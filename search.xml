<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Oozie学习</title>
      <link href="/2019/05/05/Oozie%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/05/05/Oozie%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Oozie介绍"><a href="#一、Oozie介绍" class="headerlink" title="一、Oozie介绍"></a>一、Oozie介绍</h1><h2 id="1、基本内容"><a href="#1、基本内容" class="headerlink" title="1、基本内容"></a>1、基本内容</h2><p>• Oozie是用于 Hadoop 平台的开源的工作流调度引擎。</p><blockquote><p>工作流： </p><p>分为开始 ， 中间过程  ，结束  等流程</p><p>定时调度： Crontab</p></blockquote><p>• 用来管理Hadoop作业。<br>• 属于web应用程序，由Oozie client和Oozie Server两个组件构成。<br>• Oozie Server运行于Java Servlet容器（<code>Tomcat</code>）中的web程序。<br>• 官网：<a href="https://oozie.apache.org/" target="_blank" rel="noopener">https://oozie.apache.org/</a></p><h2 id="2、Oozie作用"><a href="#2、Oozie作用" class="headerlink" title="2、Oozie作用"></a>2、Oozie作用</h2><p>– 统一调度hadoop系统中常见的mr任务启动、hdfs操作、shell调度、hive操作等<br>– 使得复杂的依赖关系、时间触发、事件触发使用xml语言进行表达，开发效率提高<br>– 一组任务使用一个DAG来表示，使用图形表达流程逻辑更加清晰<br>– 支持很多种任务调度，能完成大部分hadoop任务处理<br>– 程序定义支持EL常量和函数，表达更加丰富</p><h1 id="二、Oozie架构"><a href="#二、Oozie架构" class="headerlink" title="二、Oozie架构"></a>二、Oozie架构</h1><p><img src="https://static.imcoder.site/blog/upload/image/article/2017/09/1504700098315_16_a.png" alt=""></p><p>三者是递进关系：Workflow（工作流） → Coordinator（协调器） → Bundle（ 束；捆）</p><h1 id="三、Oozie安装"><a href="#三、Oozie安装" class="headerlink" title="三、Oozie安装"></a>三、Oozie安装</h1><p>• 安装方式：<br>– 手动安装<br>– Cloudera Manger添加服务</p><p>• Oozie Web控制台<br>– 解压ext-2.2到/var/lib/oozie目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install unzip -y </span><br><span class="line">unzip ext-2.2.zip -d /var/lib/oozie/</span><br><span class="line"><span class="meta">#</span># unzip --help</span><br><span class="line"><span class="meta">#</span>#-d  extract files into exdir   将文件解压到指定目录</span><br></pre></td></tr></table></figure><p>– Oozie服务中配置启用web控制台<br>– 保存，重启oozie服务</p><p>• Oozie配置</p><blockquote><p>– 1、节点内存配置<br>– 2、 oozie.service.callablequeueservice.callable.concurrency（节点并发）<br>– 3、 oozie.service.callablequeueservice.queue.size（队列大小）<br>– 4、 oozie.service.ActionService.executor.ext.classes（扩展）</p></blockquote><p>• Oozie共享库  （存放一些常用的  jar 包）</p><blockquote><p>（该路径在HDFS上） /user/oozie/share/lib   </p></blockquote><p>• web管理地址</p><p>入口一：</p><blockquote><p>原生入口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  http://oozie_host_ip:11000/oozie/</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>提交任务后，需要手点击刷新</p></blockquote><p>入口二：</p><p>HUE</p><blockquote><p>在CDH中添加HUE服务，点击进入HUE→快速链接：Hue Web UI</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  http://oozie_host_ip:8888</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>在Query Editor中选择Hive/Impala查询 ，直接写SQL</p><p>无需手动刷新</p><p>有文件浏览器 、 Job Browser （MR的Job）</p></blockquote><p>• oozie管理</p><blockquote><p>– 任务列表查看<br>– 任务状态查看<br>– 流程返回信息<br>– 节点信息查看<br>– 流程图信息<br>– 日志查看<br>– 系统信息查看和配置</p></blockquote><h1 id="四、Oozie-CLI-命令"><a href="#四、Oozie-CLI-命令" class="headerlink" title="四、Oozie CLI 命令"></a>四、Oozie CLI 命令</h1><p>• 启动任务：</p><p>（涉及到权限问题时，切换用户： su - hdfs     切换文件用户组：chown hdfs:hdfs job.properties）</p><p>（查询端口号：netstat -anpt | grep 8032       或 lsof -i 8032 ）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://ip:11000/oozie/ -config job.properties -run</span><br></pre></td></tr></table></figure><p>• 停止任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://ip:11000/oozie/ -kill 0000002-150713234209387-oozie-oozi-W</span><br></pre></td></tr></table></figure><p>• 提交任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://ip:11000/oozie/ -config job.properties –submit</span><br></pre></td></tr></table></figure><p>• 开始任务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://ip:11000/oozie/ -config job.properties –start 0000003-150713234209387-oozie-oozi-W</span><br></pre></td></tr></table></figure><p>• 查看任务执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie job -oozie http://ip:11000/oozie/ -config job.properties –info 0000003-150713234209387-oozie-oozi-W</span><br></pre></td></tr></table></figure><h1 id="五、Oozie配置"><a href="#五、Oozie配置" class="headerlink" title="五、Oozie配置"></a>五、Oozie配置</h1><h2 id="1、Job配置"><a href="#1、Job配置" class="headerlink" title="1、Job配置"></a>1、Job配置</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2qqehjmi7j30pz0e00wo.jpg" alt=""></p><blockquote><ul><li>共享库在HDFS上</li><li>后两个地址可以不配</li></ul></blockquote><h2 id="2、WorkFlow配置"><a href="#2、WorkFlow配置" class="headerlink" title="2、WorkFlow配置"></a>2、WorkFlow配置</h2><p>workflow.xml</p><h3 id="1、版本信息"><a href="#1、版本信息" class="headerlink" title="1、版本信息"></a>1、版本信息</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.4"</span> <span class="attr">name</span>=<span class="string">“workflow</span> <span class="attr">name</span>"&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2、EL函数"><a href="#2、EL函数" class="headerlink" title="2、EL函数"></a>2、EL函数</h3><p>– 基本的EL函数</p><p>• String firstNotNull(String value1, String value2)<br>• String concat(String s1, String s2)<br>• String replaceAll(String src, String regex, String replacement)<br>• String appendAll(String src, String append, String delimeter)<br>• String trim(String s)<br>• String urlEncode(String s)<br>• String timestamp()<br>• String toJsonStr(Map) (since Oozie 3.3)<br>• String toPropertiesStr(Map) (since Oozie 3.3)<br>• String toConfigurationStr(Map) (since Oozie 3.3)<br>​<br>– WorkFlow EL</p><p>• String wf:id() – 返回当前workflow作业ID<br>• String wf:name() – 返回当前workflow作业NAME<br>• String wf:appPath() – 返回当前workflow的路径<br>• String wf:conf(String name) – 获取当前workflow的完整配置信息<br>• String wf:user() – 返回启动当前job的用户<br>• String wf:callback(String stateVar) – 返回结点的回调URL，其中参数为动作指定的退出状态<br>• int wf:run() – 返回workflow的运行编号，正常状态为0<br>• Map wf:actionData(String node) – 返回当前节点完成时输出的信息<br>• int wf:actionExternalStatus(String node) – 返回当前节点的状态<br>• String wf:lastErrorNode() – 返回最后一个ERROR状态推出的节点名称<br>• String wf:errorCode(String node) – 返回指定节点执行job的错误码，没有则返回空<br>• String wf:errorMessage(String message) – 返回执行节点执行job的错误信息，没有则返回空<br>​<br>– HDFS EL<br>• boolean fs:exists(String path)<br>• boolean fs:isDir(String path)<br>• long fs:dirSize(String path) – 目录则返回目录下所有文件字节数；否则返回-1<br>• long fs:fileSize(String path) – 文件则返回文件字节数；否则返回-1<br>• long fs:blockSize(String path) – 文件则返回文件块的字节数；否则返回-1                                                                 </p><h3 id="3、节点"><a href="#3、节点" class="headerlink" title="3、节点"></a>3、节点</h3><p>A、流程控制节点</p><p>• start – 定义workflow开始<br>• end – 定义workflow结束<br>• decision – 实现switch功能</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">decision</span> <span class="attr">name</span>=<span class="string">"[NODE-NAME]"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">switch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">case</span> <span class="attr">to</span>=<span class="string">"[NODE_NAME]"</span>&gt;</span>[PREDICATE]<span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="tag">&lt;<span class="name">case</span> <span class="attr">to</span>=<span class="string">"[NODE_NAME]"</span>&gt;</span>[PREDICATE]<span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">default</span> <span class="attr">to</span>=<span class="string">"[NODE_NAME]"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">switch</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">decision</span>&gt;</span></span><br></pre></td></tr></table></figure><p>• sub-workflow – 调用子workflow<br>• kill – 杀死workflow<br>• fork – 并发执行workflow</p><p>• join – 并发执行结束（与fork一起使用）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">fork</span> <span class="attr">name</span>=<span class="string">"[FORK-NODE-NAME]"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--并发执行--&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">"[NODE-NAME]"</span> /&gt;</span></span><br><span class="line">      ...</span><br><span class="line">     <span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">"[NODE-NAME]"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">fork</span>&gt;</span></span><br><span class="line">...</span><br><span class="line"><span class="tag">&lt;<span class="name">join</span> <span class="attr">name</span>=<span class="string">"[JOIN-NODE-NAME]"</span> <span class="attr">to</span>=<span class="string">"[NODE-NAME]"</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>B、动作节点</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">action</span>&gt;</span></span><br></pre></td></tr></table></figure><p>• shell<br>• java<br>• fs<br>• MR<br>• hive<br>• sqoop</p><ul><li><h4 id="Shell节点"><a href="#Shell节点" class="headerlink" title="Shell节点"></a>Shell节点</h4><p> —job.properties</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nameNode=hdfs://node1:8020</span><br><span class="line">jobTracker=node1:8021</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=examples</span><br><span class="line">##workflow.xml文件所在的路径</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/workflow/oozie/shell</span><br></pre></td></tr></table></figure><p>–-workflow.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.3"</span> <span class="attr">name</span>=<span class="string">"shell-wf"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"shell-node"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"shell-node"</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.1"</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">           </span><br><span class="line"><span class="comment">&lt;!--真正的执行业务逻辑： 打印--&gt;</span>     </span><br><span class="line">       <span class="tag">&lt;<span class="name">exec</span>&gt;</span>echo<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">argument</span>&gt;</span>hi shell in oozie<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line">           </span><br><span class="line">       <span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">message</span>&gt;</span>Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><h4 id="调用impala"><a href="#调用impala" class="headerlink" title="调用impala"></a>调用impala</h4></li></ul><p>—job.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nameNode=hdfs://node1:8020</span><br><span class="line">jobTracker=node1:8032</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=examples</span><br><span class="line">oozie.usr.system.libpath=true</span><br><span class="line">oozie.libpath=$&#123;namenode&#125;/user/$&#123;user.name&#125;/workflow/impala/lib</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/workflow/impala</span><br></pre></td></tr></table></figure><p>–-workflow.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.4"</span> <span class="attr">name</span>=<span class="string">"impala-wf"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"shell-node"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"shell-node"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">shell</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:shell-action:0.1"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">exec</span>&gt;</span>impala-shell<span class="tag">&lt;/<span class="name">exec</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>-i<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>node2<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>-q<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">argument</span>&gt;</span>invalidate metadata<span class="tag">&lt;/<span class="name">argument</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">capture-output</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">shell</span>&gt;</span></span><br><span class="line">......</span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">.......</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><h4 id="fs节点"><a href="#fs节点" class="headerlink" title="fs节点"></a>fs节点</h4></li></ul><p>– - workflow.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">"[WF-DEF-NAME]"</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.5"</span>&gt;</span></span><br><span class="line">...</span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"[NODE-NAME]"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">fs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">'[PATH]'</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mkdir</span> <span class="attr">path</span>=<span class="string">'[PATH]'</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">move</span> <span class="attr">source</span>=<span class="string">'[SOURCE-PATH]'</span> <span class="attr">target</span>=<span class="string">'[TARGET-PATH]'</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">chmod</span> <span class="attr">path</span>=<span class="string">'[PATH]'</span> <span class="attr">permissions</span>=<span class="string">'[PERMISSIONS]'</span> <span class="attr">dir-files</span>=<span class="string">'false'</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">touchz</span> <span class="attr">path</span>=<span class="string">'[PATH]'</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">chgrp</span> <span class="attr">path</span>=<span class="string">'[PATH]'</span> <span class="attr">group</span>=<span class="string">'[GROUP]'</span> <span class="attr">dir-files</span>=<span class="string">'false'</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">fs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"[NODE-NAME]"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"[NODE-NAME]"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>Java节点</li></ul><p>—- job.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nameNode=hdfs://node1:8020</span><br><span class="line">jobTracker=node1:8032</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=examples</span><br><span class="line">oozie.usr.system.libpath=true</span><br><span class="line">oozie.libpath=$&#123;nameNode&#125;/user/workflow/lib/lib4java</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/workflow/oozie/java</span><br></pre></td></tr></table></figure><p>—-workflow.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">xmlns</span>=<span class="string">"uri:oozie:workflow:0.3"</span> <span class="attr">name</span>=<span class="string">"mr-wc-wf"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">"mr-node"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">"mr-node"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">java</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;jobTracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;nameNode&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">prepare</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">"$&#123;nameNode&#125;/user/path"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mkdir</span> <span class="attr">path</span>=<span class="string">"$&#123;nameNode&#125;/user/path"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;queueName&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">   <span class="comment">&lt;!--&lt;arg&gt;args1&lt;/arg&gt;</span></span><br><span class="line"><span class="comment">&lt;arg&gt;args2&lt;/arg&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">main-class</span>&gt;</span>com.pagename.classname<span class="tag">&lt;/<span class="name">main-class</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">java</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">"fail"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">"fail"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">message</span>&gt;</span>Map/Reduce failed, error</span><br><span class="line">message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">"end"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工作流调度引擎 </tag>
            
            <tag> Hadoop </tag>
            
            <tag> web </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>impala学习</title>
      <link href="/2019/04/28/impala%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/04/28/impala%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、-impala-架构"><a href="#一、-impala-架构" class="headerlink" title="一、==impala==架构"></a>一、==impala==架构</h1><p><a href="https://www.cloudera.com/products/open-source/apache-hadoop/impala.html" target="_blank" rel="noopener">https://www.cloudera.com/products/open-source/apache-hadoop/impala.html</a></p><p><a href="http://impala.apache.org/" target="_blank" rel="noopener">http://impala.apache.org/</a></p><p>cloudera公司：提供对HDFS、HBase数据的高性能、低延迟的交互式sql查询功能。</p><p>基于Hive使用内存计算，兼顾数据仓库、具有实时、批处理、多并发等优点。</p><p>是CDH平台首选的PB级别大数据实时查询分析引擎。</p><ul><li>计算性能对比图：</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2ijxv43utj309r06cdgn.jpg" alt=""></p><p>1、impala的优势</p><ul><li>基于Hive使用内存计算（数据存储在Hive中），能够对PB级数据进行交互式实时查询分析</li><li>无需转换成MR，直接读取HDFS数据</li><li>基于C++编写，LLVM统一编译运行</li><li>兼容HiveSQL</li><li>具有数据仓库的特性，可以直接对Hive数据做数据分析</li><li>支持Data Local（数据本地化，移动计算）</li><li>支持列式存储（如HBase）</li><li>支持JDBC/ODBC远程访问（ODBC：Windows端控制面板搜索：数据源）</li></ul><p>2、impala的劣势</p><ul><li>内存依赖大</li><li>C++编写，部分开源</li><li>完全依赖Hive做sql解析</li><li>实践过程中，分区数超过1W时，性能严重下降</li><li>稳定性不如Hive</li></ul><p>3、安装方式</p><ul><li>ClouderaManager （CDH）</li><li>手动安装比较麻烦</li></ul><p>4、组件</p><ul><li>Statestore Daemon<ul><li>实例：statestored</li><li>负责收集在集群中各个impala进程的资源信息、各节点健康状况，同步节点信息</li><li>负责query的调度</li></ul></li><li>Catalog Daemon<ul><li>实例：catalogd</li><li>分发表的元数据信息到各个impala中</li><li>接收来自statestore的所有请求</li></ul></li><li>Impala Daemon<ul><li>实例：impalad</li><li>接收client 、hue 、 jdbc/odbc请求、query执行并返回给中心协调节点</li><li>子节点上的守护进程，负责和statestore保持通信，汇报工作</li></ul></li></ul><p> <img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2ijxv9whbj30nq0a442o.jpg" alt=""></p><p>Query Planner ：</p><p>查询计划，有缓存效果</p><p>Query Coordinator</p><p>分发查询任务</p><p>Query  Excutor</p><p>执行查询任务</p><p>5、impala shell</p><p>进入交互式界面</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#外部项</span></span></span><br><span class="line">impala shell -h        ##显示impala的帮助信息</span><br><span class="line">impala shell -v        ##显示impala的版本信息</span><br><span class="line">impala shell -V        ##启用详细输出</span><br><span class="line">impala shell --quiet   ##关闭详细输出</span><br><span class="line">impala shell -p        ##开启显示详细执行计划</span><br><span class="line">impala shell -i hostname（--impalad=hostname） ##指定连接主机，默认端口21000</span><br><span class="line">impala shell -r（--refresh after connect） ##刷新所有元数据，可以快一点查看如新创建的表</span><br><span class="line">impala shell -q query（--query=query）  ##在命令行执行查询，不进入impala shell中，相当于 hive -e "select * from tb"</span><br><span class="line">impala shell -d default_db（--database=default_db）   ##指定数据库</span><br><span class="line">impala shell -B（--delimited）  ##去格式化输出，用于外部执行sql时，标准输出到指定文件中时，去格式化后的数据更适用于其他业务需要，没有多余的字符</span><br><span class="line">               * --output delimited=chatactor ##指定分隔符</span><br><span class="line">               * --print_header               ##打印列名</span><br><span class="line">impala shell -f query_file（query_file=query_file）  ##执行查询文件，以分号分隔 </span><br><span class="line">impala shell -o filename（--output_file filename） #将结果输出到指定文件 类似于 &gt; </span><br><span class="line">impala shell -c  ##查询执行失败时继续执行 与 -f 组合使用</span><br><span class="line">impala shell -k（--kerberos）  ##使用kerberos安全加密方式运行impala-shell</span><br><span class="line">kerberos -l  ##启用LDAP认证</span><br><span class="line">kerberos -u  ##启用LDAP时，指定用户名</span><br></pre></td></tr></table></figure><p>开启LDAP认证模式：<img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2plpe6u34j30c701dmxk.jpg" alt=""></p><p>进入交互式界面：<img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2plhvwid8j30lx04pdie.jpg" alt=""></p><ul><li>impala中的sql语句读取的数据是来自Hive中的</li><li>在内部项中使用sql，用 ；（分号） 结束每个语句。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##内部项</span></span></span><br><span class="line">help               ## 进入帮助界面</span><br><span class="line">connect node01     ##connect &lt;hostname:port&gt;  连接其他主机服务器，默认端口21000</span><br><span class="line">refresh &lt;tablename&gt;  ##增量刷新元数据库</span><br><span class="line">invalidate metadata  ##全量刷新元数据库</span><br><span class="line">explain &lt;sql&gt;        ##显示查询计划 、 步骤信息</span><br><span class="line">          set explain level 设置显示级别 （0,1,2,3）</span><br><span class="line">shell &lt;shell&gt;        ##不退出impala-shell而执行Linux命令  ，相当于Hive中的！</span><br><span class="line">profile              ##打印最近一次的查询计划的信息</span><br><span class="line">&lt;sql&gt;                  ## 内部输入sql语句</span><br></pre></td></tr></table></figure><h1 id="二-、-impala监控管理"><a href="#二-、-impala监控管理" class="headerlink" title="二 、 impala监控管理"></a>二 、 impala监控管理</h1><p>Web UI</p><ul><li>查看StateStore（状态信息，只配置在一台节点上，impala）</li></ul><p><a href="http://node00:25010" target="_blank" rel="noopener">http://node00:25010</a></p><ul><li>查看Catalog</li></ul><p><a href="http://node00:25020" target="_blank" rel="noopener">http://node00:25020</a></p><h1 id="三、impala的存储和分区"><a href="#三、impala的存储和分区" class="headerlink" title="三、impala的存储和分区"></a>三、impala的存储和分区</h1><h2 id="1、存储"><a href="#1、存储" class="headerlink" title="1、存储"></a>1、存储</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2pmi7oet6j30j50a4q67.jpg" alt=""></p><h2 id="2、压缩"><a href="#2、压缩" class="headerlink" title="2、压缩"></a>2、压缩</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2pnbkuxljj30gt099tbv.jpg" alt=""></p><p>需要压缩的情况：</p><ul><li>网络IO时</li><li>资源可能一段时间不用时</li></ul><h2 id="3、分区"><a href="#3、分区" class="headerlink" title="3、分区"></a>3、分区</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2pne27zxsj30j20acgq9.jpg" alt=""></p><p>与Hive类似</p><blockquote><ul><li>Hive中分区表在创建表的时候，就需要指定分区字段，之后可以动态添加分区，但是如果是非分区表，之后是不能添加分区字段。</li><li>动态分区的字段与普通字段相比，相当于伪列，占用空间小</li></ul></blockquote><h2 id="4、impala中支持的数据类型"><a href="#4、impala中支持的数据类型" class="headerlink" title="4、impala中支持的数据类型"></a>4、impala中支持的数据类型</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2pnx336zrj30hk09staz.jpg" alt=""></p><p>complex ：  复数</p><h2 id="5、impala中不支持的类型"><a href="#5、impala中不支持的类型" class="headerlink" title="5、impala中不支持的类型"></a>5、impala中不支持的类型</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2pny9q2x4j30kx08fjvc.jpg" alt=""></p><ul><li>自定义序列化反序列化：（例：Hive中导入Json文件）</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2po0dz6s4j309x043q3j.jpg" alt=""></p><ul><li>不支持自定义函数</li></ul><h1 id="四、Impala-SQL"><a href="#四、Impala-SQL" class="headerlink" title="四、Impala SQL"></a>四、Impala SQL</h1><ul><li>创建数据库</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db1;</span><br><span class="line"><span class="keyword">use</span> db1;</span><br></pre></td></tr></table></figure><ul><li>删除数据库</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> db1;</span><br></pre></td></tr></table></figure><ul><li><p>创建表（内部表：存储在配置的默认路径中）</p><p>默认方式创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_b1(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>指定存储方式创建表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_b2(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span></span><br><span class="line"><span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\0'</span>   <span class="comment">--(impala 1.3.1版本以上支持)</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><p>其他方式创建内部表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--使用现有表结构</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> t_b3 <span class="keyword">like</span> t_b2;</span><br><span class="line"><span class="comment">--指定文本表字段分隔符</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> t_b3 <span class="keyword">set</span> serdeproperties</span><br><span class="line">(<span class="string">'serialization.format'</span> = <span class="string">','</span>,<span class="string">'field.delim'</span> = <span class="string">','</span>);</span><br></pre></td></tr></table></figure></li><li><p>插入数据</p><ul><li>直接插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t_b3 <span class="keyword">values</span>(<span class="number">1</span>,<span class="keyword">hex</span>(<span class="string">'hello world'</span>));</span><br></pre></td></tr></table></figure><ul><li>从其他表插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> (overwrite) <span class="keyword">into</span>  t_b3 <span class="keyword">select</span> * <span class="keyword">from</span> t_b2;</span><br></pre></td></tr></table></figure><ul><li>批量导入文件</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/xxx/xxx'</span> <span class="keyword">into</span> <span class="keyword">table</span> t_b3;</span><br></pre></td></tr></table></figure></li><li><p>创建表（外部表：需指定存储路径）</p></li></ul><p>​      默认方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> t_b4(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">location <span class="string">'/user/xxxx.txt'</span>;</span><br></pre></td></tr></table></figure><p>   指定存储方式</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> t_b4 <span class="keyword">like</span> parquet_tbl <span class="string">'/user/xxx/1.dat'</span></span><br><span class="line"><span class="keyword">partition</span> (</span><br><span class="line"><span class="keyword">year</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">month</span> tinyint,</span><br><span class="line"><span class="keyword">day</span> tinyint</span><br><span class="line">)</span><br><span class="line">location <span class="string">'user/xxx/xxx.txt'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> parquet;</span><br></pre></td></tr></table></figure><p>视图：（简化查询语句，封装sql，提高数据安全性）</p><ul><li>创建视图</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> v1 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">count</span>（<span class="keyword">id</span>） <span class="keyword">as</span> total <span class="keyword">from</span> t_b1；</span><br></pre></td></tr></table></figure><ul><li>查询视图</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> v1;</span><br></pre></td></tr></table></figure><ul><li>查看视图定义</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">describe</span> formatted v1；</span><br></pre></td></tr></table></figure><p>注意note：</p><blockquote><ul><li>不能向impala的视图进行插入操作</li><li>insert表可来自视图</li></ul></blockquote><ul><li>数据文件处理</li></ul><p>加载数据：</p><blockquote><ul><li>insert语句：插入数据时每条数据产生一个数据文件（不建议加载批量数据，如果遇到需要一条一条插入数据的业务场景时，可借助Hbase）</li><li>load data方式：推荐应用于批量数据插入时</li><li>来自中间表：适用于从一个小文件较多的大表中读取文件并写入新的表产生少量的数据文件。也可适用于进行格式转换</li></ul></blockquote><p>空值处理：</p><blockquote><ul><li>impala将 ‘\n’ 表示为NULL ，在结合sqoop使用时注意做相应的空字段过滤</li><li>也可用一下方式处理：</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; alter table tb1 set tblpropertites ("serialization.null.format" = "null");</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h1 id="五、impala整合Hbase"><a href="#五、impala整合Hbase" class="headerlink" title="五、impala整合Hbase"></a>五、impala整合Hbase</h1><p>Impala可通过Hive外部表方式和Hbase整合，步骤如下：</p><p>1、创建Hbase表，向表中添加数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create 'tb_test','info'</span><br><span class="line"></span><br><span class="line">put 'tb_test','1','info:name','zhangsan'</span><br><span class="line">put 'tb_test','2','info:name','lisi'</span><br></pre></td></tr></table></figure><p>2、在Hive中创建外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> tb_test(</span><br><span class="line"><span class="keyword">key</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> serde<span class="string">'org.apache.hadoop.hive.hbase.HbaseSerDe'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">by</span> <span class="string">'org.apache.hadoop.hive.hbase.HbaseStorageHandler'</span></span><br><span class="line"><span class="keyword">with</span> serdeproperties(<span class="string">"hbase.columns.mapping"</span>=<span class="string">":key,info:name"</span>)</span><br><span class="line">tblproperties(<span class="string">"hbase.table.name"</span>=<span class="string">"tb_test"</span>);</span><br></pre></td></tr></table></figure><p>3、刷新impala表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#外部项 </span></span></span><br><span class="line">impal-shell -r</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#内部项</span></span></span><br><span class="line">invalidate metadata;</span><br></pre></td></tr></table></figure><p><code>注意</code>可能报错：impala中查询表中数据时，报错无法获得Hbase表的信息</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2qlrxwc8mj30g9035ab0.jpg" alt=""></p><p><code>解决方案</code> ，配置完后，记得重启CDH集群</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2qlwse8d1j30lo0j5q87.jpg" alt=""></p><h1 id="六、impala和JDBC整合"><a href="#六、impala和JDBC整合" class="headerlink" title="六、impala和JDBC整合"></a>六、impala和JDBC整合</h1><p>数据库接口标准：JDBC</p><ul><li><p>配置：<br>– impala.driver=org.apache.hive.jdbc.HiveDriver<br>– impala.url=jdbc:hive2://node2:21050/;auth=noSasl<br>– impala.username=<br>– impala.password=</p></li><li><p>尽量使用PreparedStatement执行SQL语句：<br>– 1.性能上PreparedStatement（ps）要好于Statement，ps可以有缓存，执行预加载，性能优<br>– 2.Statement存在查询不出数据的情况，ps可以使用占位符 ， 防止sql注入</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">impalaClient</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">//加载Hive驱动</span></span><br><span class="line">Class.forName(<span class="string">"org.apache.hive.jdbc.HiveDriver"</span>);</span><br><span class="line">         <span class="comment">//创建连接 ：数据库url  ， 用户名 ， 密码  </span></span><br><span class="line">Connection conn =</span><br><span class="line"><span class="comment">//21050  是impalad的端口    ，因为auth=noSasl 所以可以不用密码       DriverManager.getConnection("jdbc:hive2://node04:21050/test;auth=noSasl",</span></span><br><span class="line">                                       <span class="string">""</span>, </span><br><span class="line">                                       <span class="string">""</span>);</span><br><span class="line">        <span class="comment">//创建sql的预处理块</span></span><br><span class="line">PreparedStatement pstm = </span><br><span class="line">            conn.prepareStatement(<span class="string">"select id,type,level from sales limit 50"</span>);</span><br><span class="line">        <span class="comment">//预处理快执行查询操作，得到查询结果</span></span><br><span class="line">ResultSet rs = pstm.executeQuery();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (rs.next()) &#123;</span><br><span class="line">System.out.println(rs.getString(<span class="number">1</span>) + <span class="string">"  "</span> +</span><br><span class="line">                               rs.getString(<span class="string">"type"</span>) + <span class="string">"  "</span> + </span><br><span class="line">                               rs.getString(<span class="string">"level"</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rs.close();</span><br><span class="line">pstm.close();</span><br><span class="line">conn.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="七、impala性能优化"><a href="#七、impala性能优化" class="headerlink" title="七、impala性能优化"></a>七、impala性能优化</h1><ul><li>执行计划 ，（查看执行计划，便于性能优化）<br>– 查询sql执行之前，先对该sql做一个分析，列出需要完成这一项查询的详细方案<br>– 命令：explain sql  （级别默认为1）、profile （查询最后一次执行计划）</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2qmgd7ek6j30g9070adj.jpg" alt=""></p><ul><li><p><strong>要点：</strong><br>• 1、SQL优化，使用之前调用执行计划</p><p>• 2、选择合适的文件格式进行存储</p><p>• 3、避免产生很多小文件（如果有其他程序产生的小文件，可以使用中间表）</p><p>• 4、使用合适的分区技术，根据分区粒度测算  （分区数膨胀速度 ；  分区内的数据）</p><p>• 5、使用compute stats进行表信息搜集</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--impala 内部：</span></span><br><span class="line">&gt; show table stats tb_test;</span><br><span class="line"></span><br><span class="line">&gt; show column stats tb_test; </span><br><span class="line"></span><br><span class="line"><span class="comment">--执行优化分析</span></span><br><span class="line">&gt; compute stats tb_test；</span><br><span class="line"></span><br><span class="line">再执行查询就会快很多</span><br></pre></td></tr></table></figure><blockquote><p>显示表的信息</p><p>#Rows   #File   Size      Bytes Cached      Cache Replication       Format       Incremental stats</p></blockquote><blockquote><p>显示表中字段的信息</p><p>Column       Type       #Distinct Values        #Nulls       Max Size       Avg Size        </p></blockquote><p>• 6、网络io的优化：<br>​          – a.避免把整个数据发送到客户端（避免select * ）<br>​          – b.尽可能的做条件过滤<br>​          – c.使用limit字句<br>​          – d.输出文件时，避免使用美化输出 （impal-shell -B  去格式化）</p><p>• 7、使用profile输出底层信息计划，在做相应环境优化</p></li></ul><p>注意note：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------</span><br><span class="line">Hive函数：</span><br><span class="line">analyse table</span><br><span class="line">over  （开窗函数）</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Null ： 在Hive中不占空间，在MySql中占空间；</span><br><span class="line">       Hive允许有稀疏</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">Hive中查看表信息</span><br><span class="line">hive&gt;  desc tablename;(表的字段信息)</span><br><span class="line">hive&gt; desc formatted tablename;（表的非常详细的信息）</span><br><span class="line">hive&gt;show tables from databasename；（impala不支持）</span><br><span class="line">------------------------------------------------------------     </span><br><span class="line"> CDH搭建无法找到JAVA_HOME问题</span><br><span class="line">  su username 使用该命令要保证在/home/路径下用户的目录</span><br><span class="line">  将java的配置在全局环境变量中</span><br><span class="line">------------------------------------------------------------  </span><br><span class="line">  CDH环境中启动hive会遇到权限问题</span><br><span class="line"> 解决方案： su - hdfs</span><br><span class="line">-------------------------------------------------------------   </span><br><span class="line">  mysql 授权：</span><br><span class="line">   grant all on *.* to &apos;temp&apos;@&apos;CDH-NODE-01&apos; identified by &apos;temp&apos; with grant option</span><br><span class="line">                       （库名）（库的连接名）                （用户名）</span><br><span class="line">------------------------------------------------------------                           </span><br><span class="line">  hive建表语句中</span><br><span class="line">  stored as textfile    指定在hive中保存的文件格式，保存在HDFS上                     </span><br><span class="line">  导入数据到hive中</span><br><span class="line">  附加方法：</span><br><span class="line">  先创建指定格式的表</span><br><span class="line">  将本地满足表的格式的数据文件，加载到表所在的HDFS路径下</span><br><span class="line">  ------------------------------------------------------------     </span><br><span class="line">  HBase的rowkey设计原则：</span><br><span class="line">  * 唯一性</span><br><span class="line">  * 定长 ： 满足2的次幂（与计算机的位处理有关）</span><br><span class="line">  * 越短越好： 64KB        </span><br><span class="line">            若设计过长，影响数据存储的利用率，数据有价值的部分在value中</span><br><span class="line">  * 散列原则：  数据均匀分布在不同的服务器上</span><br><span class="line">           * 取反 （连贯的情况）</span><br><span class="line">           * 取hash</span><br><span class="line">           * 拼接随机前缀</span><br><span class="line">           * 取模有点危险</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> impala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> CDH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
      <url>/2019/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>title: 推荐系统项目学习<br>date: 2019-1-18<br>update: 2019-1-18<br>tags:</p><pre><code>- Linux系统环境    - HDFS    - Base</code></pre><p>categories: flume<br>grammar_cjkRuby: true<br>mathjax: true<br>overdue: true #这一行文章提醒<br>no_word_count: false<br>description: “线性回归”</p><p>[TOC]</p><h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p>是人工智能的分支，主要学习常用算法</p><p>深度学习是机器学习的分支</p><h1 id="一、人工智能与Spark-MLLib"><a href="#一、人工智能与Spark-MLLib" class="headerlink" title="一、人工智能与Spark MLLib"></a>一、人工智能与Spark MLLib</h1><p>分类：强人工智能 ，弱人工智能（目前使用最多）</p><p>训练模型：使用概率论，需要大量的数据样本，不断的迭代计算</p><p>HDFS：海量数据的存储</p><p>迭代计算：<br>使用MR受限（MR只能把中间结果存储在磁盘上，不利于下次计算的重新读取，对迭代算法是性能瓶颈，使用时，耗时，耗磁盘IO）</p><p>使用Spark（Spark基于内存计算，适合迭代计算，同时提供基于海量数据的ML库）</p><p>Spark MLLib ： spark + machine + learning + lib（库）</p><p>基本数据类型：</p><p>向量（有大小和方向，相当于元组，数字化描述特征）</p><h1 id="二、-线性回归"><a href="#二、-线性回归" class="headerlink" title="二、==线性回归=="></a>二、==线性回归==</h1><h1 id="利用历史数据找出规律用于预测。"><a href="#利用历史数据找出规律用于预测。" class="headerlink" title="利用历史数据找出规律用于预测。"></a>利用历史数据找出规律用于预测。</h1><p><code>回归（regression）：</code>关注自变量和因变量之间的对应关系,</p><p>用于预测  （相当于M=ax+by+cx  ，M就是因变量（就是需要预测的值） ，x,y,z就是自变量（参与预测的变量），a,b,c（就是训练模型提供的））</p><p>使用最小二乘法算出：</p><p>（损失函数）总误差: <img src="https://ws1.sinaimg.cn/large/005zftzDgy1g27svzh2m9j309l01qjrc.jpg" alt=""></p><p>梯度下降法：不断调试，迭代计算，求出局部最小的误差（局部收敛）</p><p>模拟出来的图线，成为拟合函数</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g27sy8gqqjj30hz0ddn0f.jpg" alt=""></p><h3 id="LabeledPoint标注点"><a href="#LabeledPoint标注点" class="headerlink" title="LabeledPoint标注点"></a>LabeledPoint标注点</h3><p>训练模型：需要多维度的历史数据，以LabeledPoint数据结构</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.<span class="type">LabeledPoint</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataTypeTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"DataTypeTest"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//稠密向量</span></span><br><span class="line">    <span class="keyword">val</span> dv: <span class="type">Vector</span> = <span class="type">Vectors</span>.dense(<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>)</span><br><span class="line">    <span class="comment">//稀疏向量                          个数， 索引 ， 值</span></span><br><span class="line">    <span class="keyword">val</span> sv1: <span class="type">Vector</span> = <span class="type">Vectors</span>.sparse(<span class="number">10</span>, <span class="type">Array</span>(<span class="number">0</span>, <span class="number">2</span>,<span class="number">5</span>), <span class="type">Array</span>(<span class="number">1.0</span>, <span class="number">3.0</span>,<span class="number">8.0</span>))</span><br><span class="line"><span class="comment">//    val sv2: Vector = Vectors.sparse(10, Seq((0, 1.0), (2, 3.0)))</span></span><br><span class="line"></span><br><span class="line">    println(dv)</span><br><span class="line">    println(sv1)</span><br><span class="line"><span class="comment">//    println(sv2)</span></span><br><span class="line"></span><br><span class="line">    println(sv1.toDense)</span><br><span class="line"><span class="comment">//   标注点LabeledPoint ：带有标签的本地向量</span></span><br><span class="line">    <span class="keyword">val</span> pos: <span class="type">LabeledPoint</span> = <span class="type">LabeledPoint</span>(<span class="number">1.0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>))</span><br><span class="line">                                  <span class="comment">//      y    向量</span></span><br><span class="line">    <span class="keyword">val</span> neg: <span class="type">LabeledPoint</span> =</span><br><span class="line">             <span class="type">LabeledPoint</span>(<span class="number">0.0</span>, <span class="type">Vectors</span>.sparse(<span class="number">3</span>, <span class="type">Array</span>(<span class="number">0</span>, <span class="number">2</span>), <span class="type">Array</span>(<span class="number">1.0</span>, <span class="number">3.0</span>)))</span><br><span class="line">    println(pos.label)  <span class="comment">//  标注点对应的y</span></span><br><span class="line">    println(neg.features.toDense)   <span class="comment">//标注点对应的维度向量，转成dense输出</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> labeledPointRDD = sc.parallelize(<span class="type">Array</span>(pos,neg));</span><br><span class="line">    <span class="comment">//存储标注点数据</span></span><br><span class="line">    <span class="type">MLUtils</span>.saveAsLibSVMFile(labeledPointRDD,<span class="string">"labeledPointRDD.txt"</span>)</span><br><span class="line">      <span class="comment">/* 1.0 1:1.0 2:0.0 3:3.0</span></span><br><span class="line"><span class="comment">         0.0 1:1.0 3:3.0    </span></span><br><span class="line"><span class="comment">      */</span>      </span><br><span class="line">    <span class="comment">//加载标注点数据----------------------------</span></span><br><span class="line">    <span class="keyword">val</span> examples: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>] = </span><br><span class="line">              <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">"sample_libsvm_data.txt"</span>)</span><br><span class="line"></span><br><span class="line">    examples.foreach &#123; x =&gt;</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="keyword">val</span> label = x.label</span><br><span class="line">        <span class="keyword">val</span> features = x.features</span><br><span class="line">        println(<span class="string">"label:"</span> + label + <span class="string">"\tfeatures:"</span> + features.toDense)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g27y5sdf6cj31780ysaca.jpg" alt=""></p><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g27y5sl1v1j316l17jdin.jpg" alt=""></p><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.&#123;<span class="type">LabeledPoint</span>, <span class="type">LinearRegressionModel</span>, <span class="type">LinearRegressionWithSGD</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LinearRegression</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="comment">// 构建Spark对象</span></span><br><span class="line">        <span class="keyword">val</span> conf = </span><br><span class="line">        <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LinearRegressionWithSGD"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="type">Logger</span>.getRootLogger.setLevel(<span class="type">Level</span>.<span class="type">WARN</span>)</span><br><span class="line">        <span class="comment">//    sc.setLogLevel("WARN")</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//读取样本数据</span></span><br><span class="line"><span class="comment">//-0.4307829,-1.63735562648104 -2.00621178480549 -1.86242597251066 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306        </span></span><br><span class="line">        <span class="keyword">val</span> data_path1 = <span class="string">"lpsa.data"</span></span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(data_path1)</span><br><span class="line">        <span class="keyword">val</span> examples: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>] = data.map &#123; line =&gt;</span><br><span class="line">            <span class="keyword">val</span> parts = line.split(',')</span><br><span class="line">            <span class="type">LabeledPoint</span>(parts(<span class="number">0</span>).toDouble, </span><br><span class="line">                         <span class="type">Vectors</span>.dense(parts(<span class="number">1</span>).split(' ').map(_.toDouble)))</span><br><span class="line">        &#125;.cache()</span><br><span class="line">        <span class="comment">//1 为随机种子 ， 将数据集按比例随机切分为0.8训练集（0） ， 0.2测试集（1）</span></span><br><span class="line">        <span class="keyword">val</span> train2TestData: <span class="type">Array</span>[<span class="type">RDD</span>[<span class="type">LabeledPoint</span>]] =</span><br><span class="line">                                       examples.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//    val numExamples = examples.count(</span></span><br><span class="line">        <span class="comment">// 迭代次数</span></span><br><span class="line">        <span class="keyword">val</span> numIterations = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//在每次迭代的过程中 梯度下降算法的下降步长大小</span></span><br><span class="line">        <span class="keyword">val</span> stepSize = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//每一次下山后，是否计算所有样本的误差值   fraction：n. 分数；部分；小部分；稍微</span></span><br><span class="line">        <span class="keyword">val</span> miniBatchFraction = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">val</span> lrs = <span class="keyword">new</span> <span class="type">LinearRegressionWithSGD</span>()<span class="comment">//结合梯度的线性回归算法</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//设置需不需要有截距</span></span><br><span class="line">        lrs.setIntercept(<span class="literal">true</span>)</span><br><span class="line">        lrs.optimizer.setStepSize(stepSize)</span><br><span class="line">        lrs.optimizer.setNumIterations(numIterations)</span><br><span class="line">        lrs.optimizer.setMiniBatchFraction(miniBatchFraction)</span><br><span class="line">        <span class="comment">//开始不停的训练：针对训练集</span></span><br><span class="line">        <span class="keyword">val</span> model: <span class="type">LinearRegressionModel</span> = lrs.run(train2TestData(<span class="number">0</span>))</span><br><span class="line">       <span class="comment">//训练模型的权重：a ， b</span></span><br><span class="line">        println(model.weights)</span><br><span class="line">        <span class="comment">//训练模型的截距：     intercept截；截断；窃听  n. 拦截；[数] 截距；截获的情报</span></span><br><span class="line">        println(model.intercept)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对样本进行测试 ：使用测试集数据    使用测试集的特征值测试</span></span><br><span class="line">        <span class="keyword">val</span> prediction: <span class="type">RDD</span>[<span class="type">Double</span>] =      </span><br><span class="line">                                   model.predict(train2TestData(<span class="number">1</span>).map(_.features))</span><br><span class="line">        <span class="comment">//K：训练集的预测   V：测试集的预测</span></span><br><span class="line">        <span class="keyword">val</span> predictionAndLabel: <span class="type">RDD</span>[(<span class="type">Double</span>, <span class="type">Double</span>)] =</span><br><span class="line">                                   prediction.zip(train2TestData(<span class="number">1</span>).map(_.label))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">val</span> print_predict: <span class="type">Array</span>[(<span class="type">Double</span>, <span class="type">Double</span>)] = predictionAndLabel.take(<span class="number">100</span>)</span><br><span class="line">        println(<span class="string">"prediction"</span> + <span class="string">"\t"</span> + <span class="string">"label"</span>)</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to print_predict.length - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="comment">// val tuple: (Double, Double) = print_predict(i)</span></span><br><span class="line">            println(print_predict(i)._1 + <span class="string">"\t"</span> + print_predict(i)._2)</span><br><span class="line">            <span class="comment">//用以对比预测值和实际值  ，可能存在误差，  因为会有一些噪声数据</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 计算测试误差</span></span><br><span class="line">        <span class="keyword">val</span> loss = predictionAndLabel.map &#123;</span><br><span class="line">            <span class="keyword">case</span> (p, v) =&gt;</span><br><span class="line">                <span class="keyword">val</span> err = p - v</span><br><span class="line">                <span class="type">Math</span>.abs(err)</span><br><span class="line">        &#125;.reduce(_ + _)</span><br><span class="line">        <span class="keyword">val</span> error = loss / train2TestData(<span class="number">1</span>).count</span><br><span class="line">        println(<span class="string">s"Test RMSE = "</span> + error)</span><br><span class="line">        <span class="comment">// 模型保存</span></span><br><span class="line">        <span class="keyword">val</span> <span class="type">ModelPath</span> = <span class="string">"model"</span></span><br><span class="line">        <span class="comment">//  model.save(sc, ModelPath)</span></span><br><span class="line"><span class="comment">// 使用模型预测        </span></span><br><span class="line"><span class="comment">// val sameModel: LinearRegressionModel =  LinearRegressionModel.load(sc, ModelPath)</span></span><br><span class="line"><span class="comment">// RDD[Double] rdd = sameModel.pridict(testData:RDD[Vector])</span></span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="三、-贝叶斯分类算法"><a href="#三、-贝叶斯分类算法" class="headerlink" title="三、==贝叶斯分类算法=="></a>三、==贝叶斯分类算法==</h1><h1 id="依据概率原则进行垃圾邮件分类"><a href="#依据概率原则进行垃圾邮件分类" class="headerlink" title="依据概率原则进行垃圾邮件分类"></a>依据概率原则进行垃圾邮件分类</h1><p>朴素贝叶斯算法：依据概率原则进行分类，应用先前事件的有关数据来估计未来事件发生的概率。</p><p>基于贝叶斯定理的条件概率：事件A和事件B为相互独立事件</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2874tfslcj30dm03d74y.jpg" alt=""><br>$$<br>P(A|B) = P(A)*P(B|A)/P(B)<br>$$</p><p>案例应用：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g287bnorbtj318q0uewfq.jpg" alt=""></p><p>垃圾邮件案例：</p><p>某邮件出现几个单词，判断它是垃圾邮件的概率</p><p>但是因为其中某一单词的出现，抵消了或否决了所有其他证据</p><h1 id="拉普拉斯估计"><a href="#拉普拉斯估计" class="headerlink" title="拉普拉斯估计"></a>拉普拉斯估计</h1><blockquote><p>拉普拉斯估计本质上是给频率表中的每个计数加上一个较小的数，这样就保证了每一类中每个特征发生<strong>概率非零</strong>。</p></blockquote><blockquote><p> 通常情况下，拉普拉斯估计中加上的数值设定为1，这样就保证每一个特征至少在数据中出现一次</p></blockquote><h3 id="二分类→正负例"><a href="#二分类→正负例" class="headerlink" title="二分类→正负例"></a>二分类→正负例</h3><p>二分类：只有两种情况</p><p>​           正：期望结果（如垃圾邮件）     负： 与之相对的结果（正常邮件）</p><p>方法一：基于ML （针对DataFrame）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.&#123;<span class="type">NaiveBayes</span>, <span class="type">NaiveBayesModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.&#123;<span class="type">CountVectorizer</span>, <span class="type">CountVectorizerModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">DataFrame</span>, <span class="type">Row</span>, <span class="type">SQLContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Naive_bayes1</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"word2vector"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line">        <span class="comment">//加载数据</span></span><br><span class="line">        <span class="comment">// ham,It took Mr owl 3 licks</span></span><br><span class="line">        <span class="comment">//spam,"complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT </span></span><br><span class="line">        <span class="keyword">val</span> idData = sc.textFile(<span class="string">"sms_spam.txt"</span>).map(_.split(<span class="string">","</span>)).cache()</span><br><span class="line">        <span class="comment">//1.0为正常邮件 0.0为垃圾邮件</span></span><br><span class="line">        <span class="keyword">val</span> idDataRows: <span class="type">RDD</span>[<span class="type">Row</span>] = idData.map(x =&gt; </span><br><span class="line">                <span class="type">Row</span>((<span class="keyword">if</span> (x(<span class="number">0</span>) == <span class="string">"ham"</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>), x(<span class="number">1</span>).split(<span class="string">" "</span>).map(_.trim)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"label"</span>, <span class="type">DoubleType</span>, nullable = <span class="literal">false</span>),</span><br><span class="line">            <span class="type">StructField</span>(<span class="string">"words"</span>, <span class="type">ArrayType</span>(<span class="type">StringType</span>, <span class="literal">true</span>), nullable = <span class="literal">false</span>)</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> df = sqlContext.createDataFrame(idDataRows, schema)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//构建词汇表/词袋 【i hate you love dont 】</span></span><br><span class="line">        <span class="keyword">val</span> countVectorizer: <span class="type">CountVectorizerModel</span> =      </span><br><span class="line">       <span class="keyword">new</span> <span class="type">CountVectorizer</span>().setInputCol(<span class="string">"words"</span>).setOutputCol(<span class="string">"features"</span>).fit(df)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//查看词汇表</span></span><br><span class="line">        countVectorizer.vocabulary.take(<span class="number">100</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//文本向量化 CountVector (1,2,1) IdfVector  WordVertor</span></span><br><span class="line">        <span class="keyword">val</span> cvDF: <span class="type">DataFrame</span> = countVectorizer.transform(df)</span><br><span class="line">        <span class="comment">//是否最多只显示20个字符，默认为true。</span></span><br><span class="line">        cvDF.show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//正负例样本，显示前10个</span></span><br><span class="line">        <span class="keyword">val</span> example: <span class="type">DataFrame</span> = cvDF.drop(<span class="string">"words"</span>)</span><br><span class="line">        example.show(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 切分数据集与训练集</span></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(trainingData, testData) = </span><br><span class="line">                                 example.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>), seed = <span class="number">1234</span>L)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 训练朴素贝叶斯模型</span></span><br><span class="line">        <span class="keyword">val</span> model: <span class="type">NaiveBayesModel</span> = </span><br><span class="line">                           <span class="keyword">new</span> <span class="type">NaiveBayes</span>() .fit(trainingData)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 预测  predict</span></span><br><span class="line">        <span class="keyword">val</span> predictions: <span class="type">DataFrame</span> = model.transform(testData)</span><br><span class="line"></span><br><span class="line">        predictions.show()</span><br><span class="line">        <span class="comment">//okmail: Dear Dave this is your final notice to collect your!</span></span><br><span class="line">        <span class="comment">//模型评估</span></span><br><span class="line">        <span class="comment">//将结果注册为临时表</span></span><br><span class="line">        predictions.registerTempTable(<span class="string">"result"</span>)</span><br><span class="line">        <span class="comment">//计算正确率</span></span><br><span class="line">        <span class="keyword">val</span> accuracy: <span class="type">DataFrame</span> = sqlContext.sql(</span><br><span class="line">    <span class="string">"select (1- (sum(abs(label-prediction)))/count(label)) as accuracy from result"</span>)</span><br><span class="line">        accuracy.show()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//保存模型</span></span><br><span class="line">        <span class="comment">//    model.save("sms_spam")</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二种：基于Ml （RDD）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.<span class="type">NaiveBayes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.<span class="type">LabeledPoint</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 贝叶斯算法</span></span><br><span class="line"><span class="comment">2,0 0 3</span></span><br><span class="line"><span class="comment">2,0 0 4</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Naive_bayes2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="comment">//1 构建Spark对象</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Naive_bayes"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="comment">//读取样本数据1</span></span><br><span class="line">        <span class="keyword">val</span> data = sc.textFile(<span class="string">"sample_naive_bayes_data.txt"</span>)</span><br><span class="line">        <span class="keyword">val</span> parsedData = data.map &#123; line =&gt;</span><br><span class="line">            <span class="keyword">val</span> parts = line.split(',')</span><br><span class="line">            <span class="type">LabeledPoint</span>(parts(<span class="number">0</span>).toDouble, </span><br><span class="line">                         <span class="type">Vectors</span>.dense(parts(<span class="number">1</span>).split(' ').map(_.toDouble)))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//样本数据划分训练样本与测试样本</span></span><br><span class="line">        <span class="keyword">val</span> splits = parsedData.randomSplit(<span class="type">Array</span>(<span class="number">0.6</span>, <span class="number">0.4</span>), seed = <span class="number">11</span>L)</span><br><span class="line">        <span class="keyword">val</span> training = splits(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">val</span> test = splits(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//新建贝叶斯分类模型模型，并训练</span></span><br><span class="line">        <span class="keyword">val</span> model = <span class="type">NaiveBayes</span>.train(training, lambda = <span class="number">1.0</span>)</span><br><span class="line">        <span class="comment">//对测试样本进行测试</span></span><br><span class="line">        <span class="keyword">val</span> predictionAndLabel = test.map(p =&gt; (model.predict(p.features), p.label))</span><br><span class="line">        <span class="keyword">val</span> print_predict = predictionAndLabel.take(<span class="number">20</span>)</span><br><span class="line">        println(<span class="string">"prediction"</span> + <span class="string">"\t"</span> + <span class="string">"label"</span>)</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to print_predict.length - <span class="number">1</span>) &#123;</span><br><span class="line">            println(print_predict(i)._1 + <span class="string">"\t"</span> + print_predict(i)._2)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> accuracy = </span><br><span class="line">        <span class="number">1.0</span> * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / test.count()</span><br><span class="line">        println(accuracy)</span><br><span class="line">        <span class="comment">//保存模型</span></span><br><span class="line">        <span class="keyword">val</span> <span class="type">ModelPath</span> = <span class="string">"naive_bayes_model"</span></span><br><span class="line">        <span class="comment">//    model.save(sc, ModelPath)</span></span><br><span class="line">        <span class="comment">//    val sameModel = NaiveBayesModel.load(sc, ModelPath)</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四、-Kmeans聚类算法"><a href="#四、-Kmeans聚类算法" class="headerlink" title="四、==Kmeans聚类算法=="></a>四、==Kmeans聚类算法==</h1><p><a href="http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html" target="_blank" rel="noopener">http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html</a><br><a href="http://shabal.in/visuals/kmeans/4.html" target="_blank" rel="noopener">http://shabal.in/visuals/kmeans/4.html</a></p><h1 id="根据聚类中心点将数据自动划分成类"><a href="#根据聚类中心点将数据自动划分成类" class="headerlink" title="根据聚类中心点将数据自动划分成类"></a>根据聚类中心点将数据自动划分成类</h1><p>K均值聚类</p><p>聚类：给事物打标签，寻找同一组内的个体之间的一些潜在的相似模式。力图找到数据的自然分组kmeans</p><h3 id="Kmeans算法的基本原理："><a href="#Kmeans算法的基本原理：" class="headerlink" title="Kmeans算法的基本原理："></a>Kmeans算法的基本原理：</h3><ul><li><p>聚类是一种<strong>无监督</strong>的机器学习任务，它会自动将数据划分成类cluster。因此聚类分组不需要提前被告知所划分的组应该是什么样的。因为我们甚至可能都不知道我们再寻找什么，所以聚类是用于知识发现而不是预测。</p></li><li><p>聚类原则是一个组内的记录彼此必须非常相似，而与该组之外的记录截然不同。所有聚类做的就是遍历所有数据然后找到这些相似性</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g296i8zzjcj30d309a75z.jpg" alt=""></p></li><li><p>使用距离来分配和更新类</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g296m4ue3bj30bk08igmr.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g297p9ueblj30bu081gmn.jpg" alt=""></p><p>选择适当的聚类数：</p><h3 id="聚类原则："><a href="#聚类原则：" class="headerlink" title="聚类原则："></a>聚类原则：</h3><p>类内部成员越相似越好；类与类之间的成员差异越大越好</p><p>肘部法：求拐点的值</p><h3 id="算法思想："><a href="#算法思想：" class="headerlink" title="算法思想："></a>算法思想：</h3><p>–以空间中K个点为中心进行聚类，对最靠近他们的对象归类，通过迭代的方法<br>，逐次更新各聚类中心的值，直到得到最好的聚类结果</p><h3 id="算法流程总结："><a href="#算法流程总结：" class="headerlink" title="算法流程总结："></a>算法流程总结：</h3><p>–1、适当选择c个类的初始中心<br>–2、在第K次迭代中，对任意一个样本，求其到c各中心的距离，将该样本归到距离最短的中心所在的类<br>–3、利用均值等方法更新该类的中心值<br>–4、对于多有的c个聚类中心，如果利用2,3的迭代法更新后，值保持不变，则迭代结束，否则继续迭代</p><h3 id="Kmeans算法的缺陷"><a href="#Kmeans算法的缺陷" class="headerlink" title="Kmeans算法的缺陷"></a>Kmeans算法的缺陷</h3><p> 聚类中心的个数K 需要事先给定，但在实际中这个 K 值的选定是非常难以估计的，很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适<br> Kmeans需要人为地确定初始聚类中心，不同的初始聚类中心可能导致<br>完全不同的聚类结果。（可以使用Kmeans++算法来解决）</p><h1 id="Kmeans-算法"><a href="#Kmeans-算法" class="headerlink" title="Kmeans++算法"></a>Kmeans++算法</h1><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p> 从输入的数据点集合中随机选择一个点作为第一个聚类中心<br> 对于数据集中的每一个点x，计算它与最近聚类中心(指已选择的聚类中心)的距离D(x)<br> 选择一个新的数据点作为新的聚类中心，选择的原则是：D(x)较大的点，被选取作为聚类中心的概率较大<br> 重复2和3直到k个聚类中心被选出来<br> 利用这k个初始的聚类中心来运行标准的k-means算法</p><h3 id="Kmeans算法的应用："><a href="#Kmeans算法的应用：" class="headerlink" title="Kmeans算法的应用："></a>Kmeans算法的应用：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.clustering._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Vectors</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KMeans</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="comment">//1 构建Spark对象</span></span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KMeans"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取样本数据1，格式为LIBSVM format     </span></span><br><span class="line">       <span class="comment">/*  0.0 0.0 0.0</span></span><br><span class="line"><span class="comment">           0.1 0.1 0.1  */</span></span><br><span class="line">        <span class="keyword">val</span> data = sc.textFile(<span class="string">"kmeans_data.txt"</span>)</span><br><span class="line">        <span class="keyword">val</span> parsedData: <span class="type">RDD</span>[linalg.<span class="type">Vector</span>] = </span><br><span class="line">                  data.map(s =&gt; <span class="type">Vectors</span>.dense(s.split(' ').map(_.toDouble))).cache()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> numClusters = <span class="number">4</span></span><br><span class="line">        <span class="keyword">val</span> numIterations = <span class="number">100</span></span><br><span class="line">        <span class="keyword">val</span> model: <span class="type">KMeansModel</span> = <span class="keyword">new</span> <span class="type">KMeans</span>().</span><br><span class="line">            setK(numClusters).</span><br><span class="line">            setMaxIterations(numIterations).</span><br><span class="line">            run(parsedData)</span><br><span class="line">        <span class="keyword">val</span> centers: <span class="type">Array</span>[linalg.<span class="type">Vector</span>] = model.clusterCenters</span><br><span class="line">        println(<span class="string">"centers"</span>)</span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to centers.length - <span class="number">1</span>) &#123;</span><br><span class="line">            println(centers(i)(<span class="number">0</span>) + <span class="string">"\t"</span> + centers(i)(<span class="number">1</span>) + <span class="string">"\t"</span> + centers(i)(<span class="number">2</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 误差计算</span></span><br><span class="line">        <span class="keyword">val</span> <span class="type">WSSSE</span> = model.computeCost(parsedData)</span><br><span class="line">        println(<span class="string">"Errors = "</span> + <span class="type">WSSSE</span>)</span><br><span class="line">        <span class="comment">//打印出属于哪个聚类中心的类 ，的索引</span></span><br><span class="line">        println(model.predict(<span class="type">Vectors</span>.dense(<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>)))</span><br><span class="line">        <span class="comment">//保存模型</span></span><br><span class="line">        <span class="comment">//    val ModelPath = "KMeans_Model"</span></span><br><span class="line">        <span class="comment">//    model.save(sc, ModelPath)</span></span><br><span class="line">        <span class="comment">//    val sameModel = KMeansModel.load(sc, ModelPath)</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="五、-关联规则"><a href="#五、-关联规则" class="headerlink" title="五、==关联规则=="></a>五、==关联规则==</h1><h1 id="指定商品之间关系模式（如啤酒尿布）"><a href="#指定商品之间关系模式（如啤酒尿布）" class="headerlink" title="指定商品之间关系模式（如啤酒尿布）"></a>指定商品之间关系模式（如啤酒尿布）</h1><p>一个典型的规则可以表述为： {花生酱，果酱} –&gt; {面包}</p><h3 id="支持度和置信度："><a href="#支持度和置信度：" class="headerlink" title="支持度和置信度："></a>支持度和置信度：</h3><p>一个项集或者规则度量法的支持度是指其在数据中出现的频率</p><p>置信度是指该规则的预测能力或者准确度的度量</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g298xxfo38j30f408ajtu.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g299p8axdfj31fd0rejry.jpg" alt=""></p><p>理解：</p><p>​       项集 的支持度：指该相集在数据集中的概率</p><p>​       关联规则:  </p><p>​                     就是 由项集 A ， 推出项集 B 发生的概率 ， 而这个概率就是置信度</p><p>​       置信度   ： 指由 {AB} 的支持度 /  {A }的支持度 </p><p>  Apriori算法是通过设置支持度阈值 ， 找出频繁项集  ，结合关联规则 ， 得出置信度。    </p><p>Spark中没有Apriori算法，它的关联规则算法是FPGrowth算法。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g299p1nk7yj31au0o1gmi.jpg" alt=""></p><h1 id="Apriori算法-FPGrowth算法"><a href="#Apriori算法-FPGrowth算法" class="headerlink" title="Apriori算法/FPGrowth算法"></a>Apriori算法/FPGrowth算法</h1><p> Apriori原则指的是一个频繁项集的所有子集也必须是频繁的，如果{A,B}是频繁的，那么{A}和{B}都必须是频繁的<br> 根据定义，支持度表示一个项集出现在数据中的频率。因此，如果知道{A}不满足所期望的支持度阈值，那么就没有考虑{A,B}或者任何包含{A}的项集，这些项集绝对不可能是频繁的</p><p> Apriori算法利用这个逻辑在实际评估他们之前潜在的关联规则</p><ul><li><p>分为两个阶段：</p><ul><li><p>识别所有满足最小支持度阈值的项集</p></li><li><p>根据满足最小支持度阈值的这些项集来创建规则 </p></li></ul></li><li><p>例如，迭代1需要评估一组1项集，迭代2评估2项集，以此类推。在迭代中没有产生新的项集，算法将停止。之后，算法会根据产生的频繁项集，根据所有可能的子集产生关联规则。例如，{A，B}将产生候选规则{A}-&gt;{B}和{B}-&gt;{A}。这些规则将根据最小置信度阈值评估，任何不满足所期望的置信度的规则将被排除</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g299p7k23jj31a20qdt9w.jpg" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.fpm.<span class="type">FPGrowth</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AssociationRule</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * Spark购物篮关联规则算法</span></span><br><span class="line"><span class="comment">      a,b,c</span></span><br><span class="line"><span class="comment">      a,b,d</span></span><br><span class="line"><span class="comment">      a,c</span></span><br><span class="line"><span class="comment">      a,d</span></span><br><span class="line"><span class="comment">      **/</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> inputPath = <span class="string">"shopping_cart"</span></span><br><span class="line">        <span class="keyword">val</span> outputPath = <span class="string">"rs/shopping_cart"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">            .setMaster(<span class="string">"local"</span>)</span><br><span class="line">            .setAppName(<span class="string">"AssociationRule"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="type">SparkContext</span>.getOrCreate(sparkConf)</span><br><span class="line">        <span class="keyword">val</span> transactions: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(inputPath)</span><br><span class="line">        <span class="comment">//求出商品组合：(List(a,b),1) (List(b,c),1) (List(a,b,c),1)...</span></span><br><span class="line">        <span class="keyword">val</span> patterns: <span class="type">RDD</span>[(<span class="type">List</span>[<span class="type">String</span>], <span class="type">Int</span>)] = </span><br><span class="line">                         transactions.flatMap(line =&gt; &#123;</span><br><span class="line">                         <span class="keyword">val</span> items = line.split(<span class="string">","</span>).toList</span><br><span class="line">                         <span class="comment">//combinations 对项集执行两两自由组合</span></span><br><span class="line">            (<span class="number">0</span> to items.size).flatMap(items.combinations).filter(xs =&gt;</span><br><span class="line">                          !xs.isEmpty)</span><br><span class="line">        &#125;).map((_, <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//商品组合出现的频度计算</span></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">          * (List(b, c, e),1)</span></span><br><span class="line"><span class="comment">          * (List(b, d, e),1)</span></span><br><span class="line"><span class="comment">          * (List(c, d),2)</span></span><br><span class="line"><span class="comment">          * (List(c, e),2)</span></span><br><span class="line"><span class="comment">          * (List(b),9)</span></span><br><span class="line"><span class="comment">          * (List(a, b, d),1)</span></span><br><span class="line"><span class="comment">          * (List(b, d),5)</span></span><br><span class="line"><span class="comment">          * (List(a, b),3)</span></span><br><span class="line"><span class="comment">          * (List(d, e),2)</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        <span class="keyword">val</span> combined: <span class="type">RDD</span>[(<span class="type">List</span>[<span class="type">String</span>], <span class="type">Int</span>)] = patterns.reduceByKey(_ + _)</span><br><span class="line">        combined.collect().foreach(println)</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">          * 算出所有的关联规则</span></span><br><span class="line"><span class="comment">          * (List(b, c, e),(List(),1))</span></span><br><span class="line"><span class="comment">          * (List(c, e),(List(b, c, e),1))</span></span><br><span class="line"><span class="comment">          * (List(b, e),(List(b, c, e),1))</span></span><br><span class="line"><span class="comment">          * (List(b, c),(List(b, c, e),1))</span></span><br><span class="line"><span class="comment">          * (List(b, d, e),(List(),1))</span></span><br><span class="line"><span class="comment">          * (List(e),(List(c, e),2))</span></span><br><span class="line"><span class="comment">          * (List(c),(List(c, e),2))</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">        <span class="keyword">val</span> subpatterns: <span class="type">RDD</span>[(<span class="type">List</span>[<span class="type">String</span>], (<span class="type">List</span>[<span class="type">String</span>], <span class="type">Int</span>))] =</span><br><span class="line">             combined.flatMap(pattern =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> result = </span><br><span class="line">            <span class="type">ListBuffer</span>.empty[<span class="type">Tuple2</span>[<span class="type">List</span>[<span class="type">String</span>], <span class="type">Tuple2</span>[<span class="type">List</span>[<span class="type">String</span>], <span class="type">Int</span>]]]</span><br><span class="line">            result += ((pattern._1, (<span class="type">Nil</span>, pattern._2)))</span><br><span class="line">            print(result)</span><br><span class="line">            <span class="keyword">val</span> sublist= <span class="keyword">for</span> &#123;</span><br><span class="line">                i &lt;- <span class="number">0</span> until pattern._1.size</span><br><span class="line">                xs = pattern._1.take(i) ++ pattern._1.drop(i + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> xs.size &gt; <span class="number">0</span></span><br><span class="line">            &#125; <span class="keyword">yield</span> (xs, (pattern._1, pattern._2))</span><br><span class="line">            result ++= sublist</span><br><span class="line">            println(<span class="string">" : "</span> + result.toList)</span><br><span class="line">            result.toList</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        subpatterns.collect().foreach(x =&gt; &#123;println(x + <span class="string">"-----------"</span>)&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> rules: <span class="type">RDD</span>[(<span class="type">List</span>[<span class="type">String</span>], <span class="type">Iterable</span>[(<span class="type">List</span>[<span class="type">String</span>], <span class="type">Int</span>)])] =</span><br><span class="line">                subpatterns.groupByKey()</span><br><span class="line"></span><br><span class="line">        <span class="comment">//计算每个规则的概率</span></span><br><span class="line">        <span class="keyword">val</span> assocRules: <span class="type">RDD</span>[<span class="type">List</span>[(<span class="type">List</span>[<span class="type">String</span>], <span class="type">List</span>[<span class="type">String</span>], <span class="type">Double</span>)]] =</span><br><span class="line">        rules.map(in =&gt; &#123;</span><br><span class="line"><span class="comment">//          val a: Iterable[(List[String], Int)] = in._2</span></span><br><span class="line">            <span class="keyword">val</span> fromCount = in._2.find(p =&gt; p._1 == <span class="type">Nil</span>).get</span><br><span class="line">            <span class="keyword">val</span> lstData = in._2.filter(p =&gt; p._1 != <span class="type">Nil</span>).toList</span><br><span class="line">            <span class="keyword">if</span> (lstData.isEmpty) <span class="type">Nil</span></span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">val</span> result = &#123;</span><br><span class="line">                    <span class="keyword">for</span> &#123;</span><br><span class="line">                        t2 &lt;- lstData</span><br><span class="line">                        confidence = t2._2.toDouble / fromCount._2.toDouble</span><br><span class="line">                        difference = t2._1 diff in._1</span><br><span class="line">                    &#125; <span class="keyword">yield</span> (((in._1, difference, confidence)))</span><br><span class="line">                &#125;</span><br><span class="line">                result</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">val</span> formatResult: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">Double</span>)] = </span><br><span class="line">       assocRules.flatMap(f =&gt; &#123;</span><br><span class="line">            f.map(s =&gt; </span><br><span class="line">            (s._1.mkString(<span class="string">"["</span>, <span class="string">","</span>, <span class="string">"]"</span>), s._2.mkString(<span class="string">"["</span>, <span class="string">","</span>, <span class="string">"]"</span>), s._3))</span><br><span class="line">        &#125;).sortBy(tuple =&gt; tuple._3, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">//保存结果</span></span><br><span class="line">        <span class="comment">//formatResult.saveAsTextFile(outputPath)</span></span><br><span class="line">        <span class="comment">//打印商品组合频度</span></span><br><span class="line">        combined.foreach(println)</span><br><span class="line">        <span class="comment">//打印商品关联规则和置信度</span></span><br><span class="line">        formatResult.foreach(println)</span><br><span class="line">        sc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="六、-逻辑回归"><a href="#六、-逻辑回归" class="headerlink" title="六、==逻辑回归=="></a>六、==逻辑回归==</h1><p>官网：<a href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#classification" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/mllib-linear-methods.html#classification</a></p><p>逻辑回归是一种<strong>线性</strong><code>有监督</code><strong>分类</strong>模型</p><p>主要用于做分类 ， 常见的是二分类（分成两个类）， 被命名为 正负例类。还有多分类（多于两个类型）</p><p>有监督：有Y值用来测试结果</p><p>无监督：无Y值用来参考</p><h1 id="预测是否生病"><a href="#预测是否生病" class="headerlink" title="预测是否生病"></a>预测是否生病</h1><p><code>逻辑回归</code>是一种用于分类的模型，就相当于y=f(x)，表明输入与输出（类别）的关系。</p><p>最常见问题有如医生治病时的望、闻、问、切，之后判定病人是否生病或生了什么病，其中的望闻问切就是输入，即特征数据，判断是否生病就相当于获取因变量y，即分类结果。</p><p>二分类：结合训练医疗模型，根据输入的特征数据，判断因变量Y的值（要么健康，要么生病）。</p><h2 id="逻辑回归和线性回归的差别："><a href="#逻辑回归和线性回归的差别：" class="headerlink" title="逻辑回归和线性回归的差别："></a>逻辑回归和线性回归的差别：</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:center"><em>线性回归</em></th><th style="text-align:center"><em>逻辑回归</em></th></tr></thead><tbody><tr><td style="text-align:center">目的</td><td style="text-align:center">预测</td><td style="text-align:center">分类</td></tr><tr><td style="text-align:center"><img src="https://private.codecogs.com/gif.latex?%5Cdpi%7B100%7D%20y%5E%7B%28i%29%7D" alt="y^{(i)}"><span class="img-alt">y^{(i)}</span></td><td style="text-align:center">未知</td><td style="text-align:center">{0,1}</td></tr><tr><td style="text-align:center">函数</td><td style="text-align:center">拟合函数</td><td style="text-align:center">预测函数</td></tr><tr><td style="text-align:center">参数计算方式</td><td style="text-align:center">最小二乘</td><td style="text-align:center">最大似然估计</td></tr></tbody></table><h2 id="代码案例："><a href="#代码案例：" class="headerlink" title="代码案例："></a>代码案例：</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;<span class="type">LogisticRegressionWithLBFGS</span>, <span class="type">LogisticRegressionWithSGD</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.optimization.&#123;<span class="type">L1Updater</span>, <span class="type">SquaredL2Updater</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.<span class="type">LabeledPoint</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogisticRegression4</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"spark"</span>).setMaster(<span class="string">"local[3]"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">     <span class="comment">/*数据类型：</span></span><br><span class="line"><span class="comment">     1 1:56 2:1 3:0 4:3 5:4 6:3 </span></span><br><span class="line"><span class="comment">     0 1:18 2:0 3:0 4:4 5:3 6:3 </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">        <span class="keyword">val</span> inputData: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>] = </span><br><span class="line">                                  <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">"健康状况训练集.txt"</span>)</span><br><span class="line">        <span class="keyword">val</span> splits = inputData.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">        <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">        lr.setIntercept(<span class="literal">true</span>）<span class="comment">//设置截距</span></span><br><span class="line">        <span class="comment">//    val model = lr.run(trainingData)</span></span><br><span class="line">        <span class="comment">//    val result = testData.map&#123;point=&gt;</span></span><br><span class="line">        <span class="comment">//               Math.abs(point.label-model.predict(point.features)) &#125;</span></span><br><span class="line">        <span class="comment">//    println("正确率="+(1.0-result.mean()))</span></span><br><span class="line">        <span class="comment">//    println(model.weights.toArray.mkString(" "))</span></span><br><span class="line">        <span class="comment">//    println(model.intercept)</span></span><br><span class="line"><span class="comment">//将模型设置为不返回 0 ， 1  结果，而返回结果为 计算的概率</span></span><br><span class="line">        <span class="keyword">val</span> model = lr.run(trainingData).clearThreshold()</span><br><span class="line">        <span class="keyword">val</span> errorRate = testData.map &#123; p =&gt;</span><br><span class="line">            <span class="keyword">val</span> score = model.predict(p.features)</span><br><span class="line">            <span class="comment">// 癌症病人宁愿错判断出得癌症也别错过一个得癌症的病人</span></span><br><span class="line">            <span class="keyword">val</span> result = score &gt; <span class="number">0.5</span> <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="literal">true</span> =&gt; <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">case</span> <span class="literal">false</span> =&gt; <span class="number">0</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//为了规避风险，可以调整固定阈值0.5</span></span><br><span class="line">            <span class="type">Math</span>.abs(result - p.label)</span><br><span class="line">        &#125;.mean()<span class="comment">//求均值</span></span><br><span class="line">        println(<span class="number">1</span> - errorRate)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>出现线性不可分的情况时，可以使用调维方法， 如将二维数据变成三维数据</p><h1 id="鲁棒性调优"><a href="#鲁棒性调优" class="headerlink" title="鲁棒性调优"></a>鲁棒性调优</h1><p>W在数值上越小越好，这样越能抵抗数据的扰动</p><h1 id="数值优化"><a href="#数值优化" class="headerlink" title="数值优化"></a>数值优化</h1><h2 id="最大值最小值法"><a href="#最大值最小值法" class="headerlink" title="最大值最小值法"></a>最大值最小值法</h2><ul><li>归一化的一种方法：最大值最小值法</li><li>缺点<ul><li>抗干扰能力 弱</li><li>受离群值得影响比较大</li><li>中间容易没有数据</li></ul></li></ul><h2 id="方差归一化"><a href="#方差归一化" class="headerlink" title="方差归一化"></a>方差归一化</h2><ul><li>归一化的一种方法：方差归一化</li><li>优点<ul><li>抗干扰能力强，和所有数据都有关, 求标准差需要所有值的介入，重要有离群值的话，会被抑<br> 制下来</li></ul></li><li>缺点<ul><li>最终未必会落到0到1之间</li></ul></li><li>牺牲归一化结果为代价提高稳定</li></ul><h2 id="均值归一化："><a href="#均值归一化：" class="headerlink" title="均值归一化："></a>均值归一化：</h2><p> 每个数量减去平均值</p><h1 id="七、-随机森林"><a href="#七、-随机森林" class="headerlink" title="七、==随机森林=="></a>七、==随机森林==</h1><h2 id="天气与车祸的关系"><a href="#天气与车祸的关系" class="headerlink" title="天气与车祸的关系"></a>天气与车祸的关系</h2><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><ul><li><p>决策树</p><ul><li>决策树是一个预测模型;他代表的是对象属性与对象值之间的一种映射关系</li><li>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试<br>输出，每个叶节点代表一种类别。</li></ul></li><li><p>决策树 思想，实际上就是寻找最纯净的划分 方法。</p></li><li><p>决策树是一种 <strong>非线性</strong> <code>有监督</code> <strong>分类</strong> 模型<br> 线性分类模型比如说逻辑回归，可能会存在不可分问题，但是非线性分类就不存在</p></li><li><p>决策树是通过固定的条件来对类别进行判断：</p></li><li><p>决策树方法</p><ul><li>决策树的生成：数据不断分裂的递归过程，每一次分裂，尽可能让类别一样的数据在树的一边，当树的叶子节点的数据都是一类的时候，则停止分类。(if else 语句)</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.<span class="type">DecisionTree</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.model.<span class="type">DecisionTreeModel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ClassificationDecisionTree</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">"analysItem"</span>)</span><br><span class="line">    conf.setMaster(<span class="string">"local[3]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        1 1:3 2:1 3:1 4:1 5:66 </span></span><br><span class="line"><span class="comment">        0 1:1 2:3 3:2 4:2 5:47 </span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        <span class="keyword">val</span> data = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">"汽车数据样本.txt"</span>)</span><br><span class="line">        <span class="comment">// Split the data into training and test sets (30% held out for testing)</span></span><br><span class="line">        <span class="keyword">val</span> splits = data.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">        <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//指明类别,二分类</span></span><br><span class="line">        <span class="keyword">val</span> numClasses = <span class="number">2</span></span><br><span class="line">        <span class="comment">//指定离散变量，未指明的都当作连续变量处理</span></span><br><span class="line">        <span class="comment">//1,2,3,4维度进来就变成了0,1,2,3</span></span><br><span class="line">        <span class="comment">//这里天气维度有3类,但是要指明4,这里是个坑,后面以此类推</span></span><br><span class="line">        <span class="keyword">val</span> categoricalFeaturesInfo = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>](<span class="number">0</span> -&gt; <span class="number">4</span>, <span class="number">1</span> -&gt; <span class="number">4</span>, <span class="number">2</span> -&gt; <span class="number">3</span>, <span class="number">3</span> -&gt; <span class="number">3</span>)</span><br><span class="line">        <span class="comment">//设定评判标准</span></span><br><span class="line">        <span class="keyword">val</span> impurity = <span class="string">"entropy"</span></span><br><span class="line">        <span class="comment">//树的最大深度,太深运算量大也没有必要  剪枝</span></span><br><span class="line">        <span class="keyword">val</span> maxDepth = <span class="number">3</span></span><br><span class="line">        <span class="comment">//设置离散化程度,连续数据需要离散化,分成32个区间,默认其实就是32,分割的区间保证数量差不多  这个参数也可以进行剪枝</span></span><br><span class="line">        <span class="keyword">val</span> maxBins =<span class="number">10</span></span><br><span class="line">        <span class="comment">//生成模型</span></span><br><span class="line">        <span class="keyword">val</span> model: <span class="type">DecisionTreeModel</span> = </span><br><span class="line">        <span class="type">DecisionTree</span>.trainClassifier(trainingData, numClasses,</span><br><span class="line">                                     categoricalFeaturesInfo, impurity, </span><br><span class="line">                                     maxDepth, maxBins)</span><br><span class="line">        <span class="comment">//测试</span></span><br><span class="line">        <span class="keyword">val</span> labelAndPreds: <span class="type">RDD</span>[(<span class="type">Double</span>, <span class="type">Double</span>)] = testData.map &#123; point =&gt;</span><br><span class="line">            <span class="keyword">val</span> prediction = model.predict(point.features)</span><br><span class="line">            (point.label, prediction)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> testErr = labelAndPreds.filter(r =&gt; r._1 != r._2).count().toDouble / testData.count()</span><br><span class="line">        println(<span class="string">"Test Error = "</span> + testErr)</span><br><span class="line">        println(<span class="string">"Learned classification tree model:\n"</span> + model.toDebugString)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p> 随机森林是一种非线性有监督分类模型<br> 森林：由树组成<br> 随机：生成树的数据都是从数据集中随机选取的</p><p>生成方式</p><p>当数据集很大的时候，我们随机选取数据集的一部分，生成一棵树，重复上述过程，我们可以生成一堆形态各异的树，这些树放在一起就叫森林。</p><table><thead><tr><th style="text-align:center">逻辑回归</th><th style="text-align:center">随机森林</th></tr></thead><tbody><tr><td style="text-align:center">软分类</td><td style="text-align:center">硬分类</td></tr><tr><td style="text-align:center">线性模型</td><td style="text-align:center">非线性模型</td></tr><tr><td style="text-align:center">输出有概率意义</td><td style="text-align:center">输出无概率意义</td></tr><tr><td style="text-align:center">抗干扰能力强</td><td style="text-align:center">抗干扰能力弱</td></tr></tbody></table><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.tree.<span class="type">RandomForest</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ClassificationRandomForest</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">"analysItem"</span>)</span><br><span class="line">    conf.setMaster(<span class="string">"local[3]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//读取数据</span></span><br><span class="line">        <span class="keyword">val</span> data = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">"汽车数据样本.txt"</span>)</span><br><span class="line">        <span class="comment">//将样本按7：3的比例分成</span></span><br><span class="line">        <span class="keyword">val</span> splits = data.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">        <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">//分类数</span></span><br><span class="line">        <span class="keyword">val</span> numClasses = <span class="number">2</span></span><br><span class="line">        <span class="comment">// categoricalFeaturesInfo 为空，意味着所有的特征为连续型变量</span></span><br><span class="line">        <span class="keyword">val</span> categoricalFeaturesInfo = <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>](<span class="number">0</span> -&gt; <span class="number">4</span>, <span class="number">1</span> -&gt; <span class="number">4</span>, <span class="number">2</span> -&gt; <span class="number">3</span>, <span class="number">3</span> -&gt; <span class="number">3</span>)</span><br><span class="line">        <span class="comment">//树的个数</span></span><br><span class="line">        <span class="keyword">val</span> numTrees = <span class="number">3</span></span><br><span class="line">        <span class="comment">//特征子集采样策略，auto 表示算法自主选取</span></span><br><span class="line">        <span class="comment">//"auto"根据特征数量在4个中进行选择</span></span><br><span class="line">    <span class="comment">// 1,all 全部特征 2,sqrt 把特征数量开根号后随机选择的 3,log2 取对数个 4,onethird 三分之一</span></span><br><span class="line">        <span class="keyword">val</span> featureSubsetStrategy = <span class="string">"auto"</span></span><br><span class="line">        <span class="comment">//纯度计算</span></span><br><span class="line">        <span class="keyword">val</span> impurity = <span class="string">"entropy"</span></span><br><span class="line">        <span class="comment">//树的最大层次</span></span><br><span class="line">        <span class="keyword">val</span> maxDepth = <span class="number">3</span></span><br><span class="line">        <span class="comment">//特征最大装箱数,即连续数据离散化的区间</span></span><br><span class="line">        <span class="keyword">val</span> maxBins = <span class="number">32</span></span><br><span class="line">        <span class="comment">//训练随机森林分类器，trainClassifier 返回的是 RandomForestModel 对象</span></span><br><span class="line">        <span class="keyword">val</span> model = </span><br><span class="line">     <span class="type">RandomForest</span>.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,</span><br><span class="line">            numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)</span><br><span class="line">        <span class="comment">//打印模型</span></span><br><span class="line">        println(model.toDebugString)</span><br><span class="line">        <span class="comment">//保存模型</span></span><br><span class="line">        <span class="comment">//model.save(sc,"汽车保险")</span></span><br><span class="line">        <span class="comment">//在测试集上进行测试</span></span><br><span class="line">        <span class="keyword">val</span> count = testData.map &#123; point =&gt;</span><br><span class="line">            <span class="keyword">val</span> prediction = model.predict(point.features)</span><br><span class="line">            <span class="comment">//    Math.abs(prediction-point.label)</span></span><br><span class="line">            (prediction, point.label)</span><br><span class="line">        &#125;.filter(r =&gt; r._1 != r._2).count()</span><br><span class="line">        println(<span class="string">"Test Error = "</span> + count.toDouble / testData.count().toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>日志分析系统</title>
      <link href="/2019/02/25/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2019/02/25/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<p>一、项目需求功能</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1pexqdtr4j30pi0bxgmm.jpg" alt=""></p><ul><li>JS | JDK 发送数据到 Nginx ；</li><li>Nginx配置日志文件access_log以及日志文件的数据格式，将访问它的数据（URL）存储在access_log文件中；</li><li>使用Flume的（exec _ hdfs ）将日志文件的数据存储在HDFS中指定的文件中；</li><li>使用MR对数据实现ETL，在map阶段对数据进行切分，然后，将清洗后的数据存储在Hbase中；</li><li>将Hbase中的数据整合到Hive中，使用hql对数据进行多维度分析</li><li>对Hive中分析的数据结果通过Sqoop导出到MySQL 中，供前台展示</li><li></li></ul><p>二、项目逻辑分布</p><p>三、项目初始化</p>]]></content>
      
      
      <categories>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大型网站日志分析系统 </tag>
            
            <tag> Java </tag>
            
            <tag> 离线分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark调优</title>
      <link href="/2019/02/23/Spark%E4%BC%98%E5%8C%96/"/>
      <url>/2019/02/23/Spark%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="1、资源调优"><a href="#1、资源调优" class="headerlink" title="1、资源调优"></a>1、资源调优</h1><h2 id="1）-在部署-spark-集群中"><a href="#1）-在部署-spark-集群中" class="headerlink" title="1）  在部署 spark 集群中"></a>1）  在部署 spark 集群中</h2><p>在部署 spark 集群中指定资源分配的默认参数 。</p><p>在 spark 安装包的 conf 下 spark-env.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SPARK_WORKER_CORES</span><br><span class="line">SPARK_WORKER_MEMORY</span><br><span class="line">SPARK_WORKER_INSTANCES  ##每台机器启动 worker 数</span><br></pre></td></tr></table></figure><h2 id="2）-在提交-Application-时"><a href="#2）-在提交-Application-时" class="headerlink" title="2）  在提交 Application 时"></a>2）  在提交 Application 时</h2><p>在提交 Application 的时候给当前的 Application 分配更多的资源</p><p>提交命令选项：（在提交 Application 的时候使用选项）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--executor-cores</span><br><span class="line">--executor-memory</span><br><span class="line">--total-executor-cores</span><br></pre></td></tr></table></figure><p>配置信息：（Application 的代码中设置或在 Spark-default.conf 中设置）</p><p>通过SparkConf  conf.set( ) 进行配置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.executor.cores</span><br><span class="line">spark. executor.memory</span><br><span class="line">spark.max.cores</span><br></pre></td></tr></table></figure><p>动态分配资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.service.enabled true ##启用 External shuffle Service 服务</span><br><span class="line">spark.shuffle.service.port 7337 ##Shuffle Service 服务端口，必须和yarn-site 中的一致</span><br><span class="line">spark.dynamicAllocation.enabled true ##开启动态资源分配</span><br><span class="line">spark.dynamicAllocation.minExecutors 1 ##每个 Application 最小分配的executor 数</span><br><span class="line">spark.dynamicAllocation.maxExecutors 30 ##每个 Application 最大并发分配的 executor 数</span><br><span class="line">spark.dynamicAllocation.schedulerBacklogTimeout 1s</span><br><span class="line">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s</span><br></pre></td></tr></table></figure><h1 id="2、并行度调优"><a href="#2、并行度调优" class="headerlink" title="2、并行度调优"></a>2、并行度调优</h1><p>1）  如果读取的数据在 HDFS 中，降低 block 大小，相当于提高了 RDD中 partition 个数 </p><p>一般产生shuffle（数据放在磁盘，shuffle就是从磁盘拉取数据到内存就可以重新部署）的算子都可以设置分区数达到并行度调优。</p><p>​       sc.textFile(xx,numPartitions)</p><p>2）  sc.parallelize(xxx, numPartitions)</p><p>3）  sc.makeRDD(xxx, numPartitions)      //Scala代码，类似于parallelize</p><p>4）  sc.parallelizePairs(xxx, numPartitions)</p><p>5）  repartions/coalesce</p><p>6）  redecByKey/groupByKey/join —(xxx, numPartitions)</p><p>7）  spark.default.parallelism net set</p><p>8）  spark.sql.shuffle.partitions—200</p><p>9）  自定义分区器（作用在K,V 格式的RDD上）partitionBy，或者在reduceByKey算子中，把自定义分区器用匿名内部类的形式作为参数传入。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, String&gt; partitionByPairRDD = </span><br><span class="line">       javaPairRDD.partitionBy(<span class="keyword">new</span> Partitioner() &#123;</span><br><span class="line">        <span class="comment">//定义分区规则</span></span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">               Integer value = Integer.parseInt((String)key);</span><br><span class="line">               <span class="keyword">return</span> value % <span class="number">3</span>;</span><br><span class="line">           &#125;</span><br><span class="line">         <span class="comment">//定义分成几个区</span></span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numPartitions</span><span class="params">()</span> </span>&#123;</span><br><span class="line">               <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br></pre></td></tr></table></figure><p>10） 如果读取数据是在 SparkStreaming 中</p><p>Receiver模式 ;设置blockInterval:   spark.streaming.blockInterval—200ms</p><p>Direct:由读取的 topic 的分区数决定并行度</p><h1 id="3、代码调优"><a href="#3、代码调优" class="headerlink" title="3、代码调优"></a>3、代码调优</h1><h2 id="1、避免创建重复的-RDD"><a href="#1、避免创建重复的-RDD" class="headerlink" title="1、避免创建重复的 RDD"></a>1、避免创建重复的 RDD</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(path1)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(path1)</span><br></pre></td></tr></table></figure><p>这就是创建了重复的 RDD<br>有什么问题？ 对于执行性能来说没有问题，但是呢，代码乱</p><h2 id="2、复用同一个-RDD"><a href="#2、复用同一个-RDD" class="headerlink" title="2、复用同一个 RDD"></a>2、复用同一个 RDD</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = <span class="type">RDD</span>&lt;<span class="type">String</span>,<span class="type">String</span>&gt;</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(_._2)</span><br></pre></td></tr></table></figure><p>这样的话 rdd2 是 rdd1 的子集。 rdd2 执行了一个操作 filter</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd2.filter()= rdd1.map（(_._2)）.filter()</span><br></pre></td></tr></table></figure><p>复用同一个RDD时，对这个RDD做cache持久化 ，就可以避免对这个RDD的重复计算。</p><h2 id="3、对多次使用的-RDD-进行持久化"><a href="#3、对多次使用的-RDD-进行持久化" class="headerlink" title="3、对多次使用的 RDD 进行持久化"></a>3、对多次使用的 RDD 进行持久化</h2><h3 id="选择最合适的持久化策略？"><a href="#选择最合适的持久化策略？" class="headerlink" title="选择最合适的持久化策略？"></a>选择最合适的持久化策略？</h3><p>​        MEMORY_ONLY    (默认)</p><p>​         性能最高，但要求内存必须足够大，可存放下整个 RDD 的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个 RDD 的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果 RDD 中数据比较多时（比如几十亿），<br>直接用这种持久化级别，会导致 JVM 的 OOM 内存溢出异常。</p><p>​         MEMORY_ONLY_SER   （建议）</p><p>​        该级别会将 RDD 数据序列化后再保存在内存中，此时每个 partition 仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比 MEMORY_ONLY 多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。如果RDD 中的数据量过多的话，还是可能会导致 OOM 内存溢出的异常。</p><p>​      MEMORY_AND_DISK_SER   （建 议）</p><p>​    适用于纯内存的级别都无法使用 ,RDD 的数据量很大，内存无法完全放下时。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</p><p>​      DISK_ONLY 和后缀为_2      （不建议）</p><p>​      因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2 的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</p><h3 id="持久化算子："><a href="#持久化算子：" class="headerlink" title="持久化算子："></a>持久化算子：</h3><h4 id="（1）cache"><a href="#（1）cache" class="headerlink" title="（1）cache:"></a>（1）cache:</h4><p> MEMORY_ONLY</p><h4 id="（2）persist："><a href="#（2）persist：" class="headerlink" title="（2）persist："></a>（2）persist：</h4><p>MEMORY_ONLY<br>MEMORY_ONLY_SER<br>MEMORY_AND_DISK_SER<br>一般不要选择带有_2 的持久化级别。</p><h4 id="（3）checkpoint"><a href="#（3）checkpoint" class="headerlink" title="（3）checkpoint:"></a>（3）checkpoint:</h4><p>① 如果一个 RDD 的计算时间比较长或者计算起来比较复杂，一般将这个 RDD 的计算结果保存到 HDFS 上，这样数据会更加安全。优化方案：在需要checkpoint的地方调用cache或persist ，提高效率。<br>② 如果一个 RDD 的依赖关系非常长，也会使用 checkpoint,会切断依赖关系，提高容错的效率。</p><h2 id="4、尽量避免使用-shuffle-类的算子"><a href="#4、尽量避免使用-shuffle-类的算子" class="headerlink" title="4、尽量避免使用 shuffle 类的算子"></a>4、尽量避免使用 shuffle 类的算子</h2><p>使用广播变量来模拟使用 join,使用情况：一个 RDD 比较大，一个 RDD比较小。</p><p>join 算子=广播变量+filter、广播变量+map、广播变量+flatMap</p><h2 id="5、使用-map-side-预聚合的-shuffle-操作"><a href="#5、使用-map-side-预聚合的-shuffle-操作" class="headerlink" title="5、使用 map-side 预聚合的 shuffle 操作"></a>5、使用 map-side 预聚合的 shuffle 操作</h2><p>即尽量使用有 combiner 的 shuffle 类算子。</p><h3 id="combiner-概念："><a href="#combiner-概念：" class="headerlink" title="combiner 概念："></a>combiner 概念：</h3><p>在 map 端，每一个 map task 计算完毕后进行的局部聚合。</p><h3 id="combiner-好处："><a href="#combiner-好处：" class="headerlink" title="combiner 好处："></a>combiner 好处：</h3><p>1) 降低 shuffle write 写磁盘的数据量。</p><p>2) 降低 shuffle read 拉取数据量的大小。</p><p>3) 降低 reduce 端聚合的次数。</p><h3 id="有-combiner-的-shuffle-类算子："><a href="#有-combiner-的-shuffle-类算子：" class="headerlink" title="有 combiner 的 shuffle 类算子："></a>有 combiner 的 shuffle 类算子：</h3><p>1) reduceByKey:</p><p>这个算子在 map 端是有 combiner 的，在一些场景中可以使用 reduceByKey 代替 groupByKey。</p><p>2) aggregateByKey</p><p>3) combineByKey</p><h2 id="6、尽量使用高性能的算子"><a href="#6、尽量使用高性能的算子" class="headerlink" title="6、尽量使用高性能的算子"></a>6、尽量使用高性能的算子</h2><p>使用 reduceByKey 替代 groupByKey</p><p>使用 mapPartition 替代 map</p><p>使用 foreachPartition 替代 foreach</p><p>filter 后使用 coalesce 减少分区数</p><p>使用使用repartitionAndSortWithinPartitions 替代repartition与sort类操作</p><p>使用 repartition 和 coalesce 算子操作分区。</p><h2 id="7、使用广播变量"><a href="#7、使用广播变量" class="headerlink" title="7、使用广播变量"></a>7、使用广播变量</h2><p>​        开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如 100M 以上的大集合），那么此时就应该使用 Spark 的广播(Broadcast）功能来提升性能，函数中使用到外部变量时，默认情况下，Spark 会将该变量复制多个副本，通过网络传输到 task 中，此时每个 task都有一个变量副本。如果变量本身比较大的话（比如 100M，甚至 1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor 中占用过多内存导致的频繁 GC，都会极大地影响性能。如果使用的外部变量比较大，建议使用 Spark 的广播功能，对该变量进行广播。广播后的变量，会保证每个 Executor 的内存中，只驻留一份变量副本，</p><p>​       而 Executor 中的 task 执行时共享该 Executor 中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对 Executor 内存的占用开销，降低 GC 的频率。</p><p>​       广播大变量发送方式：</p><p>Executor 一开始并没有广播变量，而是 task 运行需 要 用 到 广 播 变 量 ， 会 找 executor 的 blockManager 要 ，bloackManager 找 Driver 里面的 blockManagerMaster 要。使用广播变量可以大大降低集群中变量的副本数。不使用广播变量，变量的副本数和 task 数一致。使用广播变量变量的副本和 Executor 数一致。</p><h2 id="8、使用-Kryo-优化序列化性能"><a href="#8、使用-Kryo-优化序列化性能" class="headerlink" title="8、使用 Kryo 优化序列化性能"></a>8、使用 Kryo 优化序列化性能</h2><p>在 Spark 中，主要有三个地方涉及到了序列化：</p><p>1) 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输。</p><p>2) 将自定义的类型作为 RDD 的泛型类型时（比如 JavaRDD<ttt>，TTT是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现 Serializable 接口。</ttt></p><p>3) 使用可序列化的持久化策略时（比如 MEMORY_ONLY_SER），Spark会将 RDD 中的每个 partition 都序列化成一个大的字节数组。</p><p>Kryo 序列化器介绍：<br>Spark 支持使用 Kryo 序列化机制。Kryo 序列化机制，比默认的 Java 序列化机制，速度要快，序列化后的数据要更小，大概是 Java 序列化机制的 1/10。所以 Kryo 序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。对于这三种出现序列化的地方，我们都可以通过使用 Kryo 序列化类库，来优化序列化和反序列化的性能。Spark 默认使用的是 Java 的序列化机制，也就是 ObjectOutputStream/ObjectInputStream API 来进行序列化和反序列化。但是 Spark 同时支持使用 Kryo 序列化库，Kryo 序列化类库的性能比 Java 序列化类库的性能要高很多。官方介绍，Kryo 序列化机制比 Java 序列化机制，性能高 10 倍左右。Spark 之所以默认没有使用Kryo 作为序列化类库，是因为 Kryo 要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说这种方式比较麻烦。</p><p>Spark 中使用 Kryo：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Sparkconf.set(&quot;spark.serializer&quot;,</span><br><span class="line">&quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">.registerKryoClasses(new Class[]&#123;SpeedSortKey.class&#125;)</span><br></pre></td></tr></table></figure><h2 id="9、优化数据结构"><a href="#9、优化数据结构" class="headerlink" title="9、优化数据结构"></a>9、优化数据结构</h2><h3 id="java-中有三种类型比较消耗内存："><a href="#java-中有三种类型比较消耗内存：" class="headerlink" title="java 中有三种类型比较消耗内存："></a>java 中有三种类型比较消耗内存：</h3><p>1) 对象，每个 Java 对象都有对象头、引用等额外的信息，因此比较占用内存空间。</p><p>2) 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</p><p>3) 集合类型，比如 HashMap、LinkedList 等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如 Map.Entry。</p><p>因此 Spark 官方建议，在 Spark 编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如 Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低 GC 频率，提升性能。</p><h2 id="10、使用高性能的库-fastutil"><a href="#10、使用高性能的库-fastutil" class="headerlink" title="10、使用高性能的库 fastutil"></a>10、使用高性能的库 fastutil</h2><h3 id="fasteutil-介绍："><a href="#fasteutil-介绍：" class="headerlink" title="fasteutil 介绍："></a>fasteutil 介绍：</h3><ul><li><p>fastutil 是扩展了 Java 标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的 map、set、list 和 queue；</p></li><li><p>fastutil 能够提供更小的内存占用，更快的存取速度；</p></li></ul><p>我们使用 fastutil提供的集合类，来替代自己平时使用的 JDK 的原生的 Map、List、Set，好处在于，fastutil 集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者 key）获取元素的值和设置元素的值的时候，提供更快的存取速度。fastutil 的每一种集合类型，都实现了对应的 Java 中的标准接口（比如 fastutil 的 map，实现了 Java 的 Map 接口），因此可以直接放入已有系统的任何代码中。</p><ul><li>fastutil 最新版本要求 Java 7 以及以上版本。</li></ul><h3 id="使用："><a href="#使用：" class="headerlink" title="使用："></a>使用：</h3><p>见 RandomExtractCars.java 类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String, Map&lt;String, IntList&gt;&gt; fastutilDateHourExtractMap = </span><br><span class="line">                                <span class="keyword">new</span> HashMap&lt;String, Map&lt;String, IntList&gt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (Map.Entry&lt;String, Map&lt;String, List&lt;Integer&gt;&gt;&gt; dateHourExtractEntry : dateHourExtractMap.entrySet()) &#123;</span><br><span class="line">String date = dateHourExtractEntry.getKey();</span><br><span class="line">Map&lt;String, List&lt;Integer&gt;&gt; hourExtractMap =</span><br><span class="line">                                       dateHourExtractEntry.getValue();</span><br><span class="line">            </span><br><span class="line">Map&lt;String, IntList&gt; fastutilHourExtractMap = <span class="keyword">new</span> HashMap&lt;String, IntList&gt;();</span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;Integer&gt;&gt; hourExtractEntry : hourExtractMap.entrySet()) &#123;</span><br><span class="line">String hour = hourExtractEntry.getKey();</span><br><span class="line">List&lt;Integer&gt; extractList = hourExtractEntry.getValue();</span><br><span class="line"></span><br><span class="line">IntList fastutilExtractList = <span class="keyword">new</span> IntArrayList();</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; extractList.size(); i++) &#123;</span><br><span class="line">fastutilExtractList.add(extractList.get(i));</span><br><span class="line">&#125;</span><br><span class="line">fastutilHourExtractMap.put(hour, fastutilExtractList);</span><br><span class="line">&#125;</span><br><span class="line">fastutilDateHourExtractMap.put(date, fastutilHourExtractMap);</span><br></pre></td></tr></table></figure><h1 id="4、数据本地化"><a href="#4、数据本地化" class="headerlink" title="4、数据本地化"></a>4、数据本地化</h1><h2 id="（1）数据本地化的级别："><a href="#（1）数据本地化的级别：" class="headerlink" title="（1）数据本地化的级别："></a>（1）数据本地化的级别：</h2><h3 id="1-PROCESS-LOCAL"><a href="#1-PROCESS-LOCAL" class="headerlink" title="1) PROCESS_LOCAL"></a>1) PROCESS_LOCAL</h3><p>task 要计算的数据在本进程（Executor）的内存中。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21btcmi0aj309d06tt9e.jpg" alt=""></p><h3 id="2-NODE-LOCAL"><a href="#2-NODE-LOCAL" class="headerlink" title="2) NODE_LOCAL"></a>2) NODE_LOCAL</h3><p>① task 所计算的数据在本节点所在的磁盘上。<br>② task 所计算的数据在本节点其他 Executor 进程的内存中。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21bugz8otj30bp0cdwfu.jpg" alt=""></p><h3 id="3-NO-PREF"><a href="#3-NO-PREF" class="headerlink" title="3) NO_PREF"></a>3) NO_PREF</h3><p>task 所计算的数据在关系型数据库中，如 mysql。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21bvbr619j309j07a3yr.jpg" alt=""></p><h3 id="4-RACK-LOCAL"><a href="#4-RACK-LOCAL" class="headerlink" title="4) RACK_LOCAL"></a>4) RACK_LOCAL</h3><p>task所计算的数据在同机架的不同节点的磁盘或者Executor进程的内存中</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21bw26vmxj30910bcmxv.jpg" alt=""></p><h3 id="5-ANY"><a href="#5-ANY" class="headerlink" title="5) ANY"></a>5) ANY</h3><p>跨机架。</p><h2 id="（2）Spark-数据本地化调优："><a href="#（2）Spark-数据本地化调优：" class="headerlink" title="（2）Spark 数据本地化调优："></a>（2）Spark 数据本地化调优：</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21bwjhtyfj30ek08owf7.jpg" alt=""></p><p>​          Spark 中任务调度时，TaskScheduler 在分发之前需要依据数据的位置来分发，最好将 task 分发到数据所在的节点上，如果 TaskScheduler 分发的 task在默认 3s 依然无法执行的话，TaskScheduler 会重新发送这个 task 到相同的 Executor 中去执行，会重试 5 次，如果依然无法执行，那么 TaskScheduler会降低一级数据本地化的级别再次发送 task。</p><p>​         如上图中，会先尝试 1,PROCESS_LOCAL 数据本地化级别，如果重试 5 次每次等待 3s,会默认这个 Executor 计算资源满了，那么会降低一级数据本地化级别到 2，NODE_LOCAL,如果还是重试 5 次每次等待 3s 还是失败，那么还是会降低一级数据本地化级别到 3，RACK_LOCAL。这样数据就会有网络传输，降低了执行效率。</p><h3 id="1-如何提高数据本地化的级别？"><a href="#1-如何提高数据本地化的级别？" class="headerlink" title="1)  如何提高数据本地化的级别？"></a>1)  如何提高数据本地化的级别？</h3><p>可以增加每次发送 task 的等待时间（默认都是 3s），将 3s 倍数调大，  结合 WEBUI 来调节：</p><p>• spark.locality.wait</p><p>• spark.locality.wait.process</p><p>• spark.locality.wait.node</p><p>• spark.locality.wait.rack</p><p>注意：等待时间不能调大很大，调整数据本地化的级别不要本末倒置，虽然每一个 task 的本地化级别是最高了，但整个 Application 的执行时间反而加长。</p><h3 id="2-如何查看数据本地化的级别？"><a href="#2-如何查看数据本地化的级别？" class="headerlink" title="2)  如何查看数据本地化的级别？"></a>2)  如何查看数据本地化的级别？</h3><p>通过日志或者 WEBUI</p><h1 id="5、内存调优"><a href="#5、内存调优" class="headerlink" title="5、内存调优"></a>5、内存调优</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21bxft53yj30d709iq33.jpg" alt=""></p><p>​          JVM堆内存分为一块较大的Eden和两块较小的Survivor，每次只使用Eden和其中一块 Survivor，当回收时将 Eden 和 Survivor 中还存活着的对象一次性复制到另外一块Survivor上，最后清理掉Eden和刚才用过的Survivor。<br>也就是说当 task 创建出来对象会首先往 Eden 和 survivor1 中存放，survivor2是空闲的，当Eden和survivor1区域放满以后就会触发minor gc小型垃圾回收，清理掉不再使用的对象。会将存活下来的对象放入 survivor2中。<br>如果存活下来的对象大小大于 survivor2 的大小，那么 JVM 就会将多余的对象直接放入到老年代中。<br>如果这个时候年轻代的内存不是很大的话，就会经常的进行 minor gc，频繁的 minor gc 会导致短时间内有些存活的对象（多次垃圾回收都没有回收掉，一直在用的又不能被释放,这种对象每经过一次 minor gc 都存活下来）<br>频繁的倒来倒去，会导致这些短生命周期的对象（不一定长期使用）每进行一次垃圾回收就会长一岁。年龄过大，默认 15 岁，垃圾回收还是没有回收回去就会跑到老年代里面去了。<br>这样会导致在老年代中存放大量的短生命周期的对象，老年代应该存放的是数量比较少并且会长期使用的对象，比如数据库连接池对象。这样的话，老年代就会满溢（full gc 因为本来老年代中的对象很少，很少进行 full gc 因此采取了不太复杂但是消耗性能和时间的垃圾回收算法）。不管 minor gc 还是 full gc 都会导致 JVM 的工作线程停止。</p><h2 id="总结-堆内存不足造成的影响："><a href="#总结-堆内存不足造成的影响：" class="headerlink" title="总结-堆内存不足造成的影响："></a>总结-堆内存不足造成的影响：</h2><p>1) 频繁的 minor gc。</p><p>2) 老年代中大量的短声明周期的对象会导致 full gc。</p><p>3) gc 多了就会影响 Spark 的性能和运行的速度。</p><p>Spark JVM 调优主要是降低 gc时间，可以修改 Executor 内存的比例参数。<br>RDD 缓存、task 定义运行的算子函数，可能会创建很多对象，这样会占用大量的堆内存。堆内存满了之后会频繁的 GC，如果 GC 还不能够满足内存的需要的话就会报 OOM。比如一个 task 在运行的时候会创建 N 个对象，这些对象首先要放入到 JVM 年轻代中。比如在存数据的时候我们使用了foreach 来将数据写入到内存，每条数据都会封装到一个对象中存入数据库中，那么有多少条数据就会在 JVM 中创建多少个对象。</p><h2 id="Spark-中如何内存调优？"><a href="#Spark-中如何内存调优？" class="headerlink" title="Spark 中如何内存调优？"></a>Spark 中如何内存调优？</h2><p>Spark Executor 堆内存中存放（以静态内存管理为例）：</p><p>RDD 的缓存数据和广播变量（spark.storage.memoryFraction 0.6），shuffle 聚合内存<br>（spark.shuffle.memoryFraction 0.2）,task 的运行（0.2）那么如何调优呢？</p><p>1) 提高 Executor 总体内存的大小</p><p>2) 降低储存内存比例或者降低聚合内存比例</p><h2 id="如何查看-gc？"><a href="#如何查看-gc？" class="headerlink" title="如何查看 gc？"></a>如何查看 gc？</h2><p>Spark WEBUI 中 job-&gt;stage-&gt;task</p><h1 id="6、Spark-Shuffle-调优"><a href="#6、Spark-Shuffle-调优" class="headerlink" title="6、Spark Shuffle 调优"></a>6、Spark Shuffle 调优</h1><ol><li>buffer 大小——32KB</li><li>shuffle read 拉取数据量的大小——48M</li><li>shuffle 聚合内存的比例——20%</li><li>拉取数据重试次数——5 次</li><li>重试间隔时间 60s</li><li>Spark Shuffle 的种类</li><li>HashShuffle 合并机制</li><li>SortShuffle bypass 机制 200 次</li></ol><h1 id="7、调节-Executor-的堆外内存"><a href="#7、调节-Executor-的堆外内存" class="headerlink" title="7、调节 Executor 的堆外内存"></a>7、调节 Executor 的堆外内存</h1><p>​        Spark 底层 shuffle 的传输方式是使用 netty 传输，netty 在进行网络传输的过程会申请堆外内存（netty 是零拷贝），所以使用了堆外内存。默认情况下，这个堆外内存上限默认是每一个 executor 的内存大小的 10%；真正处理大数据的时候，这里都会出现问题，导致 spark 作业反复崩溃，无法运行；此时就会去调节这个参数，到至少 1G（1024M），甚至说 2G、4G。<br>​     executor 在进行 shuffle write，优先从自己本地block manager中获取某份数据地址，如果本地 block manager 没有的话，那么会通过 TransferService，去远程连接其他节点上 executor 的 blockmanager 去获取，尝试建立远程的网络连接，并且去拉取数据。频繁创建对象让 JVM 堆内存满溢，进行垃圾回收。正好碰到那个 exeuctor 的 JVM 在垃圾回收。处于垃圾回过程中，所有的工作线程全部停止；相当于只要一旦进行垃圾回收，spark / executor 停止工作，无法提供响应，spark 默认的网络连接的超时时长是 60s；如果卡住 60s 都无法建立连接的话，那么这个 task 就失败了。task  失败了就会出现 shuffle file cannot find  的错误。</p><p>那么如何调节等待的时长呢？</p><p>在./spark-submit 提交任务的脚本里面添加：</p><p>–conf spark.core.connection.ack.wait.timeout=300</p><p>Executor 由于内存不足或者堆外内存不足了，挂掉了，对应的 Executor 上面的 block manager 也挂掉了，找不到对应的 shuffle map output 文件，Reducer 端不能够拉取数据。</p><p>我们可以调节堆外内存的大小，如何调节？</p><p>在./spark-submit 提交任务的脚本里面添加</p><p>yarn 下：<br>–conf spark.yarn.executor.memoryOverhead=2048 单位 M</p><p>standalone 下：<br>–conf spark.executor.memoryOverhead=2048 单位 M</p><h1 id="8、解决数据倾斜"><a href="#8、解决数据倾斜" class="headerlink" title="8、解决数据倾斜"></a>8、解决数据倾斜</h1><h2 id="1、使用-Hive-ETL-预处理数据"><a href="#1、使用-Hive-ETL-预处理数据" class="headerlink" title="1、使用 Hive ETL 预处理数据"></a>1、使用 Hive ETL 预处理数据</h2><h3 id="方案适用场景："><a href="#方案适用场景：" class="headerlink" title="方案适用场景："></a>方案适用场景：</h3><p>如果导致数据倾斜的是 Hive 表。如果该 Hive 表中的数据本身很不均匀（比如某个 key 对应了 100 万数据，其他 key 才对应了 10 条数据），而且业务场景需要频繁使用 Spark 对 Hive 表执行某个分析操作，那么比较适合使用这种技术方案。</p><h3 id="方案实现思路："><a href="#方案实现思路：" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>此时可以评估一下，是否可以通过 Hive 来进行数据预处理（即通过 HiveETL 预先对数据按照 key 进行聚合，或者是预先和其他表进行 join），然后在 Spark 作业中针对的数据源就不是原来的 Hive 表了，而是预处理后的 Hive 表。此时由于数据已经预先进行过聚合或 join 操作了，那么在Spark 作业中也就不需要使用原先的 shuffle 类算子执行这类操作了。</p><h3 id="方案实现原理："><a href="#方案实现原理：" class="headerlink" title="方案实现原理："></a>方案实现原理：</h3><p>这种方案从根源上解决了数据倾斜，因为彻底避免了在 Spark 中执行shuffle 类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以 Hive ETL 中进行 group by 或者 join 等 shuffle 操作时，还是会出现数据倾斜，导致 Hive ETL 的速度很慢。我们只是把数据倾斜的发生提前到了 Hive ETL 中，避免 Spark 程序发生数据倾斜而已。</p><h2 id="2、过滤少数导致倾斜的-key"><a href="#2、过滤少数导致倾斜的-key" class="headerlink" title="2、过滤少数导致倾斜的 key"></a>2、过滤少数导致倾斜的 key</h2><h3 id="方案适用场景：-1"><a href="#方案适用场景：-1" class="headerlink" title="方案适用场景："></a>方案适用场景：</h3><p>如果发现导致倾斜的 key 就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如 99%的 key 就对应 10 条数据，但是只有一个 key 对应了 100 万数据，从而导致了数据倾斜。</p><h3 id="方案实现思路：-1"><a href="#方案实现思路：-1" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>如果我们判断那少数几个数据量特别多的 key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个 key。比如，在Spark SQL 中可以使用 where 子句过滤掉这些 key 或者在 Spark Core中对 RDD 执行 filter 算子过滤掉这些 key。如果需要每次作业执行时，动态判定哪些 key 的数据量最多然后再进行过滤，那么可以使用 sample算子对 RDD 进行采样，然后计算出每个 key 的数量，取数据量最多的 key过滤掉即可。</p><h3 id="方案实现原理：-1"><a href="#方案实现原理：-1" class="headerlink" title="方案实现原理："></a>方案实现原理：</h3><p>将导致数据倾斜的 key 给过滤掉之后，这些 key 就不会参与计算了，自然不可能产生数据倾斜。</p><h2 id="3、提高-shuffle-操作的并行度"><a href="#3、提高-shuffle-操作的并行度" class="headerlink" title="3、提高 shuffle 操作的并行度"></a>3、提高 shuffle 操作的并行度</h2><h3 id="方案实现思路：-2"><a href="#方案实现思路：-2" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>在对 RDD 执行 shuffle 算子时，给 shuffle 算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个 shuffle 算子执行时 shuffleread task的数量。对于Spark SQL中的shuffle类语句，比如group by、join 等，需要设置一个参数，即 spark.sql.shuffle.partitions，该参数代表了 shuffle read task 的并行度，该值默认是 200，对于很多场景来说都有点过小。</p><h3 id="方案实现原理：-2"><a href="#方案实现原理：-2" class="headerlink" title="方案实现原理："></a>方案实现原理：</h3><p>增加 shuffle read task 的数量，可以让原本分配给一个 task 的多个 key分配给多个 task，从而让每个 task 处理比原来更少的数据。举例来说，如果原本有 5 个不同的 key，每个 key 对应 10 条数据，这 5 个 key 都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffleread task 以后，每个 task 就分配到一个 key，即每个 task 就处理 10 条数据，那么自然每个 task 的执行时间都会变短了。</p><h2 id="4、双重聚合"><a href="#4、双重聚合" class="headerlink" title="4、双重聚合"></a>4、双重聚合</h2><h3 id="方案适用场景：-2"><a href="#方案适用场景：-2" class="headerlink" title="方案适用场景："></a>方案适用场景：</h3><p>对 RDD 执行 reduceByKey 等聚合类 shuffle 算子或者在 Spark SQL 中使用 group by 语句进行分组聚合时，比较适用这种方案。</p><h3 id="方案实现思路：-3"><a href="#方案实现思路：-3" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个 key 都打上一个随机数，比如 10 以内的随机数，此时原先一样的 key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行 reduceByKey 等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个 key 的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><h3 id="方案实现原理：-3"><a href="#方案实现原理：-3" class="headerlink" title="方案实现原理："></a>方案实现原理：</h3><p>将原本相同的 key 通过附加随机前缀的方式，变成多个不同的 key，就可以让原本被一个 task 处理的数据分散到多个 task 上去做局部聚合，进而解决单个 task 处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21c00ck9yj30ek078t9g.jpg" alt=""></p><p>如果一个 RDD 中有一个 key 导致数据倾斜，同时还有其他的 key，那么一般先对数据集进行抽样，然后找出倾斜的 key,再使用 filter 对原始的RDD 进行分离为两个 RDD，一个是由倾斜的 key 组成的 RDD1，一个是由其他的key 组成的 RDD2，那么对于 RDD1 可以使用加随机前缀进行多分区多 task 计算，对于另一个 RDD2 正常聚合计算，最后将结果再合并起来。</p><h2 id="5、将-reduce-join-转为-map-join"><a href="#5、将-reduce-join-转为-map-join" class="headerlink" title="5、将 reduce join 转为 map join"></a>5、将 reduce join 转为 map join</h2><p>BroadCast+filter(或者 map)</p><h3 id="方案适用场景：-3"><a href="#方案适用场景：-3" class="headerlink" title="方案适用场景："></a>方案适用场景：</h3><p>在对 RDD 使用 join 类操作，或在 Spark SQL 中使用 join 语句时，而且 join 操作中的一个 RDD 或表的数据量比较小（比如几百 M 或者一两 G），比较适用此方案。</p><h3 id="方案实现思路：-4"><a href="#方案实现思路：-4" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>不使用 join 算子进行连接操作，而使用 Broadcast 变量与 map 类算子实现 join 操作，进而完全规避掉 shuffle 类的操作，彻底避免数据倾斜的发生和出现。将较小 RDD 中的数据直接通过 collect 算子拉取到 Driver端的内存中来，然后对其创建一个 Broadcast 变量；接着对另外一个 RDD执行 map 类算子，在算子函数内，从 Broadcast 变量中获取较小 RDD的全量数据，与当前 RDD 的每一条数据按照连接 key 进行比对，如果连接 key 相同的话，那么就将两个 RDD 的数据用你需要的方式连接起来。</p><h3 id="方案实现原理：-4"><a href="#方案实现原理：-4" class="headerlink" title="方案实现原理："></a>方案实现原理：</h3><p>普通的 join 是会走 shuffle 过程的，而一旦 shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个 RDD 是比较小的，则可以采用广播小 RDD 全量数据+map 算子来实现与 join 同样的效果，也就是 map join，此时就不会发生 shuffle 操作，也就不会发生数据倾斜。</p><h2 id="6、采样倾斜-key-并分拆-join-操作"><a href="#6、采样倾斜-key-并分拆-join-操作" class="headerlink" title="6、采样倾斜 key 并分拆 join 操作"></a>6、采样倾斜 key 并分拆 join 操作</h2><h3 id="方案适用场景：-4"><a href="#方案适用场景：-4" class="headerlink" title="方案适用场景："></a>方案适用场景：</h3><p>两个 RDD/Hive 表进行 join 的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个 RDD/Hive 表中的 key 分布情况。如果出现数据倾斜，是因为其中某一个 RDD/Hive 表中的少数几个 key的数据量过大，而另一个 RDD/Hive 表中的所有 key 都分布比较均匀，那么采用这个解决方案是比较合适的。</p><h3 id="方案实现思路：-5"><a href="#方案实现思路：-5" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>对包含少数几个数据量过大的 key 的那个 RDD，通过 sample 算子采样出一份样本来，然后统计一下每个 key 的数量，计算出来数据量最大的是哪几个key。然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的 RDD，并给每个 key 都打上 n 以内的随机数作为前缀，而不会导致倾斜的大部分 key 形成另外一个 RDD。接着将需要 join 的另一个 RDD，也过滤出来那几个倾斜 key 对应的数据并形成一个单独的RDD，将每条数据膨胀成 n 条数据，这 n 条数据都按顺序附加一个 0~n的前缀，不会导致倾斜的大部分 key 也形成另外一个 RDD。再将附加了随机前缀的独立 RDD 与另一个膨胀 n 倍的独立 RDD 进行 join，此时就可以将原先相同的 key 打散成 n 份，分散到多个 task 中去进行 join 了。而另外两个普通的 RDD 就照常 join 即可。最后将两次 join 的结果使用<br>union 算子合并起来即可，就是最终的 join 结果 。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21c0ylpoxj30ek06ijsi.jpg" alt=""></p><h2 id="7、使用随机前缀和扩容-RDD-进行-join"><a href="#7、使用随机前缀和扩容-RDD-进行-join" class="headerlink" title="7、使用随机前缀和扩容 RDD 进行 join"></a>7、使用随机前缀和扩容 RDD 进行 join</h2><h3 id="方案适用场景：-5"><a href="#方案适用场景：-5" class="headerlink" title="方案适用场景："></a>方案适用场景：</h3><p>如果在进行 join 操作时，RDD 中有大量的 key 导致数据倾斜，那么进行分拆 key 也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><h3 id="方案实现思路：-6"><a href="#方案实现思路：-6" class="headerlink" title="方案实现思路："></a>方案实现思路：</h3><p>该方案的实现思路基本和“解决方案六”类似，首先查看 RDD/Hive 表中的数据分布情况，找到那个造成数据倾斜的 RDD/Hive 表，比如有多个 key 都对应了超过 1 万条数据。然后将该 RDD 的每条数据都打上一个n 以内的随机前缀。同时对另外一个正常的 RDD 进行扩容，将每条数据都扩容成 n 条数据，扩容出来的每条数据都依次打上一个 0~n 的前缀。最后将两个处理后的 RDD 进行 join 即可。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g21c1yursmj30ek08fmyp.jpg" alt=""></p><h1 id="9、Spark-故障解决（troubleshooting）"><a href="#9、Spark-故障解决（troubleshooting）" class="headerlink" title="9、Spark 故障解决（troubleshooting）"></a>9、Spark 故障解决（troubleshooting）</h1><h2 id="1、shuffle-file-cannot-find：磁盘小文件找不到。"><a href="#1、shuffle-file-cannot-find：磁盘小文件找不到。" class="headerlink" title="1、shuffle file cannot find：磁盘小文件找不到。"></a>1、shuffle file cannot find：磁盘小文件找不到。</h2><p>1) connection timeout —-shuffle file cannot find<br>提高建立连接的超时时间，或者降低 gc，降低 gc 了那么 spark 不能堆外提供服务的时间就少了，那么超时的可能就会降低。</p><p>2) fetch data fail —- shuffle file cannot find<br>提高拉取数据的重试次数以及间隔时间。</p><p>3) OOM/executor lost —- shuffle file cannot find<br>提高堆外内存大小，提高堆内内存大小。</p><h2 id="2、reduce-OOM"><a href="#2、reduce-OOM" class="headerlink" title="2、reduce OOM"></a>2、reduce OOM</h2><p>BlockManager 拉取的数据量大，reduce task 处理的数据量小</p><p>解决方法：<br>1) 降低每次拉取的数据量<br>2) 提高 shuffle 聚合的内存比例<br>3) 提高 Executor 的内存比例</p><h2 id="3、序列化问题"><a href="#3、序列化问题" class="headerlink" title="3、序列化问题"></a>3、序列化问题</h2><h2 id="4、Null-值问题"><a href="#4、Null-值问题" class="headerlink" title="4、Null 值问题"></a>4、Null 值问题</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = rdd.map&#123;x=&gt;&#123;</span><br><span class="line">x+”~”;<span class="type">W</span></span><br><span class="line">&#125;&#125;</span><br><span class="line">rdd.foreach&#123;x=&gt;&#123;</span><br><span class="line"><span class="type">System</span>.out.println(x.getName())</span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 资源 </tag>
            
            <tag> 内存 </tag>
            
            <tag> 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习（六）</title>
      <link href="/2019/02/22/Spark%E5%AD%A6%E4%B9%A0%EF%BC%886%EF%BC%89/"/>
      <url>/2019/02/22/Spark%E5%AD%A6%E4%B9%A0%EF%BC%886%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、-SparkStreaming简介"><a href="#一、-SparkStreaming简介" class="headerlink" title="一、 SparkStreaming简介"></a>一、 SparkStreaming简介</h1><ul><li><p>SparkStreaming是流式处理框架，是Spark API的扩展，<code>支持</code>可扩展、高吞吐、容错的实时数据处理。</p></li><li><p>实时数据<code>来源</code>：kafka、Flume、Twitter、ZeroMQ、TCP Socket</p></li><li>可以使用高级功能的复杂算子来<code>处理</code>流数据：如 map、reduce、join、window</li><li>处理后的数据可以<code>存放</code>在文件系统、数据库等，方便实时展现</li></ul><h1 id="二、SparkStreaming-与-Storm-的区别"><a href="#二、SparkStreaming-与-Storm-的区别" class="headerlink" title="二、SparkStreaming 与 Storm 的区别"></a>二、SparkStreaming 与 Storm 的区别</h1><blockquote><ul><li><p>Storm 是纯实时的流式处理框架，SparkStreaming 是准实时的处理框架（微批处理）。因为微批处理，SparkStreaming 的吞吐量比 Storm 要高。</p></li><li><p>Storm 的事务机制要比 SparkStreaming 的要完善。</p></li><li><p>Storm 支持动态资源调度。(spark1.2 开始和之后也支持)</p></li><li><p>SparkStreaming 擅长复杂的业务处理，Storm 不擅长复杂的业务处理，擅长简单的汇总型计算</p></li></ul></blockquote><table><thead><tr><th style="text-align:center">SparkStreaming</th><th style="text-align:center">Storm</th></tr></thead><tbody><tr><td style="text-align:center">微批处理，准实时的流式处理框架</td><td style="text-align:center">实时计算框架，来一条数据马上处理</td></tr><tr><td style="text-align:center">支持动态调整资源</td><td style="text-align:center">支持动态调整资源</td></tr><tr><td style="text-align:center">支持事务</td><td style="text-align:center">支持事务</td></tr><tr><td style="text-align:center">支持复杂的业务场景</td><td style="text-align:center">处理场景相对简单一些</td></tr></tbody></table><h1 id="三、SparkStreaming的详情"><a href="#三、SparkStreaming的详情" class="headerlink" title="三、SparkStreaming的详情"></a>三、SparkStreaming的详情</h1><h2 id="1、运行流程"><a href="#1、运行流程" class="headerlink" title="1、运行流程"></a>1、运行流程</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0ff16n6p0j30rp08075q.jpg" alt=""></p><blockquote><p>SparkStreaming会启动receive task一直接受数据，每个batchInterval的时间周期，就会把数据变成一个batch，然后进一步封装成RDD，最后变成DStream ,用户操作DStream 时，可以使用一系列算子： map、flatmap、filter。。。。。</p></blockquote><blockquote><p><code>情况：</code></p><p>1、batchInterval为5s ，计算这批数据的时间为3s ，则此时 0—5s，在接收数据；5—10s，一边接收数据，一边处理上一批数据；依次类推。</p><p>2、batchInterval为5s ，计算这批数据的时间为6s ，则此时0—5s，在接收数据；5—10s，一边接收第二批数据，一边处理第一批数据；10—11s,一边接收第三批数据，一边处理第一批数据，第二批数据等待计算，就会造成<code>数据堆积</code>，如果SparkStreaming的数据存储是仅在内存中，就会发生OOM；如果设置StorageLevel 包含 disk, 则内存存放不下的数据会溢写至 disk, 加大延迟</p></blockquote><blockquote><p><code>注意；</code></p><ul><li>receiver task 是 7*24 小时一直在执行</li></ul></blockquote><h2 id="2、SparkStreaming-代码"><a href="#2、SparkStreaming-代码" class="headerlink" title="2、SparkStreaming 代码"></a>2、SparkStreaming 代码</h2><h3 id="（1）关于SparkStreaming-框架我们必须要知道的几点"><a href="#（1）关于SparkStreaming-框架我们必须要知道的几点" class="headerlink" title="（1）关于SparkStreaming 框架我们必须要知道的几点"></a>（1）关于SparkStreaming 框架我们必须要知道的几点</h3><blockquote><p><code>注意：</code></p><ul><li><p>receiver模式下接收数据，local的模拟线程必须大于等于2：</p><ul><li>一个线程用receiver的数据接收</li><li>一个线程用于执行job</li></ul></li><li><p><code>Duration</code>时间设置就是我们能接受的延迟度，需要根据集群的资源情况以及监控每一个job的执行时间来调节出最佳时间。</p></li><li><p>创建JavaStreamingContext有两种方式：SparkConf 、 SparkContext</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;   <span class="comment">//由conf 创建</span></span><br><span class="line">&gt;    <span class="keyword">final</span> JavaStreamingContext jsc = </span><br><span class="line">&gt;               <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line">&gt;   <span class="comment">//     JavaSparkContext  →   JavaStreamingContext  </span></span><br><span class="line">&gt;   JavaStreamingContext jsc = </span><br><span class="line">&gt;               <span class="keyword">new</span> JavaStreamingContext(sc,Durations.seconds(<span class="number">5</span>));  </span><br><span class="line">&gt;   <span class="comment">//    JavaStreamingContext    →    JavaSparkContext</span></span><br><span class="line">&gt;   <span class="keyword">final</span> JavaSparkContext sparkContext = jsc.sparkContext();</span><br><span class="line">&gt;   </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></li></ul></blockquote><blockquote><ul><li><p>所有的代码逻辑完成以后，必须要有一个ouput opertion类算子</p></li><li><p>JavaStreamingContext.start()   ，Streaming框架便启动，之后，就不能再次添加业务逻辑</p></li><li><p>JavaStreamingContext.stop()   ，无参的stop( )  会把SparkContext一同关闭；stop(false) , 只会关闭StreamingContext ,SparkContext依然存在</p></li><li><p>JavaStreamingContext.stop()停止之后不能再调用 start</p></li></ul></blockquote><h3 id="（2）代码举例：WordCount"><a href="#（2）代码举例：WordCount" class="headerlink" title="（2）代码举例：WordCount"></a>（2）代码举例：WordCount</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.Accumulator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.broadcast.Broadcast;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountOnline</span> </span>&#123;</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">     The master URL to connect to, </span></span><br><span class="line"><span class="comment">     such as "local" to run locally with one thread, </span></span><br><span class="line"><span class="comment">     "local[4]" to run locally with 4 cores, </span></span><br><span class="line"><span class="comment">      or "spark://master:7077" to run on a Spark standalone cluster.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"> <span class="keyword">final</span> SparkConf conf = </span><br><span class="line">               <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"WordCountOnline"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 在创建streamingContext的时候 设置batch Interval</span></span><br><span class="line"><span class="comment"> * 创建streamingContext有两种方式：conf， context</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//    final JavaStreamingContext jsc = </span></span><br><span class="line"><span class="comment">//             new JavaStreamingContext(conf, Durations.seconds(5));</span></span><br><span class="line"></span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line"><span class="comment">//创建StreamContext，及间隔时间：每个5秒处理数据        </span></span><br><span class="line">JavaStreamingContext jsc = </span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(sc,Durations.seconds(<span class="number">5</span>));         </span><br><span class="line"><span class="comment">//final JavaSparkContext sparkContext = jsc.sparkContext();</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//设置监听节点及端口，接收从这个节点的这个port输入的数据，最后封装成DStream</span></span><br><span class="line"><span class="comment">//避免端口被占用        </span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; lines = </span><br><span class="line">                                   jsc.socketTextStream(<span class="string">"node00"</span>, <span class="number">9999</span>);</span><br><span class="line"><span class="comment">//切割每一行数据</span></span><br><span class="line">        JavaDStream&lt;String&gt; words = </span><br><span class="line">            lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> Arrays.asList(s.split(<span class="string">" "</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; ones = </span><br><span class="line">            words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"><span class="comment">//给每个单词计为1</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//累加，并指定分区数</span></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; counts = </span><br><span class="line">            ones.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> i1 + i2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//output operator类的算子   </span></span><br><span class="line">        <span class="comment">// counts.print();</span></span><br><span class="line">counts.foreachRDD(<span class="keyword">new</span> VoidFunction&lt;JavaPairRDD&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(JavaPairRDD&lt;String, Integer&gt; pairRDD)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"=============="</span>);</span><br><span class="line">           pairRDD.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123;</span><br><span class="line">                    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple)</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                     System.out.println(<span class="string">"tuple ---- "</span>+tuple );</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//框架启动必须调用start</span></span><br><span class="line"> jsc.start();</span><br><span class="line"><span class="comment">//等待spark程序被终止</span></span><br><span class="line"> jsc.awaitTermination();</span><br><span class="line"><span class="comment">//这个期间可用于一些扫尾操作，如获取SparkContext，如果直接stop，就无法实现了        </span></span><br><span class="line">                </span><br><span class="line"><span class="comment">//任务执行结束</span></span><br><span class="line">        jsc.stop();</span><br><span class="line">        System.out.println(<span class="string">"stop====================="</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）代码运行"><a href="#（3）代码运行" class="headerlink" title="（3）代码运行"></a>（3）代码运行</h3><p>在Linux系统中：</p><ul><li>启动socket server 服务：node00</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install nc -y</span><br><span class="line">nc -lk 9999</span><br></pre></td></tr></table></figure><ul><li>在该节点上输入输入数据 （避免端口被占用）</li></ul><blockquote><p>在Windows端运行代码，便能接收到数据，从而执行运算处理</p></blockquote><h3 id="广播黑名单："><a href="#广播黑名单：" class="headerlink" title="广播黑名单："></a>广播黑名单：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.sxt.java.sparkstreaming;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.broadcast.Broadcast;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.google.common.base.Optional;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">// 过滤黑名单（使用广播变量）</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransformOperator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"transform"</span>);</span><br><span class="line">JavaStreamingContext jsc = </span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(conf,Durations.seconds(<span class="number">5</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//模拟黑名单</span></span><br><span class="line">List&lt;String&gt; blackList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">blackList.add(<span class="string">"zhangsan"</span>);</span><br><span class="line"><span class="comment">//广播黑名单</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;String&gt;&gt; broadcastList =</span><br><span class="line">            jsc.sparkContext().broadcast(blackList);</span><br><span class="line"></span><br><span class="line"><span class="comment">//接受socket数据源: 1 zhangsan     2  lisi       3   wangwu</span></span><br><span class="line">JavaReceiverInputDStream&lt;String&gt; nameList = </span><br><span class="line">            jsc.socketTextStream(<span class="string">"node01"</span>, <span class="number">7777</span>);</span><br><span class="line">        </span><br><span class="line">JavaPairDStream&lt;String, String&gt; pairNameList = </span><br><span class="line">            nameList.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(s.split(<span class="string">" "</span>)[<span class="number">1</span>], s);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">//对DStream使用transform算子，在算子内部实现RDD到RDD的转换</span></span><br><span class="line">JavaDStream&lt;String&gt; transFormResult = </span><br><span class="line">pairNameList.transform(<span class="keyword">new</span> Function&lt;JavaPairRDD&lt;String,String&gt;,JavaRDD&lt;String&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JavaRDD&lt;String&gt; <span class="title">call</span><span class="params">(JavaPairRDD&lt;String, String&gt; nameRDD)</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">JavaPairRDD&lt;String, String&gt; filter =</span><br><span class="line">nameRDD.filter(</span><br><span class="line">             <span class="keyword">new</span> Function&lt;Tuple2&lt;String,String&gt;, Boolean&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(Tuple2&lt;String, String&gt; v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//得到广播变量</span></span><br><span class="line">List&lt;String&gt; blackList = broadcastList.value();</span><br><span class="line"><span class="keyword">return</span> !blackList.contains(v1._1);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">return</span> filter.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">(Tuple2&lt;String, String&gt; v1)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> v1._2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">transFormResult.print();</span><br><span class="line">jsc.start();</span><br><span class="line">jsc.awaitTermination();</span><br><span class="line">jsc.stop();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="统计累计值："><a href="#统计累计值：" class="headerlink" title="统计累计值："></a>统计累计值：</h3><p>从程序启动，到当前 ， 所有批次数据的累加值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> com.google.common.base.Optional;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UpdateStateByKeyOperator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        SparkConf conf = <span class="keyword">new</span>  SparkConf()</span><br><span class="line">        conf.setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"UpdateStateByKeyDemo"</span>);</span><br><span class="line">        JavaStreamingContext jsc =</span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"><span class="comment">// 开启checkpoint   同时设置checkpoint文件路径</span></span><br><span class="line"><span class="comment">// jsc.checkpoint("hdfs://sukie/spark/checkpoint");</span></span><br><span class="line">        jsc.checkpoint(<span class="string">"./checkpoint"</span>);</span><br><span class="line">        </span><br><span class="line"><span class="comment">/* 数据格式：</span></span><br><span class="line"><span class="comment">   hello hh </span></span><br><span class="line"><span class="comment">   hello tt</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; lines = </span><br><span class="line">                        jsc.socketTextStream(<span class="string">"node00"</span>, <span class="number">9999</span>);</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; words = </span><br><span class="line">            lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">","</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; ones = </span><br><span class="line">            words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">//updateStateByKey  更新key值状态</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; counts =ones.updateStateByKey(</span><br><span class="line">            <span class="keyword">new</span> Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> Optional&lt;Integer&gt; <span class="title">call</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">              List&lt;Integer&gt; values, Optional&lt;Integer&gt; state)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        <span class="comment">/**</span></span><br><span class="line"><span class="comment">                         * values:经过分组最后 这个key所对应的values  [1,1,1,1,1]</span></span><br><span class="line"><span class="comment">                         * state:这个key在前一个批次的状态</span></span><br><span class="line"><span class="comment">                         */</span></span><br><span class="line">                        Integer updateValue = <span class="number">0</span>;</span><br><span class="line">                        <span class="keyword">if</span> (state.isPresent()) &#123;</span><br><span class="line">                            <span class="comment">//如果存在值，便获取</span></span><br><span class="line">                            updateValue = state.get();</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        System.out.println(updateValue + <span class="string">" ========  "</span>);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (Integer value : values) &#123;</span><br><span class="line">                            updateValue += value;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">return</span> Optional.of(updateValue);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//output operator</span></span><br><span class="line">        counts.print();</span><br><span class="line"><span class="comment">//        counts.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">// public void call(JavaPairRDD&lt;String, Integer&gt; pairRDD) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                System.out.println(pairRDD.getNumPartitions());</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                pairRDD.collect();</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;);</span></span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">        jsc.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><blockquote><p>Optional 类</p><ul><li>Java 8 引入的类</li><li>主要用于解决空指针异常的问题</li><li>从本质上说，这是一个包含有可选值的包装类，意味着Optional 类既可以包含有对象，也可以为空</li></ul></blockquote><h2 id="3、SparkStreaming算子操作"><a href="#3、SparkStreaming算子操作" class="headerlink" title="3、SparkStreaming算子操作"></a>3、SparkStreaming算子操作</h2><h3 id="1、ouput-opertion类算子"><a href="#1、ouput-opertion类算子" class="headerlink" title="1、ouput opertion类算子"></a>1、ouput opertion类算子</h3><ul><li><blockquote><p><strong>foreachRDD</strong></p><p>参数：RDD    返回值：无</p><ul><li>foreachRDD可以遍历得到DStream中的RDD</li><li><p>可以对RDD使用RDD的Transformation类算子进行转化</p></li><li><p>但是在这个算子内 <strong>必须对抽取出来的RDD执行Action类算子</strong>，代码才能执行</p></li><li>在这个算子<strong>内</strong>，RDD算子<strong>外</strong>执行的代码是在Driver端执行，RDD算子<strong>内</strong>的代码是在Executor中执行。</li></ul><p><strong>print</strong></p><p>参数：无        返回值：无</p><p>直接将DStream中的数据输出打印</p></blockquote></li></ul><h3 id="2、transformation类算子"><a href="#2、transformation类算子" class="headerlink" title="2、transformation类算子"></a>2、transformation类算子</h3><ul><li><blockquote><p><strong>transform</strong></p><p>参数：RDD   返回：另一RDD</p><p>transform算子可将DStream做RDD到RDD的任意操作</p><ul><li>在这个算子<strong>内</strong>，RDD算子<strong>外</strong>执行的代码是在Driver端执行，RDD算子<strong>内</strong>的代码是在Executor中执行。</li></ul><p><strong>updateStateByKey</strong></p><ul><li>此算子为SparkStreaming中每一个key维护一个state，state可以是任意类型，也可以是自定义对象，更新函数也可以是自定义</li><li>与reduceByKey相似的地方就是会先按key进行分组</li><li>通过更新函数对该 key 的状态不断更新，对于每个新的 batch 而言，SparkStreaming 会在使用 updateStateByKey 的时候为已经存在的 key 进行 state 的状态更新。</li><li><p>如果要不断的更新每个key的state，就一定涉及到了状态的保存和容错，这个时候就需要开启checkpoint机制和功能</p></li><li><p>有何用？全面的广告点击分析，统计广告点击流量，统计这一天的车流量，统计点击量</p></li></ul></blockquote></li></ul><h3 id="3、注意"><a href="#3、注意" class="headerlink" title="3、注意"></a>3、注意</h3><ul><li><h4 id="使用到-updateStateByKey-要开启-checkpoint-机制和功能。"><a href="#使用到-updateStateByKey-要开启-checkpoint-机制和功能。" class="headerlink" title="使用到 updateStateByKey 要开启 checkpoint 机制和功能。"></a>使用到 updateStateByKey 要开启 checkpoint 机制和功能。</h4></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//设置checkpoint目录: </span><br><span class="line">// 落地到本地磁盘</span><br><span class="line">  jsc.checkpoint(&quot;./checkpoint&quot;);</span><br><span class="line">//保存在hdfs</span><br><span class="line">  jsc.checkpoint(&quot;hdfs://shsxt/spark/checkpoint&quot;);</span><br></pre></td></tr></table></figure><ul><li><h4 id="多久会将内存中的数据-每一个key所对应的状态-写入到磁盘一份？"><a href="#多久会将内存中的数据-每一个key所对应的状态-写入到磁盘一份？" class="headerlink" title="多久会将内存中的数据(每一个key所对应的状态)写入到磁盘一份？"></a>多久会将内存中的数据(每一个key所对应的状态)写入到磁盘一份？</h4><blockquote><p>如果batchInterval设置的时间小于10秒，那么10秒写入磁盘一份。</p><p>如果 batchInterval 设置的时间大于 10 秒，那么就会 batchInterval时间间隔写入磁盘一份。</p><p>这样做是为了防止频繁的写HDFS</p></blockquote></li></ul><h2 id="4、窗口操作"><a href="#4、窗口操作" class="headerlink" title="4、窗口操作"></a>4、窗口操作</h2><h4 id="1-窗口操作理解图："><a href="#1-窗口操作理解图：" class="headerlink" title="(1)窗口操作理解图："></a>(1)窗口操作理解图：</h4><p><img src="http://spark.apache.org/docs/latest/img/streaming-dstream-window.png" alt=""></p><p>假设每隔 5s 1 个 batch,上图中窗口长度为 15s，窗口滑动间隔 10s。</p><ul><li>窗口长度和滑动间隔必须是 batchInterval 的整数倍。如果不是整数倍会检测报错。</li></ul><blockquote><p>用于计算最近一段时间的数据</p></blockquote><h4 id="2-优化后的-window-窗口操作示意图："><a href="#2-优化后的-window-窗口操作示意图：" class="headerlink" title="(2)优化后的 window 窗口操作示意图："></a>(2)优化后的 window 窗口操作示意图：</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0fh4r65nej30g507b0vo.jpg" alt=""></p><ul><li>优化后的 window 操作要保存状态所以要设置 checkpoint 路径，没有优化的 window 操作可以不设置 checkpoint 路径</li></ul><h4 id="3-代码实现-："><a href="#3-代码实现-：" class="headerlink" title="(3)代码实现 ："></a>(3)代码实现 ：</h4><p>​     searchWordPairDStream.reduceByKeyAndWindow(function1 ， function2 ， 窗口时间 ， 滑块时间）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="comment">//基于滑动窗口的热点搜索词实时统计</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WindowOperator</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">conf.setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"WindowHotWord"</span>); </span><br><span class="line"></span><br><span class="line">JavaStreamingContext jssc = </span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"><span class="comment">//设置日志级别为WARN</span></span><br><span class="line">jssc.sparkContext().setLogLevel(<span class="string">"WARN"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 注意：</span></span><br><span class="line"><span class="comment"> *  没有优化的窗口函数可以不设置checkpoint目录</span></span><br><span class="line"><span class="comment"> *  优化的窗口函数必须设置checkpoint目录 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//      jssc.checkpoint("hdfs://node1:9000/spark/checkpoint");</span></span><br><span class="line">   jssc.checkpoint(<span class="string">"./checkpoint"</span>);</span><br><span class="line">JavaReceiverInputDStream&lt;String&gt; searchLogsDStream =</span><br><span class="line">                            jssc.socketTextStream(<span class="string">"node00"</span>, <span class="number">9999</span>);</span><br><span class="line"><span class="comment">//word1</span></span><br><span class="line">JavaDStream&lt;String&gt; searchWordsDStream = </span><br><span class="line">            searchLogsDStream.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(String t)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(t + <span class="string">"*************"</span>);</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(t.split(<span class="string">" "</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将搜索词映射为(searchWord, 1)的tuple格式</span></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; searchWordPairDStream = </span><br><span class="line">            searchWordsDStream.mapToPair(</span><br><span class="line"><span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String searchWord)</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(searchWord, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 每隔10秒，计算最近60秒内的数据，</span></span><br><span class="line"><span class="comment"> * 那么这个窗口大小就是60秒，里面有12个rdd，在没有计算之前，这些rdd是不会进行计算的。</span></span><br><span class="line"><span class="comment"> * 那么在计算的时候会将这12个rdd聚合起来，然后一起执行reduceByKeyAndWindow操作 ，</span></span><br><span class="line"><span class="comment"> * reduceByKeyAndWindow是针对窗口操作的而不是针对DStream操作的。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">// JavaPairDStream&lt;String, Integer&gt; searchWordCountsDStream =</span></span><br><span class="line"><span class="comment">//searchWordPairDStream.reduceByKeyAndWindow(</span></span><br><span class="line"><span class="comment">//                 new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//private static final long serialVersionUID = 1L;</span></span><br><span class="line"><span class="comment">//@Override</span></span><br><span class="line"><span class="comment">//public Integer call(Integer v1, Integer v2) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//return v1 + v2;</span></span><br><span class="line"><span class="comment">//&#125;</span></span><br><span class="line"><span class="comment">//&#125;, Durations.minutes(30), Durations.seconds(60));</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        JavaPairDStream&lt;String, Integer&gt; searchWordCountsDStream =</span></span><br><span class="line"><span class="comment">//            searchWordPairDStream.reduceByKeyAndWindow(</span></span><br><span class="line"><span class="comment">//        new Function2&lt;Integer, Integer, Integer&gt;() &#123;</span></span><br><span class="line"><span class="comment">//                @Override</span></span><br><span class="line"><span class="comment">//                public Integer call(Integer v1, Integer v2) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                    System.out.println(v1 + " : " + v2);</span></span><br><span class="line"><span class="comment">//                    return v1 + v2;</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//            &#125;,Durations.seconds(15),Durations.seconds(5));</span></span><br><span class="line">               <span class="comment">// 窗口时间               滑块时间     </span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//window窗口操作优化：</span></span><br><span class="line">        <span class="comment">//将划出串窗口的数据排除，将新划入窗口的数据添加</span></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; searchWordCountsDStream =</span><br><span class="line">          searchWordPairDStream.reduceByKeyAndWindow(</span><br><span class="line">            <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"v1:"</span> + v1 + <span class="string">" v2:"</span> + v2 + <span class="string">"  ++++++++++"</span>);</span><br><span class="line">                    <span class="keyword">return</span> v1 + v2;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                    System.out.println(<span class="string">"v1:"</span> + v1 + <span class="string">" v2:"</span> + v2 + <span class="string">"------------"</span>);</span><br><span class="line">                    <span class="keyword">return</span> v1 - v2;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;, Durations.seconds(<span class="number">15</span>), Durations.seconds(<span class="number">5</span>));</span><br><span class="line">              <span class="comment">//窗口时间                //滑块时间</span></span><br><span class="line">  searchWordCountsDStream.print();</span><br><span class="line"></span><br><span class="line">jssc.start();</span><br><span class="line">jssc.awaitTermination();</span><br><span class="line">jssc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>reduceByKeyAndWindow</li></ul><blockquote><p>reduceByKeyAndWindow是针对窗口操作的而不是针对DStream操作的。且必须是（K,V）格式</p></blockquote><h2 id="5-Driver-HA（Standalone-或者-Mesos）"><a href="#5-Driver-HA（Standalone-或者-Mesos）" class="headerlink" title="5. Driver HA（Standalone 或者 Mesos）"></a>5. Driver HA（Standalone 或者 Mesos）</h2><p>代码举例：</p><h3 id="产生文件："><a href="#产生文件：" class="headerlink" title="产生文件："></a>产生文件：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 此复制文件的程序是模拟在data目录下动态生成相同格式的txt文件，用于给sparkstreaming 中 textFileStream提供输入流。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CopyFile_data</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">String uuid = UUID.randomUUID().toString();</span><br><span class="line">System.out.println(uuid);</span><br><span class="line">copyFile(</span><br><span class="line">       <span class="keyword">new</span> File(<span class="string">"./data/scores.txt"</span>),<span class="keyword">new</span> File(<span class="string">"./dataTest/"</span>+uuid+<span class="string">"----words.txt"</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">copyFile</span><span class="params">(File fromFile, File toFile)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">FileInputStream ins = <span class="keyword">new</span> FileInputStream(fromFile);</span><br><span class="line">FileOutputStream out = <span class="keyword">new</span> FileOutputStream(toFile);</span><br><span class="line"><span class="keyword">byte</span>[] b = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>*<span class="number">1024</span>];</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unused"</span>)</span><br><span class="line"><span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> ((n = ins.read(b)) != -<span class="number">1</span>) &#123;</span><br><span class="line">out.write(b, <span class="number">0</span>, b.length);</span><br><span class="line">&#125;</span><br><span class="line">ins.close();</span><br><span class="line">out.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="处理数据："><a href="#处理数据：" class="headerlink" title="处理数据："></a>处理数据：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContextFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.DStream;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  Spark standalone or Mesos with cluster deploy mode only:</span></span><br><span class="line"><span class="comment"> *  在提交application的时候  添加 --supervise 选项  如果Driver挂掉 会自动启动一个Driver</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingOnHDFS</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> SparkConf conf =  </span><br><span class="line">     <span class="keyword">new</span>  SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SparkStreamingOnHDFS"</span>);   </span><br><span class="line"></span><br><span class="line"><span class="comment">//      final String checkpointDirectory =</span></span><br><span class="line"><span class="comment">//        "hdfs://shsxt/spark/SparkStreaming/CheckPoint2017";</span></span><br><span class="line"><span class="keyword">final</span> String checkpointDirectory = <span class="string">"./checkpoint"</span>;</span><br><span class="line"></span><br><span class="line">        JavaStreamingContextFactory factory = <span class="keyword">new</span> JavaStreamingContextFactory() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JavaStreamingContext <span class="title">create</span><span class="params">()</span> </span>&#123;  </span><br><span class="line"><span class="keyword">return</span> createContext(checkpointDirectory,conf);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line">        </span><br><span class="line"><span class="comment">//getOrCreate() 该方法会先到checkpointDirectory的文件中检查是否有checkpoint记录，如果没有就会让 factory 去调用 create() 来创建JavaStreamingContext ；如果存在就执行checkpoint的任务</span></span><br><span class="line">JavaStreamingContext jsc = JavaStreamingContext.getOrCreate(checkpointDirectory, factory);</span><br><span class="line">        </span><br><span class="line">jsc.start();</span><br><span class="line">jsc.awaitTermination();</span><br><span class="line">jsc.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings</span>(<span class="string">"deprecation"</span>)</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> JavaStreamingContext </span><br><span class="line">        createContext(String checkpointDirectory,SparkConf conf) &#123;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// If you do not see this printed, that means the StreamingContext has been loaded</span></span><br><span class="line"><span class="comment">// from the new checkpoint</span></span><br><span class="line">System.out.println(<span class="string">"Creating new context"</span>);</span><br><span class="line">SparkConf sparkConf = conf;</span><br><span class="line"><span class="comment">// Create the context with a 1 second batch size</span></span><br><span class="line"></span><br><span class="line">JavaStreamingContext ssc = </span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(sparkConf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"><span class="comment">//ssc.sparkContext().setLogLevel("WARN");</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  checkpoint 保存：</span></span><br><span class="line"><span class="comment"> *1.配置信息</span></span><br><span class="line"><span class="comment"> *2.DStream操作逻辑</span></span><br><span class="line"><span class="comment"> *3.job的执行进度</span></span><br><span class="line"><span class="comment"> *      4.offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">ssc.checkpoint(checkpointDirectory);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 监控的是HDFS上的一个目录，监控文件数量的变化     文件内容如果追加监控不到。</span></span><br><span class="line"><span class="comment"> * 只监控文件夹下新增的文件，减少的文件时监控不到的，文件的内容有改动也监控不到。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//JavaDStream&lt;String&gt; lines = </span></span><br><span class="line"><span class="comment">//        ssc.textFileStream("hdfs://node1:9000/spark/sparkstreaming");</span></span><br><span class="line">JavaDStream&lt;String&gt; lines = ssc.textFileStream(<span class="string">"./dataTest"</span>);</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; words = </span><br><span class="line">            lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> Arrays.asList(s.split(<span class="string">" "</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; ones = </span><br><span class="line">            words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s.trim(), <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; counts = </span><br><span class="line">            ones.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">                </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> i1 + i2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        counts.print();</span></span><br><span class="line"></span><br><span class="line">        DStream&lt;Tuple2&lt;String, Integer&gt;&gt; dstream = counts.dstream();</span><br><span class="line"></span><br><span class="line">        dstream.saveAsTextFiles(<span class="string">"./data/write/xxxxx"</span>,<span class="string">"yyyyyy"</span>);</span><br><span class="line"><span class="keyword">return</span> ssc;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong><code>注意</code></strong></p><ul><li><p>因为 SparkStreaming 是 7*24 小时运行，Driver 只是一个简单的进程，有可能挂掉，所以实现 Driver 的 HA 就有必要</p></li><li><p>如果使用的 Client 模式就无法实现 Driver HA ，这里针对的是 cluster 模式</p></li><li><p>Yarn 平台 cluster 模式提交任务，AM(AplicationMaster)相当于 Driver，如果挂掉会自动启动 AM</p></li><li><p>这里所说的 DriverHA 针对的是 Spark standalone 和 Mesos 资源调度的情况下</p></li></ul><h3 id="实现-Driver-的高可用有两个步骤："><a href="#实现-Driver-的高可用有两个步骤：" class="headerlink" title="实现 Driver 的高可用有两个步骤："></a>实现 Driver 的高可用有两个<strong><code>步骤</code></strong>：</h3><ul><li><p>第一：提交任务层面，在提交任务的时候加上选项 –supervise,当 Driver挂掉的时候会自动重启 Driver。</p></li><li><p>第二：代码层面，使用 JavaStreamingContext.getOrCreate（checkpoint路径，JavaStreamingContextFactory）</p></li><li>Driver 中元数据包括：<ol><li>创建应用程序的配置信息。</li><li>DStream 的操作逻辑</li><li>job 中没有完成的批次数据，也就是 job 的执行进度。</li></ol></li></ul><h2 id="6、SparkStreaming-Kafka"><a href="#6、SparkStreaming-Kafka" class="headerlink" title="6、SparkStreaming+Kafka"></a>6、SparkStreaming+Kafka</h2><h3 id="（1）receiver-模式"><a href="#（1）receiver-模式" class="headerlink" title="（1）receiver 模式"></a>（1）receiver 模式</h3><h4 id="receiver-模式原理图"><a href="#receiver-模式原理图" class="headerlink" title="receiver 模式原理图"></a>receiver 模式原理图</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0g5e9n7b7j315j0ywn1a.jpg" alt=""></p><h4 id="receiver-模式理解："><a href="#receiver-模式理解：" class="headerlink" title="receiver 模式理解："></a>receiver 模式理解：</h4><blockquote><p>1.在 SparkStreaming 程序运行起来后，Executor 中会启动 receivertasks 来接收 kafka 推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。</p><p>2.receiver task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。</p><p>3.备份完成后去 zookeeper 中更新消费偏移量，然后向 Driver 中的 receiver tracker 汇报数据的位置。最后 Driver 根据数据本地化将 task 分发到不同节点上执行</p></blockquote><blockquote><p> 数据本地化原则：将task分配到data所在节点</p></blockquote><h4 id="receiver-模式中存在的问题："><a href="#receiver-模式中存在的问题：" class="headerlink" title="receiver 模式中存在的问题："></a>receiver 模式中存在的问题：</h4><p><strong><em>场景一：</em></strong></p><blockquote><p>1、当 Driver 进程挂掉后，Driver 下的 Executor 都会被杀掉，当更新完 zookeeper 消费偏移量后，Driver 如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。如何解决这个问题？</p><p>2、开启<strong><code>WAL</code></strong>(write ahead log)<em>预写日志机制</em>， 在接受过来数据备份到其他节点的时候，同时备份到 HDFS 上一份（我们需要将接收来的数据的持久化级别降级到 MEMORY_AND_DISK），这样就能保证数据的安全性。</p><p>3、不过，因为写 HDFS 比较消耗性能，要在备份完数据之后才能进行更新 zookeeper 以及汇报位置等，这样会增加 job 的执行时间，这样对于任务的执行提高了延迟度</p></blockquote><p><strong><em>场景二：</em></strong></p><blockquote><p>开启了WAL机制</p><p>若数据接收完后（50~100），也将数据备份到另一节点和HDFS上，正准备更新偏移量（100）的时候，driver挂掉了，重启后，就会到HDFS上去获取未计算的数据，然后，检查偏移量（50），再根据偏移量去消费topic。这就出现了重复消费的现象。kafka的偏移量的更新是异步提交的</p></blockquote><p><strong><code>术语解释：</code></strong></p><blockquote><p>SparkStreaming的receive模式能保证 at least模型，只能保证至少消费一次，不能保证仅被消费一次</p><p>exactly-once模型   能保证仅被消费一次，较理想的模型可以避免重复消费，direct模式可以实现，但是输出不能保证</p></blockquote><h4 id="receiver-的并行度设置："><a href="#receiver-的并行度设置：" class="headerlink" title="receiver 的并行度设置："></a>receiver 的并行度设置：</h4><blockquote><p>1、receiver 的并行度是由 spark.streaming.blockInterval 来决定的，默认为200ms,</p><p>2、假设 batchInterval 为 5s,那么每隔 blockInterval 就会产生一个 block,这里就对应每批次产生 RDD 的 partition,这样 5 秒产生的这个 Dstream 中的这个 RDD 的 partition 为 25 个，并行度就是25。</p><p>3、如果想提高并行度，可以减少 blockInterval 的数值，但是最好不要低于 50ms。</p></blockquote><h4 id="receiver-模式代码："><a href="#receiver-模式代码：" class="headerlink" title="receiver 模式代码："></a>receiver 模式代码：</h4><h5 id="产生数据："><a href="#产生数据：" class="headerlink" title="产生数据："></a>产生数据：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> kafka.javaapi.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> kafka.producer.KeyedMessage;</span><br><span class="line"><span class="keyword">import</span> kafka.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> kafka.serializer.StringEncoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="comment">//向kafka中生产数据</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProducer</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">  <span class="comment">// sparkstreaming  storm  flink 两三年后变成主流  流式处理，可能更复杂，数据处理性能要非常好</span></span><br><span class="line">    <span class="keyword">private</span> String topic; <span class="comment">//发送给Kafka的数据,topic</span></span><br><span class="line">    <span class="keyword">private</span> Producer&lt;Integer, String&gt; producerForKafka;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyProducer</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">        Properties conf = <span class="keyword">new</span> Properties();</span><br><span class="line">        conf.put(<span class="string">"metadata.broker.list"</span>, <span class="string">"node01:9092,node02:9092,node03:9092"</span>);</span><br><span class="line">        conf.put(<span class="string">"serializer.class"</span>, StringEncoder.class.getName());</span><br><span class="line">        conf.put(<span class="string">"acks"</span>,<span class="number">1</span>);</span><br><span class="line">        producerForKafka = <span class="keyword">new</span> Producer&lt;Integer, String&gt;(<span class="keyword">new</span> ProducerConfig(conf));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> counter = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          counter++;</span><br><span class="line">         <span class="comment">// String value = "tt" + counter;</span></span><br><span class="line">            String value = <span class="string">"tt"</span></span><br><span class="line">          KeyedMessage&lt;Integer, String&gt; message = <span class="keyword">new</span> KeyedMessage&lt;&gt;(topic, value);</span><br><span class="line"></span><br><span class="line">            producerForKafka.send(message);</span><br><span class="line">            System.out.println(value + <span class="string">" - -- -- --- -- - -- - -"</span>);</span><br><span class="line"><span class="comment">//  hash partitioner 当有key时，则默认通过key 取hash后 ，对partition_number 取余数</span></span><br><span class="line"><span class="comment">//producerForKafka.send(new KeyedMessage&lt;Integer, String&gt;(topic,22,userLog));</span></span><br><span class="line"><span class="comment">//            每2条数据暂停2秒</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="number">0</span> == counter % <span class="number">2</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> MyProducer(<span class="string">"sk1"</span>).start();</span><br><span class="line">        <span class="keyword">new</span> MyProducer(<span class="string">"sk2"</span>).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="操作："><a href="#操作：" class="headerlink" title="操作："></a>操作：</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> kafka.serializer.StringDecoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.StorageLevel;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="comment">//receiver 模式并行度是由blockInterval决定的</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingOnKafkaReceiver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">         conf.setAppName(<span class="string">"SparkStreamingOnKafkaReceiver"</span>).setMaster(<span class="string">"local[2]"</span>);</span><br><span class="line">        <span class="comment">//开启预写日志 WAL机制</span></span><br><span class="line">        conf.set(<span class="string">"spark.streaming.receiver.writeAheadLog.enable"</span>, <span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">        JavaStreamingContext jsc = </span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">10</span>));</span><br><span class="line">        jsc.checkpoint(<span class="string">"./receivedata"</span>);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, Integer&gt; topicConsumerConcurrency = </span><br><span class="line">            <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line">       <span class="comment">//设置读取的topic和接受数据的线程数</span></span><br><span class="line">        topicConsumerConcurrency.put(<span class="string">"sk1"</span>, <span class="number">1</span>);</span><br><span class="line">        topicConsumerConcurrency.put(<span class="string">"sk2"</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 第一个参数是StreamingContext</span></span><br><span class="line"><span class="comment">         * 第二个参数是ZooKeeper集群信息</span></span><br><span class="line"><span class="comment">         （接受Kafka数据的时候会从Zookeeper中获得Offset等元数据信息）</span></span><br><span class="line"><span class="comment">         * 第三个参数是Consumer Group 消费者组</span></span><br><span class="line"><span class="comment">         * 第四个参数是消费的Topic以及并发读取Topic中Partition的线程数</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 注意：</span></span><br><span class="line"><span class="comment">         * KafkaUtils.createStream 使用五个参数的方法，设置receiver的存储级别</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"><span class="comment">//JavaPairReceiverInputDStream&lt;String,String&gt; lines = KafkaUtils.createStream(</span></span><br><span class="line"><span class="comment">//jsc,</span></span><br><span class="line"><span class="comment">//"node3:2181,node4:2181,node5:2181",</span></span><br><span class="line"><span class="comment">//"MyFirstConsumerGroup", </span></span><br><span class="line"><span class="comment">//topicConsumerConcurrency,</span></span><br><span class="line"><span class="comment">//StorageLevel.MEMORY_AND_DISK());</span></span><br><span class="line"></span><br><span class="line">        JavaPairReceiverInputDStream&lt;String, String&gt; lines =</span><br><span class="line">            KafkaUtils.createStream(</span><br><span class="line">                jsc,</span><br><span class="line">                <span class="string">"node01:2181,node02:2181,node03:2181"</span>,</span><br><span class="line">                <span class="string">"MyFirstConsumerGroup"</span>,</span><br><span class="line">                topicConsumerConcurrency);</span><br><span class="line"></span><br><span class="line">        JavaDStream&lt;String&gt; words = </span><br><span class="line">              lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;Tuple2&lt;String, String&gt;, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">              <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;String, String&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(tuple._2.split(<span class="string">"\t"</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; pairs = </span><br><span class="line">            words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairDStream&lt;String, Integer&gt; wordsCount = </span><br><span class="line">            pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="comment">//对相同的Key，进行Value的累计（包括Local和Reducer级别同时Reduce）</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        wordsCount.print();</span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">        jsc.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="运行代码："><a href="#运行代码：" class="headerlink" title="运行代码："></a>运行代码：</h5><p>Linux端</p><ul><li>启动zookeeper：3台</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><ul><li>启动kafka：3台</li></ul><p>在kafka的解压路径下的/bin目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh ../config/server.properties</span><br></pre></td></tr></table></figure><p>终止kafka：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh</span><br></pre></td></tr></table></figure><h3 id="（2）Driect-模式"><a href="#（2）Driect-模式" class="headerlink" title="（2）Driect 模式"></a>（2）Driect 模式</h3><h4 id="Driect-模式原理图"><a href="#Driect-模式原理图" class="headerlink" title="Driect 模式原理图"></a>Driect 模式原理图</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0hviwdqgkj30my0ba3z2.jpg" alt=""></p><h4 id="Direct-模式理解："><a href="#Direct-模式理解：" class="headerlink" title="Direct 模式理解："></a>Direct 模式理解：</h4><blockquote><ul><li><p>SparkStreaming+kafka 的 Driect 模式就是将 kafka 看成存数据的一方，SparkStreaming不是被动接收数据，而是主动去拉取数据。拉取数据后先进行计算，成功后再更新偏移量。</p></li><li><p>消费者偏移量也不是用 zookeeper 来管理，而是 SparkStreaming 内部对消费者偏移量自动来维护，默认消费偏移量是在内存中，当然如果设置了checkpoint 目录，那么消费偏移量也会保存在 checkpoint 中。当然也可以实现用 zookeeper 来管理</p></li><li>能保证消费exactly-once  仅且消费一次 ， 但是， 不能保证计算过程中的数据库操作仅一次  ， 可以借助幂等 ， 或悲观锁 来解决。 </li></ul></blockquote><h4 id="Direct-模式并行度设置："><a href="#Direct-模式并行度设置：" class="headerlink" title="Direct 模式并行度设置："></a>Direct 模式并行度设置：</h4><blockquote><p>Direct 模式的并行度是由读取的 kafka 中 topic 的 partition 数决定的。</p><p>可以在sparksteaming中使用算子改变分区数，如reartition</p></blockquote><h4 id="Direct-模式代码："><a href="#Direct-模式代码：" class="headerlink" title="Direct 模式代码："></a>Direct 模式代码：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> kafka.serializer.DefaultEncoder;</span><br><span class="line"><span class="keyword">import</span> kafka.serializer.StringDecoder;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Durations;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.KafkaUtils;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 并行度:</span></span><br><span class="line"><span class="comment"> * 1、linesDStram里面封装到的是RDD， RDD里面有partition与读取topic的parititon数是一致的。</span></span><br><span class="line"><span class="comment"> * 2、从kafka中读来的数据封装一个DStram里面，可以对这个DStream重分区                 </span></span><br><span class="line"><span class="comment"> *     reaprtitions(numpartition)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingOnKafkaDirected</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf =  <span class="keyword">new</span> SparkConf()</span><br><span class="line">            .setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SparkStreamingOnKafkaDirected"</span>);</span><br><span class="line">        <span class="comment">//设置反压机制</span></span><br><span class="line"><span class="comment">//conf.set("spark.streaming.backpressure.enabled", "false");</span></span><br><span class="line">        <span class="comment">//设置每一个分区最大执行速度</span></span><br><span class="line"><span class="comment">//conf.set("spark.streaming.kafka.maxRatePerPartition", "100");</span></span><br><span class="line">        <span class="comment">//是否优雅的关闭SparkStreaming  </span></span><br><span class="line">        conf.set(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>,<span class="string">"true"</span>);</span><br><span class="line">JavaStreamingContext jsc = </span><br><span class="line">            <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">5</span>));</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 可以不设置checkpoint 不设置保存offset,offset默认在内存中有一份，</span></span><br><span class="line"><span class="comment">   如果设置checkpoint，在checkpoint也有一份offset， 一般要设置。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">jsc.checkpoint(<span class="string">"./checkpoint"</span>);</span><br><span class="line">Map&lt;String, String&gt; kafkaParameters = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">kafkaParameters.put(</span><br><span class="line">            <span class="string">"metadata.broker.list"</span>, <span class="string">"node01:9092,node02:9092,node03:9092"</span>);</span><br><span class="line"><span class="comment">//kafkaParameters.put("auto.offset.reset", "smallest");</span></span><br><span class="line"></span><br><span class="line">HashSet&lt;String&gt; topics = <span class="keyword">new</span> HashSet&lt;String&gt;();</span><br><span class="line">topics.add(<span class="string">"sk1"</span>);</span><br><span class="line">        topics.add(<span class="string">"sk2"</span>);</span><br><span class="line"></span><br><span class="line">JavaPairInputDStream&lt;String,String&gt; lines =</span><br><span class="line">                      KafkaUtils.createDirectStream(jsc,</span><br><span class="line">                                    String.class,  </span><br><span class="line">                                    String.class,</span><br><span class="line">                                    StringDecoder.class,</span><br><span class="line">                                    StringDecoder.class,</span><br><span class="line">                                                    kafkaParameters,</span><br><span class="line">                                                    topics);</span><br><span class="line"></span><br><span class="line">JavaDStream&lt;String&gt; words = lines.flatMap(</span><br><span class="line">            <span class="keyword">new</span> FlatMapFunction&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123; </span><br><span class="line">                <span class="comment">//如果是Scala，由于SAM转换，</span></span><br><span class="line">                <span class="comment">//所以可以写成val words = lines.flatMap &#123; line =&gt; line.split(" ")&#125;</span></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;String,String&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> Arrays.asList(tuple._2.split(<span class="string">"\t"</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(</span><br><span class="line">            <span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(word, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">JavaPairDStream&lt;String, Integer&gt; wordsCount = </span><br><span class="line">            pairs.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123; </span><br><span class="line">            <span class="comment">//对相同的Key，进行Value的累计（包括Local和Reducer级别同时Reduce）</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">return</span> v1 + v2;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">wordsCount.print();</span><br><span class="line">jsc.start();</span><br><span class="line">jsc.awaitTermination();</span><br><span class="line">jsc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="7、相关配置"><a href="#7、相关配置" class="headerlink" title="7、相关配置"></a>7、相关配置</h2><p>预写日志:（receive模式中）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.receiver.writeAheadLog.enable 默认 false 没有开启</span><br></pre></td></tr></table></figure><p>blockInterval:（receive模式中）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.blockInterval 默认 200ms</span><br></pre></td></tr></table></figure><p>反压机制: （设置自动调整每一批次的数据量的理想范围，但调整的比较慢）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.backpressure.enabled 默认 false</span><br></pre></td></tr></table></figure><p>每一批次接收数据速率:（receive模式中）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.receiver.maxRate 默认没有设置</span><br></pre></td></tr></table></figure><p>每一分区接收数据速率 :（  direct模式）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.kafka.maxRatePerpartition   默认没有设置</span><br></pre></td></tr></table></figure><h2 id="8、如何优雅的关闭Spark-Streaming作业"><a href="#8、如何优雅的关闭Spark-Streaming作业" class="headerlink" title="8、如何优雅的关闭Spark Streaming作业"></a>8、如何优雅的关闭Spark Streaming作业</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.streaming.stopGracefullyOnShutdown  设置成true</span><br></pre></td></tr></table></figure><p>执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -15/sigterm driverpid</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark框架 </tag>
            
            <tag> SparkStreaming </tag>
            
            <tag> 流式处理框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习（五）</title>
      <link href="/2019/02/21/Spark%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89/"/>
      <url>/2019/02/21/Spark%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、Shark"><a href="#一、Shark" class="headerlink" title="一、Shark"></a>一、Shark</h1><h2 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h2><p>基于 Spark 计算框架之上且兼容 Hive 语法的 SQL 执行引擎，</p><h2 id="2、特点"><a href="#2、特点" class="headerlink" title="2、特点"></a>2、特点</h2><ul><li>基于 Spark 的特性</li></ul><p>由于底层的计算采用了 Spark，性能比 MapReduce 的 Hive 普遍快 2 倍以上，当数据全部 load 在内存的话，将快 10 倍以上，因此 Shark 可以作为交互式查询应用服务来使用。</p><ul><li>基于 Hive的特性</li></ul><p>Shark 是完全兼容 Hive的语法，表结构以及UDF函数等，已有的HiveSql可以直接进行迁移至Shark上。 Shark 底层依赖于 Hive 的解析器，查询优化器。</p><ul><li>缺点</li></ul><p>由于 Shark 的整体设计架构对 Hive 的依赖性太强，难以支持其长远发展，比如不能和 Spark的其他组件进行很好的集成，无法满足 Spark 的一栈式解决大数据处理的需求。</p><h1 id="二、SparkSql"><a href="#二、SparkSql" class="headerlink" title="二、SparkSql"></a>二、SparkSql</h1><h2 id="1、SparkSQL介绍"><a href="#1、SparkSQL介绍" class="headerlink" title="1、SparkSQL介绍"></a>1、SparkSQL介绍</h2><p>Hive 是 Shark 的前身，Shark 是 SparkSQL 的前身。</p><p>SparkSQL 特点：</p><ul><li><p>其完全脱离了 Hive 的限制。</p></li><li><p>SparkSQL支持查询原生的RDD。</p><p>RDD是Spark平台的核心概念，是 Spark 能够高效的处理大数据的各种场景的基础。</p></li><li><p>能够在 Scala 中写 SQL 语句。</p></li></ul><p>支持简单的 SQL 语法检查，能够在Scala中写Hive语句访问Hive数据，并将结果取回作为RDD使用。</p><h2 id="2、Spark-on-Hive-和-Hive-on-Spark"><a href="#2、Spark-on-Hive-和-Hive-on-Spark" class="headerlink" title="2、Spark on Hive 和 Hive on Spark"></a>2、Spark on Hive 和 Hive on Spark</h2><p><strong><code>Spark on Hive</code></strong>：</p><p> Hive 只作为储存角色，Spark 负责 sql 解析优化，执行。</p><p>数据源在hive上，解析引擎是sparksql，执行任务是spark。</p><p><strong><code>Hive on Spark</code></strong>：</p><p>Hive 即作为存储又负责 sql 的解析优化，Spark 负责执行。</p><p>数据源在hive上，解析引擎是hive，执行任务是spark</p><h2 id="3、DataFrame"><a href="#3、DataFrame" class="headerlink" title="3、DataFrame"></a>3、DataFrame</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0e3717grfj30dq07amzi.jpg" alt=""></p><ul><li>分布式数据容器;</li><li>与 RDD 类似，然而 DataFrame更像传统数据库的二维表格;</li><li>DataFrame 的底层封装的是 RDD，只不过 RDD 的泛型是 Row 类型;</li><li>相当于RDD+schema  （数据+数据的结构信息）</li><li>与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map）</li><li>从 API 易用性的角度上 看， DataFrame API提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。</li></ul><h2 id="4、SparkSql-的数据源"><a href="#4、SparkSql-的数据源" class="headerlink" title="4、SparkSql 的数据源"></a>4、SparkSql 的数据源</h2><p> JSON 类型的字符串，JDBC、Parquent、Hive，HBASE、HDFS </p><h2 id="5、SparkSql底层架构"><a href="#5、SparkSql底层架构" class="headerlink" title="5、SparkSql底层架构"></a>5、SparkSql底层架构</h2><p>sql——&gt;逻辑计划——&gt;优化逻辑计划——&gt;物理计划——&gt;RDD（Spark任务）</p><p><code>解释：</code></p><blockquote><p> 首先拿到 sql 后解析一批未被解决的逻辑计划，再经过分析得到分析后的逻辑计划，再经过一批优化规则转换成一批最佳优化的逻辑计划，再经过 SparkPlanner 的策略转化成一批物理计划，随后经过消费模型转换<br>成一个个的 Spark 任务执行。</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g20v9y5irfj30tz0cvq66.jpg" alt=""></p><h2 id="6、谓词下推（predicate-Pushdown）"><a href="#6、谓词下推（predicate-Pushdown）" class="headerlink" title="6、谓词下推（predicate Pushdown）"></a>6、谓词下推（predicate Pushdown）</h2><p><code>sql</code>:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> table1.name,table2.score <span class="keyword">from</span> table1 <span class="keyword">join</span> table2 <span class="keyword">on</span> table1.id=table2.id <span class="keyword">where</span> table1.age &gt; <span class="number">50</span> <span class="keyword">and</span> table2.score &gt; <span class="number">90</span></span><br></pre></td></tr></table></figure><p><code>执行顺序</code> </p><p> join:t1,t2<br>过滤：where : t1.age&gt;50,t2.score&gt;90<br>列裁剪：from:  select:</p><p><strong><code>谓词下推</code></strong><br>先各自过滤：where<br>然后列裁剪：t1:name,id  ;  t2:score,id<br>join</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0e98ac1j9j30df09vt9h.jpg" alt="谓词下推"><span class="img-alt">谓词下推</span></p><h1 id="三、创建DataFrame的几种方式"><a href="#三、创建DataFrame的几种方式" class="headerlink" title="三、创建DataFrame的几种方式"></a>三、创建DataFrame的几种方式</h1><h2 id="1、Json格式文件"><a href="#1、Json格式文件" class="headerlink" title="1、Json格式文件"></a>1、Json格式文件</h2><p>读取Json格式文件创建DataFrame</p><p><code>注意：</code></p><blockquote><p>1、json文件中不能<code>嵌套</code>json格式的内容</p><p>2、读取json文件格式（转换成DataFrame）的两种方式。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment">//创建SQLContext（实现了序列化）</span></span><br><span class="line">&gt; SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sparkContext);</span><br><span class="line">&gt; DataFrame df = sqlContext.read().format(<span class="string">"json"</span>).load(<span class="string">"./data/jsonfile"</span>);</span><br><span class="line">&gt; DataFrame df = sqlContext.read().json(<span class="string">"./data/jsonfile"</span>);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>3、dataFrame.show( )默认显示前20行数据，使用dataFrame.show(行数）可显示指定行数的数据</p><p>4、将DataFrame转换成RDD：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; Java: df.javaRDD( )  </span><br><span class="line">&gt; Scala: df.rdd</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>5、显示DataFrame的Schema信息（树形的形式）：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; df.printSchema(  )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>6、dataFrame自带API操作dataFrame ,不常用</p><p>7、使用sql查询：</p><p>​         a，将DataFrame注册临时表： df.registerTemptable(“mytable”)   </p><p>​         b，使用sql： sqlContext.sql(“sql语句”)</p><p>8、df中的数据加载过之后，显示时，会默认将列按ASCII码进行排序</p></blockquote><p><code>Java：</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"jsonfile"</span>);</span><br><span class="line">SparkContext context = <span class="keyword">new</span> SparkContext(conf);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建SQLContext（实现了序列化）</span></span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sparkContext);</span><br><span class="line"></span><br><span class="line"><span class="comment">//文件格式：&#123;"name":"zhangsan","age": 18&#125;</span></span><br><span class="line"><span class="comment">//读取json文件的两种方式,得到DataFrame（底层是RDD）</span></span><br><span class="line">DataFrame df = sqlContext.read().format(<span class="string">"json"</span>).load(<span class="string">"./data/jsonfile"</span>);</span><br><span class="line"><span class="comment">//DataFrame df = sqlContext.read().json("./data/jsonfile");</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//显示df中的内容的两种情况（以二维表显示，空值用null代替，列自动按ASCII码排序）</span></span><br><span class="line">df.show();</span><br><span class="line">df.show(<span class="number">100</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//df转换成RDD</span></span><br><span class="line"><span class="comment">//RDD&lt;ROW&gt; rdd = df.rdd()</span></span><br><span class="line">JavaRDD&lt;Row&gt; javaRDD = df.javaRDD();</span><br><span class="line"></span><br><span class="line"><span class="comment">//显示数据结构信息</span></span><br><span class="line">df.printSchema();</span><br><span class="line"></span><br><span class="line"><span class="comment">//自带操作DataFrame的API</span></span><br><span class="line"><span class="comment">//select name from table</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show();</span><br><span class="line"><span class="comment">//select name ,age+10 as addage from table</span></span><br><span class="line">df.select(df.col(<span class="string">"name"</span>),df.col(<span class="string">"age"</span>).plus(<span class="number">10</span>).alias(<span class="string">"addage"</span>)).show();</span><br><span class="line"><span class="comment">//select name ,age from table where age&gt;19</span></span><br><span class="line">df.select(df.col(<span class="string">"name"</span>),df.col(<span class="string">"age"</span>)).where(df.col(<span class="string">"age"</span>).gt(<span class="number">19</span>)).show();</span><br><span class="line"><span class="comment">//select age,count(*) from table group by age</span></span><br><span class="line">df.groupBy(df.col(<span class="string">"age"</span>)).count().show();</span><br><span class="line">    </span><br><span class="line"><span class="comment">//使用SQL查询</span></span><br><span class="line"><span class="comment">//将DataFrame注册成临时的一张表，这张表相当于临时注册到内存中，是逻辑上的表，不会雾化到磁盘</span></span><br><span class="line">df.registerTempTable(<span class="string">"table"</span>);</span><br><span class="line">DataFrame sqlDF = sqlContext.sql(<span class="string">"sekect * from table where name like 'zhang&amp;'"</span>);</span><br><span class="line">sqlDF.show();</span><br><span class="line">context.stop()</span><br></pre></td></tr></table></figure><p><code>Scala:</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"json"</span>)</span><br><span class="line"><span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQlContext</span>(context)</span><br><span class="line"></span><br><span class="line"><span class="comment">//读取json文件</span></span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(<span class="string">"./data/jsonfile"</span>)</span><br><span class="line"><span class="comment">//val df = sqlContext.read.format("json").load("./data/jsonfile)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//将df转化成RDD</span></span><br><span class="line"><span class="comment">//val rdd = df.rdd</span></span><br><span class="line">df.show()</span><br><span class="line">de.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">//select * from table</span></span><br><span class="line">df.select(df.col(<span class="string">"name"</span>)).show()</span><br><span class="line"><span class="comment">//select name from table where age&gt;19</span></span><br><span class="line">df.select(df.col(<span class="string">"name"</span>),df.col(<span class="string">"age"</span>)).where(df.col(<span class="string">"age"</span>).gt(<span class="number">19</span>)).show()</span><br><span class="line"><span class="comment">//select count(*) from table group by age</span></span><br><span class="line">df.groupBy(df.col(<span class="string">"age"</span>)).count().show();</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用sql </span></span><br><span class="line"><span class="comment">//注册临时表</span></span><br><span class="line">df.registerTempTable(<span class="string">"table"</span>)</span><br><span class="line"><span class="keyword">val</span> result = sqlContext.sql(<span class="string">"select * from table"</span>)</span><br><span class="line">result.show()</span><br><span class="line">context.stop()</span><br></pre></td></tr></table></figure><h2 id="2、Json格式的RDD"><a href="#2、Json格式的RDD" class="headerlink" title="2、Json格式的RDD"></a>2、Json格式的RDD</h2><p>通过Json格式的RDD创建DataFrame</p><p><strong><code>Java</code></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"jsonRdd"</span>);</span><br><span class="line">JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(context);</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建RDD</span></span><br><span class="line">JavaRDD&lt;String&gt; nameRDD = context.parallelize(Array.asList(</span><br><span class="line"><span class="string">"&#123;'name','zs','age','18'&#125;"</span>,</span><br><span class="line"><span class="string">"&#123;\"name\",\"ls\",\"age\",\"21\"&#125;"</span></span><br><span class="line">));</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; scoreRDD = context.parallelize(Array.asList(</span><br><span class="line"><span class="string">"&#123;'name':'zs','score':'90'&#125;"</span>,</span><br><span class="line"><span class="string">"&#123;\"name\":\"ls\",\"score\":\"88\"&#125;"</span></span><br><span class="line">));</span><br><span class="line"></span><br><span class="line"><span class="comment">//将jsonRDD转换成DataFrame</span></span><br><span class="line">DataFrame namedf = sqlContext.read().json(nameRDD);</span><br><span class="line">DataFrame scoredf = sqlContext.read().json(scoreRDD);</span><br><span class="line"></span><br><span class="line"><span class="comment">//为df注册临时表</span></span><br><span class="line">namedf.registerTempTable(<span class="string">"nameTable"</span>);</span><br><span class="line">scoredf.registerTempTable(<span class="string">"scoreTable"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用sql查询</span></span><br><span class="line">DataFrame df = sqlContext.sql(<span class="string">"select nameTable.name,nameTable.age,"</span>+</span><br><span class="line">                             <span class="string">"scoretable.score from nameTable join scoreTabel"</span>+</span><br><span class="line">                              <span class="string">"on nameTable.name = scoreTable.name "</span>);</span><br><span class="line">df.show();</span><br><span class="line">context.stop();</span><br></pre></td></tr></table></figure><p><strong><code>Scala</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"jsonRdd"</span>)</span><br><span class="line"><span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(context)</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建RDD</span></span><br><span class="line"><span class="keyword">val</span> nameRDD = context.makeRDD(</span><br><span class="line"> <span class="string">"&#123;'name','zs','age','18'&#125;"</span>,</span><br><span class="line"><span class="string">"&#123;\"name\",\"ls\",\"age\",\"21\"&#125;"</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> scoreRDD = context.makeRDD(                                                     </span><br><span class="line"><span class="string">"&#123;'name':'zs','score':'90'&#125;"</span>,</span><br><span class="line"><span class="string">"&#123;\"name\":\"ls\",\"score\":\"88\"&#125;"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">//获取dataFrame</span></span><br><span class="line"><span class="keyword">val</span> namedf = sqlContext.read.json(nameRDD)</span><br><span class="line"><span class="keyword">val</span> scoredf = sqlContext.read.json(scoreRDD)</span><br><span class="line"></span><br><span class="line"><span class="comment">//为DataFrame指定临时表</span></span><br><span class="line">namedf.registerTempTable(<span class="string">"nameTable"</span>)</span><br><span class="line">scoredf.registerTempTable(<span class="string">"scoreTable"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用sql</span></span><br><span class="line"><span class="keyword">val</span> df = sqlContext.sql(<span class="string">"select nameTable.name,nameTable.age,"</span>+</span><br><span class="line">                         <span class="string">"scoretable.score from nameTable join scoreTabel"</span>+</span><br><span class="line">                         <span class="string">"on nameTable.name = scoreTable.name "</span>)</span><br><span class="line">df.show()</span><br><span class="line">context.stop()</span><br></pre></td></tr></table></figure><h2 id="3、非Json格式的文件"><a href="#3、非Json格式的文件" class="headerlink" title="3、非Json格式的文件"></a>3、非Json格式的文件</h2><p>非Json格式的文件创建DataFrame</p><h3 id="1）反射的方式"><a href="#1）反射的方式" class="headerlink" title="1）反射的方式"></a>1）反射的方式</h3><p>通过反射的方式将非json格式的RDD转换成DataFrame（不推荐）</p><ul><li>自定义类要实现序列化</li><li>自定义类的访问级别是public</li><li>RDD转换成DataFrame后会根据映射按ASCII码排序</li><li>将DataFrame转换成RDD时，获取字段的范式有两种：<ul><li>1）row.getInt(0）；df.getString(1) 通过下标获取，返回Row类型的数据，注意列顺序问题（不推荐）</li><li>2）row.getAs(“列名”)  通过列名获取对应列值（推荐）</li></ul></li></ul><p><strong><code>Java</code></strong>:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.sql.dataframe;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> <span class="keyword">implements</span> <span class="title">Serializable</span></span>&#123;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="keyword">private</span> String id ;</span><br><span class="line"><span class="keyword">private</span>  String name;</span><br><span class="line"><span class="keyword">private</span> Integer age;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> id;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(String id)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.id = id;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> name;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">getAge</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> age;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setAge</span><span class="params">(Integer age)</span> </span>&#123;</span><br><span class="line"><span class="keyword">this</span>.age = age;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="string">"Person [id="</span> + id + <span class="string">", name="</span> + name + <span class="string">", age="</span> + age + <span class="string">"]"</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"RDD"</span>);</span><br><span class="line">JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line"><span class="comment">//获取RDD（文件格式：1,zhangsan,18）</span></span><br><span class="line">JavaRDD&lt;String&gt; lineRDD = context.textFile(<span class="string">"./data/person"</span>);</span><br><span class="line"><span class="comment">//反射</span></span><br><span class="line">JavaRDD&lt;Person&gt; personRDD = </span><br><span class="line">    lineRDD.map(<span class="keyword">new</span> Funcation&lt;String,Person&gt;()&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Person <span class="title">call</span><span class="params">(String str)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            Person person = <span class="keyword">new</span> Person();</span><br><span class="line">            person.setId(str.split(<span class="string">","</span>)[<span class="number">0</span>]);</span><br><span class="line">            person.setName(str.split(<span class="string">","</span>)[<span class="number">1</span>]);</span><br><span class="line">            person.setAge(Integer.valueOf(str.split(<span class="string">","</span>)[<span class="number">2</span>]));</span><br><span class="line">            <span class="keyword">return</span> person;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">传入Person.class后，sqlContext就通过反射的方式创建DataFrame</span></span><br><span class="line"><span class="comment">因为在底层通过反射的方式可以获得Person类的所有field，再结合RDD，即可创建DataFrame</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//将RDD转换成DataFram</span></span><br><span class="line">DataFrame df = sqlContext.createDataFrame(personRDD,Person.class);</span><br><span class="line">df.show();</span><br><span class="line">df.printSchema()</span><br><span class="line">df.registerTempTable(<span class="string">"table"</span>);</span><br><span class="line">DataFrame sqldf = sqlContext.sql(<span class="string">"select * from table"</span>);</span><br><span class="line">sqldf.show()</span><br><span class="line">  </span><br><span class="line"><span class="comment">//将DataFrame转换成RDD（两种方式）</span></span><br><span class="line"><span class="comment">//因为排序的原因：df中列的顺序变为：age ， id ， name</span></span><br><span class="line">JavaRDD&lt;Row&gt; dfRDD = df.javaRDD();</span><br><span class="line">JavaRDD&lt;Person&gt; dfMap = </span><br><span class="line">   dfRDD.map(<span class="keyword">new</span> Function(Row,Person)&#123;</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Person <span class="title">call</span><span class="params">(Row row)</span> <span class="keyword">throws</span> Exception </span>&#123;    </span><br><span class="line">        Person p = <span class="keyword">new</span> Person();</span><br><span class="line"><span class="comment">//        p.setId(row.getString(1));</span></span><br><span class="line"><span class="comment">//        p.setName(row.getString(2));</span></span><br><span class="line"><span class="comment">//        p.setAge(row.getInt(0));</span></span><br><span class="line">        p.setId((String)row.getAs(<span class="string">"id"</span>));</span><br><span class="line">p.setName((String)row.getAs(<span class="string">"name"</span>));</span><br><span class="line">p.setAge((Integer)row.getAs(<span class="string">"age"</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> p;</span><br><span class="line">    &#125;    </span><br><span class="line">&#125;);</span><br><span class="line">dfMap.foreach(<span class="keyword">new</span> VoidFunction&lt;Person&gt;()&#123;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Person person)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.out.println(person);       </span><br><span class="line">    &#125;     </span><br><span class="line">&#125;);</span><br><span class="line">context.stop();</span><br></pre></td></tr></table></figure><p><strong><code>Scala</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"> conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"rddreflect"</span>)</span><br><span class="line"> <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"> <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"> <span class="keyword">val</span> lineRDD = sc.textFile(<span class="string">"./data/person"</span>)</span><br><span class="line"><span class="comment">//文件格式：1,zhangsan,18</span></span><br><span class="line"><span class="comment">//将RDD转换成DataFrame</span></span><br><span class="line"><span class="keyword">val</span> personRDD = linRDD.map&#123;x=&gt;&#123;</span><br><span class="line"> <span class="keyword">val</span> person = <span class="type">Person</span>(x.split(<span class="string">","</span>)(<span class="number">0</span>),x.split(<span class="string">","</span>)(<span class="number">1</span>),<span class="type">Intger</span>.valueOf(x.split(<span class="string">","</span>)(<span class="number">2</span>))</span><br><span class="line"> person</span><br><span class="line">&#125;&#125;</span><br><span class="line"><span class="comment">//将personRDD转化成DataFrame                     </span></span><br><span class="line"><span class="keyword">val</span> df = personRDD.toDF() </span><br><span class="line">df.show()  </span><br><span class="line">                     </span><br><span class="line"><span class="comment">//将DataFrame转换成RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd = df.rdd</span><br><span class="line"><span class="keyword">val</span> personRDD = rdd.map&#123;x=&gt;&#123;</span><br><span class="line">   <span class="type">Person</span>(x.getAs(<span class="string">"id"</span>),x.getAs(<span class="string">"name"</span>),x.getAs(<span class="string">"age"</span>)) </span><br><span class="line">&#125;&#125; </span><br><span class="line">personRDD.foreach(println)</span><br><span class="line">context.stop()</span><br></pre></td></tr></table></figure><h3 id="2）动态创建Schema"><a href="#2）动态创建Schema" class="headerlink" title="2）动态创建Schema"></a>2）动态创建Schema</h3><p>通过动态创建Schema将非json格式RDD转成DataFrame（RowRDD ， Schema）</p><p><strong><code>Java</code></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"rddStruct"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">JavaRDD&lt;String&gt; lineRDD = sc.textFile(<span class="string">"./data/person"</span>);</span><br><span class="line"><span class="comment">//文件格式：1,zhangsan,18</span></span><br><span class="line"><span class="comment">//将RDD转换成DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//先将RDD转成Row类型的RDD</span></span><br><span class="line"><span class="keyword">final</span> JavaRDD&lt;Row&gt; rowRDD =</span><br><span class="line">    lineRDD.map(<span class="keyword">new</span> Function&lt;String,Row&gt;()&#123;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">             val row = RowFactory.create(</span><br><span class="line">             s.split(<span class="string">","</span>)[<span class="number">0</span>],</span><br><span class="line">             s.split(<span class="string">","</span>)[<span class="number">1</span>],</span><br><span class="line">            Integer.valueOf(s.split(<span class="string">","</span>)[<span class="number">2</span>]) </span><br><span class="line">             );</span><br><span class="line">            <span class="keyword">return</span> row;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"><span class="comment">//再动态创建DataFrame的的元数据（Schema），字段的来源：字符串或外部数据库，是否非空</span></span><br><span class="line">List&lt;StructField&gt; asList = Arrays.asList(</span><br><span class="line">    DataTypes.createStructField(<span class="string">"id"</span>,DataTypes.StringType,<span class="keyword">true</span>),</span><br><span class="line">    DataTypes.createStructField(<span class="string">"name"</span>,DataTypes.StringType,<span class="keyword">true</span>)，</span><br><span class="line">    DataTypes.createStructField（<span class="string">"age"</span>,DataTypes.IntegerType,<span class="keyword">true</span>)</span><br><span class="line">);</span><br><span class="line">StructType schema = DataTypes.createStructType(asList);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取DataFrame</span></span><br><span class="line">DataFrame df = sqlContext.createDataFrame(rowRDD,schema);</span><br><span class="line">df.printSchema();</span><br><span class="line">df.show();</span><br><span class="line"></span><br><span class="line"><span class="comment">//将dataframe转换成rowRDD</span></span><br><span class="line"><span class="comment">//JavaRDD&lt;Row&gt; javaRDD = df.javaRDD();</span></span><br><span class="line"><span class="comment">//javaRDD.foreach(new VoidFunction&lt;Row&gt;() &#123;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//private static final long serialVersionUID = 1L;</span></span><br><span class="line"><span class="comment">//@Override</span></span><br><span class="line"><span class="comment">//public void call(Row row) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//System.out.println(row.getString(0));</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//                System.out.println(row);</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//&#125;);</span></span><br><span class="line">context.stop();</span><br></pre></td></tr></table></figure><p><strong><code>Scala</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"rddStruct"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">val</span> lineRDD = sc.textFile(<span class="string">"./data/person"</span>)</span><br><span class="line"><span class="comment">//文件格式：1,zhangsan,18</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//将RDD转换成RowRDD</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = lineRDD.map&#123;x=&gt;&#123;</span><br><span class="line">    <span class="keyword">val</span> split = x.split(<span class="string">","</span>)</span><br><span class="line">    <span class="type">RowFactory</span>.create(split(<span class="number">0</span>),split(<span class="number">1</span>),<span class="type">Integer</span>.valueOf(split(<span class="number">2</span>))</span><br><span class="line">&#125;&#125;</span><br><span class="line"><span class="comment">//获取schema</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line"><span class="type">StructField</span>(<span class="string">"id"</span>,<span class="type">StringType</span>，<span class="literal">true</span>),</span><br><span class="line"><span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line"><span class="type">StructField</span>(<span class="string">"age"</span>,<span class="type">IntegerType</span>,<span class="literal">true</span>)</span><br><span class="line">))</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.createDataFrame(rowRDD,shema)</span><br><span class="line">df.show()</span><br><span class="line">df.printSchema()                      </span><br><span class="line">context.stop()</span><br></pre></td></tr></table></figure><h2 id="4、parquet文件"><a href="#4、parquet文件" class="headerlink" title="4、parquet文件"></a>4、parquet文件</h2><p>读取parquet文件创建DataFrame</p><p><strong><code>注意：</code></strong></p><ul><li><p>可以将 DataFrame 存储成 parquet 文件。保存成 parquet 文件的方式有两种</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.write().mode(SaveMode.Overwrite)format(&quot;parquet&quot;).save(&quot;./sparksql/parquet&quot;);</span><br><span class="line">df.write().mode(SaveMode.Overwrite).parquet(&quot;./sparksql/parquet&quot;);</span><br></pre></td></tr></table></figure></li><li><blockquote><p>将DataFrame保存成parquet文件，指定SaveMode 来指定文件保存时的模式。</p><p>Overwrite：覆盖<br>Append：追加<br>ErrorIfExists：如果存在就报错<br>Ignore：如果存在就忽略</p></blockquote></li></ul><p><strong><code>Java</code></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"parquet"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">JavaRDD&lt;String&gt; jsonRDD = sc.textFile(<span class="string">"./data/json"</span>);</span><br><span class="line"><span class="comment">//读取json格式的文件</span></span><br><span class="line">DataFrame df = sqlContext.read().json(jsonRDD);</span><br><span class="line"><span class="comment">//sqlContext.read().format("json").load("./spark/json");</span></span><br><span class="line">df.show();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存成parquet文件有以下两种方式：</span></span><br><span class="line">df.write().mode(SaveMode.Overwrite).parquet(<span class="string">"./sparksql/parquet"</span>);</span><br><span class="line"><span class="comment">//df.write().mode(SaveMode.Overwrite).format("parquet").save("data/parquet");</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 加载parquet文件成DataFrame,有以下两种方式： </span></span><br><span class="line">load = sqlContext.read().parquet(<span class="string">"data/parquet"</span>);</span><br><span class="line"><span class="comment">// DataFrame load = sqlContext.read().format("parquet").load("data/parquet");</span></span><br><span class="line">load.show();</span><br><span class="line">sc.stop();</span><br></pre></td></tr></table></figure><p><strong><code>Scala</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"parquet"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">val</span> jsonRDD = sc.textFile(<span class="string">"data/json"</span>)</span><br><span class="line"><span class="keyword">val</span> df = sqlContext.read.json(jsonRDD)</span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 将DF保存为parquet文件</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).format(<span class="string">"parquet"</span>).save(<span class="string">"data/parquet"</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).parquet(<span class="string">"data/parquet"</span>)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 读取parquet文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">var</span> result = sqlContext.read.parquet(<span class="string">"data/parquet"</span>)</span><br><span class="line">result = sqlContext.read.format(<span class="string">"parquet"</span>).load(<span class="string">"data/parquet"</span>)</span><br><span class="line">result.show()</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h2 id="5、JDBC中的数据"><a href="#5、JDBC中的数据" class="headerlink" title="5、JDBC中的数据"></a>5、JDBC中的数据</h2><p>读取JDBC中的数据创建DataFrame（MySQL为例）</p><p>两种方式创建 DataFrame</p><p><strong><code>Java</code></strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"mysql"</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 配置join或者聚合操作shuffle数据时分区的数量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        conf.set(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"1"</span>);</span><br><span class="line"></span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        SQLContext sqlContext = <span class="keyword">new</span> SQLContext(sc);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 第一种方式读取MySql数据库表，加载为DataFrame</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Map&lt;String, String&gt; optionsMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">        options.put(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/spark"</span>);</span><br><span class="line">        options.put(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        options.put(<span class="string">"user"</span>, <span class="string">"root"</span>);</span><br><span class="line">        options.put(<span class="string">"password"</span>, <span class="string">"root"</span>);</span><br><span class="line">        options.put(<span class="string">"dbtable"</span>, <span class="string">"person"</span>);</span><br><span class="line"></span><br><span class="line">     DataFrame person = sqlContext.read().format(<span class="string">"jdbc"</span>).options(optionsMap).load();</span><br><span class="line">        person.show();</span><br><span class="line"></span><br><span class="line">        person.registerTempTable(<span class="string">"person"</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 第二种方式用DataFrameReader分步读取MySql数据表加载为DataFrame</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        DataFrameReader reader = sqlContext.read().format(<span class="string">"jdbc"</span>);</span><br><span class="line">        reader.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://127.0.0.1:3306/spark"</span>);</span><br><span class="line">        reader.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        reader.option(<span class="string">"user"</span>, <span class="string">"root"</span>);</span><br><span class="line">        reader.option(<span class="string">"password"</span>, <span class="string">"root"</span>);</span><br><span class="line">        reader.option(<span class="string">"dbtable"</span>, <span class="string">"score"</span>);</span><br><span class="line">        DataFrame score = reader.load();</span><br><span class="line">        score.show();</span><br><span class="line">        score.registerTempTable(<span class="string">"score"</span>);</span><br><span class="line"></span><br><span class="line">        DataFrame result =</span><br><span class="line">               sqlContext.sql(<span class="string">"select person.id,person.name,person.age,score.score "</span></span><br><span class="line">                        + <span class="string">"from person,score "</span></span><br><span class="line">                        + <span class="string">"where person.name = score.name  and score.score&gt; 90"</span>);</span><br><span class="line">        result.show();</span><br><span class="line"></span><br><span class="line">        result.registerTempTable(<span class="string">"result"</span>);</span><br><span class="line">DataFrame df = sqlContext.sql(<span class="string">"select id,name,age,score from result where ag&gt;18"</span>);</span><br><span class="line">        df.show();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 将DataFrame结果保存到Mysql中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>);</span><br><span class="line">        properties.setProperty(<span class="string">"password"</span>, <span class="string">"root"</span>);</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * SaveMode:</span></span><br><span class="line"><span class="comment">         * Overwrite：覆盖</span></span><br><span class="line"><span class="comment">         * Append:追加</span></span><br><span class="line"><span class="comment">         * ErrorIfExists:如果存在就报错</span></span><br><span class="line"><span class="comment">         * Ignore:如果存在就忽略</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">      result.write().mode(SaveMode.Append)</span><br><span class="line">              .jdbc(<span class="string">"jdbc:mysql://127.0.0.1:3306/spark"</span>, <span class="string">"result2"</span>, properties);</span><br><span class="line">        System.out.println(<span class="string">"----Finish----"</span>);</span><br><span class="line">        sc.stop();</span><br></pre></td></tr></table></figure><p><strong><code>Scala</code></strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"mysql"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 第一种方式读取Mysql数据库表创建DF</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>,<span class="type">String</span>]();</span><br><span class="line">options.put(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://192.168.100.111:3306/spark"</span>)</span><br><span class="line">options.put(<span class="string">"driver"</span>,<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">options.put(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">options.put(<span class="string">"password"</span>, <span class="string">"1234"</span>)</span><br><span class="line">options.put(<span class="string">"dbtable"</span>,<span class="string">"person"</span>)</span><br><span class="line"><span class="keyword">val</span> person = sqlContext.read.format(<span class="string">"jdbc"</span>).options(options).load()</span><br><span class="line">person.show()</span><br><span class="line">person.registerTempTable(<span class="string">"person"</span>)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 第二种方式读取Mysql数据库表创建DF</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> reader = sqlContext.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">reader.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://192.168.100.111:3306/spark"</span>)</span><br><span class="line">reader.option(<span class="string">"driver"</span>,<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">reader.option(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">reader.option(<span class="string">"password"</span>,<span class="string">"1234"</span>)</span><br><span class="line">reader.option(<span class="string">"dbtable"</span>, <span class="string">"score"</span>)</span><br><span class="line"><span class="keyword">val</span> score = reader.load()</span><br><span class="line">score.show()</span><br><span class="line">score.registerTempTable(<span class="string">"score"</span>)</span><br><span class="line"><span class="keyword">val</span> result = sqlContext.sql(<span class="string">"select person.id,person.name,score.score from                                        person,score where person.name = score.name"</span>)</span><br><span class="line">result.show()</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将数据写入到Mysql表中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"password"</span>, <span class="string">"1234"</span>)</span><br><span class="line">result.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).</span><br><span class="line">             jdbc(<span class="string">"jdbc:mysql://192.168.100.111:3306/spark"</span>, <span class="string">"result"</span>, properties)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h2 id="6、Hive中的数据"><a href="#6、Hive中的数据" class="headerlink" title="6、Hive中的数据"></a>6、Hive中的数据</h2><p>读取Hive中的数据加载成DataFrame</p><ul><li><blockquote><p> HiveContext 是 SQLContext 的子类，连接 Hive 建议使用HiveContext</p></blockquote></li><li><blockquote><p>由于本地没有 Hive 环境，要提交到集群运行，提交命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> ./spark-submit</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --master spark://node00:7077,node01:7077</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-cores 1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --executor-memory 1G</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --total-executor-cores 1</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> --class com.bd.sparksql.dataframe.CreateDFFromHive</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> /usr/soft/spark-test.jar</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote></li></ul><h3 id="代码详情"><a href="#代码详情" class="headerlink" title="代码详情"></a>代码详情</h3><h4 id="Java"><a href="#Java" class="headerlink" title="Java"></a><code>Java</code></h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"hive"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line"><span class="comment">//HiveContext是SQLContext的子类。（2.0之后就将两个类就合成一个类了）</span></span><br><span class="line">HiveContext hiveContext = <span class="keyword">new</span> HiveContext(sc);<span class="comment">//用于操作Hive上的数据</span></span><br><span class="line"><span class="comment">//创建实例库</span></span><br><span class="line">hiveContext.sql(<span class="string">"CREATE database spark"</span>);</span><br><span class="line"><span class="comment">//切换实例库</span></span><br><span class="line">hiveContext.sql(<span class="string">"USE spark"</span>);</span><br><span class="line"><span class="comment">//删除已存在的表</span></span><br><span class="line">hiveContext.sql(<span class="string">"DROP TABLE IF EXISTS student_infos"</span>);</span><br><span class="line"><span class="comment">//在hive中创建student_infos表</span></span><br><span class="line">hiveContext.sql(<span class="string">"CREATE TABLE IF NOT EXISTS student_infos (name STRING,age INT) row format delimited fields terminated by '\t' "</span>);</span><br><span class="line"><span class="comment">//从本地加载数据到表中</span></span><br><span class="line">hiveContext.sql(</span><br><span class="line">    <span class="string">"load data local inpath '/root/student_infos' into table student_infos"</span>);</span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">"DROP TABLE IF EXISTS student_scores"</span>); </span><br><span class="line">hiveContext.sql(</span><br><span class="line">    <span class="string">"CREATE TABLE IF NOT EXISTS student_scores (name STRING, score INT) row format delimited fields terminated by '\t'"</span>);  </span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">"LOAD DATA "</span></span><br><span class="line">+ <span class="string">"LOCAL INPATH '/root/student_scores'"</span></span><br><span class="line">+ <span class="string">"INTO TABLE student_scores"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//      查询表生成DataFrame</span></span><br><span class="line"><span class="comment">//DataFrame df = hiveContext.table("student_infos");//第二种读取Hive表加载DF方式</span></span><br><span class="line">DataFrame goodStudentsDF = hiveContext.sql(<span class="string">"SELECT si.name, si.age, ss.score "</span></span><br><span class="line">+ <span class="string">"FROM student_infos si "</span></span><br><span class="line">+ <span class="string">"JOIN student_scores ss "</span></span><br><span class="line">+ <span class="string">"ON si.name=ss.name "</span></span><br><span class="line">+ <span class="string">"WHERE ss.score&gt;=80"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//将df注册成临时表，才能使用sql</span></span><br><span class="line">goodStudentsDF.registerTempTable(<span class="string">"goodstudent"</span>);</span><br><span class="line">DataFrame result = hiveContext.sql(<span class="string">"select * from goodstudent"</span>);</span><br><span class="line">result.show();</span><br><span class="line">        </span><br><span class="line">         <span class="comment">//将结果保存到hive表 good_student_infos</span></span><br><span class="line">hiveContext.sql(<span class="string">"DROP TABLE IF EXISTS good_student_infos"</span>);goodStudentsDF.write().mode(SaveMode.Overwrite).saveAsTable(<span class="string">"good_student_infos"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame table = hiveContext.table(<span class="string">"good_student_infos"</span>);</span><br><span class="line">Row[] goodStudentRows = table.collect();</span><br><span class="line"><span class="keyword">for</span>(Row goodStudentRow : goodStudentRows) &#123;</span><br><span class="line">System.out.println(goodStudentRow);</span><br><span class="line">&#125;</span><br><span class="line">sc.stop();</span><br></pre></td></tr></table></figure><h4 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a><code>Scala</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setAppName(<span class="string">"HiveSource"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * HiveContext是SQLContext的子类。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> hiveContext = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line">hiveContext.sql(<span class="string">"use spark"</span>)</span><br><span class="line">hiveContext.sql(<span class="string">"drop table if exists student_infos"</span>)</span><br><span class="line">hiveContext.sql(<span class="string">"create table if not exists student_infos (name string,age int) row format  delimited fields terminated by '\t'"</span>)</span><br><span class="line">hiveContext.sql(<span class="string">"load data local inpath '/root/test/student_infos' into table student_infos"</span>)</span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">"drop table if exists student_scores"</span>)</span><br><span class="line">hiveContext.sql(<span class="string">"create table if not exists student_scores (name string,score int) row format delimited fields terminated by '\t'"</span>)</span><br><span class="line">hiveContext.sql(<span class="string">"load data local inpath '/root/test/student_scores' into table student_scores"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = hiveContext.sql(<span class="string">"select si.name,si.age,ss.score from student_infos si,student_scores ss where si.name = ss.name"</span>)</span><br><span class="line"></span><br><span class="line">hiveContext.sql(<span class="string">"drop table if exists good_student_infos"</span>)</span><br><span class="line"><span class="comment">//将结果写入到hive表中</span></span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).saveAsTable(<span class="string">"good_student_infos"</span>)    </span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h1 id="四、Spark-On-Hive-的配置"><a href="#四、Spark-On-Hive-的配置" class="headerlink" title="四、Spark On Hive 的配置"></a>四、Spark On Hive 的配置</h1><h2 id="Hive配置：（在Linux端）"><a href="#Hive配置：（在Linux端）" class="headerlink" title="Hive配置：（在Linux端）"></a><strong><code>Hive配置：</code></strong>（在Linux端）</h2><p>Hive服务端的配置：hive-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive2/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node00:3306/hive2?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="（1）在Spark客户端配置Spark-On-Hive"><a href="#（1）在Spark客户端配置Spark-On-Hive" class="headerlink" title="（1）在Spark客户端配置Spark  On  Hive"></a>（1）在Spark客户端配置Spark  On  Hive</h3><p>在Spark客户端安装包下spark-1.6.0/conf路径下创建hive-site.xml：</p><p>编辑内容：配置hive的metastore路径（即hive服务端的IP）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.198.131:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="（2）启动-zookeeper-集群，启动-HDFS-集群。"><a href="#（2）启动-zookeeper-集群，启动-HDFS-集群。" class="headerlink" title="（2）启动 zookeeper 集群，启动 HDFS 集群。"></a>（2）启动 zookeeper 集群，启动 HDFS 集群。</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start  (3台)</span><br><span class="line">start-all.sh       (任一台)</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><blockquote><p>由于我们这里是使用Spark作为计算框架 所以不需要启动yarn</p><p>启动yarn是在使用MapReduce作为计算框架时</p></blockquote><h3 id="（3）启动spark服务（在spark解压目录的-sbin路径下）"><a href="#（3）启动spark服务（在spark解压目录的-sbin路径下）" class="headerlink" title="（3）启动spark服务（在spark解压目录的/sbin路径下）"></a>（3）启动spark服务（在spark解压目录的/sbin路径下）</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="（4）启动mysql服务"><a href="#（4）启动mysql服务" class="headerlink" title="（4）启动mysql服务"></a>（4）启动mysql服务</h3><p>(mysql  :node00     hive  ：服务端：node02    ； 客户端 ： node01)</p><ul><li>检查mysql服务是否启动：</li></ul><blockquote><p>命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; chkconfig</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>显示：</p><blockquote><p>mysqld             0:off    1:off    2:off    3:off    4:off    5:off    6:off</p></blockquote><p>没有启动</p></blockquote><ul><li>启动mysql服务</li></ul><blockquote><p>命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; [root@node00 conf]# service mysqld start</span><br><span class="line">&gt; Starting mysqld:                                           [  OK  ]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><ul><li>登录mysql</li></ul><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; [root@node00 conf]# mysql -u root -p</span><br><span class="line">&gt; Enter password: 123456</span><br><span class="line">&gt; mysql&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h3 id="（5）启动-Hive服务端"><a href="#（5）启动-Hive服务端" class="headerlink" title="（5）启动 Hive服务端"></a>（5）启动 Hive服务端</h3><p>启动 Hive 的 metastore 服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">后台启动hive服务端</span></span><br><span class="line">hive --service metastore &amp;</span><br><span class="line"><span class="meta">#</span><span class="bash">启动打印服务日志</span></span><br><span class="line">Start Hive MetaStore Server</span><br></pre></td></tr></table></figure><h3 id="（6）打开hive交互式页面-在任一台"><a href="#（6）打开hive交互式页面-在任一台" class="headerlink" title="（6）打开hive交互式页面(在任一台)"></a>（6）打开hive交互式页面(在任一台)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>创建数据库spark</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create database spark;</span><br></pre></td></tr></table></figure><h3 id="（7）启动-SparkShell"><a href="#（7）启动-SparkShell" class="headerlink" title="（7）启动 SparkShell"></a>（7）启动 SparkShell</h3><p>读取 Hive 中的表总数，对比 hive 中查询同一表 查询总数测试时间。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./spark-shell</span><br><span class="line">--master spark://node00:7077,node01:7077</span><br><span class="line">--executor-cores 1</span><br><span class="line">--executor-memory 1g</span><br><span class="line">--total-executor-cores 1</span><br><span class="line">import org.apache.spark.sql.hive.HiveContext</span><br><span class="line">val hc = new HiveContext(sc)</span><br><span class="line">hc.sql("show databases").show</span><br><span class="line">hc.sql("user default").show</span><br><span class="line">hc.sql("select count(*) from jizhan").show</span><br></pre></td></tr></table></figure><p><strong><code>注意</code></strong></p><blockquote><p>如果使用 Spark on Hive 查询数据时，出现错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; Cause by: java.net.UknownHostException： XXX</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>找不到 HDFS 集群路径，要在客户端机器 conf/spark-env.sh 中设置HDFS 的 路 径 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h2 id="spark-On-hive：（在windows端）"><a href="#spark-On-hive：（在windows端）" class="headerlink" title="spark On hive：（在windows端）"></a><strong>spark On hive</strong>：（在windows端）</h2><h3 id="1、配置文件："><a href="#1、配置文件：" class="headerlink" title="1、配置文件："></a>1、<code>配置文件：</code></h3><p>在项目中新建文件夹conf（标记为资源文件）：</p><p>添加一下三个配置文件:（其中hive-site.xml文件用于连接hive 服务端， 其余两个文件用于连接hdfs）</p><ul><li><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.Sunrise<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.Sunrise.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.Sunrise.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.Sunrise.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.Sunrise.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/hadoop/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.fsdataset.volume.choosing.policy<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://node00:8485;node01:8485;node02:8485/shsxt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/jn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.shsxt<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.max.xcievers<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.balance.bandwidthPerSec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>10485760<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.socket.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>900000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>20<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>30<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.socket.write.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1800000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span>9</span><br></pre></td></tr></table></figure><ul><li><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the "License");</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:2181,node01:2181,node02:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><h4 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h4></li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://192.168.198.131:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2、Windows端运行"><a href="#2、Windows端运行" class="headerlink" title="2、Windows端运行"></a>2、Windows端运行</h3><p><code>注意bug</code></p><ul><li><blockquote><p>若需要将上面类（连接Hive的类）打包到Linux系统上运行时，代码中conf.setMaster(“local”）中setMaster(“local”)就不需要了</p><p>否则会报错：</p><p>xxxxxx</p></blockquote></li><li></li></ul><blockquote><p>OOM(内存溢出)</p><p>Edit Configurations  —&gt;添加VM options的配置，用以调整JVM的内存</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xms800m -Xmx800m  -XX:PermSize=64M -XX:MaxNewSize=256m -XX:MaxPermSize=128m</span><br></pre></td></tr></table></figure><ul><li><blockquote><p>java.io.IOException: Failed to delete: C:\Users\SunRise\AppData\Local\Temp\spark-64f2b5a7-f8b8-4da4-b1af-137bb278e3a4</p><p>临时目录 删除失败，不影响程序的正常运行</p></blockquote></li></ul><ul><li><blockquote><p>org.apache.hadoop.hive.ql.metadata.HiveException: copyFiles: error while checking/creating destination directory!!</p><p>数据加载失败，远程连接拒绝：因为我把core-site.xml  、 hdfs-site.xml  这两个资源文件删除了。</p><p>配置这两个作为资源文件时，注意在使用textFile( )时要取消，因为要避免从hdfs上拿文件</p></blockquote></li></ul><h3 id="3、打包在Linux上运行"><a href="#3、打包在Linux上运行" class="headerlink" title="3、打包在Linux上运行"></a>3、打包在Linux上运行</h3><ul><li><h4 id="项目打包"><a href="#项目打包" class="headerlink" title="项目打包"></a>项目打包</h4></li></ul><blockquote><p>1、点击Project Structure—&gt;Artifacts—&gt; ‘+’—&gt;JAR—&gt;如图：所使用的的Spark包就不用打进去了，因为Linux中也有。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0hfk3nqwfj30qg0g20tt.jpg" alt=""></p><p>2、点击Build—&gt;Build Project ,之后就会在指定路径下生成对应的jar包</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0hfjxuoi5j30vo0bxjsy.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0hfkaf6rej30r40czq4l.jpg" alt=""></p><p>3、将生成的jar包放在Linux系统上对应的Spark客户端节点上</p></blockquote><p><code>注意bug</code></p><ul><li><p>如果打包项目的时候，没有将hive-site.xml文件打包进去，运行时，会报错，说数据库不存在</p></li><li><blockquote><p>解决方法：将它打包进去，或者将该文件放在spark解压目录的conf路径下（最好在三台节点都配置，因为driver启动在哪台节点不确定）</p></blockquote></li></ul><p>启动spark，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>启动提交前提：</p><ul><li>zookeeper集群启动</li><li>hdfs集群启动</li><li>hive服务端启动</li><li>spark集群启动</li></ul><p>启动提交（在node00上，保证要有，两个文件，+  运行jar包）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master spark://node00:7077 --class com.bd.spark.java.sparkstream.CreateDFFromHive /usr/soft/spark-test.jar</span><br></pre></td></tr></table></figure><p>运行结果：</p><blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0hg723i9kj30qf0e50u8.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0hg5sximqj30qf0e5dhq.jpg" alt=""></p></blockquote><h1 id="五、关于序列化"><a href="#五、关于序列化" class="headerlink" title="五、关于序列化"></a>五、关于序列化</h1><h2 id="1、关于序列化的问题你要知道的"><a href="#1、关于序列化的问题你要知道的" class="headerlink" title="1、关于序列化的问题你要知道的"></a>1、关于序列化的问题你要知道的</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">测试java中以下几种情况下不被序列化的问题：</span><br><span class="line"></span><br><span class="line">1.反序列化时serializable 版本号不一致时会导致不能反序列化。</span><br><span class="line"></span><br><span class="line">2.子类中实现了serializable接口，父类中没有实现，</span><br><span class="line">父类中的变量不能被序列化,序列化后父类中的变量会得到null。</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">父类实现serializable接口,子类没有实现serializable接口时，子类可以正常序列化</span><br><span class="line"></span><br><span class="line">3.被关键字transient修饰的变量不能被序列化。</span><br><span class="line"></span><br><span class="line">4.静态变量不能被序列化，属于类，不属于方法和对象，所以不能被序列化。</span><br></pre></td></tr></table></figure><h2 id="2、储存-DataFrame"><a href="#2、储存-DataFrame" class="headerlink" title="2、储存 DataFrame"></a>2、储存 DataFrame</h2><ul><li><p>将 DataFrame 存储为 parquet 文件。</p></li><li><p>将 DataFrame 存储到 JDBC 数据库。</p></li><li><p>将 DataFrame 存储到 Hive 表。</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.ObjectOutputStream;</span><br><span class="line"><span class="keyword">import</span> com.sxt.java.sql.dataframe.serializeTest.bean.User;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 测试：反序列化时 实体类中的serialVersionUID  改变后能否反序列化？（不能）</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 步骤：  1.将本地bean serialVersionUID 版本号改为1L，然后序列化</span></span><br><span class="line"><span class="comment"> * 2.将本地bean serialVersionUID 版本号改为2L，然后反序列化</span></span><br><span class="line"><span class="comment"> * 3.观察能否完成反序列化.</span></span><br><span class="line"><span class="comment"> * 结论：简单来说，Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，</span></span><br><span class="line"><span class="comment"> * JVM会把传来的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，</span></span><br><span class="line"><span class="comment"> * 可以进行反序列化，否则就会出现序列化版本不一致的异常。</span></span><br><span class="line"><span class="comment"> * 当实现java.io.Serializable接口的实体（类）没有显式地定义一个名为serialVersionUID，类型为long的变量时，</span></span><br><span class="line"><span class="comment"> * Java序列化机制会根据编译的class自动生成一个serialVersionUID作序列化版本比较用，这种情况下，</span></span><br><span class="line"><span class="comment"> * 只有同一次编译生成的class才会生成相同的serialVersionUID 。如果我们不希望通过编译来强制划分软件版本，</span></span><br><span class="line"><span class="comment"> * 即实现序列化接口的实体能够兼容先前版本，未作更改的类，就需要显式地定义一个名为serialVersionUID，类型为long的变量，</span></span><br><span class="line"><span class="comment"> * 不修改这个变量值的序列化实体都可以相互进行串行化和反串行化。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeSerializableTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">//        User user = new User();</span></span><br><span class="line"><span class="comment">//        user.setUsername("zhangsan");</span></span><br><span class="line"><span class="comment">//        user.setPasswd("1234");</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 这里注意第一次运行的时候要把下面代码中的反序列部分注释，</span></span><br><span class="line"><span class="comment">             * 然后，在反序列化前 改正User对象中的serializableVersion的版本号，</span></span><br><span class="line"><span class="comment">             * 运行反序列化时，要把序列化代码注释，这样才能保证版本不一致。</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line"><span class="comment">//          将对象序列化到本地磁盘            </span></span><br><span class="line"><span class="comment">//ObjectOutputStream out = </span></span><br><span class="line"><span class="comment">//                new ObjectOutputStream(new FileOutputStream("data/user.txt"));</span></span><br><span class="line"><span class="comment">//out.writeObject(user);</span></span><br><span class="line"><span class="comment">//out.flush();</span></span><br><span class="line"><span class="comment">//out.close();</span></span><br><span class="line">            </span><br><span class="line"><span class="comment">//          将对象从本地磁盘反序列化</span></span><br><span class="line">            ObjectInputStream in = </span><br><span class="line">                <span class="keyword">new</span> ObjectInputStream(<span class="keyword">new</span> FileInputStream(<span class="string">"data/user.txt"</span>));</span><br><span class="line">            User user = (User) in.readObject();</span><br><span class="line"></span><br><span class="line">            System.out.println(user);</span><br><span class="line">            System.out.println(user.getPasswd() + <span class="string">"   "</span> + user.getUsername());</span><br><span class="line">            in.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="六、自定义函数UDF和UDAF"><a href="#六、自定义函数UDF和UDAF" class="headerlink" title="六、自定义函数UDF和UDAF"></a>六、自定义函数UDF和UDAF</h1><h2 id="1、UDF-用户自定义函数"><a href="#1、UDF-用户自定义函数" class="headerlink" title="1、UDF:用户自定义函数"></a>1、UDF:用户自定义函数</h2><p><code>Java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"udf"</span>);</span><br><span class="line">        JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        SQLContext sqlContext = <span class="keyword">new</span> SQLContext(context);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; paraRDD = context.parallelize(Arrays.asList(<span class="string">"zs1"</span>,<span class="string">"ls12"</span>,<span class="string">"ww123"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//rowRDD</span></span><br><span class="line">        JavaRDD&lt;Row&gt; rowRDD = paraRDD.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> RowFactory.create(v1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//schema</span></span><br><span class="line">        List&lt;StructField&gt; list = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line">        list.add(DataTypes.createStructField(<span class="string">"name"</span>,DataTypes.StringType,<span class="keyword">true</span>));</span><br><span class="line">        StructType schema = DataTypes.createStructType(list);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//DataFrame</span></span><br><span class="line">        DataFrame df = sqlContext.createDataFrame(rowRDD,schema);</span><br><span class="line">        df.registerTempTable(<span class="string">"names"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//udf</span></span><br><span class="line">      sqlContext.udf().register(<span class="string">"StringLen"</span>,<span class="keyword">new</span> UDF1&lt;String,Integer&gt;()&#123;</span><br><span class="line">          <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">              <span class="keyword">return</span> s.length();</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;,DataTypes.IntegerType);</span><br><span class="line"></span><br><span class="line">      <span class="comment">//udf2</span></span><br><span class="line">      sqlContext.udf().register(<span class="string">"StringLens"</span>, <span class="keyword">new</span> UDF2&lt;String, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(String s, Integer s2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(s2.toString());</span><br><span class="line">                <span class="keyword">return</span> s.length()+s2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,DataTypes.IntegerType);</span><br><span class="line"></span><br><span class="line">      <span class="comment">//使用sql</span></span><br><span class="line">    sqlContext.sql(<span class="string">"select name ,StringLens(name,100) as length from names"</span>).show();</span><br><span class="line"></span><br><span class="line">        context.stop();</span><br></pre></td></tr></table></figure><p><code>Scala</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"> conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"udf"</span>)</span><br><span class="line"> <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"> <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(context)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> rdd = context.makeRDD(<span class="type">Array</span>(<span class="string">"zs1"</span>,<span class="string">"ls12"</span>,<span class="string">"ww123"</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rowRDD = rdd.map(x=&gt;&#123;</span><br><span class="line">  <span class="type">RowFactory</span>.create(x)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> field = <span class="type">Array</span>(<span class="type">DataTypes</span>.createStructField(<span class="string">"name"</span>,<span class="type">DataTypes</span>.<span class="type">StringType</span>,<span class="literal">true</span>))</span><br><span class="line"> <span class="keyword">val</span> schema = <span class="type">DataTypes</span>.createStructType(field)</span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> df = sqlContext.createDataFrame(rowRDD,schema)</span><br><span class="line"> df.registerTempTable(<span class="string">"names"</span>)</span><br><span class="line"></span><br><span class="line"> sqlContext.udf.register(<span class="string">"StringLen"</span>,(x:<span class="type">String</span>)=&gt;&#123;</span><br><span class="line">   x.length</span><br><span class="line"> &#125;)</span><br><span class="line"></span><br><span class="line"> sqlContext.udf.register(<span class="string">"StringLens"</span>,(x:<span class="type">String</span>,y:<span class="type">Integer</span>)=&gt;&#123;</span><br><span class="line">   x.length+y</span><br><span class="line"> &#125;)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(<span class="string">"select name , StringLen(name) from names"</span>).show()</span><br><span class="line"></span><br><span class="line"> sqlContext.sql(<span class="string">"select name,StringLens(name,100)from names"</span>).show()</span><br><span class="line"></span><br><span class="line"> context.stop()</span><br></pre></td></tr></table></figure><h2 id="2、UDAF-用户自定义聚合函数"><a href="#2、UDAF-用户自定义聚合函数" class="headerlink" title="2、UDAF:用户自定义聚合函数"></a>2、UDAF:用户自定义聚合函数</h2><ul><li><blockquote><p> 实现 UDAF 函数如果要自定义类要实现UserDefinedAggregateFunction 类（这是一个抽象类）</p></blockquote></li></ul><p>功能：实现统计相同值得个数</p><p>数据：</p><pre><code>*     zhangsan*     zhangsan*     lisi*     lisi*     wangwu*     wangwu*     zhangsan**     select count(*)  from user group by name</code></pre><p><code>Java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"udaf"</span>);</span><br><span class="line">        JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        SQLContext sqlContext = <span class="keyword">new</span> SQLContext(context);</span><br><span class="line"><span class="comment">//指定了两个分区</span></span><br><span class="line"> JavaRDD&lt;String&gt; rdd = context.parallelize(</span><br><span class="line">       Arrays.asList(<span class="string">"zhangsan"</span>, <span class="string">"lisi"</span>, <span class="string">"wangwu"</span>, <span class="string">"zhangsan"</span>, <span class="string">"zhangsan"</span>,<span class="string">"lisi"</span>,                <span class="string">"zhangsan"</span>, <span class="string">"lisi"</span>, <span class="string">"wangwu"</span>, <span class="string">"zhangsan"</span>, <span class="string">"zhangsan"</span>, <span class="string">"lisi"</span>), <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;Row&gt; rowRDD = rdd.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Row <span class="title">call</span><span class="params">(String v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> RowFactory.create(v1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        List&lt;StructField&gt; field = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        field.add(DataTypes.createStructField(<span class="string">"name"</span>,DataTypes.StringType,<span class="keyword">true</span>));</span><br><span class="line"></span><br><span class="line">        StructType schema = DataTypes.createStructType(field);</span><br><span class="line">        DataFrame df  = sqlContext.createDataFrame(rowRDD, schema);</span><br><span class="line">        df.registerTempTable(<span class="string">"names"</span>);</span><br><span class="line"></span><br><span class="line">      sqlContext.udf().register(<span class="string">"CountString"</span>, <span class="keyword">new</span> UserDefinedAggregateFunction() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//select name ,StringCount(name) as number from user group by name</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">//初始化一个内部的自己定义的值,在Aggregate之前每组数据的初始化结果</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(MutableAggregationBuffer buffer)</span> </span>&#123;</span><br><span class="line">                <span class="comment">//初始化buffer第0位置的元素为0</span></span><br><span class="line">                buffer.update(<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">                System.out.println(<span class="string">"buffer initialize ----"</span>+buffer.get(<span class="number">0</span>));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 更新 可以认为一个一个地将组内的字段值传递进来 实现拼接的逻辑</span></span><br><span class="line"><span class="comment">             * buffer.getInt(0)获取的是上一次聚合后的值</span></span><br><span class="line"><span class="comment">             * 相当于map端的combiner，combiner就是对每一个map task的处理结果进行一次小聚合</span></span><br><span class="line"><span class="comment">             * 大聚合发生在reduce端.</span></span><br><span class="line"><span class="comment">             * 这里即是:在进行聚合的时候，每当有新的值进来，对分组后的聚合如何进行计算</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="comment">//相当于分区内</span></span><br><span class="line">            <span class="comment">//buffer1:表示上一次的累加值   buffer2:本次传进来的值</span></span><br><span class="line">            <span class="comment">//将函数输入的参数理解为一行（Row）</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(MutableAggregationBuffer buffer, Row arg1)</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"class buffer :"</span>+buffer.getClass()+<span class="string">"-------"</span>+buffer.hashCode());</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"class  arg1:"</span>+arg1.getClass()+<span class="string">"-------"</span>+arg1.hashCode());</span><br><span class="line"></span><br><span class="line">                buffer.update(<span class="number">0</span>,buffer.getInt(<span class="number">0</span>)+<span class="number">1</span>);</span><br><span class="line">System.out.println(<span class="string">"update----buffer:"</span>+buffer.toString()+<span class="string">",arg1:"</span>+arg1.toString());</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 合并 update操作，</span></span><br><span class="line"><span class="comment">             可能是针对一个分组内的部分数据，在某个节点上发生的 </span></span><br><span class="line"><span class="comment">             但是可能一个分组内的数据，会分布在多个节点上处理</span></span><br><span class="line"><span class="comment">             * 此时就要用merge操作，将各个节点上分布式拼接好的串，合并起来</span></span><br><span class="line"><span class="comment">             * buffer1.getInt(0) : 大聚合的时候 上一次聚合后的值</span></span><br><span class="line"><span class="comment">             * buffer2.getInt(0) : 本次计算传入进来的update的结果</span></span><br><span class="line"><span class="comment">             * 这里即是：最后在分布式节点完成后需要进行全局级别的Merge操作</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">//相当于分区之间</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MutableAggregationBuffer buffer1, Row buffer2)</span> </span>&#123;</span><br><span class="line">System.out.println(<span class="string">"class buffer1 :"</span>+buffer1.getClass()+<span class="string">"----"</span>+buffer1.hashCode());</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"class buffer2 :"</span>+buffer2.getClass()+<span class="string">"----"</span>+buffer2.hashCode());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                buffer1.update(<span class="number">0</span>,buffer1.getInt(<span class="number">0</span>)+buffer2.getInt(<span class="number">0</span>));</span><br><span class="line"> System.out.println(<span class="string">"merge：b1:"</span>+buffer1.toString()+<span class="string">",buffer2:"</span>+buffer2.toString());</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//指定输入字段的字段及类型</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> StructType <span class="title">inputSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> DataTypes.createStructType(                        Arrays.asList(DataTypes.createStructField(<span class="string">"name"</span>,DataTypes.StringType,<span class="keyword">true</span>))</span><br><span class="line">                );</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 在进行聚合操作的时候所要处理的数据的结果的字段的类型</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> StructType <span class="title">bufferSchema</span><span class="params">()</span> </span>&#123;</span><br><span class="line">               <span class="keyword">return</span> DataTypes.createStructType(                       Arrays.asList(DataTypes.createStructField(<span class="string">"buffer"</span>,DataTypes.IntegerType,<span class="keyword">true</span>)));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//指定UDAF函数计算后返回的结果类型</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> DataType <span class="title">dataType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> DataTypes.IntegerType;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//最后返回一个和DataType的类型要一致的类型，返回UDAF最后的计算结果</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(Row buffer)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> buffer.getInt(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//确保一致性 一般用true,用以标记针对给定的一组输入，UDAF是否总是生成相同的结果。</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">deterministic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        sqlContext.sql(<span class="string">"select name , CountString(name) from names"</span>).show();</span><br><span class="line">        context.stop();</span><br></pre></td></tr></table></figure><p><code>Scala</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">RowFactory</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">MutableAggregationBuffer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">UserDefinedAggregateFunction</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">DataType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">DataTypes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">IntegerType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StringType</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">StructType</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyUDAF</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span>  </span>&#123;</span><br><span class="line">  <span class="comment">// 为每个分组的数据执行初始化值</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">     buffer(<span class="number">0</span>) = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 每个组，有新的值进来的时候，进行分组对应的聚合值的计算</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getAs[<span class="type">Int</span>](<span class="number">0</span>)+<span class="number">1</span></span><br><span class="line">  &#125;       </span><br><span class="line">  <span class="comment">// 最后merger的时候，在各个节点上的聚合值，要进行merge，也就是合并</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getAs[<span class="type">Int</span>](<span class="number">0</span>)+buffer2.getAs[<span class="type">Int</span>](<span class="number">0</span>) </span><br><span class="line">  &#125;    </span><br><span class="line">  <span class="comment">//输入数据的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">DataTypes</span>.createStructType(</span><br><span class="line">        <span class="type">Array</span>(<span class="type">DataTypes</span>.createStructField(<span class="string">"input"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line">  &#125;    </span><br><span class="line">    <span class="comment">// 聚合操作时，所处理的数据的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">DataTypes</span>.createStructType(</span><br><span class="line">        <span class="type">Array</span>(<span class="type">DataTypes</span>.createStructField(<span class="string">"aaa"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 最终函数返回值的类型</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = &#123;</span><br><span class="line">    <span class="type">DataTypes</span>.<span class="type">IntegerType</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 最后返回一个最终的聚合值   要和dataType的类型一一对应</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer.getAs[<span class="type">Int</span>](<span class="number">0</span>)</span><br><span class="line">  &#125;    </span><br><span class="line"><span class="comment">//保证数据一致性</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDAF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"udaf"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line">    <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>(<span class="string">"zhangsan"</span>,<span class="string">"lisi"</span>,<span class="string">"wangwu"</span>,<span class="string">"zhangsan"</span>,<span class="string">"lisi"</span>))</span><br><span class="line">    <span class="keyword">val</span> rowRDD = rdd.map &#123; x =&gt; &#123;<span class="type">RowFactory</span>.create(x)&#125; &#125;   </span><br><span class="line">    <span class="keyword">val</span> schema =<span class="type">DataTypes</span>.createStructType(</span><br><span class="line">        <span class="type">Array</span>(<span class="type">DataTypes</span>.createStructField(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line">    <span class="keyword">val</span> df = sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line">    df.show()</span><br><span class="line">    df.registerTempTable(<span class="string">"user"</span>)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 注册一个udaf函数</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    sqlContext.udf.register(<span class="string">"StringCount"</span>, <span class="keyword">new</span> <span class="type">MyUDAF</span>())</span><br><span class="line">    sqlContext.sql(<span class="string">"select name ,StringCount(name) from user group by name"</span>).show()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="七、开窗函数"><a href="#七、开窗函数" class="headerlink" title="七、开窗函数"></a>七、开窗函数</h1><p><strong><code>注意：</code></strong></p><ul><li><p>row_number() 开窗函数是按照某个字段分组，再按某个字段排序，然后取排序字段的前几个的值，相当于 分组取 topN</p></li><li><p>如果 SQL 语句里面使用到了开窗函数，那么这个 SQL 语句必须使用HiveContext 来执行，HiveContext 默认情况下在本地无法创建。</p></li><li><p>开窗函数格式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">row_number() over (partitin by XXX order by XXX) </span><br><span class="line"><span class="comment">--group by .... order by  .... limit 0, 5 ;</span></span><br><span class="line"><span class="comment">--row_number() over (partition by xxx order by xxx desc) xxx</span></span><br></pre></td></tr></table></figure></li></ul><p><code>Java</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.DataFrame;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SaveMode;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.HiveContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RowNumberWindowFun</span> </span>&#123;</span><br><span class="line">    <span class="comment">//-Xms800m -Xmx800m  -XX:PermSize=64M -XX:MaxNewSize=256m -XX:MaxPermSize=128m</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setAppName(<span class="string">"windowfun"</span>).setMaster(<span class="string">"local"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        conf.set(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"1"</span>);</span><br><span class="line">HiveContext hiveContext = <span class="keyword">new</span> HiveContext(sc);</span><br><span class="line">hiveContext.sql(<span class="string">"use spark"</span>);</span><br><span class="line">hiveContext.sql(<span class="string">"drop table if exists sales"</span>);</span><br><span class="line">hiveContext.sql(</span><br><span class="line">            <span class="string">"create table if not exists sales (riqi string,leibie string,jine Int) "</span></span><br><span class="line">    + <span class="string">"row format delimited fields terminated by '\t'"</span>);</span><br><span class="line">hiveContext.sql(</span><br><span class="line">            <span class="string">"load data local inpath './data/sales.txt' into table sales"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 开窗函数格式：</span></span><br><span class="line"><span class="comment"> * 【 row_number() over (partition by XXX order by XXX) as rank】</span></span><br><span class="line"><span class="comment"> * 注意：rank 从1开始</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 以类别分组，按每种类别金额降序排序，显示 【日期，种类，金额】 结果，如：</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 1 A 100</span></span><br><span class="line"><span class="comment">         * 2 B 200</span></span><br><span class="line"><span class="comment">         * 3 A 300</span></span><br><span class="line"><span class="comment">         * 4 B 400</span></span><br><span class="line"><span class="comment">         * 5 A 500</span></span><br><span class="line"><span class="comment">         * 6 B 600</span></span><br><span class="line"><span class="comment"> * 排序后：</span></span><br><span class="line"><span class="comment"> * 5 A 500  --rank 1</span></span><br><span class="line"><span class="comment"> * 3 A 300  --rank 2 </span></span><br><span class="line"><span class="comment"> * 1 A 100  --rank 3</span></span><br><span class="line"><span class="comment"> * 6 B 600  --rank 1</span></span><br><span class="line"><span class="comment"> * 4 B 400--rank 2</span></span><br><span class="line"><span class="comment">         * 2 B 200  --rank 3</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment">         * 2018 A 400     1</span></span><br><span class="line"><span class="comment">         * 2017 A 500     2</span></span><br><span class="line"><span class="comment">         * 2016 A 550     3</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 2016 A 550     1</span></span><br><span class="line"><span class="comment">         * 2017 A 500     2</span></span><br><span class="line"><span class="comment">         * 2018 A 400     3</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//无法取前三</span></span><br><span class="line"><span class="comment">//hiveContext.sql("select riqi,leibie,jine,"</span></span><br><span class="line"><span class="comment">//             + "row_number() over (partition by leibie order by jine desc) rank "</span></span><br><span class="line"><span class="comment">//             + "from sales").show();</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DataFrame result = hiveContext.sql(</span><br><span class="line"><span class="string">"select riqi,leibie,jine,rank from ( select riqi,leibie,jine,"</span></span><br><span class="line">+ <span class="string">"row_number() over (partition by leibie order by jine desc) rank from sales) t"</span></span><br><span class="line">+ <span class="string">"where t.rank&lt;=3"</span>);</span><br><span class="line">result.show(<span class="number">100</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将结果保存到hive表sales_result</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">result.write().mode(SaveMode.Overwrite).saveAsTable(<span class="string">"sales_result"</span>);</span><br><span class="line">sc.stop();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Scala</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RowNumberWindowFun</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setAppName(<span class="string">"windowfun"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> hiveContext = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line">    hiveContext.sql(<span class="string">"use spark"</span>);</span><br><span class="line">    hiveContext.sql(<span class="string">"drop table if exists sales"</span>);</span><br><span class="line">    hiveContext.sql(</span><br><span class="line">       <span class="string">"create table if not exists sales (riqi string,leibie string,jine Int) "</span></span><br><span class="line">   + <span class="string">"row format delimited fields terminated by '\t'"</span>);</span><br><span class="line">hiveContext.sql(</span><br><span class="line">            <span class="string">"load data local inpath '/root/test/sales' into table sales"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 开窗函数格式：</span></span><br><span class="line"><span class="comment"> * 【 rou_number() over (partitin by XXX order by XXX) 】</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> result = hiveContext.sql(</span><br><span class="line">        <span class="string">"select riqi,leibie,jine from (select riqi,leibie,jine,"</span></span><br><span class="line">+<span class="string">"row_number() over (partition by leibie order by jine desc) rank"</span></span><br><span class="line">    + <span class="string">"from sales) t where t.rank&lt;=3"</span>);</span><br><span class="line">result.show();</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark框架 </tag>
            
            <tag> SparkSql </tag>
            
            <tag> SQL语句 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkShuffle调优</title>
      <link href="/2019/02/20/Shuffle%E8%B0%83%E4%BC%98/"/>
      <url>/2019/02/20/Shuffle%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.file.buffer</span><br></pre></td></tr></table></figure><p>默认值：32k<br>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.reducer.maxSizeInFlight</span><br></pre></td></tr></table></figure><p>默认值：48m<br>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。<br>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.io.maxRetries</span><br></pre></td></tr></table></figure><p>默认值：3<br>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。<br>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。<br>shuffle file not find    taskScheduler不负责重试task，由DAGScheduler负责重试stage</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.io.retryWait</span><br></pre></td></tr></table></figure><p>默认值：5s<br>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。<br>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.memoryFraction</span><br></pre></td></tr></table></figure><p>默认值：0.2<br>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。<br>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.manager</span><br></pre></td></tr></table></figure><p>默认值：sort|hash<br>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。<br>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.sort.bypassMergeThreshold</span><br></pre></td></tr></table></figure><p>默认值：200<br>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。<br>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.shuffle.consolidateFiles</span><br></pre></td></tr></table></figure><p>默认值：false<br>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。<br>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SparkCore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习（四）</title>
      <link href="/2019/02/19/Spark%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/"/>
      <url>/2019/02/19/Spark%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、广播变量"><a href="#一、广播变量" class="headerlink" title="一、广播变量"></a>一、广播变量</h1><h2 id="1、广播变量理解图"><a href="#1、广播变量理解图" class="headerlink" title="1、广播变量理解图"></a>1、广播变量理解图</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0cxiwpy6gj30fy0h2tax.jpg" alt=""></p><blockquote><p>使用广播变量将list广播出去，spark会将广播变量list放到Excutor中的BlockManager中管理；</p><p>tasks（若干个task）会到BlockManager（管理数据）中获取广播变量 ，</p></blockquote><h2 id="2、广播变量的使用"><a href="#2、广播变量的使用" class="headerlink" title="2、广播变量的使用"></a>2、广播变量的使用</h2><p>Java:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.core;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.broadcast.Broadcast;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadCast</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"broadcast"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//List中已经实现了序列化，可以用于跨网络传输</span></span><br><span class="line">List&lt;String&gt; list = Arrays.asList(<span class="string">"hello bjsxt"</span>);</span><br><span class="line"><span class="comment">//广播变量将list广播出去</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;String&gt;&gt; broadCastList = sc.broadcast(list);</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">"data/word.txt"</span>);</span><br><span class="line">JavaRDD&lt;String&gt; result = lines.filter(<span class="keyword">new</span> Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;                       </span><br><span class="line">                <span class="comment">//匿名内部类中使用的变量在声明时必须使用final修饰</span></span><br><span class="line"><span class="keyword">return</span> broadCastList.value().contains(s);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">result.foreach(<span class="keyword">new</span> VoidFunction&lt;String&gt;() &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(String t)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">System.out.println(t);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">sc.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Scala:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"brocast"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hello xasxt"</span>)</span><br><span class="line"><span class="keyword">val</span> broadCast = sc.broadcast(list)</span><br><span class="line"><span class="keyword">val</span> lineRDD = sc.textFile(<span class="string">"./words.txt"</span>)</span><br><span class="line">lineRDD.filter &#123; x =&gt; &#123;</span><br><span class="line">    println(broadCast.value)</span><br><span class="line">    broadCast.value.contains(x) </span><br><span class="line">&#125;.foreach &#123; println&#125;</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h2 id="3、注意事项"><a href="#3、注意事项" class="headerlink" title="3、注意事项"></a>3、注意事项</h2><blockquote><ul><li><p>为什么使用广播变量？</p><p>提高网络传输速率，避免worker端内存浪费。 sparkcontext.broadcast(list)；将一个集合作为参数。</p></li><li><p>能不能将一个 RDD 使用广播变量广播出去？<br>不能，因为RDD是不存储数据的。可以将RDD的collect结果广播出去。</p></li><li><p>广播变量只能在 Driver 端定义，在Executor端使用，不能在 Executor 端定义。</p></li><li><p>在 Driver 端可以修改广播变量的值，在 Executor 端无法修改广播变量的值</p></li><li><p>代码中，算子内部执行是在Executor端，其余的在Driver端</p></li><li><p>序列化：用于机器之间跨网络传输时，要将文件序列化到磁盘才可完成传输</p></li><li><p>内存大会频繁的gc（垃圾回收）就会卡顿，如果内存还不够，就会报oom（内存溢出）</p></li></ul></blockquote><h1 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h1><h2 id="1、累加器理解图"><a href="#1、累加器理解图" class="headerlink" title="1、累加器理解图"></a>1、累加器理解图</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0da58qfv9j30m40c4q9q.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0da61a9htj30lx0ebn4i.jpg" alt=""></p><h2 id="2、累加器的使用"><a href="#2、累加器的使用" class="headerlink" title="2、累加器的使用"></a>2、累加器的使用</h2><p>Java：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.core;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.Accumulator;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 累加器在Driver端定义赋初始值和读取，在Executor端累加。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AccumulatorOperator</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"accumulator"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">//获取累加器：初始值为0</span></span><br><span class="line"><span class="keyword">final</span> Accumulator&lt;Integer&gt; accumulator = sc.accumulator(<span class="number">0</span>);</span><br><span class="line">sc.textFile(<span class="string">"data/word.txt"</span>,<span class="number">2</span>).foreach(<span class="keyword">new</span> VoidFunction&lt;String&gt;() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(String t)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">accumulator.add(<span class="number">1</span>);</span><br><span class="line">                <span class="comment">//不能再Executor端获取accumulator.value()来触发累加</span></span><br><span class="line"><span class="comment">//System.out.println(accumulator.value());</span></span><br><span class="line">System.out.println(accumulator);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">  <span class="comment">// accumulator.value 写法只能在driver端，用来汇总累加器的值</span></span><br><span class="line">  <span class="comment">//excutor端的task只能用accumulator的写法来查看数据</span></span><br><span class="line">System.out.println(accumulator.value());</span><br><span class="line">sc.stop();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Scala:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"accumulator"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> accumulator = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">val count = 0</span></span><br><span class="line"><span class="comment">sc.textFile("./words.txt").foreach &#123; x =&gt;&#123;</span></span><br><span class="line"><span class="comment">count+=1</span></span><br><span class="line"><span class="comment">println("count:"+count)</span></span><br><span class="line"><span class="comment">&#125;&#125; </span></span><br><span class="line"><span class="comment">//結果count打印为0 ， 因为count未能序列化，无法实现跨网络传输</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">sc.textFile(<span class="string">"./words.txt"</span>).foreach &#123; x =&gt;&#123;accumulator.add(<span class="number">1</span>)&#125;&#125;</span><br><span class="line">println(accumulator.value)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><ul><li>累加器在Driver端定义赋初始值，累加器只能在Driver端读取，在 Excutor 端更新。</li><li>不能再Executor端获取accumulator.value()来触发累加，它是用来汇总累加器的值</li><li>accumulator.value 写法只能在driver端，excutor端的task只能用accumulator的写法来查看数据</li><li>序列化：用于机器之间跨网络传输时，要将文件序列化到磁盘才可完成传输</li></ul><h1 id="三、SparkShuffle"><a href="#三、SparkShuffle" class="headerlink" title="三、SparkShuffle"></a>三、SparkShuffle</h1><h2 id="1、SparkShuffle-概念"><a href="#1、SparkShuffle-概念" class="headerlink" title="1、SparkShuffle 概念"></a>1、SparkShuffle 概念</h2><p>reduceByKey 会将上一个 RDD 中的每一个 key 对应的所有 value 聚合成一个 value，然后生成一个新的 RDD，元素类型是&lt;key,value&gt;对的形式，这样每一个 key 对应一个聚合起来的 value。</p><p><strong><code>问题</code>：</strong>聚合之前，每一个 key 对应的 value 不一定都是在一个 partition中，也不太可能在同一个节点上，因为 RDD 是分布式的弹性的数据集，RDD 的 partition 极有可能分布在各个节点上。</p><p><strong><code>如何聚合？</code></strong></p><ul><li>– – <strong>Shuffle Write</strong> ：上一个 stage 的每个 map task 就必须保证将自己处理的当前分区的数据相同的 key 写入一个分区文件中，可能会写入多个不同的分区文件中。</li><li>– – <strong>Shuffle Read</strong> ：reduce task 就会从上一个 stage 的所有 task 所在的机器上寻找属于自己的那些分区文件，这样就可以保证每一个 key 所对应的 value 都会汇聚到同一个节点上去处理和聚合。</li><li>Spark 中有两种 Shuffle 类型，HashShuffle 和 SortShuffle，Spark1.2之<code>前</code>是 <code>HashShuffle</code> 默认的分区器是 HashPartitioner，Spark1.2 引入<code>SortShuffle</code> 默认的分区器是 RangePartitioner。</li></ul><h2 id="2、HashShuffle"><a href="#2、HashShuffle" class="headerlink" title="2、HashShuffle"></a>2、HashShuffle</h2><h3 id="1-gt-普通机制"><a href="#1-gt-普通机制" class="headerlink" title="1&gt; 普通机制"></a>1&gt; 普通机制</h3><ul><li>普通机制示意图</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0dax2jtxwj30qv0enwqa.jpg" alt=""></p><ul><li><p>执行流程<br>a) 每一个 map task 将不同结果写到不同的 buffer 中，每个buffer 的大小为 <strong><code>32K</code></strong>。buffer 起到数据缓存的作用。</p><p>b) 每个 buffer 文件最后对应一个磁盘小文件。</p><p>c) reduce task 来拉取对应的磁盘小文件。</p></li><li><p>总结<br>① .map task 的计算结果会根据分区器（默认是hashPartitioner）来决定写入到哪一个磁盘小文件中去。<br>ReduceTask 会去 Map 端拉取相应的磁盘小文件。<br>② .产生的磁盘小文件的个数：M（map task 的个数）*R（reduce task 的个数）</p></li><li><p>存在的问题<br> 产生的磁盘小文件过多，会导致以下问题：<br> a) 在 Shuffle Write 过程中会产生很多写磁盘小文件的对象。</p><p> b) 在 Shuffle Read 过程中会产生很多读取磁盘小文件的对象。</p><p> c) 在JVM堆内存中对象过多会造成频繁的gc,gc还无法解决运行所需要的内存 的话，就会 OOM。</p><p> d) 在数据传输过程中会有频繁的网络通信，频繁的网络通信出现通信故障的可能性大大增加，一旦网络通信出现了故障会导致 shuffle file cannot find ，由于这个错误导致的 task 失败，TaskScheduler 不负责重试，由 DAGScheduler 负责重试 Stage。</p></li></ul><h3 id="2-gt-合并机制"><a href="#2-gt-合并机制" class="headerlink" title="2&gt; 合并机制"></a>2&gt; 合并机制</h3><p> 合并机制示意图</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0db1l3k43j30sd0euwov.jpg" alt=""></p><ul><li>执行流程</li></ul><p>1）在只有一个核的Excutor中运行的tasks共用buffer。</p><p>2）在提交Application时 ， 会给它指定核数core</p><p>3)在同一个核core中处理的task属于同一个句柄（操作系统的概念），处理的便是同一个文件。</p><ul><li>总结<br>产生磁盘小文件的个数：C(core 的个数)*R（reduce 的个数）</li></ul><h2 id="3、SortShuffle"><a href="#3、SortShuffle" class="headerlink" title="3、SortShuffle"></a>3、SortShuffle</h2><h3 id="1-gt-普通机制-1"><a href="#1-gt-普通机制-1" class="headerlink" title="1&gt; 普通机制"></a>1&gt; 普通机制</h3><p> 普通机制示意图</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g206dhorcnj30kl0jatbt.jpg" alt=""></p><ul><li><p>执行流程</p><p>a) map task 的计算结果会写入到一个内存数据结构里面，内存数据结构默认是 5M</p><p>b) 在 shuffle 的时候会有一个定时器，不定期的去估算这个内存结构的大小，当内存结构中的数据超过 5M 时，比如现在内存结构中的数据为 5.01M，那么他会申请 5.01*2-5=5.02M 内存给内存数据结构。</p><p>c) 如果申请成功不会进行溢写，如果申请不成功，这时候会发生溢写磁盘。</p><p>d) 在溢写之前内存结构中的数据会进行排序分区</p><p>e) 然后开始溢写磁盘，写磁盘是以batch的形式去写，一个batch是 1 万条数据，</p><p>f) map task 执行完成后，会将这些磁盘小文件合并成一个大的磁盘文件，同时生成一个索引文件。</p><p>g) reduce task 去 map 端拉取数据的时候，首先解析索引文件，根据索引文件再去拉取对应的数据。</p></li><li><p>总结<br>产生磁盘小文件的个数： 2*M（map task 的个数）</p></li></ul><h3 id="2-gt-bypass-机制"><a href="#2-gt-bypass-机制" class="headerlink" title="2&gt;bypass 机制"></a>2&gt;bypass 机制</h3><p> bypass 机制示意图</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g206eobjrdj30sf0ie45i.jpg" alt=""></p><p> 总结</p><p>① .bypass 运行机制的触发条件如下：相较于普通机制少了排序<br>shuffle  reduce  task 的 数 量 小 于spark.shuffle.sort.bypassMergeThreshold 的参数值。这个值默认是 200。<br>② .产生的磁盘小文件为：2*M（map task 的个数）</p><h2 id="4、Shuffle-文件寻址"><a href="#4、Shuffle-文件寻址" class="headerlink" title="4、Shuffle 文件寻址"></a>4、Shuffle 文件寻址</h2><h3 id="1-MapOutputTracker"><a href="#1-MapOutputTracker" class="headerlink" title="1)MapOutputTracker"></a>1)MapOutputTracker</h3><p>MapOutputTracker是Spark架构中的一个模块，是一个主从架构。管理磁盘小文件的地址。</p><ul><li><p>MapOutputTrackerMaster 是主对象，存在于 Driver 中。</p></li><li><p>MapOutputTrackerWorker 是从对象，存在于 Excutor 中。 </p></li></ul><h3 id="2-BlockManager"><a href="#2-BlockManager" class="headerlink" title="2) BlockManager"></a>2) BlockManager</h3><p>BlockManager 块管理者（数据管理），是 Spark 架构中的一个模块，也是一个主从架构。</p><ul><li>BlockManagerMaster,主对象，存在于 Driver 中。BlockManagerMaster 会在集群中有用到广播变量和缓存数据或者删除缓存数据的时候，通知 BlockManagerSlave 传输或者删除数据。</li><li>BlockManagerWorker，从对象，存在于 Excutor 中。BlockManagerWorker 会与 BlockManagerWorker 之间通信。</li><li>无论在 Driver 端的 BlockManager 还是在 Excutor 端的BlockManager 都含有四个对象：<br>① DiskStore:负责磁盘的管理。<br>② MemoryStore：负责内存的管理。<br>③ ConnectionManager：负责连接其他的BlockManagerWorker。<br>④ BlockTransferService:负责数据的传输。</li></ul><h3 id="3-Shuffle-文件寻址图"><a href="#3-Shuffle-文件寻址图" class="headerlink" title="3) Shuffle 文件寻址图"></a>3) Shuffle 文件寻址图</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g206dzvwhkj30ut0lt450.jpg" alt=""></p><h3 id="4-Shuffle-文件寻址流程"><a href="#4-Shuffle-文件寻址流程" class="headerlink" title="4) Shuffle 文件寻址流程"></a>4) Shuffle 文件寻址流程</h3><p>a) 当 map task 执行完成后，会将 task 的执行情况和磁盘小文件的地址封装到 MapStatus 对象中，通过MapOutputTrackerWorker对象向 Driver 中的MapOutputTrackerMaster 汇报。</p><p>b) 在所有的 map task 执行完毕后，Driver 中就掌握了所有的磁盘小文件的地址。</p><p>c) 在 reduce task 执行之前，会通过 Excutor 中MapOutPutTrackerWorker 向 Driver 端的MapOutputTrackerMaster 获取磁盘小文件的地址。</p><p>d) 获取到磁盘小文件的地址后，会通过 BlockManager 中的ConnectionManager 连接数据所在节点上的<br>ConnectionManager,然后通过 BlockTransferService 进行数据的传输。</p><p>e) BlockTransferService 默认启动 5 个 task 去节点拉取数据。默认情况下，5 个 task 拉取数据量不能超过 48M。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g207jecc9oj311y0lcq7m.jpg" alt=""></p><h1 id="四、Spark-内存管理"><a href="#四、Spark-内存管理" class="headerlink" title="四、Spark 内存管理"></a>四、Spark 内存管理</h1><p>Spark 执行应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，</p><ul><li>Driver 负责创建 SparkContext 上下文，提交任务，task 的分发等。</li><li>Executor 负责 task 的计算任务，并将结果返回给 Driver。同时需要为需要持久化的 RDD 提供储存。</li><li>Driver 端的内存管理比较简单，这里所说的 Spark内存管理针对 Executor 端的内存管理。</li></ul><p>Spark 内存管理分为<code>静态内存管理</code>和<code>统一内存管理</code>，Spark1.6 之前使用的是静态内存管理，Spark1.6 之后引入了统一内存管理。</p><ul><li>静态内存管理中存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以在应用程序启动前进行配置。</li><li>统一内存管理与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以互相借用对方的空间。<br>Spark1.6 以 上 版 本 默 认 使 用 的 是 统 一 内 存 管 理 ， 可 以 通 过 参 数spark.memory.useLegacyMode  设置为 true (默认为 false)使用静态内存管理。</li></ul><p>官网：<a href="http://spark.apache.org/docs/1.6.1/configuration.html#memory-management" target="_blank" rel="noopener">内存管理配置</a></p><h2 id="1、静态内存管理分布图"><a href="#1、静态内存管理分布图" class="headerlink" title="1、静态内存管理分布图"></a>1、静态内存管理分布图</h2><p>(<a href="https://ws1.sinaimg.cn/large/005zftzDgy1g2087fcm4xj30t80hm0vy.jpg" target="_blank" rel="noopener">https://ws1.sinaimg.cn/large/005zftzDgy1g2087fcm4xj30t80hm0vy.jpg</a>)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.useLegacyModefalse</span><br></pre></td></tr></table></figure><blockquote><p>是否开启静态内存管理， 默认是false ，若设置为true ， 以下三个配置才会生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; spark.storage.memoryFraction0.6</span><br><span class="line">&gt; spark.shuffle.memoryFraction0.2</span><br><span class="line">&gt; spark.storage.unrollFraction0.2</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h2 id="2、统一内存管理分布图"><a href="#2、统一内存管理分布图" class="headerlink" title="2、统一内存管理分布图"></a>2、统一内存管理分布图</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.memory.fraction0.75</span><br><span class="line">spark.memory.storageFraction0.5</span><br><span class="line">spark.memory.offHeap.enabledfalse</span><br><span class="line">spark.memory.offHeap.size0</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g2078y69bzj30ff0dkt9w.jpg" alt=""></p><h2 id="3、reduce-中-OOM-如何处理？"><a href="#3、reduce-中-OOM-如何处理？" class="headerlink" title="3、reduce 中 OOM 如何处理？"></a>3、reduce 中 OOM 如何处理？</h2><p>1) 减少每次拉取的数据量</p><p>2) 提高 shuffle 聚合的内存比例</p><p>3) 提高 Excutor 的总内存</p><h1 id="五、Shuffle-调优"><a href="#五、Shuffle-调优" class="headerlink" title="五、Shuffle  调优"></a>五、Shuffle  调优</h1><h2 id="1、SparkShuffle-调优配置项如何使用？"><a href="#1、SparkShuffle-调优配置项如何使用？" class="headerlink" title="1、SparkShuffle 调优配置项如何使用？"></a>1、SparkShuffle 调优配置项如何使用？</h2><p>1) 在代码中,不推荐使用，硬编码。</p><p>new SparkConf().set(“spark.shuffle.file.buffer”,”64”)</p><p>2) 在提交 spark 任务的时候，推荐使用。<br>spark-submit –conf spark.shuffle.file.buffer=64 –conf ….</p><p>3) 在 conf 下的 spark-default.conf 配置文件中,不推荐，因为是写死后所有应用程序都要用。</p><p><code>详情请看下一篇：SparkShuffle调优</code></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 广播 </tag>
            
            <tag> 累加 </tag>
            
            <tag> SparkShuffle </tag>
            
            <tag> spark内存管理 </tag>
            
            <tag> shuffle调优 </tag>
            
            <tag> sparkcore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习（三）</title>
      <link href="/2019/02/18/Spark%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89/"/>
      <url>/2019/02/18/Spark%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="案例一、统计网站-pv-和-uv"><a href="#案例一、统计网站-pv-和-uv" class="headerlink" title="案例一、统计网站 pv 和 uv"></a>案例一、统计网站 pv 和 uv</h1><h2 id="0、概念理解"><a href="#0、概念理解" class="headerlink" title="0、概念理解"></a>0、概念理解</h2><p><code>PV</code> 是网站分析的一个术语，用以衡量网站用户访问的网页的数量。</p><p>对于广告主，PV 值可预期它可以带来多少广告收入。一般来说，PV 与来访者的数量成正比，但是 PV 并不直接决定页面的真实来访者数量，如同一个来访者通过不断的刷新页面，也可以制造出非常高的 PV。</p><h2 id="1、什么是-PV-值"><a href="#1、什么是-PV-值" class="headerlink" title="1、什么是 PV 值"></a>1、什么是 PV 值</h2><p>PV （page view ）即页面浏览量或点击量，是衡量一个网站或网页用户访问量。</p><p>PV 值就是所有访问者在 24 小时（0 点到 24 点）内看了某个网站多少个页面或某个网页多少次。</p><p>PV 是指页面刷新的次数，每一次页面刷新，就算做一次 PV 流量。</p><h2 id="2、什么是UV-值"><a href="#2、什么是UV-值" class="headerlink" title="2、什么是UV 值"></a>2、什么是UV 值</h2><p>UV （unique visitor ）即独立访客数，指访问某个站点或点击某个网页的不同 IP 地址的人数。</p><p>在同一天内，UV  只记录第一次进入网站的具有独立IP  的访问者，在同一天内再次访问该网站则不计数。</p><p>UV 提供了一定时间内不同观众数量的统计指标，而没有反应出网站的全面活动。</p><p><code>PV</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.core;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.deploy.master.Master;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestPV</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        sparkConf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"pv"</span>);</span><br><span class="line">        JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; lineRDD = context.textFile(<span class="string">"./data/pvuvdata"</span>);</span><br><span class="line">        <span class="comment">/**求每个页面 PV</span></span><br><span class="line"><span class="comment">         * 文件每一行的内容</span></span><br><span class="line"><span class="comment">115.77.12.186安徽2017-10-1015120123070845641635304912151098www.suning.com</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//方法一：mapToPair().reduceByKey().foeach()</span></span><br><span class="line">     lineRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">      <span class="meta">@Override</span></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               String[] str = line.split(<span class="string">"\t"</span>);</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(str[<span class="number">5</span>],<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">       &#125;).reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                   <span class="keyword">return</span> v1 + v2;</span><br><span class="line">           &#125;</span><br><span class="line">      &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               System.out.println(tuple2);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line">          <span class="comment">//方法二:  mapToPair().groupByKey().foeach()</span></span><br><span class="line">     JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupByKeyRDD =                               lineRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">          <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               String[] str = line.split(<span class="string">"\t"</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(str[<span class="number">5</span>], <span class="number">1</span>);</span><br><span class="line">           &#125;</span><br><span class="line">      &#125;).groupByKey();</span><br><span class="line"></span><br><span class="line">    groupByKeyRDD.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">               Iterator&lt;Integer&gt; iterator = tuple2._2.iterator();</span><br><span class="line">                <span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">                   count++;</span><br><span class="line">               &#125;</span><br><span class="line">               System.out.println(<span class="string">"url : "</span> + tuple2._1 + <span class="string">" value: "</span> + count);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);  </span><br><span class="line">           <span class="comment">//方法三： mapToPair().countByKey()--&gt;对map遍历</span></span><br><span class="line">    Map&lt;String, Object&gt; map = </span><br><span class="line">        lineRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">               String[] str = line.split(<span class="string">"\t"</span>);</span><br><span class="line">               <span class="comment">// url,1</span></span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(str[<span class="number">5</span>], <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">       &#125;).countByKey();</span><br><span class="line">       <span class="keyword">for</span> (String key :map.keySet())&#123;</span><br><span class="line">           System.out.println(<span class="string">"key : "</span> + key  + <span class="string">"   value :"</span> + map.get(key) );</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>UV</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.core;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.broadcast.Broadcast;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.deploy.master.Master;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.scheduler.DAGScheduler;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestUV</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span>  <span class="keyword">int</span> sum = <span class="number">10</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        sparkConf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"uv"</span>);</span><br><span class="line">        JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br><span class="line">        </span><br><span class="line">        JavaRDD&lt;String&gt; lineRDD = context.textFile(<span class="string">"./data/pvuvdata"</span>);</span><br><span class="line">         <span class="comment">/**求每个网站 UV： 用IP唯一标识用户 ，注意去重</span></span><br><span class="line"><span class="comment">         * 文件每一行的内容</span></span><br><span class="line"><span class="comment">115.77.12.186安徽2017-10-1015120123070845641635304912151098www.suning.com</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//方法一：mapToPair().groupByKey().foreach()</span></span><br><span class="line">        JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; rdd1 = </span><br><span class="line">            lineRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String url = s.split(<span class="string">"\t"</span>)[<span class="number">5</span>];</span><br><span class="line">                String ip = s.split(<span class="string">"\t"</span>)[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(url, ip);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).groupByKey();</span><br><span class="line"></span><br><span class="line">        rdd1.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Iterable&lt;String&gt;&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Iterable&lt;String&gt;&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">//set:无序，不可重复，所以它可以自动去重</span></span><br><span class="line">                HashSet&lt;Object&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">                Iterator&lt;String&gt; iterator = tuple2._2.iterator();</span><br><span class="line">                <span class="keyword">while</span>(iterator.hasNext())&#123;</span><br><span class="line">                    set.add(iterator.next());</span><br><span class="line">                &#125;</span><br><span class="line">                System.out.println(tuple2._1  + <span class="string">" : "</span> + set.size());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//方法二：mapToPair().ditinct().countByKey()--&gt;遍历map</span></span><br><span class="line">        Map&lt;String, Object&gt; map = </span><br><span class="line">            lineRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">                String url = s.split(<span class="string">"\t"</span>)[<span class="number">5</span>];</span><br><span class="line">                String ip = s.split(<span class="string">"\t"</span>)[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(url, ip);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).distinct().countByKey();</span><br><span class="line">        <span class="keyword">for</span> (String key : map.keySet()) &#123;</span><br><span class="line">            System.out.println(<span class="string">"key : "</span> + key + <span class="string">"   value :"</span> + map.get(key));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.sxt.scala.core</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PVUV</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"pvuv"</span>)</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> records = sc.textFile(<span class="string">"data/pvuvdata"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//pv</span></span><br><span class="line">        records.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> fields = x.split(<span class="string">"\t"</span>)</span><br><span class="line">            (fields(<span class="number">5</span>), <span class="number">1</span>)</span><br><span class="line">        &#125;).reduceByKey(_ + _).sortBy(_._2).foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//uv</span></span><br><span class="line">        <span class="keyword">val</span> result: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = records.map(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> fields = x.split(<span class="string">"\t"</span>)</span><br><span class="line">            (fields(<span class="number">5</span>), fields(<span class="number">0</span>))</span><br><span class="line">        &#125;).groupByKey()</span><br><span class="line"></span><br><span class="line">        result.foreach(x =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> key = x._1</span><br><span class="line">            <span class="keyword">val</span> iteratable = x._2</span><br><span class="line"></span><br><span class="line">            println(<span class="string">"key : "</span> + key + <span class="string">" size : "</span> + iteratable.toSet.size)</span><br><span class="line">        &#125;)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.scala.core</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PVUV2</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">       <span class="comment">// "local[4]"  指定本地以及使用的核数     </span></span><br><span class="line">        conf.setMaster(<span class="string">"local[4]"</span>).setAppName(<span class="string">"pvuv"</span>)</span><br><span class="line">        <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">        <span class="keyword">val</span> linesRDD = context.textFile(<span class="string">"data/pvuvdata"</span>)</span><br><span class="line">        <span class="comment">//pv</span></span><br><span class="line">        <span class="comment">//(www.jd.com,1000)</span></span><br><span class="line">        linesRDD.map(x=&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> fields: <span class="type">Array</span>[<span class="type">String</span>] = x.split(<span class="string">"\t"</span>)</span><br><span class="line">            (fields(<span class="number">5</span>),<span class="number">1</span>)</span><br><span class="line">        &#125;).reduceByKey((x,y)=&gt;&#123;x + y&#125;).sortBy(_._2,<span class="literal">false</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">        <span class="comment">//uv</span></span><br><span class="line">        <span class="comment">//(www.taobao.com,10.20.30.18)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = linesRDD.map(x=&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> fields = x.split(<span class="string">"\t"</span>)</span><br><span class="line">            (fields(<span class="number">5</span>),fields(<span class="number">0</span>))</span><br><span class="line">        &#125;).groupByKey()</span><br><span class="line"></span><br><span class="line">        groupRDD.map(x=&gt;&#123;</span><br><span class="line">            <span class="keyword">val</span> key  = x._1</span><br><span class="line">            <span class="keyword">val</span> size = x._2.toSet.size</span><br><span class="line">            (key,size)</span><br><span class="line">        &#125;).sortBy(_._2,<span class="literal">false</span>)foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="案例二：二次排序"><a href="#案例二：二次排序" class="headerlink" title="案例二：二次排序"></a>案例二：二次排序</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.core;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SecondSortKey</span>  <span class="keyword">implements</span> <span class="title">Serializable</span> , <span class="title">Comparable</span>&lt;<span class="title">SecondSortKey</span>&gt;</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> first;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> second;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getFirst</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> first;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setFirst</span><span class="params">(<span class="keyword">int</span> first)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getSecond</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSecond</span><span class="params">(<span class="keyword">int</span> second)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.second = second;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SecondSortKey</span><span class="params">(<span class="keyword">int</span> first, <span class="keyword">int</span> second)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.first = first;</span><br><span class="line">        <span class="keyword">this</span>.second = second;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(SecondSortKey o1)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (getFirst() - o1.getFirst() == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// 5     6</span></span><br><span class="line">            <span class="comment">// this  &lt; o1</span></span><br><span class="line">            <span class="comment">// 6   5</span></span><br><span class="line">            <span class="comment">// this &gt; o1</span></span><br><span class="line">            <span class="keyword">return</span> o1.getSecond() - getSecond();</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*if(getSecond() - o1.getSecond() == 0) &#123;</span></span><br><span class="line"><span class="comment">                return getThree() - o1.getThree();</span></span><br><span class="line"><span class="comment">            &#125;else &#123;</span></span><br><span class="line"><span class="comment">                return getSecond() - o1.getSecond();</span></span><br><span class="line"><span class="comment">            &#125;*/</span></span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span>  o1.getFirst() - getFirst();</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bd.java.core;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SecondKeyTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        sparkConf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"SecondarySortTest"</span>);</span><br><span class="line">        <span class="keyword">final</span> JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; secondRDD = sc.textFile(<span class="string">"./data/secondSort.txt"</span>);</span><br><span class="line">        <span class="comment">/*文件内容格式</span></span><br><span class="line"><span class="comment">         * 1 3</span></span><br><span class="line"><span class="comment">         * 1 4</span></span><br><span class="line"><span class="comment">         * 2 3</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="comment">//maptoPair --&gt;sortByKey --&gt;foreach</span></span><br><span class="line">        JavaPairRDD&lt;SecondSortKey, String&gt; secRDD = </span><br><span class="line">            secondRDD.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, SecondSortKey, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> Tuple2&lt;SecondSortKey, String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] fields = line.split(<span class="string">" "</span>);</span><br><span class="line">                SecondSortKey secondSortKey =<span class="keyword">new</span> SecondSortKey(</span><br><span class="line">                    Integer.parseInt(fields[<span class="number">0</span>]),</span><br><span class="line">                    Integer.parseInt(fields[<span class="number">1</span>])</span><br><span class="line">                );     </span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(secondSortKey, line);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">      secRDD.sortByKey().foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;SecondSortKey, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;SecondSortKey, String&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(tuple2._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="案例三：分组取topN"><a href="#案例三：分组取topN" class="headerlink" title="案例三：分组取topN"></a>案例三：分组取topN</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.sxt.java.core;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopN</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf;</span><br><span class="line">        conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[5]"</span>).setAppName(<span class="string">"TopOps"</span>);</span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//hdfs://shsxt/wc.txt</span></span><br><span class="line">        JavaRDD&lt;String&gt; linesRDD = sc.textFile(<span class="string">"data/scores.txt"</span>,<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> List n = <span class="keyword">new</span> ArrayList();</span><br><span class="line"><span class="comment">//      linesRDD.count();</span></span><br><span class="line"><span class="comment">/*a 86</span></span><br><span class="line"><span class="comment">  a 58</span></span><br><span class="line"><span class="comment">  b 78</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; pairRDD = </span><br><span class="line">        linesRDD.mapToPair( <span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                 <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">                 <span class="meta">@Override</span></span><br><span class="line">                 <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String str)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String[] splited = str.split(<span class="string">" "</span>);</span><br><span class="line">                        String clazzName = splited[<span class="number">0</span>];</span><br><span class="line">                        Integer score = Integer.valueOf(splited[<span class="number">1</span>]);</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(clazzName, score);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">     JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupByKeyRDD =pairRDD.groupByKey();</span><br><span class="line">     groupByKeyRDD.foreach( <span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">       <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; tuple)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                        String clazzName = tuple._1;</span><br><span class="line">                        Iterator&lt;Integer&gt; iterator = tuple._2.iterator();</span><br><span class="line">                        System.out.println(tuple);</span><br><span class="line">                        <span class="comment">//取前三：大的放前，小的后移</span></span><br><span class="line">                        Integer[] top3 = <span class="keyword">new</span> Integer[<span class="number">3</span>];</span><br><span class="line">                        <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                            Integer score = iterator.next();</span><br><span class="line">                            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; top3.length; i++) &#123;</span><br><span class="line">                                <span class="keyword">if</span> (top3[i] == <span class="keyword">null</span>) &#123;</span><br><span class="line">                                    top3[i] = score;</span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (score &gt; top3[i]) &#123;</span><br><span class="line">                                    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">2</span>; j &gt; i; j--) &#123;</span><br><span class="line">                                        top3[j] = top3[j - <span class="number">1</span>];</span><br><span class="line">                                    &#125;</span><br><span class="line">                                    top3[i] = score;</span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        System.out.println(<span class="string">"class Name:"</span> + clazzName);</span><br><span class="line">                        <span class="keyword">for</span> (Integer sscore : top3) &#123;</span><br><span class="line">                            System.out.println(sscore);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//     groupByKeyRDD.foreach(new VoidFunction&lt;Tuple2&lt;String, Iterable&lt;Integer&gt;&gt;&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//     public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; tuple2) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                String key  = tuple2._1;</span></span><br><span class="line"><span class="comment">//                Iterator&lt;Integer&gt; iterator = tuple2._2.iterator();</span></span><br><span class="line"><span class="comment">//                List list = IteratorUtils.toList(iterator);</span></span><br><span class="line"><span class="comment">//                Collections.sort(list);//正序</span></span><br><span class="line"><span class="comment">//                for(int i=0;i&lt;Math.min(3,list.size());i++)&#123;</span></span><br><span class="line"><span class="comment">//                    // list.size = 3  list.get(2)   要去最后三个</span></span><br><span class="line"><span class="comment">//                    System.out.println(key + " " +  list.get(list.size()-i-1));</span></span><br><span class="line"><span class="comment">//                &#125;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;);</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-----------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.sxt.scala.core</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"> * contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"> * this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"> * The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"> * (the "License"); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"> * the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Computes the PageRank of URLs from an input file. Input file should</span></span><br><span class="line"><span class="comment"> * be in format of:</span></span><br><span class="line"><span class="comment"> * URL         neighbor URL</span></span><br><span class="line"><span class="comment"> * URL         neighbor URL</span></span><br><span class="line"><span class="comment"> * URL         neighbor URL</span></span><br><span class="line"><span class="comment"> * ...</span></span><br><span class="line"><span class="comment"> * where URL and their neighbors are separated by space(s).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkPageRank</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//    if (args.length &lt; 1) &#123;</span></span><br><span class="line">    <span class="comment">//      System.err.println("Usage: SparkPageRank &lt;file&gt; &lt;iter&gt;")</span></span><br><span class="line">    <span class="comment">//      System.exit(1)</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"PageRank"</span>).setMaster(<span class="string">"local[1]"</span>)</span><br><span class="line">    <span class="keyword">val</span> iters = <span class="number">20</span>;</span><br><span class="line">    <span class="comment">//    val iters = if (args.length &gt; 0) args(1).toInt else 10</span></span><br><span class="line">    <span class="keyword">val</span> ctx = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> lines = ctx.textFile(<span class="string">"./page.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据边关系数据生成 邻接表 如：(1,(2,3,4,5)) (2,(1,5))..</span></span><br><span class="line">    <span class="keyword">val</span> links = lines.map&#123; s =&gt;</span><br><span class="line">      <span class="keyword">val</span> parts = s.split(<span class="string">"\\s+"</span>)<span class="comment">//匹配任意空白字符</span></span><br><span class="line">      (parts(<span class="number">0</span>), parts(<span class="number">1</span>))</span><br><span class="line">    &#125;.distinct().groupByKey().cache()</span><br><span class="line">    links.foreach(println)</span><br><span class="line">    <span class="comment">// (1,1.0) (2,1.0)..初始化</span></span><br><span class="line">    <span class="keyword">var</span> ranks = links.mapValues(v =&gt; <span class="number">1.0</span>)</span><br><span class="line">    ranks.foreach(println)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to iters) &#123;</span><br><span class="line">      <span class="comment">// (1,((2,3,4,5), 1.0))</span></span><br><span class="line">      <span class="keyword">val</span> contribs = links.join(ranks).values.flatMap&#123; <span class="keyword">case</span> (urls, rank) =&gt;</span><br><span class="line">        <span class="keyword">val</span> size = urls.size</span><br><span class="line">        urls.map(url =&gt; (url, rank / size))</span><br><span class="line">      &#125;</span><br><span class="line">      ranks = contribs.reduceByKey(_ + _).mapValues(<span class="number">0.15</span> + <span class="number">0.85</span> * _)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> output = ranks.collect()</span><br><span class="line"><span class="comment">//    output.foreach(tup =&gt; println(tup._1 + " has rank: " + tup._2 + "."))</span></span><br><span class="line">    ctx.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Spark-Shell"><a href="#Spark-Shell" class="headerlink" title="Spark Shell"></a>Spark Shell</h1><h2 id="1、-概念："><a href="#1、-概念：" class="headerlink" title="1、 概念："></a>1、 概念：</h2><p>SparkShell 是 Spark 自带的一个快速原型开发工具，也可以说是Spark 的 scala REPL(Read-Eval-Print-Loop),即交互式 shell。<strong>支持使用 scala 语言来进行 Spark 的交互式编程。</strong></p><h2 id="2、使用"><a href="#2、使用" class="headerlink" title="2、使用:"></a>2、使用:</h2><p>（配置从HDFS上获取文件）</p><h3 id="1-启动HDFS，上传文件"><a href="#1-启动HDFS，上传文件" class="headerlink" title="(1)启动HDFS，上传文件"></a>(1)启动HDFS，上传文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start   (3台)</span><br><span class="line">start-all.sh        (任一台)</span><br><span class="line">hadoop dfs -put test.txt /   (任一台：将test.txt文件上传至hdfs的根目录)</span><br></pre></td></tr></table></figure><h3 id="2-启动standalone集群：在-sbin路径下"><a href="#2-启动standalone集群：在-sbin路径下" class="headerlink" title="(2)启动standalone集群：在/sbin路径下"></a>(2)启动standalone集群：在/sbin路径下</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h3 id="3-在客户端上启动-spark-shell"><a href="#3-在客户端上启动-spark-shell" class="headerlink" title="(3)在客户端上启动 spark-shell:"></a>(3)在客户端上启动 spark-shell:</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-shell    (local模式：在控制台打印)</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-shell --master spark://node00:7077   （client模式：控制台无打印，可通过web页面查看）</span><br></pre></td></tr></table></figure><h3 id="（4）运行-wordcount："><a href="#（4）运行-wordcount：" class="headerlink" title="（4）运行 wordcount："></a>（4）运行 wordcount：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;sc.textFile(<span class="string">"hdfs://Sunrise/test.txt"</span>)</span><br><span class="line">.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).foreach(println)</span><br></pre></td></tr></table></figure><h2 id="3、参数解释：spark-shell"><a href="#3、参数解释：spark-shell" class="headerlink" title="3、参数解释：spark-shell"></a>3、参数解释：spark-shell</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 bin]# ./spark-shell -h</span><br><span class="line">Usage: ./bin/spark-shell [options]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally                                         ("client") or on one of the worker machines inside the                               cluster ("cluster")(Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application's main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated list of local jars to include on the                                 driver and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to                                 include on the driver and executor classpaths. Will                                 search the local maven repo, then maven central and                                 any additional remote repositories given by --                                       repositories. The format for thecoordinates should be                               groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude                               while  resolving the dependencies provided in --                                     packages to avoid dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories                               to search for the maven coordinates given with --                                   packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to                                 place on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the                                   working directory of each executor.</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If                               not specified, this will look for conf/spark-                                       defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note                                 that jars added with --jars are automatically included                               in the classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit</span><br><span class="line">  --verbose, -v               Print additional debug output</span><br><span class="line">  --version,                  Print the version of current Spark</span><br><span class="line"></span><br><span class="line"> Spark standalone with cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Cores for driver (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN                                   mode, or all available cores on the worker in                                       standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster                                 mode (Default: 1).</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: "default").</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into                               the working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for                               the principal specified above. This keytab will be                                   copied to the node running the Application Master via                               the Secure Distributed Cache, for renewing the login                                 tickets and the delegation tokens periodically.</span><br></pre></td></tr></table></figure><h1 id="SparkUI"><a href="#SparkUI" class="headerlink" title="SparkUI"></a>SparkUI</h1><h2 id="1、SparkUI-界面介绍"><a href="#1、SparkUI-界面介绍" class="headerlink" title="1、SparkUI 界面介绍"></a>1、SparkUI 界面介绍</h2><p>提交spark：在/bin路径下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master spark://node00:7077 --name sp --class org.apache.spark.example.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar --100</span><br></pre></td></tr></table></figure><p><code>注意</code>：</p><p>–name  指定Application的名称</p><p>指代的参数在代码中也有配置，所以对同一参数均有配置时，以<code>代码中</code>的配置为主</p><p>浏览器页面访问：node00:8080  </p><p><code>页面显示</code></p><blockquote><p>点击：Application ID列中的值  →  Application  Detail UI  会显示查看不了事件日志</p><h3 id="Event-logging-is-not-enabled"><a href="#Event-logging-is-not-enabled" class="headerlink" title="Event logging is not enabled"></a>Event logging is not enabled</h3><p>No event logs were found for this application! To <a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">enable event logging</a>, set spark.eventLog.enabled to true and spark.eventLog.dir to the directory to which your event logs are written.</p></blockquote><h2 id="2、配置-historyServer"><a href="#2、配置-historyServer" class="headerlink" title="2、配置 historyServer"></a>2、配置 historyServer</h2><ul><li>临时配置（对本次提交的应用程序起作用）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./spark-shell --master spark://node00:7077</span><br><span class="line">--name myapp1</span><br><span class="line">--conf spark.eventLog.enabled=true</span><br><span class="line">--conf spark.eventLog.dir=hdfs://Sunrise/spark/test</span><br></pre></td></tr></table></figure><p>停止程序，在 Web UI 中 Completed Applications 对应的ApplicationID 中能查看 history。</p><ul><li>spark-default.conf 配置文件中配置 HistoryServer，对所有提交的Application 都起作用</li></ul><p>在 客 户 端 节 点 ， 进 入 ../spark-1.6.0/conf/spark-defaults.conf 最后加入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#  开启记录事件日志的功能</span><br><span class="line">spark.eventLog.enabled true</span><br><span class="line">#  设置事件日志存储的目录</span><br><span class="line">spark.eventLog.dir hdfs://Sunrise/spark/test</span><br><span class="line">#  设置 HistoryServer  加载事件日志的位置</span><br><span class="line">spark.history.fs.logDirectory hdfs://Sunrise/spark/test</span><br><span class="line"># 日志优化选项, 压缩日志</span><br><span class="line">spark.eventLog.compress true</span><br></pre></td></tr></table></figure><ul><li>发送到其他节点（如果节点上没有以上配置，就不会有对应的作用）</li></ul><p>在HDFS上一定要先存在路径/spark/test</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">hdfs集群一定要启动</span></span><br><span class="line">hadoop dfs -mkdir -p /spark/test</span><br></pre></td></tr></table></figure><p><code>页面显示</code></p><blockquote><p>点击：Application ID列中的值  →  Application  Detail UI  就会有显示内容</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0d8nkvqzrj310k06zmxl.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0d8r7qsrzj30q50apwgd.jpg" alt=""></p><ul><li>启动 HistoryServer：(在/sbin路径下)</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-history-server.sh</span><br></pre></td></tr></table></figure><p>（Sunrise在这里是HDFS集群的名字）</p><p>访问 HistoryServer：node00:18080,</p><p>之后所有提交的应用程序运行状况都会被记录。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0d8t4eeftj30xj079go4.jpg" alt=""></p><h1 id="Master-HA"><a href="#Master-HA" class="headerlink" title="Master HA"></a>Master HA</h1><h2 id="1、Master-的高可用原理"><a href="#1、Master-的高可用原理" class="headerlink" title="1、Master 的高可用原理"></a>1、Master 的高可用原理</h2><p>Standalone 集群只有一个 Master，如果 Master 挂了就无法提交应用程序，但不影响正在执行的worker。</p><p>给 Master 进行高可用配置可以使用<strong>fileSystem</strong>(文件系统)和 <strong>zookeeper</strong>（分布式协调服务）。</p><ul><li><p>fileSystem 只有存储功能，可以存储 Master 的元数据信息，用fileSystem 搭建的 Master 高可用，在 Master 失败时，需要我们手动启动另外的备用 Master，这种方式不推荐使用。</p></li><li><p>zookeeper 有选举和存储功能，可以存储 Master 的元数据信息，使用zookeeper 搭建的 Master 高可用，当 Master 挂掉时，备用的 Master会自动切换，推荐使用这种方式搭建 Master 的 HA。</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0d8unf1yfj30gn0beac3.jpg" alt=""></p><h2 id="2、Master-高可用搭建"><a href="#2、Master-高可用搭建" class="headerlink" title="2、Master 高可用搭建"></a>2、Master 高可用搭建</h2><h3 id="1-配置主-Master：在-Spark-Master-节点上"><a href="#1-配置主-Master：在-Spark-Master-节点上" class="headerlink" title="1) 配置主 Master：在 Spark Master 节点上"></a>1) 配置主 Master：在 Spark Master 节点上</h3><p>编辑配置文件 .spark1.6.0/conf/ spark-env.sh</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"</span></span><br><span class="line"><span class="string">-Dspark.deploy.recoveryMode=ZOOKEEPER</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.url=node00:2181,node01:2181,node02:2181</span></span><br><span class="line"><span class="string">-Dspark.deploy.zookeeper.dir=/sparkmaster"</span></span><br></pre></td></tr></table></figure><h3 id="2-发送到其他-worker-节点上"><a href="#2-发送到其他-worker-节点上" class="headerlink" title="2) 发送到其他 worker 节点上"></a>2) 发送到其他 worker 节点上</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node01:`pwd`</span><br><span class="line">.......</span><br></pre></td></tr></table></figure><h3 id="3-配置备用-Maste：找一台节点（非主-Master-节点-node01）"><a href="#3-配置备用-Maste：找一台节点（非主-Master-节点-node01）" class="headerlink" title="3)配置备用 Maste：找一台节点（非主 Master 节点:node01）"></a>3)配置备用 Maste：找一台节点（非主 Master 节点:node01）</h3><p> 配置备用 Master,修改spark-env.sh 配置节点上的 MasterIP</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_MASTER_IP=192.168.198.130</span><br></pre></td></tr></table></figure><h3 id="4-启动-zookeeper-集群：启动spark集群之前"><a href="#4-启动-zookeeper-集群：启动spark集群之前" class="headerlink" title="4) 启动 zookeeper 集群：启动spark集群之前"></a>4) 启动 zookeeper 集群：启动spark集群之前</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="5-启动-spark-Standalone-集群，启动备用-Master"><a href="#5-启动-spark-Standalone-集群，启动备用-Master" class="headerlink" title="5) 启动 spark Standalone 集群，启动备用 Master"></a>5) 启动 spark Standalone 集群，启动备用 Master</h3><p>(在/sbin路径下)</p><p>node00:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>node01:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-master.sh</span><br></pre></td></tr></table></figure><h3 id="6-打开主-Master-和备用-Master-WebUI-页面，观察状态"><a href="#6-打开主-Master-和备用-Master-WebUI-页面，观察状态" class="headerlink" title="6) 打开主 Master 和备用 Master WebUI 页面，观察状态"></a>6) 打开主 Master 和备用 Master WebUI 页面，观察状态</h3><p>主master：</p><blockquote><h3 id="1-6-0-Spark-Master-at-spark-192-168-198-128-7077"><a href="#1-6-0-Spark-Master-at-spark-192-168-198-128-7077" class="headerlink" title=" 1.6.0 Spark Master at spark://192.168.198.128:7077"></a><a href="http://node00:8080/" target="_blank" rel="noopener"><img src="http://node00:8080/static/spark-logo-77x50px-hd.png" alt="img"><span class="img-alt">img</span> 1.6.0 </a>Spark Master at spark://192.168.198.128:7077</h3><ul><li><strong>URL:</strong> spark://192.168.198.128:7077</li><li><strong>REST URL:</strong> spark://192.168.198.128:6066 (cluster mode)</li><li><strong>Alive Workers:</strong> 2</li><li><strong>Cores in use:</strong> 2 Total, 0 Used</li><li><strong>Memory in use:</strong> 2.0 GB Total, 0.0 B Used</li><li><strong>Applications:</strong> 0 Running, 6 Completed</li><li><strong>Drivers:</strong> 0 Running, 0 Completed</li><li><strong>Status:</strong> ALIVE</li></ul></blockquote><p>备用master：</p><blockquote><h3 id="1-6-0-Spark-Master-at-spark-192-168-198-130-7077"><a href="#1-6-0-Spark-Master-at-spark-192-168-198-130-7077" class="headerlink" title=" 1.6.0 Spark Master at spark://192.168.198.130:7077"></a><a href="http://node01:8080/" target="_blank" rel="noopener"><img src="http://node01:8080/static/spark-logo-77x50px-hd.png" alt="img"><span class="img-alt">img</span> 1.6.0 </a>Spark Master at spark://192.168.198.130:7077</h3><ul><li><strong>URL:</strong> spark://192.168.198.130:7077</li><li><strong>REST URL:</strong> spark://192.168.198.130:6066 (cluster mode)</li><li><strong>Alive Workers:</strong> 0</li><li><strong>Cores in use:</strong> 0 Total, 0 Used</li><li><strong>Memory in use:</strong> 0.0 B Total, 0.0 B Used</li><li><strong>Applications:</strong> 0 Running, 0 Completed</li><li><strong>Drivers:</strong> 0 Running, 0 Completed</li><li><strong>Status:</strong> ALIVE</li></ul></blockquote><h2 id="3、注意点"><a href="#3、注意点" class="headerlink" title="3、注意点"></a>3、注意点</h2><ul><li>主备切换过程中不能提交 Application。</li><li>主备切换过程中不影响已经在集群中运行的 Application。因为Spark 是粗粒度资源调度。</li></ul><ol><li>测试验证<br> 提交 SparkPi 程序，kill 主 Master 观察现象。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master spark://node00:7077,node01:7077 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 10000</span><br></pre></td></tr></table></figure><p><code>显示：</code></p><blockquote><p>主备切换有时差，因为也不急</p><p>程序不受影响</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算框架 </tag>
            
            <tag> 参数解释 </tag>
            
            <tag> Spark shell </tag>
            
            <tag> sparkcore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习（二）</title>
      <link href="/2019/02/17/Spark%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/"/>
      <url>/2019/02/17/Spark%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、控制算子"><a href="#一、控制算子" class="headerlink" title="一、控制算子"></a>一、控制算子</h1><h2 id="1、概念："><a href="#1、概念：" class="headerlink" title="1、概念："></a>1、概念：</h2><ul><li><p>控制算子有三种，cache、persist、checkpoint</p></li><li><p>以上算子都可以将RDD 持久化，持久化的单位是 partition。</p></li><li><p>cache 和 persist 都是懒 执行的。</p></li><li><p>必须有一个 action 类算子触发执行。</p></li><li><p>cache 和 persist 算子的返回值可赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了</p></li><li><p>checkpoint 算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系（所有父RDD）。</p></li><li><code>错误：</code>rdd.cache().count() 返回的不是持久化的 RDD，而是一个数值了。</li></ul><h2 id="2、详解"><a href="#2、详解" class="headerlink" title="2、详解"></a>2、详解</h2><blockquote><p>:one:<strong>​ cache</strong><br>默认将 RDD 的数据持久化到内存中。cache 是懒执行。</p><ul><li><code>注意</code>：</li></ul><p>chche () =persist()=persist(StorageLevel.Memory_Only)</p></blockquote><blockquote><p>:two: <strong>persist</strong> </p><p>支持指定持久化级别</p><p>useOffHeap  使用堆外内存</p><p>disk、memory、offheap、deserialized（不序列化）、replication（副本数，默认为1）</p><p>序列化：压缩数据（节省空间，使用数据时要反序列化，会额外消耗CPU性能）</p><p>none 、disk_only、disk_only_2、memeory_only 、memeory_only _ser 、 memory_and_disk 、 memory_and_disk_2</p></blockquote><blockquote><p>:three: <strong>checkpoint</strong>  </p><p>checkpoint 将 RDD 持久化到磁盘，还可以切断 RDD 之间的依赖关系。</p><ul><li>checkpoint 的执行原理：</li></ul><ol><li>当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。</li><li>当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的RDD 做一个标记。</li><li>Spark 框架会自动启动一个新的 job，重新计算这个 RDD 的数据，将数据持久化到 HDFS 上。</li></ol><ul><li>优化：</li></ul><p>对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS上就可以，省去了重新计算这一步。</p></blockquote><p>持久化级别：如下</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g098pwwdc9j30f70aztbu.jpg" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cocnf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppname(<span class="string">"count"</span>)</span><br><span class="line"><span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置CP在HDFS上的路径</span></span><br><span class="line">context.setCheckPointDir(<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lineADD = context.textFile(<span class="string">"./countword.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> time1 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> c =  lineADD.count()</span><br><span class="line"><span class="keyword">val</span> time2 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> t1 = time2 - time1</span><br><span class="line"></span><br><span class="line"><span class="comment">//做缓存(persisit（m_o）)</span></span><br><span class="line">linelineADD = lineADD.cache()</span><br><span class="line"><span class="comment">//做持久化</span></span><br><span class="line">lineADD.persisit(<span class="type">StorageLevel</span>.memory_only)</span><br><span class="line"><span class="comment">//checkpoint 容错,最好还有cache</span></span><br><span class="line">lineADD.checkpoint()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> time3 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> c =  lineADD.count()</span><br><span class="line"><span class="keyword">val</span> time4 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> t2 = time4 - time3</span><br><span class="line"></span><br><span class="line"><span class="comment">//t1 远大于 t2</span></span><br></pre></td></tr></table></figure><h1 id="二、算子补充"><a href="#二、算子补充" class="headerlink" title="二、算子补充"></a>二、算子补充</h1><h2 id="transformation转换算子"><a href="#transformation转换算子" class="headerlink" title="transformation转换算子"></a>transformation转换算子</h2><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; join,leftOuterJoin,rightOuterJoin,fullOuterJoin</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>作用在 K,V 格式的 RDD 上。根据 K 进行连接，对（K,V）join(K,W)返回（K,(V,W)）</p><ul><li>join 后的分区数与父 RDD 分区数多的那一个相同 </li></ul></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; union</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>合并两个数据集。两个数据集的类型要一致。</p><ul><li>返回新的 RDD 的分区数是合并 RDD 分区数的总和。</li></ul></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; intersection</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>取两个数据集的交集</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  subtract</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>取两个数据集的差集</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  mapPartition</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>与 map 类似，遍历的单位是每个 partition 上的数据。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  distinct(map+reduceByKey+map)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; cogroup</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>当调用类型（K,V）和（K，W）的数据上时，返回一个数据集（K，（Iterable<v>,Iterable<w>））</w></v></p></blockquote><h2 id="action触发算子"><a href="#action触发算子" class="headerlink" title="action触发算子"></a>action触发算子</h2><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  foreachPartition</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>遍历的数据是每个 partition 的数据。</p></blockquote><h1 id="三、集群搭建及测试"><a href="#三、集群搭建及测试" class="headerlink" title="三、集群搭建及测试"></a>三、集群搭建及测试</h1><h2 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a><strong>Standalone</strong></h2><h3 id="1、下载安装包、解压"><a href="#1、下载安装包、解压" class="headerlink" title="1、下载安装包、解压"></a>1、下载安装包、解压</h3><p><a href="https://archive.apache.org/dist/spark/" target="_blank" rel="noopener">Spark历史版本下载</a></p><p><code>注意</code>： 与Hadoop的版本保持对应。</p><p>此处使用： <a href="https://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz" target="_blank" rel="noopener">spark-1.6.0-bin-hadoop2.6.tgz</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zvxf spark-1.6.0-bin-hadoop2.6.tgz</span><br></pre></td></tr></table></figure><h3 id="2、改名"><a href="#2、改名" class="headerlink" title="2、改名"></a>2、改名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv spark-1.6.0-bin-hadoop2.6 spark-1.6.0</span><br></pre></td></tr></table></figure><h3 id="3、修改slaves"><a href="#3、修改slaves" class="headerlink" title="3、修改slaves"></a>3、修改slaves</h3><p>进入安装包的conf目录下，修改slaves.template文件，添加从节点。并保存。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>备份</span><br><span class="line">cp slaves.template slaves</span><br><span class="line">vim slaves</span><br></pre></td></tr></table></figure><blockquote><p>常驻进程：master、worker</p></blockquote><p>配置slaves（与worker对应）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node01</span><br><span class="line">node02</span><br></pre></td></tr></table></figure><h3 id="4、修改-spark-env-sh"><a href="#4、修改-spark-env-sh" class="headerlink" title="4、修改 spark-env.sh"></a>4、修改 spark-env.sh</h3><p>改名（备份）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp spark-env.sh.template spark-env.sh</span><br></pre></td></tr></table></figure><p>配置spark-env.sh（注意与虚拟机实际配置对应）</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#locally</span></span><br><span class="line"><span class="comment">#cluster</span></span><br><span class="line"><span class="comment">#YARN client</span></span><br><span class="line"><span class="comment">#standalone deploy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#配置 java_home 路径</span></span><br><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br><span class="line"><span class="comment">#master 的 ip</span></span><br><span class="line">SPARK_MASTER_IP=192.168.198.128</span><br><span class="line"><span class="comment">#提交任务的端口，默认是 7077</span></span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="comment">#每个 worker 从节点能够支配的 core 的个数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="comment">#每个 worker 从节点能够支配的内存数</span></span><br><span class="line">SPARK_WORKER_MEMORY=1024m</span><br><span class="line"></span><br><span class="line"><span class="comment">#配置yarn</span></span><br><span class="line">HADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5/etc/hadoop</span><br></pre></td></tr></table></figure><h3 id="5、其他节点"><a href="#5、其他节点" class="headerlink" title="5、其他节点"></a>5、其他节点</h3><p>将spark解压文件发送到其他两个节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 soft]# scp -r  spark-1.6.0-bin-hadoop2.6 node01:`pwd`</span><br><span class="line">[root@node00 soft]# scp -r  spark-1.6.0-bin-hadoop2.6 node02:`pwd`</span><br></pre></td></tr></table></figure><p>6、配置环境变量（可不配，因为bin路径中包含start-all ，该命令与hdfs中的命令会冲突）</p><h3 id="7、启动：-node00"><a href="#7、启动：-node00" class="headerlink" title="7、启动：(node00)"></a>7、启动：(node00)</h3><p>在spark的解压文件的/sbin 目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><p>停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./stop-all.sh</span><br></pre></td></tr></table></figure><blockquote><p>显示：</p><p>[root@node00 sbin]# ./start-all.sh<br>starting org.apache.spark.deploy.master.Master, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploymaster.Master-1-node00.out</p><p>node01: starting org.apache.spark.deploy.worker.Worker, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spar.deploy.worker.Worker-1-node01.out</p><p>node02: starting org.apache.spark.deploy.worker.Worker, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spar.deploy.worker.Worker-1-node02.out</p></blockquote><p>查看三台节点的进程</p><p>node00（命令启动的节点）</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@node00 sbin]<span class="comment"># jps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>2343 Master<br>2408 Jps</p></blockquote><p>nose01(配置的从节点)</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@node01 ~]<span class="comment"># jps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>2292 Jps<br>2229 Worker</p></blockquote><p>node02(从节点)</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@node02 ~]<span class="comment"># jps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>6216 Worker<br>6266 Jps</p></blockquote><p><code>注意：</code></p><blockquote><p>Worker在这里不是真正干活的进程，而是相当于Yarn中的NM。</p><p>它是负责管理所在节点资源的、向Master汇报所在节点的信息（如核数、内存数）</p><p>Master： 监控任务、分发任务、回收计算结果 </p></blockquote><h3 id="8、搭建客户端"><a href="#8、搭建客户端" class="headerlink" title="8、搭建客户端"></a>8、搭建客户端</h3><ul><li>将 spark 安装包原封不动的拷贝到一个新的节点上，然后，在新的节点上提交任务即可。</li></ul><p><code>注意：</code><strong>8080</strong> 是Spark WEBUI页面的端口 ； <strong>7077</strong> 是Spark任务提交的端口</p><p>web页面访问：ip:8080</p><ul><li>修改master的WEBUI端口，</li></ul><p>方法一（永久）：通过修改start-master.sh 文件（在/sbin目录下）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim  start-master.sh</span><br></pre></td></tr></table></figure><p>找到文件内容如下的部分：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$SPARK_MASTER_WEBUI_PORT</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>方法二：在 Master 节点上导入临时环境变量，只作用于当前进程，重启就无效了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 sbin]# export SPARK_MASTER_WEBUI_PORT=8080</span><br></pre></td></tr></table></figure><p>删除临时变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 sbin]# export -n SPARK_MASTER_WEBUI_PORT</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w689aee2j311y0fodi8.jpg" alt="standalone"><span class="img-alt">standalone</span></p><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a><strong>Yarn</strong></h2><h3 id="1、步骤"><a href="#1、步骤" class="headerlink" title="1、步骤"></a>1、步骤</h3><p><strong>1。2。3。4。5。8。</strong>同standalone</p><p>不用Master和Worker，所以不用第7步，我们使用的是yarn中的RM和NM</p><h3 id="2、配置"><a href="#2、配置" class="headerlink" title="2、配置"></a>2、配置</h3><p>添加 HADOOP_CONF_DIR配置</p><p><code>（在使用Yarn时，就能找到关于hdfs的所有配置，其中就包括IP 和Port）</code></p><p>方式一：</p><p>编辑spark-env.sh文件</p><p>方式二：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5/etc/hadoop</span><br></pre></td></tr></table></figure><h2 id="测试：求π值"><a href="#测试：求π值" class="headerlink" title="测试：求π值"></a>测试：求π值</h2><p>Pi案例：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1wdcp6sfij309z0a7q2z.jpg" alt=""></p><h3 id="源码案例："><a href="#源码案例：" class="headerlink" title="源码案例："></a><strong>源码案例：</strong></h3><p>路径：在spark解压路径spark-1.6.0-bin-hadoop2.6中</p><p>spark-1.6.0-bin-hadoop2.6/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala</p><p>原理：随机产生无穷多个点落入如上图形中，求落入圆中的概率：<br>$$<br>概率   p = π<em>r</em>r/(2r*2r)=π/4<br>$$</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one or more</span></span><br><span class="line"><span class="comment"> * contributor license agreements.  See the NOTICE file distributed with</span></span><br><span class="line"><span class="comment"> * this work for additional information regarding copyright ownership.</span></span><br><span class="line"><span class="comment"> * The ASF licenses this file to You under the Apache License, Version 2.0</span></span><br><span class="line"><span class="comment"> * (the "License"); you may not use this file except in compliance with</span></span><br><span class="line"><span class="comment"> * the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// scalastyle:off println</span></span><br><span class="line"><span class="keyword">package</span> org.apache.spark.examples</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.math.random</span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Computes an approximation to pi */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkPi</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>(<span class="string">"local"</span>).setAppName(<span class="string">"Spark Pi"</span>)</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">      </span><br><span class="line">   <span class="comment">// args 运行时传入的参数   slices 分区数量 (决定task数量)</span></span><br><span class="line">    <span class="keyword">val</span> slices = <span class="keyword">if</span> (args.length &gt; <span class="number">0</span>) args(<span class="number">0</span>).toInt <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">      </span><br><span class="line">   <span class="comment">//MaxValue 一个无限大的数   n   随机产生的十万个的数</span></span><br><span class="line">    <span class="keyword">val</span> n = math.min(<span class="number">100000</span>L * slices, <span class="type">Int</span>.<span class="type">MaxValue</span>).toInt <span class="comment">// avoid overflow</span></span><br><span class="line">      </span><br><span class="line"> <span class="comment">//parallelize可以获得RDD  ，将1~n个数字放到RDD中</span></span><br><span class="line"> <span class="comment">//val count :[Int] = spark.parallelize(1 until n, slices)     </span></span><br><span class="line">   <span class="keyword">val</span> count = spark.parallelize(<span class="number">1</span> until n, slices).map &#123; i =&gt;</span><br><span class="line">      <span class="keyword">val</span> x = random * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> y = random * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">      <span class="keyword">if</span> (x*x + y*y &lt; <span class="number">1</span>) <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    &#125;.reduce(_ + _)</span><br><span class="line">      </span><br><span class="line">    println(<span class="string">"Pi is roughly "</span> + <span class="number">4.0</span> * count / n)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// scalastyle:on println</span></span><br></pre></td></tr></table></figure><p>所需使用的jar包：spark-examples-1.6.0-hadoop2.6.0.jar</p><p>位置：解压目录的lib路径下</p><p>在任一节点的/bin路径下上执行如下命令：（node00）</p><h3 id="Standalone-提交命令"><a href="#Standalone-提交命令" class="headerlink" title="Standalone 提交命令:"></a><strong>Standalone</strong> 提交命令:</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit    #提交spark </span><br><span class="line">--master spark://node1:7077   #spark主节点的地址和端口 </span><br><span class="line">--class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100   # 指明运行的jar包+路径 和 jar包中执行的包名+类名 100 为传入的参数</span><br><span class="line"></span><br><span class="line">./spark-submit --master spark://node00:7077 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 1000</span><br></pre></td></tr></table></figure><blockquote><p><code>显示：</code></p><p>提交命令的节点（node00主节点）</p><p>会显示执行日志、运算结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Starting task 999.0 <span class="keyword">in</span> stage 0.0 (TID 999, node02, partition 999,PROCESS_LOCAL, 2158 bytes)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 995.0 <span class="keyword">in</span> stage 0.0 (TID 995) <span class="keyword">in</span> 68 ms on node02 (996/1000)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 997.0 <span class="keyword">in</span> stage 0.0 (TID 997) <span class="keyword">in</span> 131 ms on node01 (997/1000)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 996.0 <span class="keyword">in</span> stage 0.0 (TID 996) <span class="keyword">in</span> 147 ms on node01 (998/1000)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 999.0 <span class="keyword">in</span> stage 0.0 (TID 999) <span class="keyword">in</span> 112 ms on node02 (999/1000)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 998.0 <span class="keyword">in</span> stage 0.0 (TID 998) <span class="keyword">in</span> 115 ms on node02 (1000/1000)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished <span class="keyword">in</span> 79.202 s</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:31 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 82.641779 s</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Pi is roughly 3.14148344      <span class="comment">#运算结果</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/metrics/json,null&#125;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/stages/stage/<span class="built_in">kill</span>,null&#125;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/api,null&#125;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 。。。。。。。。。。。。。。。。。。。。。。。。。</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/<span class="built_in">jobs</span>/json,null&#125;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/<span class="built_in">jobs</span>,null&#125;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.198.128:4040</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO cluster.SparkDeploySchedulerBackend: Shutting down all executors</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO cluster.SparkDeploySchedulerBackend: Asking each executor to shut down</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:33 INFO storage.MemoryStore: MemoryStore cleared</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:33 INFO storage.BlockManager: BlockManager stopped</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:33 INFO storage.BlockManagerMaster: BlockManagerMaster stopped</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:33 INFO scheduler.OutputCommitCoordinator<span class="variable">$OutputCommitCoordinatorEndpoint</span>: OutputCommitCoordinator stopped!</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:33 INFO remote.RemoteActorRefProvider<span class="variable">$RemotingTerminator</span>: Shutting down remote daemon.</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:33 INFO remote.RemoteActorRefProvider<span class="variable">$RemotingTerminator</span>: Remote daemon shut down; proceeding with flushing remote transports.</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:34 INFO spark.SparkContext: Successfully stopped SparkContext</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:34 INFO util.ShutdownHookManager: Shutdown hook called</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:34 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7c2019e-10f4-4b31-9308-5a94603de113</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 19/02/13 23:27:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7c2019e-10f4-4b31-9308-5a94603de113/httpd-39b8b4b3-9b80-4247-9c7e-ed6bd2dc389f</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>在命令执行期间：</p><p>在三个节点敲如下命令：jps，会显示：</p><p>node00：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@node00 ~]<span class="comment"># jps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 4903 Jps</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 2343 Master</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 4764 SparkSubmit  <span class="comment">#代表是提交spark的节点 (与主从无关)</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>node01和node02：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@node01 bin]<span class="comment"># jps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 2229 Worker</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 5096 CoarseGrainedExecutorBackend    <span class="comment">#代表是干活的节点 （仅为从节点进程）</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 5167 Jps</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>如果提交命令的节点是从节点（node01），则在该节点上会显示执行日志、运算结果</p><p>则在提交过程中，敲命令：jps  该节点会显示</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@node01 ~]<span class="comment"># jps</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 5298 CoarseGrainedExecutorBackend  <span class="comment">#代表是干活的节点 （仅为从节点进程）</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 2229 Worker</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 5323 Jps</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 5213 SparkSubmit <span class="comment">#代表是提交spark的节点 (与主从无关)</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h3 id="YARN-提交命令："><a href="#YARN-提交命令：" class="headerlink" title="YARN 提交命令："></a><strong>YARN</strong> 提交命令：</h3><p>基于Hadoop ：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">NN</th><th style="text-align:center">DN</th><th style="text-align:center">JN</th><th style="text-align:center">ZKFC</th><th style="text-align:center">ZK</th><th style="text-align:center">RM</th><th style="text-align:center">RM</th></tr></thead><tbody><tr><td style="text-align:center">node00</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center"></td><td style="text-align:center">√</td></tr><tr><td style="text-align:center">node01</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr><tr><td style="text-align:center">node02</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr></tbody></table><p>启动zookeeper ：（3台）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p>启动hdfs ：（1台）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>相当于：Instead use start-dfs.sh and start-yarn.sh</p><p>启动resourcemanager ：(在RM的主节点上启动 ：1台)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure><p>在任一节点的/bin路径下执行：（node01）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master yarn #HADOOP_CONF_DIR配置使得在使用Yarn时能找到hdfs的所有配置，其中就有IP 和Port</span><br><span class="line">--class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br><span class="line"></span><br><span class="line">./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 1000</span><br></pre></td></tr></table></figure><p><code>显示</code></p><blockquote><ul><li><p>执行日志、计算结果会在执行提交命令的节点上显示</p></li><li><p>在命令提交过程中在三台节点上敲命令：jps 会显示</p></li></ul><p>node02：</p><p>[root@node02 ~]# jps<br>3406 DataNode<br>3491 JournalNode<br>1681 QuorumPeerMain<br>4133 CoarseGrainedExecutorBackend    # 真正干活的进程<br>4092 ExecutorLauncher     # 启动executor<br>3585 NodeManager<br>3942 SparkSubmit     #提交spark的进程<br>4217 Jps</p></blockquote><h1 id="四、Standalone-模式两种提交任务方式"><a href="#四、Standalone-模式两种提交任务方式" class="headerlink" title="四、Standalone 模式两种提交任务方式"></a>四、Standalone 模式两种提交任务方式</h1><h2 id="1、Standalone-client-提交任务方式"><a href="#1、Standalone-client-提交任务方式" class="headerlink" title="1、Standalone-client 提交任务方式"></a>1、Standalone-client 提交任务方式</h2><h3 id="1-命令提交"><a href="#1-命令提交" class="headerlink" title="(1)命令提交"></a>(1)命令提交</h3><ul><li>在/sbin路径下：启动standalone</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><ul><li>提交spark</li></ul><p>方式一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master spark://node00:7077</span><br><span class="line">--class org.apache.spark.examples.SparkPi</span><br><span class="line">../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><p>方式二：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master spark://node1:7077</span><br><span class="line">--deploy-mode client</span><br><span class="line">--class org.apache.spark.examples.SparkPi</span><br><span class="line">../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><h3 id="2-执行原理图"><a href="#2-执行原理图" class="headerlink" title="(2)执行原理图"></a>(2)执行原理图</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g09rz47usdj31b40vhq5r.jpg" alt=""></p><h3 id="3-执行流程"><a href="#3-执行流程" class="headerlink" title="(3)执行流程"></a>(3)执行流程</h3><blockquote><ol><li>client 模式提交任务后，会在客户端启动 Driver 进程。</li><li>Driver 会向 Master 申请启动 Application 启动的资源。</li><li>资源申请成功，Driver 端将 task 发送到 worker 端执行。</li><li>worker 将 task 执行结果返回到 Driver 端</li></ol></blockquote><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="(4)总结"></a>(4)总结</h3><blockquote><ul><li><p>client 模式适用于测试调试程序。</p></li><li><p>Driver 进程是在客户端启动的，这里的客户端就是指提交应用程序的当前节点。</p></li><li>在 Driver 端可以看到 task 执行的情况。生产环境下不能使用 client 模式，</li></ul><p><code>是因为</code>：</p><p>假设要提交 100 个 application 到集群运行，Driver 每次都会在client 端启动，那么就会导致客户端 100 次网卡流量暴增的问题。</p></blockquote><h2 id="2、Standalone-cluster-提交任务方式"><a href="#2、Standalone-cluster-提交任务方式" class="headerlink" title="2、Standalone-cluster 提交任务方式"></a>2、Standalone-cluster 提交任务方式</h2><h3 id="（1）命令提交"><a href="#（1）命令提交" class="headerlink" title="（1）命令提交"></a>（1）命令提交</h3><ul><li>在/sbin路径下：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><ul><li>提交spark</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master spark://node00:7077</span><br><span class="line">--deploy-mode cluster</span><br><span class="line">--class org.apache.spark.examples.SparkPi</span><br><span class="line">../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><blockquote><p>Standalone-cluster 提交方式，应用程序使用的所有 jar 包和文件，必须保证所有的 worker 节点都要有，因为此种方式，spark 不会自动上传jar包。</p><p>Standalone-client 和yarn 模式会在提交命令的时候自动uploading  实现jar包共享，</p><p>解决方式：</p><p>1、将所有的依赖包和文件各放一份在 worker 节点上。</p><p>2、将所有的依赖包和文件打到同一个包中，然后放在 hdfs 上。(路径需指定为hdfs上的路径)</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; ./spark-submit</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; --master spark://node00:7077</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; --deploy-mode cluster</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; --class org.apache.spark.examples.SparkPi</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; hdfs://Sunrise/lib/spark-examples-1.6.0-hadoop2.6.0.jar</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt; 1000</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> &gt;</span></span><br></pre></td></tr></table></figure></blockquote></blockquote><h3 id="（2）执行原理图"><a href="#（2）执行原理图" class="headerlink" title="（2）执行原理图"></a>（2）执行原理图</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g09rzbjqjlj310w0qcwgh.jpg" alt=""></p><h3 id="（3）执行流程"><a href="#（3）执行流程" class="headerlink" title="（3）执行流程"></a>（3）执行流程</h3><blockquote><ol><li>cluster 模式提交应用程序后，会向 Master 请求启动 Driver.</li><li>Master 接受请求，随机在集群一台节点启动 Driver 进程。</li><li>Driver 启动后为当前的应用程序申请资源。</li><li>Driver 端发送 task 到 worker 节点上执行。</li><li>worker 将执行情况和执行结果返回给 Driver 端。</li></ol></blockquote><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><blockquote><p>Driver 进程是在集群某一台 Worker 上启动的，在客户端是无法查看 task 的执行情况的。假设要提交 100<br>个 application 到集群运行,每次 Driver 会随机在集群中某一台 Worker 上启动，那么这 100 次网卡流量暴<br>增的问题就散布在集群上</p></blockquote><h2 id="总结-Standalone"><a href="#总结-Standalone" class="headerlink" title="总结 Standalone"></a>总结 Standalone</h2><p>Standalone  两种方式提交任务，Driver  与集群的通信包括：</p><blockquote><ol><li>Driver 负责应用程序资源的申请</li><li>任务的分发。</li><li>结果的回收。</li><li>监控 task 执行情况。</li></ol></blockquote><h1 id="五、Yarn-模式两种提交任务方式"><a href="#五、Yarn-模式两种提交任务方式" class="headerlink" title="五、Yarn  模式两种提交任务方式"></a>五、Yarn  模式两种提交任务方式</h1><h2 id="1、yarn-client-提交任务方式"><a href="#1、yarn-client-提交任务方式" class="headerlink" title="1、yarn-client 提交任务方式"></a>1、yarn-client 提交任务方式</h2><h3 id="（1）命令提交-1"><a href="#（1）命令提交-1" class="headerlink" title="（1）命令提交"></a>（1）命令提交</h3><ul><li>提交spark</li></ul><p>方式一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master yarn</span><br><span class="line">--class</span><br><span class="line">org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>方式二：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master yarn–client</span><br><span class="line">--class</span><br><span class="line">org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>方式三：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master yarn</span><br><span class="line">--deploy-mode client</span><br><span class="line">--class</span><br><span class="line">org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">100</span><br></pre></td></tr></table></figure><h3 id="（2）执行原理图-1"><a href="#（2）执行原理图-1" class="headerlink" title="（2）执行原理图"></a>（2）执行原理图</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g09rzg6auij31750x4dj9.jpg" alt=""></p><h3 id="（3）执行流程-1"><a href="#（3）执行流程-1" class="headerlink" title="（3）执行流程"></a>（3）执行流程</h3><blockquote><ol><li>客户端提交一个 Application，在客户端启动一个 Driver 进程。</li><li>应用程序启动后会向 RS(ResourceManager)发送请求，启动AM(ApplicationMaster)的资源。</li><li>RS 收到请求，随机选择一台 NM(NodeManager)启动 AM。这里的 NM 相当于 Standalone 中的Worker 节点。</li><li>AM启动后，会向RS请求一批container资源，用于启动Executor.</li><li>RS 会找到一批 NM 返回给 AM,用于启动 Executor。</li><li>AM 会向 NM 发送命令启动 Executor。</li><li>Executor 启动后，会反向注册给 Driver，Driver 发送 task 到Executor,执行情况和结果返回给 Driver 端。</li></ol></blockquote><h3 id="（4）总结-1"><a href="#（4）总结-1" class="headerlink" title="（4）总结"></a>（4）总结</h3><blockquote><p>Yarn-client 模式同样是适用于测试，因为 Driver 运行在本地，Driver会与 yarn 集群中的 Executor 进行大量的通信，会造成客户机网卡流量的大量增加.</p></blockquote><blockquote><ul><li>ApplicationMaster  的作用：</li></ul><ol><li>为当前的 Application 申请资源</li><li>给 NodeManager 发送消息启动 Executor。</li></ol><ul><li>注意：</li></ul><p>ApplicationMaster 有 launchExecutor 和申请资源的功能，并没有作业调度的功能</p></blockquote><h2 id="2、yarn-cluster-提交任务方式"><a href="#2、yarn-cluster-提交任务方式" class="headerlink" title="2、yarn-cluster 提交任务方式"></a>2、yarn-cluster 提交任务方式</h2><h3 id="（1）命令提交-2"><a href="#（1）命令提交-2" class="headerlink" title="（1）命令提交"></a>（1）命令提交</h3><ul><li>提交spark</li></ul><p>方式一：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master yarn</span><br><span class="line">--deploy-mode cluster</span><br><span class="line">--class</span><br><span class="line">org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><p>方式二:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master yarn-cluster</span><br><span class="line">--class</span><br><span class="line">org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">1000</span><br></pre></td></tr></table></figure><h3 id="（2）执行原理图-2"><a href="#（2）执行原理图-2" class="headerlink" title="（2）执行原理图"></a>（2）执行原理图</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g09rzn3fuwj318x0weq60.jpg" alt=""></p><h3 id="（3）执行流程-2"><a href="#（3）执行流程-2" class="headerlink" title="（3）执行流程"></a>（3）执行流程</h3><blockquote><ol><li>客户机提交 Application 应用程序，发送请求到RS(ResourceManager),请求启动AM(ApplicationMaster)。</li><li>RS 收到请求后随机在一台 NM(NodeManager)上启动 AM（相当于 Driver 端，同一个进程）。</li><li>AM 启动，AM 发送请求到 RS，请求一批 container 用于启动Excutor。</li><li>RS 返回一批 NM 节点给 AM。</li><li>AM 连接到 NM,发送请求到 NM 启动 Excutor。</li><li>Excutor 反向注册到 AM 所在的节点的 Driver。Driver 发送 task到 Excutor。</li></ol></blockquote><h3 id="（4）总结-2"><a href="#（4）总结-2" class="headerlink" title="（4）总结"></a>（4）总结</h3><blockquote><p>Yarn-Cluster 主要用于生产环境中，</p><p>因为 Driver 运行在 Yarn 集群中某一台 nodeManager 中，每次提交任务的 Driver 所在的机器都是<br>随机的，不会产生某一台机器网卡流量激增的现象，</p><p>缺点是任务提交后不能看到日志。只能通过 yarn 查看日志。</p></blockquote><blockquote><ul><li>ApplicationMaster  的作用：</li></ul><ol><li>为当前的 Application 申请资源</li><li>给 NodeManger 发送消息启动 Excutor。</li><li>任务调度。</li></ol><ul><li>停止集群任务命令：yarn application -kill applicationID</li></ul></blockquote><h2 id="总结yarn"><a href="#总结yarn" class="headerlink" title="总结yarn"></a>总结yarn</h2><p>——————————————–TODU—————————–</p><h1 id="参数解释：spark-submit"><a href="#参数解释：spark-submit" class="headerlink" title="参数解释：spark-submit"></a>参数解释：spark-submit</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">spark-submit -h</span><br><span class="line"></span><br><span class="line">--master  （优先使用代码中的配置）</span><br><span class="line">--name    （指定APPname）</span><br><span class="line">--deploy mode  （默认为client，指定运行模式）</span><br><span class="line">--jars （可以用来为代码添加所需要的jar包依赖）</span><br><span class="line"></span><br><span class="line">IDEA代码打包：BUILD</span><br><span class="line">（注意避免jar包过大，可不打包引用的相关sparkjar包，因搭建的spark集群上已经存在）</span><br><span class="line"></span><br><span class="line">--files （添加代码所需的文件）</span><br><span class="line">--conf （PROP=value）</span><br><span class="line">--driver-memory</span><br><span class="line">--executor-memory</span><br><span class="line">--total-executor-core    （若不指明，就把所有的核均用完）</span><br><span class="line">--queue      （资源分配：若运行在yarn上  ：就是将资源 分配到队列中 ）</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 bin]# ./spark-submit -h</span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.</span><br><span class="line">  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally("client")                               or on one of the worker machines inside the  cluster                                 ("cluster") (Default: client).</span><br><span class="line">  --class CLASS_NAME          Your application's main class (for Java / Scala apps).</span><br><span class="line">  --name NAME                 A name of your application.</span><br><span class="line">  --jars JARS                 Comma-separated (逗号分隔) list of local jars to include                               on the driver and executor classpaths.(Driver 和                                     executor 依赖的第三方 jar 包)</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to                                 include on the driver and executor classpaths. Will                                 search the local maven repo, then maven central and                                 any additional remote repositories given by --                                       repositories. The format for the coordinates should be                               groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude                               while resolving the dependencies provided in --                                     packages to avoid dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories                               to search for the maven coordinates given with --                                   packages.</span><br><span class="line">  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to                                 place on the PYTHONPATH for Python apps.</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the                                   working directory of each executor.</span><br><span class="line"></span><br><span class="line">  --conf PROP=VALUE           Arbitrary Spark configuration property.</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If                               not specified, this will look for conf/spark-                                       defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).</span><br><span class="line">  --driver-java-options       Extra Java options to pass to the driver.</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note                                 that jars added with --jars are automatically included                               in the  classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line"></span><br><span class="line">  --help, -h                  Show this help message and exit</span><br><span class="line">  --verbose, -v               Print additional debug output</span><br><span class="line">  --version,                  Print the version of current Spark</span><br><span class="line"></span><br><span class="line"> Spark standalone with cluster deploy mode only:</span><br><span class="line">  --driver-cores NUM          Cores for driver (Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 If given, restarts the driver on failure.</span><br><span class="line">  --kill SUBMISSION_ID        If given, kills the driver specified.</span><br><span class="line">  --status SUBMISSION_ID      If given, requests the status of the driver specified.</span><br><span class="line"></span><br><span class="line"> Spark standalone and Mesos only:</span><br><span class="line">  --total-executor-cores NUM  Total cores for all executors.</span><br><span class="line"></span><br><span class="line"> Spark standalone and YARN only:</span><br><span class="line">  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN                                   mode, or all available cores on the worker in                                       standalone mode)</span><br><span class="line"></span><br><span class="line"> YARN-only:</span><br><span class="line">  --driver-cores NUM          Number of cores used by the driver, only in cluster                                 mode (Default: 1).</span><br><span class="line">  --queue QUEUE_NAME          The YARN queue to submit to (Default: "default").</span><br><span class="line">  --num-executors NUM         Number of executors to launch (Default: 2).</span><br><span class="line">  --archives ARCHIVES         Comma separated list of archives to be extracted into                               the working directory of each executor.</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC, while running on</span><br><span class="line">                              secure HDFS.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for                               the principal specified above. This keytab will be                                   copied to the node running the Application Master via                               the Secure Distributed Cache, for renewing the login                                 tickets and the delegation tokens periodically.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sc.textFile("hdfs://node00:8020/test.txt")</span><br><span class="line">   .flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).foreach(println)</span><br></pre></td></tr></table></figure><h1 id="六、术语解释"><a href="#六、术语解释" class="headerlink" title="六、术语解释"></a>六、术语解释</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0bt4g8pk2j30h909ajtj.jpg" alt=""></p><h1 id="七、宽窄依赖"><a href="#七、宽窄依赖" class="headerlink" title="七、宽窄依赖"></a>七、宽窄依赖</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0acxz6cfmj30mw0aygmp.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1wge4iz9fj30kb0d1tcj.jpg" alt=""></p><h2 id="1、窄依赖"><a href="#1、窄依赖" class="headerlink" title="1、窄依赖"></a>1、窄依赖</h2><p>父RDD的一个partition对应子RDD<strong><code>一个</code></strong>partition</p><p>父RDD的多个partition对应子RDD<strong><code>一个</code></strong>partition</p><p>不会产生shuffle</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">map</span><br><span class="line">flatmap</span><br><span class="line">filter</span><br><span class="line">union</span><br></pre></td></tr></table></figure><h2 id="2、宽依赖"><a href="#2、宽依赖" class="headerlink" title="2、宽依赖"></a>2、宽依赖</h2><p>父RDD的一个partition对应子RDD<strong><code>多个</code></strong>partition</p><p>会产生shuffle</p><p>会划分stage</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey</span><br><span class="line">join</span><br><span class="line">groupBy</span><br></pre></td></tr></table></figure><h1 id="八、stage"><a href="#八、stage" class="headerlink" title="八、stage"></a>八、stage</h1><h2 id="0、概念"><a href="#0、概念" class="headerlink" title="0、概念"></a>0、概念</h2><p>（1）Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是RDD之间的宽窄依赖关系：遇到宽依赖就划分stage</p><p>（2）stageN内有一组并行的task组成，这些task将以taskSet的格式提交给TaskScheduler运行</p><p>（2）task运行时，stage之间的关系可能并行，也可能串行</p><h2 id="1、stage-切割规则"><a href="#1、stage-切割规则" class="headerlink" title="1、stage 切割规则"></a>1、stage 切割规则</h2><p>切割规则：从后往前，遇到宽依赖就切割 stage。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0ae0bta21j30mh0fvn7b.jpg" alt=""></p><h2 id="2、stage-计算模式：pipeline-管道计算模式"><a href="#2、stage-计算模式：pipeline-管道计算模式" class="headerlink" title="2、stage 计算模式：pipeline 管道计算模式"></a>2、stage 计算模式：pipeline 管道计算模式</h2><p>pipeline 管道计算模式,pipeline 只是一种计算思想、模式。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0ae32q68ij30mz08s3zo.jpg" alt=""></p><blockquote><ul><li>数据在内存中流转</li><li>数据一直在管道里面什么时候数据会落地？</li></ul><ol><li>对 RDD 进行持久化(cache、persisit)。</li><li>shuffle write 的时候。</li></ol></blockquote><blockquote><ul><li>什么决定task数</li></ul><p>Stage 的 task  并行度是由 stage 的最后一个RDD的分区数来决定的 （partition分区数决定task数）</p><p>同一个stage中的task计算逻辑可能不同</p></blockquote><blockquote><ul><li>如何改变 RDD  的分区数？</li></ul><p>宽依赖可改分区数；（因为此时数据已落地到磁盘）</p><p>textFile(“  ”,5)</p><p>reduceByKey(_ +_ , 5)</p><p>GroupByKey(4)</p></blockquote><blockquote><ul><li>测试验证 pipeline 计算模式</li></ul><p><code>注意：</code>textFile(“./wc.txt”)是通过文件获得RDD，parallelize(List<string>)是通过转换参数内容获得RDD</string></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">&gt; conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"pipeline"</span>);</span><br><span class="line">&gt; <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">&gt; <span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">&gt; <span class="keyword">val</span> rdd1 = rdd.map &#123; x =&gt; &#123;</span><br><span class="line">&gt;       println(<span class="string">"map--------"</span>+x)</span><br><span class="line">&gt;        x</span><br><span class="line">&gt;       &#125;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; <span class="keyword">val</span> rdd2 = rdd1.filter &#123; x =&gt; &#123;</span><br><span class="line">&gt; println(<span class="string">"fliter********"</span>+x)</span><br><span class="line">&gt; <span class="literal">true</span></span><br><span class="line">&gt; &#125; &#125;</span><br><span class="line">&gt; rdd2.collect().foreach(print+<span class="string">","</span>) <span class="comment">//1,2,3,4</span></span><br><span class="line">&gt; sc.stop()</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p><code>显示：</code></p><p>map——–1</p><p>fliter<strong><em>**</em></strong>1</p><p>map——–2</p><p>fliter<strong><em>**</em></strong>2</p><p>map——–3</p><p>fliter<strong><em>**</em></strong>3</p><p>map——–4</p><p>fliter<strong><em>**</em></strong>4</p></blockquote><h1 id="九、Spark-资源调度和任务调度"><a href="#九、Spark-资源调度和任务调度" class="headerlink" title="九、Spark  资源调度和任务调度"></a>九、Spark  资源调度和任务调度</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0ae6j5tccj30nb0ctq8y.jpg" alt=""></p><h2 id="1、概念解释"><a href="#1、概念解释" class="headerlink" title="1、概念解释"></a>1、概念解释</h2><ul><li><strong>DAGScheduler</strong>是任务调度的高层调度器，是一个对象</li></ul><blockquote><p> DAGScheduler 的主要作用就是</p><p>将DAG 根据 RDD 之间的宽窄依赖关系划分为一个个的 Stage，然后将这些Stage 以 TaskSet 的形式提交给 TaskScheduler</p></blockquote><ul><li><p><strong>TaskScheduler</strong> 是任务调度的低层调度器</p></li><li><p><strong>TaskSet</strong> 其实就是一个集合，里面封装的就是一个个的 task 任务,也就是 stage 中的并行度 task 任务</p></li></ul><p>Application→Job→Stage→Task</p><ul><li><strong>Spark推测执行机制</strong></li></ul><blockquote><p>如果有运行缓慢的task,那么TaskScheduler就会启动一个新的task（在不同节点的excutor上）来执行相同的处理逻辑，两个task中哪个task先执行结束，就以那个task的执行结果为准。</p><p>在 Spark 中推测执行默认是关闭的。</p><p>推测执行可以通过 spark.speculation 属性来配置。<br><code>注意：</code></p><ul><li><p>对于 ETL 类型要入数据库的业务要关闭推测执行机制，这样就不会有重复的数据入库。</p></li><li><p>如果遇到数据倾斜的情况，开启推测执行则有可能导致一直会有task重新启动处理相同的逻辑，任务可能一直处于处理不完的状态。（这时候task慢是因为数据量过多，而不是执行性能不行）</p></li></ul></blockquote><h2 id="2、Spark-资源调度和任务调度的流程："><a href="#2、Spark-资源调度和任务调度的流程：" class="headerlink" title="2、Spark 资源调度和任务调度的流程："></a>2、Spark 资源调度和任务调度的流程：</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w686dtunj31s60s6tit.jpg" alt=""></p><blockquote><p>1、启动集群后，Worker 节点会向 Master 节点汇报资源情况，Master 掌握了集群资源情况。</p><p>2、当 Spark 提交一个 Application 后，根据 RDD 之间依赖关系将 Application 形成一个 DAG 有向无环图。</p><p>3、任务提交后，Spark 会在Driver 端创建两个对象：DAGScheduler 和 TaskScheduler，</p><p>DAGScheduler 将DAG 根据 RDD 之间的宽窄依赖关系划分为一个个的 Stage，然后将这些Stage 以 TaskSet 的形式提交给 TaskScheduler，</p><p>TaskSchedule 会遍历TaskSet 集合，拿到每个 task 后会将 task 发送到计算节点 Executor 中去执行（其实就是发送到 Executor 中的线程池 ThreadPool 去执行）。</p><p>task 在Executor 线程池中的运行情况会向 TaskScheduler 反馈，当 task 执行失败时，则由 TaskScheduler 负责重试，将 task 重新发送给 Executor 去执行，默认重试 3 次。如果重试 3 次依然失败，那么这个 task 所在的 stage 就失败了。</p><p>stage 失败了则由 DAGScheduler 来负责重试，重新发送 TaskSet 到TaskSchdeuler，Stage 默认重试 4 次。如果重试 4 次以后依然失败，那么这个 job 就失败了。job 失败了，Application 就失败了。</p><p>TaskScheduler 不仅能重试失败的 task,还会重试 straggling<em>&lt;落后，缓慢的&gt;</em>task（也就是执行速度比其他 task 慢太多的 task）。如果有运行缓慢的 task那么 TaskScheduler 会启动Spark 的推测执行机制先执行完，task 的执行结果为准。</p></blockquote><h2 id="3、资源调度和任务调度的流程图"><a href="#3、资源调度和任务调度的流程图" class="headerlink" title="3、资源调度和任务调度的流程图"></a>3、资源调度和任务调度的流程图</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0aeo4szikj30mt0bcgte.jpg" alt=""></p><h2 id="4、粗粒度资源申请和细粒度资源申请"><a href="#4、粗粒度资源申请和细粒度资源申请" class="headerlink" title="4、粗粒度资源申请和细粒度资源申请"></a>4、粗粒度资源申请和细粒度资源申请</h2><ul><li><p><strong>粗粒度资源申请</strong>(Spark）<br>在 Application 执行之前，将所有的资源申请完毕，当资源申请成功后，才会进行任务的调度，当所有的 task 执行完成后，才会释放这部分资源。</p><blockquote><ul><li><p><strong><code>优点：</code></strong></p><p>在 Application 执行之前，所有的资源都申请完毕，每一个task 直接使用资源就可以了，不需要 task 在执行前自己去申请资源，task 启动就快了，task 执行快了，stage 执行就快了，job 就快了，application 执行就快了。</p></li><li><p><strong><code>缺点：</code></strong></p><p>直到最后一个 task 执行完成才会释放资源，集群的资源无法充分利用。</p></li></ul></blockquote></li><li><p><strong>细粒度资源申请</strong><br>Application 执行之前不需要先去申请资源，而是直接执行，让 job中的每一个 task 在执行前自己去申请资源，task 执行完成就释放资源。</p><blockquote><p><strong><code>优点</code>：</strong></p><p>集群的资源可以充分利用。</p><p><strong><code>缺点</code></strong>：</p><p>task 自己去申请资源，task 启动变慢，Application 的运行就响应的变慢了。</p></blockquote></li></ul><h2 id="5、资源调度源码分析"><a href="#5、资源调度源码分析" class="headerlink" title="5、资源调度源码分析"></a>5、资源调度源码分析</h2><p> 资源请求简单图</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g20sl0389dj30qw0dfq6h.jpg" alt=""></p><p> 资源调度 Master 路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">路径：spark-1.6.0/core/src/main/scala/org.apache.spark/deploy/Master/Master.scala</span><br></pre></td></tr></table></figure><p> 提交应用程序，submit 的路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">路径：spark-1.6.0/core/src/main/scala/org.apache.spark/ deploy/SparkSubmit.scala</span><br></pre></td></tr></table></figure><p> 总结：</p><ol><li>Executor 在集群中分散启动，有利于 task 计算的数据本地化。</li><li>默认情况下（提交任务的时候没有设置–executor-cores 选项），每一个 Worker 为当前的 Application 启动一个 Executor,这个Executor 会使用这个 Worker 的所有的 cores 和 1G 内存。</li><li>如果想在 Worker 上启动多个 Executor，提交 Application 的时候要加–executor-cores 这个选项。</li><li>默认情况下没有设置–total-executor-cores,一个 Application 会使用 Spark 集群中所有的 cores。</li></ol><p> 结论演示</p><p>使用 Spark-submit 提交任务演示。也可以使用 spark-shell</p><ol><li><p>默认情况每个 worker 为当前的 Application 启动一个 Executor，这个 Executor 使用集群中所有的 cores 和 1G 内存。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master spark://node1:7077</span><br><span class="line">--class org.apache.spark.examples.SparkPi</span><br><span class="line">../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">10000</span><br></pre></td></tr></table></figure></li><li><p>在 workr 上启动多个 Executor,设置–executor-cores 参数指定每个executor 使用的 core 数量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master spark://node1:7077</span><br><span class="line">--executor-cores 1</span><br><span class="line">--class org.apache.spark.examples.SparkPi</span><br><span class="line">../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">10000</span><br></pre></td></tr></table></figure></li><li><p>内存不足的情况下启动 core 的情况。Spark 启动是不仅看 core 配置参数，也要看配置的 core 的内存是否够用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">  --master spark://node1:7077</span><br><span class="line">  --executor-cores 1</span><br><span class="line">  --executor-memory 3g</span><br><span class="line">  --class org.apache.spark.examples.SparkPi</span><br><span class="line">  ../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line"> 10000</span><br></pre></td></tr></table></figure></li><li><p>–total-executor-cores 集群中共使用多少 cores  。注意：一个进程不能让集群多个节点共同启动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit</span><br><span class="line">--master spark://node1:7077</span><br><span class="line">--executor-cores 1</span><br><span class="line">--executor-memory 2g</span><br><span class="line">--total-executor-cores 3</span><br><span class="line">--class org.apache.spark.examples.SparkPi</span><br><span class="line">../lib/spark-examples-1.6.0-hadoop2.6.0.jar</span><br><span class="line">10000</span><br></pre></td></tr></table></figure></li></ol><h2 id="6、任务调度源码分析"><a href="#6、任务调度源码分析" class="headerlink" title="6、任务调度源码分析"></a>6、任务调度源码分析</h2><p> Action 算子开始分析<br>任务调度可以从一个 Action 类算子开始。因为 Action 类算子会触发一个 job 的执行。</p><p> 划分 stage,以 taskSet 形式提交任务<br>DAGScheduler 类中 getMessingParentStages()方法是切割 job 划分stage 。 可 以 结 合 以 下 这 张 图 来 分 析 ：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g0ae0bta21j30mh0fvn7b.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算框架 </tag>
            
            <tag> sparkcore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark学习（一）</title>
      <link href="/2019/02/16/Spark%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/"/>
      <url>/2019/02/16/Spark%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、Spark简介"><a href="#一、Spark简介" class="headerlink" title="一、Spark简介"></a>一、Spark简介</h1><h2 id="1、什么是Spark？"><a href="#1、什么是Spark？" class="headerlink" title="1、什么是Spark？"></a>1、什么是Spark？</h2><blockquote><p>Lightning-fast unified analytics engine</p><p>Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。</p></blockquote><p>用于逻辑回归算法：</p><p>快速(100倍)：能更好的的适用于数据挖掘与机器学习等需要迭代的算法（在计算结果的基础上再计算）；Job的中间结果值在内存中流转，不需要读取HDFS，屏蔽磁盘开销；DAG调度</p><p>mr：离线，（迭代时：磁盘IO，较慢）</p><p>storm：流式</p><blockquote><p>Spark是用Scala编写的，方便快速编程</p></blockquote><h2 id="2、与MapReduce的区别"><a href="#2、与MapReduce的区别" class="headerlink" title="2、与MapReduce的区别"></a>2、与MapReduce的区别</h2><ul><li>MapReduce</li></ul><p>运行计算原理<img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w61lszw4j30nb0aa0tx.jpg" alt=""></p><p>存储原理：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w61xqpgqj30er08ymxd.jpg" alt=""></p><ul><li>Spark</li></ul><p>存储原理</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w626b6w6j30fv08eaaa.jpg" alt=""></p><p>区别：</p><blockquote><p>同：分布式计算框架</p><p>不同：</p><ul><li>Spark基于内存，MR基于HDFS</li><li>Spark处理数据的能力是MR的十倍以上</li><li>Spark除了基于内存计算之外，还有DAG有向无环图来切分任务的执行顺序</li></ul></blockquote><p>Spark API  的使用语言</p><blockquote><p>Scala（很好）<br>Python(不错)<br>Java(…)</p></blockquote><h2 id="3、Spark运行模式"><a href="#3、Spark运行模式" class="headerlink" title="3、Spark运行模式"></a>3、Spark运行模式</h2><ul><li>local</li></ul><p>多用于本地测试，如在 eclipse，idea 中写程序测试</p><ul><li>standalone</li></ul><p>standalone是Spark自带的资源调度框架，它支持完全分布式</p><ul><li>yarn</li></ul><p>Hadoop生态圈的资源调度框架，Spark也是可以基于yarn来计算的</p><blockquote><p>基于yarn来进行资源调度，必须实现ApplicationMaster接口，Spark实现的这个接口，所以可以使用</p></blockquote><ul><li>mesos</li></ul><p>资源调度框架</p><h1 id="二、Sparkcore"><a href="#二、Sparkcore" class="headerlink" title="二、Sparkcore"></a>二、Sparkcore</h1><h2 id="1、RDD"><a href="#1、RDD" class="headerlink" title="1、RDD"></a>1、RDD</h2><h3 id="（1）概念："><a href="#（1）概念：" class="headerlink" title="（1）概念："></a>（1）概念：</h3><p>RDD(Resilient Distributed Dateset)弹性分布式数据集</p><h3 id="（2）五大特性"><a href="#（2）五大特性" class="headerlink" title="（2）五大特性"></a>（2）五大特性</h3><blockquote><ol><li>RDD 是由一系列的 partition 组成的。</li><li>函数是作用在每一个 partition（split）上的。</li><li>RDD 之间有一系列的依赖关系。</li><li>分区器是作用在 K,V 格式的 RDD 上。</li><li>RDD 提供一系列最佳的计算位置。</li></ol></blockquote><blockquote><ul><li><code>获取RDD的方式</code></li><li>parallelize()</li></ul><blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt; <span class="comment">//Distribute a local Scala collection to form an RDD</span></span><br><span class="line">&gt; &gt; JavaRDD&lt;T&gt; rdd = javaSparkContext.parallelize(List&lt;T&gt; list)；</span><br><span class="line">&gt; &gt; JavaRDD&lt;T&gt; rdd = javaSparkContext.parallelize(List&lt;T&gt; list,<span class="keyword">int</span> numSlices)；     </span><br><span class="line">&gt; &gt;</span><br></pre></td></tr></table></figure></blockquote></blockquote><blockquote><ul><li>parallelizePairs</li></ul><blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt; <span class="comment">//Distribute a local Scala collection to form an RDD</span></span><br><span class="line">&gt; &gt; JavaPairRDD&lt;K,V&gt; rdd = </span><br><span class="line">&gt; &gt;      javaSparkContext.parallelizePairs(List&lt;Tuple2&lt;K, V&gt;&gt; list)；     </span><br><span class="line">&gt; &gt; JavaPairRDD&lt;K,V&gt; rdd = </span><br><span class="line">&gt; &gt;      javaSparkContext.parallelizePairs(List&lt;Tuple2&lt;K, V&gt;&gt; list,<span class="keyword">int</span> numSlices)；</span><br><span class="line">&gt; &gt;</span><br></pre></td></tr></table></figure></blockquote></blockquote><blockquote><blockquote></blockquote><ul><li>textFile(“./xx.txt”)   也可指定分区</li></ul></blockquote><h3 id="（3）RDD理解图"><a href="#（3）RDD理解图" class="headerlink" title="（3）RDD理解图"></a>（3）RDD理解图</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w679s6h6j30f70c83z7.jpg" alt=""></p><p><code>理论注解</code></p><blockquote><ul><li><p>RDD  实际上不存储数据，这里方便理解，暂时理解为存储数据。</p></li><li><p>textFile 方法底层封装的是MR 读取文件的方式(先 split,再读取文件)，默认 split 大小是一个 block 大小。</p></li><li><p>​ RDD 提供计算最佳位置，体现了数据本地化。体现了大数据中“计算移动数据不移动”的理念。</p></li></ul><p>❔  哪里体现 RDD 的分布式？</p><p>👆  RDD 是由 Partition 组成，partition 是分布在不同节点上的。</p><p>❔  哪里体现 RDD 的弹性（容错）？</p><p>👆 partition 数量，大小没有限制,默认和split（block）一致，体现了 RDD 的弹性。<br>👆 RDD 之间依赖关系，可以基于上一个 RDD 重新计算出 RDD。</p><p>❔  什么是 K,V 格式的 RDD?</p><p>👆 如果 RDD 里面存储的数据都是二元组对象，那么这个 RDD 我们就叫做 K,V 格式的 RDD。</p><p>👆 MR有分区器（根据key值求hash，来决定数据存放在哪个分区中，所以分区器必须作用在K，V格式的RDD上）</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w671xb3rj30fn09ywfd.jpg" alt=""></p><h2 id="2、Spark任务执行原理"><a href="#2、Spark任务执行原理" class="headerlink" title="2、Spark任务执行原理"></a>2、Spark任务执行原理</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w6a0je6bj30e808t0wt.jpg" alt=""></p><p>Driver：（相当于ApplicationMaster）</p><p>Worker：（相当于NodeManager）</p><p>以上图中有四个机器节点，</p><p>Driver 和 Worker 是启动在节点上的进程，运行在 JVM 中的进程。</p><ul><li>Driver 与集群节点之间有频繁的通信。</li><li>Driver：任务的调度（监控任务、 负责任务(tasks)的分发和结果的回收）。如果 task的计算结果非常大就不要回收了。会造成 oom。</li><li>Worker 是 Standalone 资源调度框架里面资源管理的从节点。也是JVM 进程。</li><li>Master 是 Standalone 资源调度框架里面资源管理的主节点。也是JVM 进程。</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1w686dtunj31s60s6tit.jpg" alt="spark任务执行图"><span class="img-alt">spark任务执行图</span></p><h2 id="3、Spark代码流程"><a href="#3、Spark代码流程" class="headerlink" title="3、Spark代码流程"></a>3、Spark代码流程</h2><h3 id="以用Scala编写WordCount为例"><a href="#以用Scala编写WordCount为例" class="headerlink" title="以用Scala编写WordCount为例"></a><code>以用Scala编写WordCount为例</code></h3><p>1、创建 SparkConf 对象</p><ul><li>可以设置 Application name。</li><li>可以设置运行模式及资源需求。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">       <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 几种运行方式：</span></span><br><span class="line"><span class="comment">         *   1.本地运行</span></span><br><span class="line"><span class="comment">         *   2.yarn</span></span><br><span class="line"><span class="comment">         *   3.standalone</span></span><br><span class="line"><span class="comment">         *   4.mesos</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line"> conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"wc"</span>)</span><br></pre></td></tr></table></figure><p>2、创建 SparkContext 对象</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span>  context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br></pre></td></tr></table></figure><p>3、基于 Spark 的上下文创建一个 RDD，对 RDD 进行处理。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获取文件中每一行数据的ADD</span></span><br><span class="line"> <span class="keyword">val</span> lineRDD= context.textFile(<span class="string">"./wc.txt"</span>)</span><br><span class="line"><span class="comment">//获取每一行数据按空格切分后的ADD</span></span><br><span class="line"> <span class="keyword">val</span> wordRDD = lineADD.flatMap(x=&gt;&#123;x.split(<span class="string">" "</span>)&#125;)</span><br><span class="line"><span class="comment">//获取每个单词加上,1 后的ADD（K,V格式）</span></span><br><span class="line"> <span class="keyword">val</span> <span class="type">KVRDD</span> = wordADD.map(x=&gt;&#123;(x,<span class="number">1</span>)&#125;)</span><br><span class="line"><span class="comment">//获取将相同key的value相加后的ADD（K,V格式），相当于Tuple2</span></span><br><span class="line"> <span class="keyword">val</span> resultRDD = <span class="type">KVADD</span>.reduceByKey((x,y)=&gt;&#123;x+y&#125;)</span><br><span class="line"><span class="comment">//降序排序</span></span><br><span class="line"> <span class="keyword">val</span> sortRDD = resultADD.sortBy(_._2,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>4、应用程序中要有 Action 类算子来触发 Transformation 类算子执行。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sortADD.foreach(println)</span><br></pre></td></tr></table></figure><p>5、关闭 Spark 上下文对象 SparkContext。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.stop()</span><br></pre></td></tr></table></figure><h2 id="4、Transformations-转换算子"><a href="#4、Transformations-转换算子" class="headerlink" title="4、Transformations 转换算子"></a>4、Transformations 转换算子</h2><h3 id="（1）概念"><a href="#（1）概念" class="headerlink" title="（1）概念"></a>（1）概念</h3><p>Transformations 类算子是一类算子（函数）叫做转换算子，如map,flatMap,reduceByKey 等。Transformations 算子是延迟执行，也叫懒加载执行。</p><blockquote><p>有action触发算子任务才能提交，才会执行runjob</p><p>算子必须作用在RDD上</p></blockquote><h3 id="（2）Transformation-类算子"><a href="#（2）Transformation-类算子" class="headerlink" title="（2）Transformation 类算子"></a>（2）Transformation 类算子</h3><blockquote><p>:arrow_up_small: <strong>filter</strong><br>过滤符合条件的记录数，true 保留，false 过滤掉。</p><p>🔼 <strong>contains</strong></p><p>作为条件，是否包含，返回true|false</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; resultRDD = </span><br><span class="line">&gt;     lines.filter(<span class="keyword">new</span> Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">&gt; <span class="meta">@Override</span></span><br><span class="line">&gt; <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt; <span class="keyword">return</span> !line.contains(<span class="string">"sh"</span>);</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &#125;);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><blockquote><p>:arrow_up_small:<strong>map</strong><br>将一个 RDD 中的每个数据项，通过 map 中的函数映射变为一个新的元素。<br>特点：输入一条，输出一条数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; mapResult = </span><br><span class="line">&gt;     line.map(<span class="keyword">new</span> Function&lt;String, String&gt;() &#123;</span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">&gt; <span class="meta">@Override</span></span><br><span class="line">&gt; <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt; <span class="keyword">return</span> s+<span class="string">"~"</span>;</span><br><span class="line">&gt; &#125; </span><br><span class="line">&gt; &#125;);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼 <strong>flatMap</strong><br>先 map 后 flat。与 map 类似，每个输入项可以映射为 0 到多个输出项。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; flatMapResult = </span><br><span class="line">&gt; lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">&gt; <span class="meta">@Override</span></span><br><span class="line">&gt; <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;     <span class="keyword">return</span> Arrays.asList(s.split(<span class="string">" "</span>));</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &#125;);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>:black_joker: <strong>mapToPair</strong>   (Java)</p><p>将RDD（如lineRDD）转换成二元组</p><p>🃏 <strong>mapValues(function)</strong></p><p>原RDD中的Key保持不变，与新的Value一起组成新的RDD中的元素。因此，该函数只适用于元素为（K,V）格式的RDD。返回Tuple2&lt;&gt;</p><p>🔼 <strong>mapPartition</strong></p><p>与 map 类似，遍历的单位是每个 partition 上的数据。一进一出</p><p>每个partition的数据，以Iterator<t> iterator形式出入，并返回Iterator<t> iterator</t></t></p><p>🔼<strong>mapPartitionWithIndex</strong><br>类似于 mapPartitions,除此之外还会携带分区的索引值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; mapPartitionsWithIndex = parallelize.mapPartitionsWithIndex(</span><br><span class="line">&gt; <span class="keyword">new</span> Function2&lt;Integer,Iterator&lt;String&gt;,</span><br><span class="line">&gt;     Iterator&lt;String&gt;&gt;() &#123;</span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">&gt; <span class="meta">@Override</span></span><br><span class="line">&gt;        <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(Integer index, Iterator&lt;String&gt; iter)</span><span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt; List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">&gt; <span class="keyword">while</span>(iter.hasNext())&#123;</span><br><span class="line">&gt; String s = iter.next();</span><br><span class="line">&gt; list.add(s+<span class="string">"~"</span>);</span><br><span class="line">&gt;     System.out.println(<span class="string">"partition id is "</span>+index +<span class="string">",value is "</span>+s );</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; <span class="keyword">return</span> list.iterator();</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &#125;, <span class="keyword">true</span>);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼 <strong>repartition</strong></p><p>repartition（3）</p><p>增加或减少分区.会产生shuffle</p><p>🔼<strong>coalesce</strong></p><p>coalesce(3,false)</p><p>常用于减少分区，第二个参数决定减少分区时是否产生shuffle：true 为产生 shuffle，false 不产生 shuffle。默认是 <code>false</code>。</p><p>如果 coalesce 设置的分区数比原来的 RDD 的分区数还多的话，第二个参数设置为 <code>false</code> 不会起作用，</p><p>如果设置成 true，效果和 repartition 一样。即 </p><p>repartition(numPartitions) = coalesce(numPartitions,true)</p></blockquote><blockquote><p>🔼<strong>union</strong></p><p>都保留 （保留总分区数）</p><p>将两个数据集合并为一个数据集。两个数据集的类型要一致。</p><p>javaRDD = rdd1.union(rdd2)</p><ul><li>返回新的 RDD 的分区数是合并 RDD 分区数的总和。</li></ul><p>🔼<strong>intersection</strong></p><p>interRDD = rdd1.intersection(rdd2)</p><p>取两个数据集的交集</p><p>🔼 <strong>subtract</strong></p><p>subRDD = rdd1.subtract(rdd2)</p><p>取两个数据集的差集</p><p>🔼 <strong>distinct</strong>(map+reduceByKey+map)</p><p>distRDD = rdd1.distinct()</p><p>去重</p></blockquote><blockquote><p>:arrow_up_small:<strong>sample</strong><br>随机抽样算子，根据传进去的小数按比例进行，有放回或者无放回的抽样。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; def sample(withReplacement : scala.Boolean, fraction : scala.Double, seed : scala.Long) </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>flatMapToPair.sample(true,0.3,4)</p><blockquote><p>withReplacement   抽出样本是否放回，true为放回，表示抽出来的样本可能重复</p><p>fraction 抽出多少，参数范围0–1</p><p>seed 用于调试是程序问题还是数据问题，将这个参数设置为定值</p></blockquote><p>:arrow_up_small:<strong>reduceByKey</strong><br>对于K，V格式的RDD，将key相同的RDD，对其value值根据相应的逻辑进行处理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">&gt; paralPairs.reduceByKey(</span><br><span class="line">&gt;     <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">&gt;            <span class="meta">@Override</span></span><br><span class="line">&gt;            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;            System.out.println(<span class="string">"v1: "</span> +v1 + <span class="string">" v2: "</span> + v2);</span><br><span class="line">&gt;                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">&gt;             &#125;</span><br><span class="line">&gt;       &#125;).foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">&gt;             <span class="meta">@Override</span></span><br><span class="line">&gt;         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;                System.out.println(tuple2);</span><br><span class="line">&gt;            &#125;</span><br><span class="line">&gt;       &#125;);</span><br><span class="line">&gt; <span class="comment">//  -----------------aggregateByKey----------------------</span></span><br><span class="line">&gt; JavaPairRDD&lt;String, Integer&gt; pairRDD = paralPairs.aggregateByKey(</span><br><span class="line">&gt;     <span class="number">80</span>, <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">&gt;            <span class="meta">@Override</span></span><br><span class="line">&gt;            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;            System.out.println(<span class="string">"map：v1: "</span> +v1+<span class="string">"v2:"</span> + v2);</span><br><span class="line">&gt;                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">&gt;            &#125;</span><br><span class="line">&gt;       &#125;, <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">&gt;           <span class="meta">@Override</span></span><br><span class="line">&gt;           <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;           System.out.println(<span class="string">"reudce：v1:"</span>+v1 +<span class="string">"v2:"</span>+ v2);</span><br><span class="line">&gt;                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">&gt;            &#125;</span><br><span class="line">&gt;       &#125;).foreach(</span><br><span class="line">&gt;     <span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">&gt;           <span class="meta">@Override</span></span><br><span class="line">&gt;          <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;              System.out.println(tuple2);</span><br><span class="line">&gt;           &#125;</span><br><span class="line">&gt;        &#125;);</span><br><span class="line">&gt; <span class="comment">//  -----------------combineByKey----------------------</span></span><br><span class="line">&gt; JavaPairRDD&lt;String, Integer&gt; pairRDD = paralPairs.combineByKey(</span><br><span class="line">&gt;     <span class="keyword">new</span> Function&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">&gt;      <span class="meta">@Override</span></span><br><span class="line">&gt;     <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;            System.out.println(<span class="string">"---初始化-----"</span> + v1);</span><br><span class="line">&gt;             <span class="keyword">return</span> v1 * <span class="number">10</span> ;</span><br><span class="line">&gt;             &#125;</span><br><span class="line">&gt;         &#125;, <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">&gt;             <span class="meta">@Override</span></span><br><span class="line">&gt;          <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;   System.out.println(<span class="string">"map.... v1: "</span> + v1 + <span class="string">" v2: "</span> + v2);</span><br><span class="line">&gt;          <span class="keyword">return</span> v1 + v2;</span><br><span class="line">&gt;             &#125;</span><br><span class="line">&gt;         &#125;, <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">&gt;             <span class="meta">@Override</span></span><br><span class="line">&gt;             <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;  System.out.println(<span class="string">"reduce.. v1: "</span> + v1 + <span class="string">" v2: "</span> + v2);</span><br><span class="line">&gt;             <span class="keyword">return</span> v1 + v2;</span><br><span class="line">&gt;             &#125;</span><br><span class="line">&gt;         &#125;).foreach(</span><br><span class="line">&gt;     <span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">&gt;             <span class="meta">@Override</span></span><br><span class="line">&gt;         <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;          System.out.println(tuple2);</span><br><span class="line">&gt;           &#125;</span><br><span class="line">&gt;    &#125;);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼<strong>reduceByKeyAndWindow</strong> (f1,f2,s1,s2)  </p><p>窗口操作</p><p>:arrow_up_small:<strong>sortByKey/sortBy</strong><br>作用在 K,V 格式的 RDD 上，对 key 进行升序或者降序排序。</p><p> mapToPair.sortByKey()</p></blockquote><blockquote><p>:arrow_up_small:<strong>join / leftOuterJoin / rightOuterJoin / fullOuterJoin</strong></p><p>以上均作用在 K,V 格式的 RDD 上。</p><p><strong>join</strong> ：保留公共元素 ，（即保留V和W中公共元素）</p><p>根据 K 进行连接，对（K,V）join(K,W)返回（K,(V,W)）</p><ul><li>join 后的分区数与父 RDD 分区数多的那一个相同 </li></ul><p><strong>leftOutJoin</strong> ：保留左边的元素</p><p>根据 K 进行连接，对（K,V）leftOutJoin(K,W)返回 （K,（V，Optional（W）））</p><p><strong>rightOutJoin</strong> ：保留右边元素</p><p>根据 K 进行连接，对（K,V）rightOutJoin(K,W)返回 （K,（Optional（V），W））</p><p><strong>fullOutJoin</strong> ：去重保留 （保留最大分区数）</p><p>根据 K 进行连接，对（K,V）rightOutJoin(K,W)返回 （K,（Optional（V），Optional（W）））</p></blockquote><blockquote><p>🔼 <strong>cogroup</strong></p><p>当调用类型（K,V）和（K，W）的数据上时，返回一个数据集（K，（Iterable<v>,Iterable<w>））</w></v></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; // cogroup 与 join不同！</span><br><span class="line">&gt; // 相当于，一个key join上所有value，都给放到一个Iterable里面去！</span><br><span class="line">&gt; JavaPairRDD&lt;String, Tuple2&lt;Iterable&lt;String&gt;, Iterable&lt;String&gt;&gt;&gt; </span><br><span class="line">&gt; studentScores = students.cogroup(scores);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼<strong>groupByKey</strong><br>作用在 K，V 格式的 RDD 上。根据 Key 进行分组。返回（K，Iterable <v>）。</v></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupByKey = </span><br><span class="line">&gt;             parallelizePairs.groupByKey();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><blockquote><p>🔼<strong>zip</strong><br>将两个 RDD 中的元素（KV 格式/非 KV 格式）变成一个 K，V 格式的 RDD,两个 RDD 的个数必须相同。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; nameRDD </span><br><span class="line">&gt; JavaRDD&lt;Integer&gt; scoreRDD</span><br><span class="line">&gt; JavaPairRDD&lt;String, Integer&gt; zip = nameRDD.zip(scoreRDD);</span><br><span class="line">&gt; </span><br><span class="line">&gt; JavaPairRDD&lt;String, String&gt; rdd1</span><br><span class="line">&gt; JavaPairRDD&lt;String, String&gt; rdd2</span><br><span class="line">&gt; JavaPairRDD&lt;Tuple2&lt;String, String&gt;, </span><br><span class="line">&gt; Tuple2&lt;String, String&gt;&gt; result = rdd1.zip(rdd2);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼<strong>zipWithIndex</strong><br>该函数将 RDD 中的元素和这个元素在 RDD 中的索引号（从 0 开始）组合成（K,V）对。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; rdd1 </span><br><span class="line">&gt; JavaPairRDD&lt;String, Long&gt; zipWithIndex = rdd2.zipWithIndex();</span><br><span class="line">&gt; </span><br><span class="line">&gt; JavaPairRDD&lt;String, String&gt; rdd2</span><br><span class="line">&gt; JavaPairRDD&lt;Tuple2&lt;String, String&gt;, Long&gt; zipWithIndex2 = rdd.zipWithIndex();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      </span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WC"</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)  <span class="comment">//用于了解集群</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> linesRDD :<span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"./words.txt"</span>)</span><br><span class="line">      </span><br><span class="line"><span class="comment">//  lineRDD.filter(x=&gt;&#123;</span></span><br><span class="line"><span class="comment">//            x.contains("sh")</span></span><br><span class="line"><span class="comment">//        &#125;).foreach(println)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//  sample 按传入小数，有放回的抽取</span></span><br><span class="line"><span class="comment">//  lineRDD.sample(true,0.2).foreach(println)</span></span><br><span class="line">     </span><br><span class="line"><span class="comment">//  lineRDD.map((_,1)).reduceByKey(_ + _).sortBy(_._2,false).foreach(println)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//  lineRDD.map((_,1)).sortByKey().foreach(println)</span></span><br><span class="line">         </span><br><span class="line">    <span class="keyword">val</span> wordRDD :<span class="type">RDD</span>[<span class="type">String</span>]  = linesRDD.flatMap&#123;lines =&gt; &#123;</span><br><span class="line">      lines.split(<span class="string">" "</span>)  <span class="comment">//匿名函数</span></span><br><span class="line">    &#125;&#125;</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> <span class="type">KVRDD</span>:<span class="type">RDD</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = wordRDD.map&#123; x =&gt; (x,<span class="number">1</span>) &#125;</span><br><span class="line"> </span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> result:<span class="type">RDD</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = <span class="type">KVRDD</span>.reduceByKey&#123;(a,b)=&gt; &#123;</span><br><span class="line">        println(<span class="string">"a:"</span>+a+<span class="string">",b:"</span>+b)</span><br><span class="line">        a+b    </span><br><span class="line">    &#125;&#125;</span><br></pre></td></tr></table></figure><p><code>补充</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    val conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">      </span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WC"</span>)</span><br><span class="line">      </span><br><span class="line">    val context = <span class="keyword">new</span> SparkContext(conf)  <span class="comment">//用于了解集群</span></span><br><span class="line">      </span><br><span class="line">   <span class="comment">//parallelizePairs</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">//join</span></span><br><span class="line">optional.absent(<span class="number">0</span>)</span><br><span class="line">optional.isPresent()</span><br><span class="line">optinal.get()</span><br></pre></td></tr></table></figure><p><code>关于Optional类</code></p><blockquote><p>Optional 类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。</p><p>Optional 是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。</p><p>Optional 类的引入很好的解决空指针异常</p></blockquote><h2 id="5、Action-行动算子"><a href="#5、Action-行动算子" class="headerlink" title="5、Action 行动算子"></a>5、Action 行动算子</h2><h3 id="（1）概念-1"><a href="#（1）概念-1" class="headerlink" title="（1）概念"></a>（1）概念</h3><p>Action 类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count 等。</p><p>Transformations 类算子是延迟执行，Action 类算子是触发执行（立即）。</p><blockquote><p>一个 application 应用程序中有几个 Action 类算子执行，就有几个 job 运行。</p></blockquote><h3 id="（2）Action-类算子"><a href="#（2）Action-类算子" class="headerlink" title="（2）Action 类算子"></a>（2）Action 类算子</h3><blockquote><p>:arrow_up_small: <strong>count</strong><br>返回数据集中的元素数。会在结果计算完成后回收到 Driver 端。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">long</span> count = linesRDD.count();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼<strong>countByKey</strong><br>作用到 K,V 格式的 RDD 上，根据 Key 计数相同 Key 的数据集元素。返回一个Map&lt;K,Object&gt;</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaPairRDD&lt;Integer, String&gt; paralPairs</span><br><span class="line">&gt; </span><br><span class="line">&gt; Map&lt;Integer, Object&gt; map = paralPairs.countByKey();</span><br><span class="line">&gt; </span><br><span class="line">&gt; <span class="keyword">for</span>(Entry&lt;Integer,Object&gt;  entry : map.entrySet())&#123;</span><br><span class="line">&gt;    System.out.println(<span class="string">"key:"</span>+entry.getKey()+<span class="string">" value:"</span>+entry.getValue());</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>🔼<strong>countByValue</strong><br>根据数据集每个元素相同的内容来计数。返回相同内容的元素对应的条数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; Map&lt;Tuple2&lt;Integer, String&gt;, Long&gt; countByValue = paralPairs.countByValue();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>:arrow_up_small: <strong>take(n)</strong><br>返回一个包含数据集前 n 个元素的集合。<br>:arrow_up_small: <strong>first</strong><br>first()=take(1),返回数据集中的第一个元素</p><p>🔼 <strong>collect</strong><br>将RDD类型的数据转换为数组，并将计算结果回收到 Driver 端。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;String&gt; resultRDD </span><br><span class="line">&gt; List&lt;String&gt; collect = resultRDD.collect();</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>:arrow_up_small: <strong>foreach</strong><br>循环遍历数据集中的每个元素，运行相应的逻辑。</p><p>:arrow_up_small: <strong>foreachPartition</strong></p><p>遍历的数据是每个 partition 的数据。所以传的参数为Iterator</p><p>:arrow_up_small:<strong>reduce</strong><br>根据聚合逻辑聚合数据集中的每个元素。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; JavaRDD&lt;Integer&gt; parallelize = </span><br><span class="line">&gt; Integer reduceResult = parallelize.reduce(</span><br><span class="line">&gt;      <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">&gt; </span><br><span class="line">&gt; <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">&gt; <span class="meta">@Override</span></span><br><span class="line">&gt; <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">&gt;        System.out.println(<span class="string">" v1 : "</span> + v1 + <span class="string">" v2 : "</span> + v2);</span><br><span class="line">&gt;         <span class="keyword">return</span> v1+v2;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; &#125;);</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"transf"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineADD = context.textFile(<span class="string">"./wc.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordADD = lineADD.flatMap(x=&gt;&#123;x.split(<span class="string">" "</span>)&#125;)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// println(wordADD.count())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//lineADD中数据回收</span></span><br><span class="line">    <span class="keyword">val</span> arr= lineADD.collect()</span><br><span class="line">    arr.foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//  val takes: Array[String] = lineRDD.take(5)</span></span><br><span class="line"><span class="comment">//  takes.foreach(println)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//  val str: String = lineRDD.first()</span></span><br><span class="line"><span class="comment">//  println(str)</span></span><br></pre></td></tr></table></figure><h2 id="6、控制算子"><a href="#6、控制算子" class="headerlink" title="6、控制算子"></a>6、控制算子</h2><h3 id="（1）概念：-1"><a href="#（1）概念：-1" class="headerlink" title="（1）概念："></a>（1）概念：</h3><ul><li>控制算子有三种，cache、persist、checkpoint</li><li>以上算子都可以将RDD 持久化，持久化的单位是 partition。</li><li>cache 和 persist 都是懒 执行的。必须有一个 action 类算子触发执行。</li><li>cache 和 persist 算子的返回值可赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了</li><li>checkpoint 算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系（所有父RDD）。</li><li><code>错误：</code>rdd.cache().count() 返回的不是持久化的 RDD，而是一个数值了。</li></ul><h3 id="（2）详解"><a href="#（2）详解" class="headerlink" title="（2）详解"></a>（2）详解</h3><blockquote><p>1️⃣<strong>​ cache</strong><br>默认将 RDD 的数据持久化到<code>内存</code>中。cache 是懒执行。</p><ul><li><code>注意</code>：</li></ul><p>chche () =persist()=persist(StorageLevel.Memory_Only)</p></blockquote><blockquote><p>2️⃣ <strong>persist</strong> </p><p>支持指定持久化级别</p><p>useOffHeap  使用堆外内存</p><p>disk、memory、offheap（堆外内存）、deserialized（不序列化）、replication（副本数，默认为1）</p><p>序列化：压缩数据（节省空间，使用数据时要反序列化，会额外消耗CPU性能）</p><p>none 、disk_only、disk_only_2、memeory_only 、memeory_only _ser 、 memory_and_disk 、 memory_and_disk_2</p></blockquote><blockquote><p>3️⃣ <strong>checkpoint</strong>  </p><p>checkpoint 将 RDD 持久化到<code>磁盘</code>，还可以切断 RDD 之间的依赖关系。</p><ul><li>checkpoint 的执行原理：</li></ul><ol><li>当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。</li><li>当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的RDD 做一个标记。</li><li>Spark 框架会自动启动一个新的 job，重新计算这个 RDD 的数据，将数据持久化到 HDFS 上。</li></ol><ul><li>优化：</li></ul><p>对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS上就可以，省去了重新计算这一步。</p></blockquote><p>持久化级别：如下</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g098pwwdc9j30f70aztbu.jpg" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.setMaster(<span class="string">"local"</span>).setAppname(<span class="string">"count"</span>)</span><br><span class="line"><span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置checkpoint在HDFS上的路径</span></span><br><span class="line">context.setCheckPointDir(<span class="string">"./checkpoint"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lineADD = context.textFile(<span class="string">"./countword.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> time1 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> c =  lineADD.count()</span><br><span class="line"><span class="keyword">val</span> time2 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> t1 = time2 - time1</span><br><span class="line"></span><br><span class="line"><span class="comment">//做缓存(persisit（m_o）)</span></span><br><span class="line">lineADD = lineADD.cache()</span><br><span class="line"><span class="comment">//做持久化</span></span><br><span class="line">lineADD.persisit(<span class="type">StorageLevel</span>.memory_only)</span><br><span class="line"><span class="comment">//checkpoint 容错,最好还有cache</span></span><br><span class="line">lineADD.checkpoint()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> time3 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> c =  lineADD.count()</span><br><span class="line"><span class="keyword">val</span> time4 = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"><span class="keyword">val</span> t2 = time4 - time3</span><br><span class="line"></span><br><span class="line"><span class="comment">//t1 远大于 t2</span></span><br></pre></td></tr></table></figure><h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h2><h3 id="以用Java编写为例"><a href="#以用Java编写为例" class="headerlink" title="以用Java编写为例"></a><code>以用Java编写为例</code></h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.shsxt.spark.java;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">        conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"wc"</span>);</span><br><span class="line">        JavaSparkContext context = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        JavaRDD&lt;String&gt; rdd = context.textFile(<span class="string">"./wc.txt"</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">long</span> count = rdd.count();</span><br><span class="line">        List&lt;String&gt; collect = rdd.collect();</span><br><span class="line">        List&lt;String&gt; take = rdd.take(<span class="number">5</span>);</span><br><span class="line">        String first = rdd.first();</span><br><span class="line">        JavaRDD&lt;String&gt; wordRDD = rdd.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                String[] split = line.split(<span class="string">" "</span>);</span><br><span class="line">                List&lt;String&gt; list = Arrays.asList(split);</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        wordRDD.map(new Function&lt;String&gt;() &#123;</span></span><br><span class="line"><span class="comment">//            @Override</span></span><br><span class="line"><span class="comment">//            public String call(String v1) throws Exception &#123;</span></span><br><span class="line"><span class="comment">//                return null;</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//        &#125;)</span></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; pairRDD = wordRDD.mapToPair(</span><br><span class="line">            <span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; resultRDD = pairRDD.reduceByKey(</span><br><span class="line">            <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairRDD&lt;Integer, String&gt; reverseRDD = resultRDD.mapToPair(</span><br><span class="line">            <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(tuple2._2, tuple2._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairRDD&lt;Integer, String&gt; sortByKey = reverseRDD.sortByKey(<span class="keyword">false</span>);</span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; result = sortByKey.mapToPair(</span><br><span class="line">            <span class="keyword">new</span> PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(Tuple2&lt;Integer, String&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(tuple2._2, tuple2._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        result.foreach(<span class="keyword">new</span> VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(tuple2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Linx系统定时调度：</p><p>crontab</p><p>定时调度脚本文件</p><p>脚本文件中，编辑spark 提交命令</p><blockquote><p> <code>注意：</code>  脚本文件中的命令必须写它的完整路径，否则找不到此命令</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算引擎 </tag>
            
            <tag> sparkcore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>String 方法</title>
      <link href="/2019/02/16/String%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/16/String%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> 方法</span><br><span class="line"></span><br><span class="line">char charAt(int index)</span><br><span class="line">返回指定位置的字符  从<span class="number">0</span>开始</span><br><span class="line"></span><br><span class="line">int compareTo(<span class="type">Object</span> o)</span><br><span class="line">比较字符串与对象</span><br><span class="line"></span><br><span class="line">int compareTo(<span class="type">String</span> anotherString)</span><br><span class="line">按字典顺序比较两个字符串</span><br><span class="line"></span><br><span class="line">int compareToIgnoreCase(<span class="type">String</span> str)</span><br><span class="line">按字典顺序比较两个字符串，不考虑大小写</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> concat(<span class="type">String</span> str)</span><br><span class="line">将指定字符串连接到此字符串的结尾</span><br><span class="line"></span><br><span class="line">boolean contentEquals(<span class="type">StringBuffer</span> sb)</span><br><span class="line">将此字符串与指定的 <span class="type">StringBuffer</span> 比较。</span><br><span class="line"></span><br><span class="line">static <span class="type">String</span> copyValueOf(char[] data)</span><br><span class="line">返回指定数组中表示该字符序列的 <span class="type">String</span></span><br><span class="line"></span><br><span class="line">static <span class="type">String</span> copyValueOf(char[] data, int offset, int count)</span><br><span class="line">返回指定数组中表示该字符序列的 <span class="type">String</span></span><br><span class="line"></span><br><span class="line">boolean endsWith(<span class="type">String</span> suffix)</span><br><span class="line">测试此字符串是否以指定的后缀结束</span><br><span class="line"></span><br><span class="line">boolean equals(<span class="type">Object</span> anObject)</span><br><span class="line">将此字符串与指定的对象比较</span><br><span class="line"></span><br><span class="line">boolean equalsIgnoreCase(<span class="type">String</span> anotherString)</span><br><span class="line">将此 <span class="type">String</span> 与另一个 <span class="type">String</span> 比较，不考虑大小写</span><br><span class="line"></span><br><span class="line">byte getBytes()</span><br><span class="line">使用平台的默认字符集将此 <span class="type">String</span> 编码为 byte 序列，并将结果存储到一个新的 byte 数组中</span><br><span class="line"></span><br><span class="line">byte[] getBytes(<span class="type">String</span> charsetName</span><br><span class="line">使用指定的字符集将此 <span class="type">String</span> 编码为 byte 序列，并将结果存储到一个新的 byte 数组中</span><br><span class="line"></span><br><span class="line">void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin)</span><br><span class="line">将字符从此字符串复制到目标字符数组</span><br><span class="line"></span><br><span class="line">int hashCode()</span><br><span class="line">返回此字符串的哈希码</span><br><span class="line"><span class="number">16</span></span><br><span class="line">int indexOf(int ch)</span><br><span class="line">返回指定字符在此字符串中第一次出现处的索引（输入的是ascii码值）</span><br><span class="line"></span><br><span class="line">int indexOf(int ch, int fromIndex)</span><br><span class="line">返返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索</span><br><span class="line"></span><br><span class="line">int indexOf(<span class="type">String</span> str)</span><br><span class="line">返回指定子字符串在此字符串中第一次出现处的索引</span><br><span class="line"></span><br><span class="line">int indexOf(<span class="type">String</span> str, int fromIndex)</span><br><span class="line">返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> intern()</span><br><span class="line">返回字符串对象的规范化表示形式</span><br><span class="line"></span><br><span class="line">int lastIndexOf(int ch)</span><br><span class="line">返回指定字符在此字符串中最后一次出现处的索引</span><br><span class="line"></span><br><span class="line">int lastIndexOf(int ch, int fromIndex)</span><br><span class="line">返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索</span><br><span class="line"></span><br><span class="line">int lastIndexOf(<span class="type">String</span> str)</span><br><span class="line">返回指定子字符串在此字符串中最右边出现处的索引</span><br><span class="line"></span><br><span class="line">int lastIndexOf(<span class="type">String</span> str, int fromIndex)</span><br><span class="line">返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索</span><br><span class="line"></span><br><span class="line">int length()</span><br><span class="line">返回此字符串的长度</span><br><span class="line"></span><br><span class="line">boolean matches(<span class="type">String</span> regex)</span><br><span class="line">告知此字符串是否匹配给定的正则表达式</span><br><span class="line"></span><br><span class="line">boolean regionMatches(boolean ignoreCase, int toffset, <span class="type">String</span> other, int ooffset, int len)</span><br><span class="line">测试两个字符串区域是否相等</span><br><span class="line"><span class="number">28</span></span><br><span class="line">boolean regionMatches(int toffset, <span class="type">String</span> other, int ooffset, int len)</span><br><span class="line">测试两个字符串区域是否相等</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> replace(char oldChar, char newChar)</span><br><span class="line">返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> replaceAll(<span class="type">String</span> regex, <span class="type">String</span> replacement</span><br><span class="line">使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> replaceFirst(<span class="type">String</span> regex, <span class="type">String</span> replacement)</span><br><span class="line">使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串</span><br><span class="line"></span><br><span class="line"><span class="type">String</span>[] split(<span class="type">String</span> regex)</span><br><span class="line">根据给定正则表达式的匹配拆分此字符串</span><br><span class="line"></span><br><span class="line"><span class="type">String</span>[] split(<span class="type">String</span> regex, int limit)</span><br><span class="line">根据匹配给定的正则表达式来拆分此字符串</span><br><span class="line"></span><br><span class="line">boolean startsWith(<span class="type">String</span> prefix)</span><br><span class="line">测试此字符串是否以指定的前缀开始</span><br><span class="line"></span><br><span class="line">boolean startsWith(<span class="type">String</span> prefix, int toffset)</span><br><span class="line">测试此字符串从指定索引开始的子字符串是否以指定前缀开始。</span><br><span class="line"></span><br><span class="line"><span class="type">CharSequence</span> subSequence(int beginIndex, int endIndex)</span><br><span class="line">返回一个新的字符序列，它是此序列的一个子序列</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> substring(int beginIndex)</span><br><span class="line">返回一个新的字符串，它是此字符串的一个子字符串</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> substring(int beginIndex, int endIndex)</span><br><span class="line">返回一个新字符串，它是此字符串的一个子字符串</span><br><span class="line"></span><br><span class="line">char[] toCharArray()</span><br><span class="line">将此字符串转换为一个新的字符数组</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> toLowerCase()</span><br><span class="line">使用默认语言环境的规则将此 <span class="type">String</span> 中的所有字符都转换为小写</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> toLowerCase(<span class="type">Locale</span> locale)</span><br><span class="line">使用给定 <span class="type">Locale</span> 的规则将此 <span class="type">String</span> 中的所有字符都转换为小写</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> toString()</span><br><span class="line">返回此对象本身（它已经是一个字符串！）</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> toUpperCase()</span><br><span class="line">使用默认语言环境的规则将此 <span class="type">String</span> 中的所有字符都转换为大写</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> toUpperCase(<span class="type">Locale</span> locale)</span><br><span class="line">使用给定 <span class="type">Locale</span> 的规则将此 <span class="type">String</span> 中的所有字符都转换为大写</span><br><span class="line"></span><br><span class="line"><span class="type">String</span> trim()</span><br><span class="line">删除指定字符串的首尾空白符</span><br><span class="line"></span><br><span class="line">static <span class="type">String</span> valueOf(primitive data <span class="class"><span class="keyword">type</span> <span class="title">x</span>)</span></span><br><span class="line"><span class="class"><span class="title">返回指定类型参数的字符串表示形式</span></span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> String </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数组方法</title>
      <link href="/2019/02/16/%E6%95%B0%E7%BB%84%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/16/%E6%95%B0%E7%BB%84%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">方法和描述</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>( x: <span class="type">T</span>, xs: <span class="type">T</span>* ): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">创建指定对象 <span class="type">T</span> 的数组, <span class="type">T</span> 的值可以是 <span class="type">Unit</span>, <span class="type">Double</span>, <span class="type">Float</span>, <span class="type">Long</span>, <span class="type">Int</span>, <span class="type">Char</span>, <span class="type">Short</span>, <span class="type">Byte</span>, <span class="type">Boolean</span>。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat</span></span>[<span class="type">T</span>]( xss: <span class="type">Array</span>[<span class="type">T</span>]* ): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">合并数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>( src: <span class="type">AnyRef</span>, srcPos: <span class="type">Int</span>, dest: <span class="type">AnyRef</span>, destPos: <span class="type">Int</span>, length: <span class="type">Int</span> ): <span class="type">Unit</span></span><br><span class="line">复制一个数组到另一个数组上。相等于 <span class="type">Java</span><span class="symbol">'s</span> <span class="type">System</span>.arraycopy(src, srcPos, dest, destPos, length)。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">empty</span></span>[<span class="type">T</span>]: <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">返回长度为 <span class="number">0</span> 的数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterate</span></span>[<span class="type">T</span>]( start: <span class="type">T</span>, len: <span class="type">Int</span> )( f: (<span class="type">T</span>) =&gt; <span class="type">T</span> ): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">返回指定长度数组，每个数组元素为指定函数的返回值。</span><br><span class="line">以上实例数组初始值为 <span class="number">0</span>，长度为 <span class="number">3</span>，计算函数为a=&gt;a+<span class="number">1</span>：</span><br><span class="line">scala&gt; <span class="type">Array</span>.iterate(<span class="number">0</span>,<span class="number">3</span>)(a=&gt;a+<span class="number">1</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fill</span></span>[<span class="type">T</span>]( n: <span class="type">Int</span> )(elem: =&gt; <span class="type">T</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">返回数组，长度为第一个参数指定，同时每个元素使用第二个参数进行填充。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fill</span></span>[<span class="type">T</span>]( n1: <span class="type">Int</span>, n2: <span class="type">Int</span> )( elem: =&gt; <span class="type">T</span> ): <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br><span class="line">返回二数组，长度为第一个参数指定，同时每个元素使用第二个参数进行填充。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ofDim</span></span>[<span class="type">T</span>]( n1: <span class="type">Int</span> ): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">创建指定长度的数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ofDim</span></span>[<span class="type">T</span>]( n1: <span class="type">Int</span>, n2: <span class="type">Int</span> ): <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br><span class="line">创建二维数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ofDim</span></span>[<span class="type">T</span>]( n1: <span class="type">Int</span>, n2: <span class="type">Int</span>, n3: <span class="type">Int</span> ): <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">T</span>]]]</span><br><span class="line">创建三维数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">range</span></span>( start: <span class="type">Int</span>, end: <span class="type">Int</span>, step: <span class="type">Int</span> ): <span class="type">Array</span>[<span class="type">Int</span>]</span><br><span class="line">创建指定区间内的数组，step 为每个元素间的步长</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">range</span></span>( start: <span class="type">Int</span>, end: <span class="type">Int</span> ): <span class="type">Array</span>[<span class="type">Int</span>]</span><br><span class="line">创建指定区间内的数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tabulate</span></span>[<span class="type">T</span>]( n: <span class="type">Int</span> )(f: (<span class="type">Int</span>)=&gt; <span class="type">T</span>): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line">返回指定长度数组，每个数组元素为指定函数的返回值，默认从 <span class="number">0</span> 开始。</span><br><span class="line">以上实例返回 <span class="number">3</span> 个元素：</span><br><span class="line">scala&gt; <span class="type">Array</span>.tabulate(<span class="number">3</span>)(a =&gt; a + <span class="number">5</span>)</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tabulate</span></span>[<span class="type">T</span>]( n1: <span class="type">Int</span>, n2: <span class="type">Int</span> )( f: (<span class="type">Int</span>, <span class="type">Int</span> ) =&gt; <span class="type">T</span>): <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br><span class="line">返回指定长度的二维数组，每个数组元素为指定函数的返回值，默认从 <span class="number">0</span> 开始。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Array </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试问题总结</title>
      <link href="/2019/02/16/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
      <url>/2019/02/16/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">悲观锁 + 乐观锁</span><br><span class="line"></span><br><span class="line">悲观锁（Pessimistic Lock）</span><br><span class="line">每次获取数据的时候，都会担心数据被修改，所以每次操作数据前都会进行加锁（读锁、写锁、行锁等），确保在自己在使用数据过程中，数据戹被其他进程修改，使用完成后再对数据进行解锁。由于数据被上了所，期间对数据进行读写的其他进程都需等待。在Java中，synchronized的思想也是悲观锁。</span><br><span class="line"></span><br><span class="line">乐观锁（Optimistic Lock）</span><br><span class="line">每次获取数据的时候，都不会担心数据被修改，所以每次获取数据的时候都不会进行加锁，但在更新数据的时候需要判断数据是否被别人修改。如果数据被其他线程更改，就不进行数据更新。如果数据没有被其他线程更改，就进行数据更新。因为数据没有进行加锁，期间数据可以被进行读写操作。</span><br><span class="line">一般会使用版本机制或CAS操作实现。</span><br><span class="line"></span><br><span class="line"> version方式：</span><br><span class="line">一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值与当前数据库中的version值相等，才更新，否则重试更新操作，直到更新成功</span><br><span class="line">SQL：update table set x=x+1, version=version+1 where id=#&#123;id&#125; and version=#&#123;version&#125;;  </span><br><span class="line"> CAS操作方式：</span><br><span class="line"> 即compare and swap 或者 compare and set，涉及到三个操作数，数据所在的内存值，预期值，新值。当需要更新时，判断当前内存值与之前取到的值是否相等，若相等，则用新值更新，若失败则重试，一般情况下是一个自旋操作，即不断的重试。</span><br><span class="line"></span><br><span class="line">适用场景：</span><br><span class="line">悲观锁：适合写操作频繁的场景。如果用于读操作频繁的场景，每次读取都进行加锁，会增加大量锁的开销，降低系统吞吐率。</span><br><span class="line"></span><br><span class="line">乐观锁：适合读操作频繁的场景如果用于写操作频繁的场景，数据发生冲突的可能性就会增加，为例保证数据的一致性，应用层需要不断的重新获取数据，这样就带来了大量的查询操作，降低系统的吞吐率。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Map </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Map方法</title>
      <link href="/2019/02/16/Map%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/16/Map%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Scala</span> <span class="type">Map</span> 方法</span><br><span class="line">下表列出了 <span class="type">Scala</span> <span class="type">Map</span> 常用的方法：</span><br><span class="line">序号方法及描述</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">++</span></span>(xs: <span class="type">Map</span>[(<span class="type">A</span>, <span class="type">B</span>)]): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回一个新的 <span class="type">Map</span>，新的 <span class="type">Map</span> xs 组成</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">-</span></span>(elem1: <span class="type">A</span>, elem2: <span class="type">A</span>, elems: <span class="type">A</span>*): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回一个新的 <span class="type">Map</span>, 移除 key 为 elem1, elem2 或其他 elems。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">--</span></span>(xs: <span class="type">GTO</span>[<span class="type">A</span>]): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回一个新的 <span class="type">Map</span>, 移除 xs 对象中对应的 key</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(key: <span class="type">A</span>): <span class="type">Option</span>[<span class="type">B</span>]</span><br><span class="line">返回指定 key 的值</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>: <span class="type">Iterator</span>[(<span class="type">A</span>, <span class="type">B</span>)]</span><br><span class="line">创建新的迭代器，并输出 key/value 对</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addString</span></span>(b: <span class="type">StringBuilder</span>): <span class="type">StringBuilder</span></span><br><span class="line">将 <span class="type">Map</span> 中的所有元素附加到<span class="type">StringBuilder</span>，可加入分隔符</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addString</span></span>(b: <span class="type">StringBuilder</span>, sep: <span class="type">String</span>): <span class="type">StringBuilder</span></span><br><span class="line">将 <span class="type">Map</span> 中的所有元素附加到<span class="type">StringBuilder</span>，可加入分隔符</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">A</span>): <span class="type">B</span></span><br><span class="line">返回指定键的值，如果不存在返回 <span class="type">Map</span> 的默认方法</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clone</span></span>(): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">从一个 <span class="type">Map</span> 复制到另一个 <span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">contains</span></span>(key: <span class="type">A</span>): <span class="type">Boolean</span></span><br><span class="line">如果 <span class="type">Map</span> 中存在指定 key，返回 <span class="literal">true</span>，否则返回 <span class="literal">false</span>。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copyToArray</span></span>(xs: <span class="type">Array</span>[(<span class="type">A</span>, <span class="type">B</span>)]): <span class="type">Unit</span></span><br><span class="line">复制集合到数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(p: ((<span class="type">A</span>, <span class="type">B</span>)) =&gt; <span class="type">Boolean</span>): <span class="type">Int</span></span><br><span class="line">计算满足指定条件的集合元素数量</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">default</span></span>(key: <span class="type">A</span>): <span class="type">B</span></span><br><span class="line">定义 <span class="type">Map</span> 的默认值，在 key 不存在时返回。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(n: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回丢弃前n个元素新集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropRight</span></span>(n: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回丢弃最后n个元素新集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropWhile</span></span>(p: ((<span class="type">A</span>, <span class="type">B</span>)) =&gt; <span class="type">Boolean</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">从左向右丢弃元素，直到条件p不成立</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">empty</span></span>: <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回相同类型的空 <span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(that: <span class="type">Any</span>): <span class="type">Boolean</span></span><br><span class="line">如果两个 <span class="type">Map</span> 相等(key/value 均相等)，返回<span class="literal">true</span>，否则返回<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span></span>(p: ((<span class="type">A</span>, <span class="type">B</span>)) =&gt; <span class="type">Boolean</span>): <span class="type">Boolean</span></span><br><span class="line">判断集合中指定条件的元素是否存在</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(p: ((<span class="type">A</span>, <span class="type">B</span>))=&gt; <span class="type">Boolean</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回满足指定条件的所有集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filterKeys</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回符合指定条件的的不可变 <span class="type">Map</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span></span>(p: ((<span class="type">A</span>, <span class="type">B</span>)) =&gt; <span class="type">Boolean</span>): <span class="type">Option</span>[(<span class="type">A</span>, <span class="type">B</span>)]</span><br><span class="line">查找集合中满足指定条件的第一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: ((<span class="type">A</span>, <span class="type">B</span>)) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">将函数应用到集合的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span></span>: <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回所有元素，除了最后一个</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span></span>: <span class="type">Boolean</span></span><br><span class="line">检测 <span class="type">Map</span> 是否为空</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keys</span></span>: <span class="type">Iterable</span>[<span class="type">A</span>]</span><br><span class="line">返回所有的key/p&gt;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">last</span></span>: (<span class="type">A</span>, <span class="type">B</span>)</span><br><span class="line">返回最后一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>: (<span class="type">A</span>, <span class="type">B</span>)</span><br><span class="line">查找最大元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>: (<span class="type">A</span>, <span class="type">B</span>)</span><br><span class="line">查找最小元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>: <span class="type">String</span></span><br><span class="line">集合所有元素作为字符串显示</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">product</span></span>: (<span class="type">A</span>, <span class="type">B</span>)</span><br><span class="line">返回集合中数字元素的积。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove</span></span>(key: <span class="type">A</span>): <span class="type">Option</span>[<span class="type">B</span>]</span><br><span class="line">移除指定 key</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retain</span></span>(p: (<span class="type">A</span>, <span class="type">B</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Map</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line">如果符合满足条件的返回 <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">size</span></span>: <span class="type">Int</span></span><br><span class="line">返回 <span class="type">Map</span> 元素的个数</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>: (<span class="type">A</span>, <span class="type">B</span>)</span><br><span class="line">返回集合中所有数字元素之和</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回一个集合中除了第一元素之外的其他元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(n: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回前 n 个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeRight</span></span>(n: <span class="type">Int</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回后 n 个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeWhile</span></span>(p: ((<span class="type">A</span>, <span class="type">B</span>)) =&gt; <span class="type">Boolean</span>): <span class="type">Map</span>[<span class="type">A</span>, <span class="type">B</span>]</span><br><span class="line">返回满足指定条件的元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toArray</span></span>: <span class="type">Array</span>[(<span class="type">A</span>, <span class="type">B</span>)]</span><br><span class="line">集合转数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toBuffer</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>]: <span class="type">Buffer</span>[<span class="type">B</span>]</span><br><span class="line">返回缓冲区，包含了 <span class="type">Map</span> 的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toList</span></span>: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回 <span class="type">List</span>，包含了 <span class="type">Map</span> 的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toSeq</span></span>: <span class="type">Seq</span>[<span class="type">A</span>]</span><br><span class="line">返回 <span class="type">Seq</span>，包含了 <span class="type">Map</span> 的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toSet</span></span>: <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回 <span class="type">Set</span>，包含了 <span class="type">Map</span> 的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>(): <span class="type">String</span></span><br><span class="line">返回字符串对象</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Map </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Set方法</title>
      <link href="/2019/02/16/Set%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/16/Set%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Scala</span> <span class="type">Set</span> 常用方法</span><br><span class="line">下表列出了 <span class="type">Scala</span> <span class="type">Set</span> 常用的方法：</span><br><span class="line">序号方法及描述</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">+</span></span>(elem: <span class="type">A</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">为集合添加新元素，x并创建一个新的集合，除非元素已存在</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">-</span></span>(elem: <span class="type">A</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">移除集合中的元素，并创建一个新的集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">contains</span></span>(elem: <span class="type">A</span>): <span class="type">Boolean</span></span><br><span class="line">如果元素在集合中存在，返回 <span class="literal">true</span>，否则返回 <span class="literal">false</span>。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">&amp;</span></span>(that: <span class="type">Set</span>[<span class="type">A</span>]): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回两个集合的交集</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">&amp;~</span></span>(that: <span class="type">Set</span>[<span class="type">A</span>]): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回两个集合的差集</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">+</span></span>(elem1: <span class="type">A</span>, elem2: <span class="type">A</span>, elems: <span class="type">A</span>*): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">通过添加传入指定集合的元素创建一个新的不可变集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">++</span></span>(elems: <span class="type">A</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">合并两个集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">-</span></span>(elem1: <span class="type">A</span>, elem2: <span class="type">A</span>, elems: <span class="type">A</span>*): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">通过移除传入指定集合的元素创建一个新的不可变集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addString</span></span>(b: <span class="type">StringBuilder</span>): <span class="type">StringBuilder</span></span><br><span class="line">将不可变集合的所有元素添加到字符串缓冲区</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addString</span></span>(b: <span class="type">StringBuilder</span>, sep: <span class="type">String</span>): <span class="type">StringBuilder</span></span><br><span class="line">将不可变集合的所有元素添加到字符串缓冲区，并使用指定的分隔符</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(elem: <span class="type">A</span>)</span><br><span class="line">检测集合中是否包含指定元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Int</span></span><br><span class="line">计算满足指定条件的集合元素个数</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copyToArray</span></span>(xs: <span class="type">Array</span>[<span class="type">A</span>], start: <span class="type">Int</span>, len: <span class="type">Int</span>): <span class="type">Unit</span></span><br><span class="line">复制不可变集合元素到数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">diff</span></span>(that: <span class="type">Set</span>[<span class="type">A</span>]): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">比较两个集合的差集</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(n: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">A</span>]]</span><br><span class="line">返回丢弃前n个元素新集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropRight</span></span>(n: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回丢弃最后n个元素新集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropWhile</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">从左向右丢弃元素，直到条件p不成立</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(that: <span class="type">Any</span>): <span class="type">Boolean</span></span><br><span class="line">equals 方法可用于任意序列。用于比较系列是否相等。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Boolean</span></span><br><span class="line">判断不可变集合中指定条件的元素是否存在。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">输出符合指定条件的所有不可变集合元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Option</span>[<span class="type">A</span>]</span><br><span class="line">查找不可变集合中满足指定条件的第一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forall</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Boolean</span></span><br><span class="line">查找不可变集合中满足指定条件的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: (<span class="type">A</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">将函数应用到不可变集合的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">A</span></span><br><span class="line">获取不可变集合的第一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span></span>: <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回所有元素，除了最后一个</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>(that: <span class="type">Set</span>[<span class="type">A</span>]): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">计算两个集合的交集</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span></span>: <span class="type">Boolean</span></span><br><span class="line">判断集合是否为空</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>: <span class="type">Iterator</span>[<span class="type">A</span>]</span><br><span class="line">创建一个新的迭代器来迭代元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">last</span></span>: <span class="type">A</span></span><br><span class="line">返回最后一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">B</span>](f: (<span class="type">A</span>) =&gt; <span class="type">B</span>): immutable.<span class="type">Set</span>[<span class="type">B</span>]</span><br><span class="line">通过给定的方法将所有元素重新计算</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>: <span class="type">A</span></span><br><span class="line">查找最大元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>: <span class="type">A</span></span><br><span class="line">查找最小元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>: <span class="type">String</span></span><br><span class="line">集合所有元素作为字符串显示</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>(sep: <span class="type">String</span>): <span class="type">String</span></span><br><span class="line">使用分隔符将集合所有元素作为字符串显示</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">product</span></span>: <span class="type">A</span></span><br><span class="line">返回不可变集合中数字元素的积。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">size</span></span>: <span class="type">Int</span></span><br><span class="line">返回不可变集合元素的数量</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitAt</span></span>(n: <span class="type">Int</span>): (<span class="type">Set</span>[<span class="type">A</span>], <span class="type">Set</span>[<span class="type">A</span>])</span><br><span class="line">把不可变集合拆分为两个容器，第一个由前 n 个元素组成，第二个由剩下的元素组成</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsetOf</span></span>(that: <span class="type">Set</span>[<span class="type">A</span>]): <span class="type">Boolean</span></span><br><span class="line">如果集合<span class="type">A</span>中含有子集<span class="type">B</span>返回 <span class="literal">true</span>，否则返回<span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>: <span class="type">A</span></span><br><span class="line">返回不可变集合中所有数字元素之和</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回一个不可变集合中除了第一元素之外的其他元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(n: <span class="type">Int</span>): <span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回前 n 个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeRight</span></span>(n: <span class="type">Int</span>):<span class="type">Set</span>[<span class="type">A</span>]</span><br><span class="line">返回后 n 个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toArray</span></span>: <span class="type">Array</span>[<span class="type">A</span>]</span><br><span class="line">将集合转换为数组</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toBuffer</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>]: <span class="type">Buffer</span>[<span class="type">B</span>]</span><br><span class="line">返回缓冲区，包含了不可变集合的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toList</span></span>: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回 <span class="type">List</span>，包含了不可变集合的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toMap</span></span>[<span class="type">T</span>, <span class="type">U</span>]: <span class="type">Map</span>[<span class="type">T</span>, <span class="type">U</span>]</span><br><span class="line">返回 <span class="type">Map</span>，包含了不可变集合的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toSeq</span></span>: <span class="type">Seq</span>[<span class="type">A</span>]</span><br><span class="line">返回 <span class="type">Seq</span>，包含了不可变集合的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>(): <span class="type">String</span></span><br><span class="line">返回一个字符串，以对象来表示</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Set </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>List方法</title>
      <link href="/2019/02/16/List%E6%96%B9%E6%B3%95/"/>
      <url>/2019/02/16/List%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">+</span></span>(elem: <span class="type">A</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">前置一个元素列表</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> </span>::(x: <span class="type">A</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">在这个列表的开头添加的元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> </span>:::(prefix: <span class="type">List</span>[<span class="type">A</span>]): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">增加了一个给定列表中该列表前面的元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> </span>::(x: <span class="type">A</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">增加了一个元素x在列表的开头</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addString</span></span>(b: <span class="type">StringBuilder</span>): <span class="type">StringBuilder</span></span><br><span class="line">追加列表的一个字符串生成器的所有元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addString</span></span>(b: <span class="type">StringBuilder</span>, sep: <span class="type">String</span>): <span class="type">StringBuilder</span></span><br><span class="line">追加列表的使用分隔字符串一个字符串生成器的所有元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(n: <span class="type">Int</span>): <span class="type">A</span></span><br><span class="line">选择通过其在列表中索引的元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">contains</span></span>(elem: <span class="type">Any</span>): <span class="type">Boolean</span></span><br><span class="line">测试该列表中是否包含一个给定值作为元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copyToArray</span></span>(xs: <span class="type">Array</span>[<span class="type">A</span>], start: <span class="type">Int</span>, len: <span class="type">Int</span>): <span class="type">Unit</span></span><br><span class="line">列表的副本元件阵列。填充给定的数组xs与此列表中最多len个元素，在位置开始。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">建立从列表中没有任何重复的元素的新列表。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(n: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回除了第n个的所有元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropRight</span></span>(n: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回除了最后的n个的元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropWhile</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">丢弃满足谓词的元素最长前缀。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">endsWith</span></span>[<span class="type">B</span>](that: <span class="type">Seq</span>[<span class="type">B</span>]): <span class="type">Boolean</span></span><br><span class="line">测试列表是否使用给定序列结束。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equals</span></span>(that: <span class="type">Any</span>): <span class="type">Boolean</span></span><br><span class="line">equals方法的任意序列。比较该序列到某些其他对象。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exists</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Boolean</span></span><br><span class="line">测试谓词是否持有一些列表的元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回列表满足谓词的所有元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forall</span></span>(p: (<span class="type">A</span>) =&gt; <span class="type">Boolean</span>): <span class="type">Boolean</span></span><br><span class="line">测试谓词是否持有该列表中的所有元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: (<span class="type">A</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span></span><br><span class="line">应用一个函数f以列表的所有元素。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">head</span></span>: <span class="type">A</span></span><br><span class="line">选择列表的第一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">indexOf</span></span>(elem: <span class="type">A</span>, from: <span class="type">Int</span>): <span class="type">Int</span></span><br><span class="line">经过或在某些起始索引查找列表中的一些值第一次出现的索引。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span></span>: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回除了最后的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">intersect</span></span>(that: <span class="type">Seq</span>[<span class="type">A</span>]): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">计算列表和另一序列之间的多重集交集。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isEmpty</span></span>: <span class="type">Boolean</span></span><br><span class="line">测试列表是否为空</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>: <span class="type">Iterator</span>[<span class="type">A</span>]</span><br><span class="line">创建一个新的迭代器中包含的可迭代对象中的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">last</span></span>: <span class="type">A</span></span><br><span class="line">返回最后一个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lastIndexOf</span></span>(elem: <span class="type">A</span>, end: <span class="type">Int</span>): <span class="type">Int</span></span><br><span class="line">之前或在一个给定的最终指数查找的列表中的一些值最后一次出现的索引</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">length</span></span>: <span class="type">Int</span></span><br><span class="line">返回列表的长度</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">B</span>](f: (<span class="type">A</span>) =&gt; <span class="type">B</span>): <span class="type">List</span>[<span class="type">B</span>]</span><br><span class="line">通过应用函数以g这个列表中的所有元素构建一个新的集合</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max</span></span>: <span class="type">A</span></span><br><span class="line">查找最大的元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">min</span></span>: <span class="type">A</span></span><br><span class="line">查找最小元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>: <span class="type">String</span></span><br><span class="line">显示列表的字符串中的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkString</span></span>(sep: <span class="type">String</span>): <span class="type">String</span></span><br><span class="line">显示的列表中的字符串中使用分隔串的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reverse</span></span>: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回新列表，在相反的顺序元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sorted</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>]: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">根据排序对列表进行排序</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startsWith</span></span>[<span class="type">B</span>](that: <span class="type">Seq</span>[<span class="type">B</span>], offset: <span class="type">Int</span>): <span class="type">Boolean</span></span><br><span class="line">测试该列表中是否包含给定的索引处的给定的序列</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>: <span class="type">A</span></span><br><span class="line">概括这个集合的元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tail</span></span>: <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回除了第一的所有元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(n: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回前n个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeRight</span></span>(n: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">A</span>]</span><br><span class="line">返回最后n个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toArray</span></span>: <span class="type">Array</span>[<span class="type">A</span>]</span><br><span class="line">列表以一个数组变换</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toBuffer</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>]: <span class="type">Buffer</span>[<span class="type">B</span>]</span><br><span class="line">列表以一个可变缓冲器转换</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toMap</span></span>[<span class="type">T</span>, <span class="type">U</span>]: <span class="type">Map</span>[<span class="type">T</span>, <span class="type">U</span>]</span><br><span class="line">此列表的映射转换</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toSeq</span></span>: <span class="type">Seq</span>[<span class="type">A</span>]</span><br><span class="line">列表的序列转换</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toSet</span></span>[<span class="type">B</span> &gt;: <span class="type">A</span>]: <span class="type">Set</span>[<span class="type">B</span>]</span><br><span class="line">列表到集合变换</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toString</span></span>(): <span class="type">String</span></span><br><span class="line">列表转换为字符串</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> List </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scala学习</title>
      <link href="/2019/02/15/Scala%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/02/15/Scala%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、Scala官网6个特征。"><a href="#一、Scala官网6个特征。" class="headerlink" title="一、Scala官网6个特征。"></a>一、Scala官网6个特征。</h1><h2 id="简捷，快速"><a href="#简捷，快速" class="headerlink" title="简捷，快速"></a>简捷，快速</h2><h2 id="1、Java"><a href="#1、Java" class="headerlink" title="1、Java"></a>1、Java</h2><ul><li>与Java无缝整合，运行在JVM上，编译形成.class文件</li></ul><h2 id="2、类型"><a href="#2、类型" class="headerlink" title="2、类型"></a>2、类型</h2><ul><li>类型自动推断:var 变量类型   val 常量类型<br> （var  s = 1 自动推断s 为int类型 ）<br>  dos窗口运行Scala语言（cmd  - &gt;scala）  </li></ul><h2 id="3、并发"><a href="#3、并发" class="headerlink" title="3、并发"></a>3、并发</h2><ul><li>底层有actor，天生用于高并发和分布式</li></ul><h2 id="4、继承"><a href="#4、继承" class="headerlink" title="4、继承"></a>4、继承</h2><ul><li>trait 特征特质（Java中接口和抽象类的结合体？？？两者区别？？单继承多实现）<br>​    静态语言  （Java静态语言  shell 、Python动态语言）</li></ul><h2 id="5、匹配"><a href="#5、匹配" class="headerlink" title="5、匹配"></a>5、匹配</h2><ul><li>模式匹配:(Java中的switch case类型必须一致)可匹配多种类型</li></ul><h2 id="6、高阶"><a href="#6、高阶" class="headerlink" title="6、高阶"></a>6、高阶</h2><ul><li>高阶函数（函数式编程）函数可以作为参数传入方法中（Jdk 8 stream流，莱姆塔表达式）</li></ul><p>​       工具类中方法为静态的</p><h1 id="二、Scala安装"><a href="#二、Scala安装" class="headerlink" title="二、Scala安装"></a>二、Scala安装</h1><h2 id="1、windows安装-配置环境变量"><a href="#1、windows安装-配置环境变量" class="headerlink" title="1、windows安装,配置环境变量"></a>1、windows安装,配置环境变量</h2><p>Ø  官网下载scala2.10：（因为spark需要的是这个版本的Scala）</p><p><a href="http://www.scala-lang.org/download/2.10.4.html" target="_blank" rel="noopener">http://www.scala-lang.org/download/2.10.4.html</a> </p><p>Ø  下载好后安装。双击msi包安装,记住安装的路径。</p><p>Ø  配置环境变量（和配置jdk一样）</p><ul><li><p>新建SCALA_HOME</p></li><li><p>编辑Path变量，在后面追加如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">;%SCALA_HOME%\bin</span><br></pre></td></tr></table></figure></li></ul><p>Ø  打开cmd,输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala  - version</span><br></pre></td></tr></table></figure><p>看是否显示版本号，确定是否安装成功</p><p><img src="images/Scala.jpg" alt=""></p><h2 id="2、eclipse-配置scala插件"><a href="#2、eclipse-配置scala插件" class="headerlink" title="2、eclipse 配置scala插件"></a>2、eclipse 配置scala插件</h2><p>Ø  下载插件（一定要对应eclipse版本下载）,并解压</p><p><a href="http://scala-ide.org/download/prev-stable.html" target="_blank" rel="noopener">http://scala-ide.org/download/prev-stable.html</a></p><p><img src="images/eclipse.jpg" alt=""></p><p>Ø  将解压目录下的features和plugins两个文件夹拷贝到eclipse安装目录中的”dropins/scala”目录下。</p><p>进入dropins，新建scala文件夹，将两个文件夹拷贝到“dropins/scala”下</p><h2 id="3、Scala编辑器：scala-ide"><a href="#3、Scala编辑器：scala-ide" class="headerlink" title="3、Scala编辑器：scala ide"></a>3、Scala编辑器：scala ide</h2><p>下载网址：<a href="http://scala-ide.org/download/sdk.html" target="_blank" rel="noopener">http://scala-ide.org/download/sdk.html</a> </p><h2 id="4、Idea-中配置scala插件"><a href="#4、Idea-中配置scala插件" class="headerlink" title="4、Idea 中配置scala插件"></a>4、Idea 中配置scala插件</h2><p>Ø  打开idea,close项目后，点击Configure-&gt;Plugins</p><p>Ø  搜索scala，点击Install安装</p><p>Ø  设置jdk，打开Project Structure,点击new 选择安装好的jdk路径</p><p>Ø 新建Scala项目</p><p><img src="images/s1.jpg" alt=""></p><p><img src="images/s2.jpg" alt=""></p><h1 id="三、Scala基础语法"><a href="#三、Scala基础语法" class="headerlink" title="三、Scala基础语法"></a>三、Scala基础语法</h1><h2 id="1、数据类型"><a href="#1、数据类型" class="headerlink" title="1、数据类型"></a>1、数据类型</h2><p><img src="images/s4.jpg" alt=""></p><p><img src="images/s5.jpg" alt=""></p><h2 id="2、变量和常量的声明"><a href="#2、变量和常量的声明" class="headerlink" title="2、变量和常量的声明"></a><strong>2</strong>、<strong>变量和常量的声明</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义变量和常量</span></span><br><span class="line"><span class="comment">     * 变量 :用 var 定义 ，可修改 </span></span><br><span class="line"><span class="comment">     * 常量 :用 val 定义，不可修改</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">var</span> name = <span class="string">"zhangsan"</span></span><br><span class="line">    println(name)</span><br><span class="line">    name =<span class="string">"lisi"</span></span><br><span class="line">    println(name)</span><br><span class="line">    <span class="keyword">val</span> gender = <span class="string">"m"</span></span><br><span class="line"><span class="comment">//    gender = "m"//错误，不能给常量再赋值</span></span><br><span class="line"><span class="comment">// 定义变量或者常量的时候，也可以写上返回的类型，一般省略，如：val a:Int = 10</span></span><br></pre></td></tr></table></figure><h2 id="3、类和对象"><a href="#3、类和对象" class="headerlink" title="3、类和对象"></a><strong>3、</strong>类和对象</h2><p>Ø  创建类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name = <span class="string">"zhangsan"</span></span><br><span class="line">  <span class="keyword">val</span> age = <span class="number">18</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayName</span></span>() = &#123;</span><br><span class="line">    <span class="string">"my name is "</span>+ name</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Ø  创建对象</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lesson_Class</span> </span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span>()</span><br><span class="line">    println(person.age);</span><br><span class="line">    println(person.sayName())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Ø  伴生类和伴生对象</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">xname :<span class="type">String</span> , xage :<span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> name = <span class="type">Person</span>.name</span><br><span class="line">  <span class="keyword">val</span> age = xage</span><br><span class="line">  <span class="keyword">var</span> gender = <span class="string">"m"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name:<span class="type">String</span>,age:<span class="type">Int</span>,g:<span class="type">String</span>)&#123;</span><br><span class="line">    <span class="keyword">this</span>(name,age)</span><br><span class="line">    gender = g</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sayName</span></span>() = &#123;</span><br><span class="line">    <span class="string">"my name is "</span>+ name</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> name = <span class="string">"zhangsanfeng"</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> person = <span class="keyword">new</span> <span class="type">Person</span>(<span class="string">"wagnwu"</span>,<span class="number">10</span>,<span class="string">"f"</span>)</span><br><span class="line">    println(person.age);</span><br><span class="line">    println(person.sayName())</span><br><span class="line">    println(person.gender)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>注意</code></p><blockquote><ul><li>建议类名首字母大写 ，方法首字母小写，类和方法命名建议符合驼峰命名法。</li><li>.一行结束，不需要分号。如果一行里有多个语句，则之间用分号隔开。</li><li>scala 中的object是单例对象，相当于java中的工具类，它里面的方法可以看成都是static静态的。object不可以传参数。另：Trait不可以传参数</li><li><p>scala中的class类默认可以传参数，默认的传参数就是默认的构造函数。</p><p>重写构造函数的时候，必须要先调用默认的构造函数。</p></li><li><p>class 类属性自带getter ，setter方法。</p></li><li>使用object时，不用new，使用class时要new ,并且new的时候，class中除了方法不执行，其他都执行。</li><li>如果在同一个文件中，object对象和class类的名称相同，则这个对象就是这个类的伴生对象，class称为object对象的伴生类，object 称为class类的伴生对象，他们可以直接访问对方的私有变量。</li></ul></blockquote><h2 id="3-if-else"><a href="#3-if-else" class="headerlink" title="3.  if else"></a><strong>3.</strong>  <strong>if else</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * if else </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">val</span> age =<span class="number">18</span> </span><br><span class="line"><span class="keyword">if</span> (age &lt; <span class="number">18</span> )&#123;</span><br><span class="line">println(<span class="string">"no allow"</span>)</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="number">18</span>&lt;=age&amp;&amp;age&lt;=<span class="number">20</span>)&#123;</span><br><span class="line">println(<span class="string">"allow with other"</span>)</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">println(<span class="string">"allow self"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-for-while-do…while"><a href="#4-for-while-do…while" class="headerlink" title="4.   for ,while,do…while"></a><strong>4.</strong>   for ,while,do…while</h2><ul><li>to和until 的用法（不带步长，带步长区别）</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**        </span></span><br><span class="line"><span class="comment">  * to和until        </span></span><br><span class="line"><span class="comment">  * 例：        </span></span><br><span class="line"><span class="comment">  * 1 to 10 返回1到10的Range数组，包含10        </span></span><br><span class="line"><span class="comment">  * 1 until 10 返回1到10 Range数组 ，不包含10        </span></span><br><span class="line"><span class="comment">  */</span>              </span><br><span class="line">println(<span class="number">1</span> to <span class="number">10</span> )<span class="comment">//打印： Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)      </span></span><br><span class="line">println(<span class="number">1.</span>to(<span class="number">10</span>))<span class="comment">//同上</span></span><br><span class="line"></span><br><span class="line">println(<span class="number">1</span> to (<span class="number">10</span> ,<span class="number">2</span>))<span class="comment">//步长为2，从1开始打印： Range(1, 3, 5, 7, 9)      </span></span><br><span class="line">println(<span class="number">1.</span>to(<span class="number">10</span>, <span class="number">2</span>)) <span class="comment">//同上</span></span><br><span class="line"></span><br><span class="line">println(<span class="number">1</span> until <span class="number">10</span> ) <span class="comment">//不包含最后一个数，打印1,2,3,4,5,6,7,8,9     </span></span><br><span class="line">println(<span class="number">1.</span>until(<span class="number">10</span>))<span class="comment">//同上          </span></span><br><span class="line">println(<span class="number">1</span> until (<span class="number">10</span> ,<span class="number">3</span> ))<span class="comment">//步长为2，从1开始打印，打印1,4,7</span></span><br></pre></td></tr></table></figure><ul><li>创建for循环</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * for 循环</span></span><br><span class="line"><span class="comment">  * </span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="keyword">for</span>( i &lt;- <span class="number">1</span> to <span class="number">10</span> )&#123;</span><br><span class="line">   println(i)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><ul><li>创建多层for循环</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//可以分号隔开，写入多个list赋值的变量，构成多层for循环</span></span><br><span class="line">    <span class="comment">//scala中 不能写count++ count-- 只能写count+</span></span><br><span class="line">    <span class="keyword">var</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>; j &lt;- <span class="number">1</span> until <span class="number">10</span>)&#123;</span><br><span class="line">      println(<span class="string">"i="</span>+ i +<span class="string">",j="</span>+j)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    println(count);</span><br><span class="line"> <span class="comment">//例子： 打印小九九</span></span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">1</span> until <span class="number">10</span> ;j &lt;- <span class="number">1</span> until <span class="number">10</span>)&#123;</span><br><span class="line">      <span class="keyword">if</span>(i&gt;=j)&#123;</span><br><span class="line">      print(i +<span class="string">" * "</span> + j + <span class="string">" = "</span>+ i*j+<span class="string">""</span>)        </span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(i==j )&#123;</span><br><span class="line">        println()</span><br><span class="line">      &#125;      </span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//九九乘法表</span></span><br><span class="line">     <span class="comment">//方法一</span></span><br><span class="line"><span class="comment">//      for(i&lt;- 1 to 9 )&#123;</span></span><br><span class="line"><span class="comment">//        for (j &lt;- 1 to i)&#123;</span></span><br><span class="line"><span class="comment">//          print(j+"*"+i+"="+i*j+"\t")</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//        println()</span></span><br><span class="line"><span class="comment">//      &#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//方法二</span></span><br><span class="line"><span class="comment">//    for(i&lt;- 1 to 9 )&#123;</span></span><br><span class="line"><span class="comment">//      for (j&lt;- 1 to 9)&#123;</span></span><br><span class="line"><span class="comment">//        if(j&lt;=i)&#123;</span></span><br><span class="line"><span class="comment">//          print(j+"*"+i+"="+i*j+"\t")</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        if(i==j)println()</span></span><br><span class="line"><span class="comment">//      &#125;</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br></pre></td></tr></table></figure><ul><li>for循环中可以加条件判断，分号隔开</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//可以在for循环中加入条件判断</span></span><br><span class="line">    <span class="keyword">for</span>(i&lt;- <span class="number">1</span> to <span class="number">10</span> ;<span class="keyword">if</span> (i%<span class="number">2</span>) == <span class="number">0</span> ;<span class="keyword">if</span> (i == <span class="number">4</span>) )&#123;</span><br><span class="line">      println(i)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>scala中没有++，–，不能使用count++，count只能使用count = count+1 ，count += 1</p></li><li><p>while循环，while（）{}，do {}while()</p></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//将for中的符合条件的元素通过yield关键字返回成一个集合</span></span><br><span class="line">    <span class="keyword">val</span> list = <span class="keyword">for</span>(i &lt;- <span class="number">1</span> to <span class="number">10</span>  ; <span class="keyword">if</span>(i &gt; <span class="number">5</span> )) <span class="keyword">yield</span> i </span><br><span class="line"><span class="comment">//&lt;-  后面是一个集合</span></span><br><span class="line">    <span class="keyword">for</span>( w &lt;- list )&#123;</span><br><span class="line">      println(w)</span><br><span class="line">&#125;</span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * while 循环</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">var</span> index = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">while</span>(index &lt; <span class="number">100</span> )&#123;</span><br><span class="line">    println(<span class="string">"第"</span>+index+<span class="string">"次while 循环"</span>)</span><br><span class="line">      index += <span class="number">1</span> </span><br><span class="line">    &#125;</span><br><span class="line"> index = <span class="number">0</span> </span><br><span class="line">    do&#123;</span><br><span class="line">    index +=<span class="number">1</span> </span><br><span class="line">    println(<span class="string">"第"</span>+index+<span class="string">"次do while 循环"</span>)</span><br><span class="line">&#125;<span class="keyword">while</span>(index &lt;<span class="number">100</span> )</span><br></pre></td></tr></table></figure><h1 id="四、Scala函数"><a href="#四、Scala函数" class="headerlink" title="四、Scala函数"></a>四、Scala函数</h1><h2 id="1、函数定义"><a href="#1、函数定义" class="headerlink" title="1、函数定义"></a>1、函数定义</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span> </span>(a: <span class="type">Int</span> , b: <span class="type">Int</span> ) : <span class="type">Unit</span> = &#123;</span><br><span class="line">   println(a+b)</span><br><span class="line"> &#125;</span><br><span class="line">fun(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun1</span> </span>(a : <span class="type">Int</span> , b : <span class="type">Int</span>)= a+b</span><br><span class="line">    println(fun1(<span class="number">1</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p><code>语法解释</code></p><blockquote><ul><li>函数定义语法 用def来定义</li><li>可以定义传入的参数，要指定传入参数的类型</li><li>scala中函数中如果返回返回值类型为Unit ，即无返回值</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(x:<span class="type">Int</span>=<span class="number">10</span>,y:<span class="type">Int</span>=<span class="number">11</span>):<span class="type">Unit</span> =&#123;</span><br><span class="line">&gt;         x+y</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt; <span class="comment">//写返回值类型是=时，一定要记得写 ：（冒号）</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><ul><li><p>scala中函数有返回值时，可以写return，也可以不写return：</p></li><li><ul><li>省略return的时候，函数自动回将最后一行的表达式的值，作为返回值</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;  <span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(x:<span class="type">Int</span>,y:<span class="type">Int</span> )=&#123;</span><br><span class="line">&gt;       <span class="keyword">if</span> (x&gt;y)</span><br><span class="line">&gt;           x</span><br><span class="line">&gt;       <span class="keyword">else</span></span><br><span class="line">&gt;          y</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt;  println(max(<span class="number">5</span>,<span class="number">7</span>))</span><br><span class="line">&gt; 结果显示：<span class="number">7</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><pre><code>* * 如果函数有retrun,则必须写返回类型。</code></pre><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;   <span class="function"><span class="keyword">def</span> <span class="title">min</span></span>(m:<span class="type">Int</span>,n:<span class="type">Int</span>):<span class="type">Int</span>=&#123;</span><br><span class="line">&gt;       <span class="keyword">if</span>(m&gt;n)</span><br><span class="line">&gt;         <span class="keyword">return</span> n</span><br><span class="line">&gt;       <span class="keyword">else</span></span><br><span class="line">&gt;         <span class="keyword">return</span> m</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt;     println(min(<span class="number">5</span>,<span class="number">7</span>))</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><pre><code>* scala中函数有返回值时，可以写返回值的类型，也可以省略，因为scala可以类型自动推断，有时候不能省略，必须写，* * 比如在递归函数中或者函数的返回值是函数类型的时候。</code></pre><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;  <span class="function"><span class="keyword">def</span> <span class="title">num</span></span>(x: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">&gt;             <span class="keyword">if</span> (x == <span class="number">1</span>)</span><br><span class="line">&gt;                 <span class="number">1</span></span><br><span class="line">&gt;             <span class="keyword">else</span> &#123;</span><br><span class="line">&gt;                 x * num(x - <span class="number">1</span>)</span><br><span class="line">&gt;             &#125;</span><br><span class="line">&gt;         &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><pre><code>* 函数定义的时候，如果去掉 = ，那么这个方法返回类型必定是Unit的。     这种说法无论方法体里面什么逻辑都成立，scala可以把任意类型转换为Unit。     假设，里面的逻辑最后返回了一个string，那么这个返回值会被转换成Unit，并且值会被丢弃。     则相当于，函数就将返回值去掉，即无返回值。* {}里的代码，如果只有一行，则可以省略{}*  传递给方法的参数可以在方法中使用，并且scala规定方法的传过来的参数为val类型，不能修改，不是var。</code></pre></blockquote><h2 id="2、递归函数"><a href="#2、递归函数" class="headerlink" title="2、递归函数"></a>2、递归函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 递归函数 </span></span><br><span class="line"><span class="comment">    * 5的阶乘</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">fun2</span></span>(num :<span class="type">Int</span>) :<span class="type">Int</span>= &#123;</span><br><span class="line">     <span class="keyword">if</span>(num ==<span class="number">1</span>)</span><br><span class="line">       num</span><br><span class="line">     <span class="keyword">else</span> </span><br><span class="line">       num * fun2(num<span class="number">-1</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   print(fun2(<span class="number">5</span>))</span><br></pre></td></tr></table></figure><h2 id="3、包含参数默认值的函数"><a href="#3、包含参数默认值的函数" class="headerlink" title="3、包含参数默认值的函数"></a>3、<strong>包含参数默认值的函数</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 包含默认参数值的函数</span></span><br><span class="line"><span class="comment">    * 注意：</span></span><br><span class="line"><span class="comment">    * 1.默认值的函数中，如果传入的参数个数与函数定义相同，则传入的数值会覆盖默认值</span></span><br><span class="line"><span class="comment">    * 2.如果不想覆盖默认值，传入的参数个数小于定义的函数的参数，则需要指定参数名称</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">fun3</span></span>(a :<span class="type">Int</span> = <span class="number">10</span>,b:<span class="type">Int</span>) = &#123;</span><br><span class="line">     println(a+b)</span><br><span class="line">   &#125;</span><br><span class="line">   fun3(b=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="4、可变参数个数的函数"><a href="#4、可变参数个数的函数" class="headerlink" title="4、可变参数个数的函数"></a><strong>4</strong>、可变参数个数的函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 可变参数个数的函数</span></span><br><span class="line"><span class="comment"> * 注意：多个参数逗号分开</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun4</span></span>(elem :<span class="type">Int</span>*)=&#123;</span><br><span class="line">  <span class="keyword">var</span> sum = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(e &lt;- elem)&#123;</span><br><span class="line">    sum += e</span><br><span class="line">  &#125;</span><br><span class="line">  sum</span><br><span class="line">&#125;</span><br><span class="line">println(fun4(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><h2 id="5、匿名函数"><a href="#5、匿名函数" class="headerlink" title="5、匿名函数"></a><strong>5、</strong>匿名函数</h2><ul><li><p>有参匿名函数</p></li><li><p>无参匿名函数</p></li><li><p>有返回值的匿名函数</p></li></ul><blockquote><p>可以将匿名函数返回给val定义的值</p><p>匿名函数不能显式声明函数的返回类型</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//有参数匿名函数</span></span><br><span class="line"><span class="keyword">val</span> fun1 = (a : <span class="type">Int</span> ， b : <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  println(a+b)</span><br><span class="line">&#125;</span><br><span class="line">value1(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//无参数匿名函数</span></span><br><span class="line"><span class="keyword">val</span> fun2 = ()=&gt;&#123;</span><br><span class="line">  println(<span class="string">"啦啦啦"</span>)</span><br><span class="line">&#125;</span><br><span class="line">fun2()</span><br><span class="line"></span><br><span class="line"><span class="comment">//有返回值的匿名函数</span></span><br><span class="line"><span class="keyword">val</span> fun3 = (a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt;&#123;</span><br><span class="line">  a+b</span><br><span class="line">&#125;</span><br><span class="line">println(fun3(<span class="number">4</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><h2 id="6、-嵌套函数"><a href="#6、-嵌套函数" class="headerlink" title="6、 嵌套函数"></a><strong>6、</strong> <strong>嵌套函数</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 嵌套函数</span></span><br><span class="line"><span class="comment">    * 例如：嵌套函数求5的阶乘</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">fun5</span></span>(num:<span class="type">Int</span>)=&#123;</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">fun6</span></span>(a:<span class="type">Int</span>,b:<span class="type">Int</span>):<span class="type">Int</span>=&#123;</span><br><span class="line">       <span class="keyword">if</span>(a == <span class="number">1</span>)&#123;</span><br><span class="line">         b</span><br><span class="line">       &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">         fun6(a<span class="number">-1</span>,a*b)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     fun6(num,<span class="number">1</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   println(fun5(<span class="number">5</span>))</span><br></pre></td></tr></table></figure><h2 id="7、偏应用函数"><a href="#7、偏应用函数" class="headerlink" title="7、偏应用函数"></a><strong>7、偏应用函数</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 偏应用函数是一种表达式</span></span><br><span class="line"><span class="comment">     * 不需要提供函数需要的所有参数，</span></span><br><span class="line"><span class="comment">     * 只需要提供部分，或不提供所需参数。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(date :<span class="type">Date</span>, s :<span class="type">String</span>)= &#123;</span><br><span class="line">      println(<span class="string">"date is "</span>+ date +<span class="string">",log is "</span>+ s)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> date = <span class="keyword">new</span> <span class="type">Date</span>()</span><br><span class="line">    log(date ,<span class="string">"log1"</span>)</span><br><span class="line">    log(date ,<span class="string">"log2"</span>)</span><br><span class="line">    log(date ,<span class="string">"log3"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//想要调用log，以上变化的是第二个参数，可以用偏应用函数处理,来优化log方法</span></span><br><span class="line">   <span class="comment">/* 绑定第一个 date 参数，</span></span><br><span class="line"><span class="comment">    * 第二个参数使用下划线(_)替换缺失的参数列表，</span></span><br><span class="line"><span class="comment">    * 并把这个新的函数值的索引的赋给变量。</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    <span class="keyword">val</span> logWithDate = log(date,_:<span class="type">String</span>)</span><br><span class="line">    logWithDate(<span class="string">"log11"</span>)</span><br><span class="line">    logWithDate(<span class="string">"log22"</span>)</span><br><span class="line">    logWithDate(<span class="string">"log33"</span>)</span><br></pre></td></tr></table></figure><h2 id="8、高阶函数"><a href="#8、高阶函数" class="headerlink" title="8、高阶函数"></a>8、<strong>高阶函数</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 高阶函数:就是操作其他函数的函数</span></span><br><span class="line"><span class="comment">     * 函数的参数是函数</span></span><br><span class="line"><span class="comment">     * 或者函数的返回是函数</span></span><br><span class="line"><span class="comment">     * 或者函数的参数和返回都是函数</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="comment">//函数作为参数或者返回累心时，只需指明函数中的类型    </span></span><br><span class="line">    <span class="comment">//函数的参数是函数：</span></span><br><span class="line">    <span class="comment">//f : (Int,Int) =&gt;Int  （两个Int型参数、Int型返回值）</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hightFun</span></span>(f : (<span class="type">Int</span>,<span class="type">Int</span>) =&gt;<span class="type">Int</span>, a:<span class="type">Int</span> ) : <span class="type">Int</span> = &#123;</span><br><span class="line">      f(a,<span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(v1 :<span class="type">Int</span>,v2: <span class="type">Int</span>):<span class="type">Int</span>  = &#123;</span><br><span class="line">      v1+v2</span><br><span class="line">    &#125;  </span><br><span class="line">    println(hightFun(f, <span class="number">1</span>))</span><br><span class="line">          <span class="comment">//***************</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">test1</span></span>(x: <span class="type">Int</span>, f: (<span class="type">Int</span>, <span class="type">Int</span>) =&gt; <span class="type">Int</span>) = &#123;</span><br><span class="line">            <span class="keyword">var</span> a = x + <span class="number">100</span></span><br><span class="line">            a * f(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        &#125;</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">            x - y</span><br><span class="line">        &#125;</span><br><span class="line">     println(test6(<span class="number">10</span>,sum))</span><br><span class="line"><span class="comment">//------------------------------------------------</span></span><br><span class="line">    <span class="comment">//函数的返回值类型为函数 ：(Int,Int)=&gt;Int  f2</span></span><br><span class="line">    <span class="comment">//1，2,3,4相加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hightFun2</span></span>(a : <span class="type">Int</span>,b:<span class="type">Int</span>) : (<span class="type">Int</span>,<span class="type">Int</span>)=&gt;<span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">f2</span> </span>(v1: <span class="type">Int</span>,v2:<span class="type">Int</span>) :<span class="type">Int</span> = &#123;</span><br><span class="line">        v1+v2+a+b</span><br><span class="line">      &#125;</span><br><span class="line">      f2</span><br><span class="line">    &#125;</span><br><span class="line">    println(hightFun2(<span class="number">1</span>,<span class="number">2</span>)(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">            <span class="comment">//***********</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test2</span></span>(x: <span class="type">Int</span>): (<span class="type">Int</span>) =&gt; <span class="type">String</span> = &#123;</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">concat</span></span>(y: <span class="type">Int</span>): <span class="type">String</span> = &#123;</span><br><span class="line">                x + <span class="string">" !! "</span> + y</span><br><span class="line">            &#125;</span><br><span class="line">            concat</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">val</span> concat: <span class="type">Int</span> =&gt; <span class="type">String</span> = test2(<span class="number">5</span>)</span><br><span class="line">    println(concat(<span class="number">7</span>))</span><br><span class="line"><span class="comment">//        println(test2(5)(7))</span></span><br><span class="line"> <span class="comment">//-------------------------------------------------   </span></span><br><span class="line">    <span class="comment">//函数的参数是函数:  f : (Int ,Int) =&gt; Int)</span></span><br><span class="line">    <span class="comment">//函数的返回是函数:  (Int,Int) =&gt; Int</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hightFun3</span></span>(f : (<span class="type">Int</span> ,<span class="type">Int</span>) =&gt; <span class="type">Int</span>) : (<span class="type">Int</span>,<span class="type">Int</span>) =&gt; <span class="type">Int</span> = &#123;</span><br><span class="line">      f</span><br><span class="line">    &#125; </span><br><span class="line">    println(hightFun3(f)(<span class="number">100</span>,<span class="number">200</span>))</span><br><span class="line">    println(hightFun3((a,b) =&gt;&#123;a+b&#125;)(<span class="number">200</span>,<span class="number">200</span>))</span><br><span class="line">    <span class="comment">//以上这句话还可以写成这样</span></span><br><span class="line">    <span class="comment">//如果函数的参数在方法体中只使用了一次 那么可以写成_表示</span></span><br><span class="line">    println(hightFun3(_+_)(<span class="number">200</span>,<span class="number">200</span>))</span><br><span class="line">            <span class="comment">//**********</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test3</span></span>(y: <span class="type">Int</span>, f: (<span class="type">Int</span>) =&gt; <span class="type">Int</span>): (<span class="type">Int</span>) =&gt; <span class="type">Int</span> = &#123;</span><br><span class="line">            <span class="keyword">var</span> sum = f(<span class="number">1</span>) + y</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">sum4</span></span>(x: <span class="type">Int</span>) = &#123;</span><br><span class="line">                x + sum</span><br><span class="line">            &#125;</span><br><span class="line">            sum4</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">//        def f2 (x: Int) =&#123;</span></span><br><span class="line"><span class="comment">//            x + 10</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line">        <span class="keyword">val</span> f = test3(<span class="number">10</span>,(x:<span class="type">Int</span>)=&gt;&#123;x + <span class="number">10</span>&#125;)</span><br><span class="line"><span class="comment">//        println(f(5))</span></span><br></pre></td></tr></table></figure><h2 id="9、柯里化函数"><a href="#9、柯里化函数" class="headerlink" title="9、柯里化函数"></a>9、柯里化函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 柯里化函数</span></span><br><span class="line"><span class="comment">    * 可以理解为高阶函数的简化</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">fun1</span></span>(a :<span class="type">Int</span>,b:<span class="type">Int</span>)(c:<span class="type">Int</span>,d:<span class="type">Int</span>) = &#123;</span><br><span class="line">     a+b+c+d</span><br><span class="line">   &#125;</span><br><span class="line">   println(fun1(<span class="number">1</span>,<span class="number">2</span>)(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">      <span class="comment">//*******</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fun</span></span>(a :<span class="type">Int</span>)(c:<span class="type">Int</span>) = &#123;</span><br><span class="line">           a+c</span><br><span class="line">       &#125;</span><br><span class="line">   println(fun(<span class="number">1</span>)(<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h1 id="五、字符串"><a href="#五、字符串" class="headerlink" title="五、字符串"></a>五、字符串</h1><h2 id="1、string-stringBuilder-可变"><a href="#1、string-stringBuilder-可变" class="headerlink" title="1、string  | stringBuilder (可变)"></a>1、string  | stringBuilder (可变)</h2><h2 id="2、操作方法"><a href="#2、操作方法" class="headerlink" title="2、操作方法"></a>2、操作方法</h2><p>Ø  比较:equals</p><p>Ø  比较忽略大小写:equalsIgnoreCase</p><p>Ø  indexOf：如果字符串中有传入的assci码对应的值，返回下标</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * String &amp;&amp; StringBuilder</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> str = <span class="string">"abcd"</span></span><br><span class="line">    <span class="keyword">val</span> str1 = <span class="string">"ABCD"</span></span><br><span class="line">    </span><br><span class="line">    println(str.indexOf(<span class="number">97</span>))</span><br><span class="line">    println(str.indexOf(<span class="string">"b"</span>))</span><br><span class="line">    </span><br><span class="line">    println(str==str1)</span><br><span class="line">    println(str.equals(str1))</span><br><span class="line">    println(str.equalsIgnoreCase(str1))</span><br><span class="line">  </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * compareToIgnoreCase</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     * 如果参数字符串等于此字符串，则返回值 0；</span></span><br><span class="line"><span class="comment">     * 如果此字符串小于字符串参数，则返回一个小于 0 的值；</span></span><br><span class="line"><span class="comment">     * 如果此字符串大于字符串参数，则返回一个大于 0 的值。</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    println(str.compareToIgnoreCase(str1))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> strBuilder = <span class="keyword">new</span> <span class="type">StringBuilder</span>() <span class="comment">//括号可省</span></span><br><span class="line">    strBuilder.append(<span class="string">"abc"</span>)</span><br><span class="line"><span class="comment">//一个 + 只能跟单个字符且必须用单引号，否则无效 </span></span><br><span class="line"><span class="comment">//    strBuilder.+('d')</span></span><br><span class="line">    strBuilder+ 'd'</span><br><span class="line"><span class="comment">//    strBuilder.+=('h')</span></span><br><span class="line">    strBuilder+= 'h' </span><br><span class="line"></span><br><span class="line"><span class="comment">//两个+ 可跟多个字符且必须用双引号</span></span><br><span class="line"><span class="comment">//    strBuilder.++=("efg")</span></span><br><span class="line">    strBuilder++= <span class="string">"efg"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//StringBuilder可以追加浮点数</span></span><br><span class="line">    strBuilder.append(<span class="number">1.0</span>)</span><br><span class="line">    strBuilder.append(<span class="number">18</span>f)</span><br><span class="line">    println(strBuilder)</span><br><span class="line"> println(strBuilder.toString())</span><br></pre></td></tr></table></figure><h1 id="六、集合"><a href="#六、集合" class="headerlink" title="六、集合"></a>六、集合</h1><h2 id="1、数组"><a href="#1、数组" class="headerlink" title="1、数组"></a>1、数组</h2><h3 id="（1）创建一维数组"><a href="#（1）创建一维数组" class="headerlink" title="（1）创建一维数组"></a>（1）创建一维数组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 创建数组两种方式：</span></span><br><span class="line"><span class="comment">    * 1.new Array[String](3)</span></span><br><span class="line"><span class="comment">    * 2.直接Array</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   </span><br><span class="line">   <span class="comment">//创建类型为Int 长度为3的数组</span></span><br><span class="line">   <span class="keyword">val</span> arr1 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">3</span>)</span><br><span class="line">   <span class="comment">//创建String 类型的数组，直接赋值</span></span><br><span class="line">   <span class="keyword">val</span> arr2 = <span class="type">Array</span>[<span class="type">String</span>](<span class="string">"s100"</span>,<span class="string">"s200"</span>,<span class="string">"s300"</span>)</span><br><span class="line">   <span class="comment">//赋值</span></span><br><span class="line">   arr1(<span class="number">0</span>) = <span class="number">100</span></span><br><span class="line">   arr1(<span class="number">1</span>) = <span class="number">200</span></span><br><span class="line">   arr1(<span class="number">2</span>) = <span class="number">300</span></span><br></pre></td></tr></table></figure><h3 id="（2）数组遍历"><a href="#（2）数组遍历" class="headerlink" title="（2）数组遍历"></a>（2）数组遍历</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 遍历两种方式</span></span><br><span class="line"><span class="comment">  * for</span></span><br><span class="line"><span class="comment">  * foreach</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="keyword">for</span>(i &lt;- arr1)&#123;</span><br><span class="line">   println(i)</span><br><span class="line"> &#125;</span><br><span class="line"> arr1.foreach(i =&gt; &#123;</span><br><span class="line">   println(i)</span><br><span class="line"> &#125;)</span><br><span class="line"> </span><br><span class="line"> <span class="keyword">for</span>(s &lt;- arr2)&#123;</span><br><span class="line">   println(s)</span><br><span class="line"> &#125;</span><br><span class="line"> arr2.foreach &#123; </span><br><span class="line">   x =&gt; println(x) </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="（3）数组操作"><a href="#（3）数组操作" class="headerlink" title="（3）数组操作"></a>（3）数组操作</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Array</span>.concat：合并数组</span><br><span class="line"><span class="type">Array</span>.fill(<span class="number">5</span>)(“shsxt”)：创建初始值的定长数组</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> a = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="number">3</span>)</span><br><span class="line">        a(<span class="number">0</span>) = <span class="number">1</span></span><br><span class="line">        a(<span class="number">1</span>) = <span class="number">2</span></span><br><span class="line">        a(<span class="number">2</span>) = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//判断数组中符合条件的元素的个数</span></span><br><span class="line"><span class="comment">//        val n = a.count(x=&gt;&#123;</span></span><br><span class="line"><span class="comment">//            if(x&gt;2)</span></span><br><span class="line"><span class="comment">//                true</span></span><br><span class="line"><span class="comment">//            else</span></span><br><span class="line"><span class="comment">//                false</span></span><br><span class="line"><span class="comment">//        &#125;)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        println(n)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> b = <span class="type">Array</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="comment">//将两个数组的元素合并在一个新的数组中</span></span><br><span class="line">        <span class="keyword">val</span> ints = <span class="type">Array</span>.concat(a,b)</span><br><span class="line"><span class="comment">//遍历数组</span></span><br><span class="line">        ints.foreach(println)</span><br><span class="line"><span class="comment">//创建包含5个指定元素的数组</span></span><br><span class="line">        <span class="keyword">val</span> strings = <span class="type">Array</span>.fill(<span class="number">5</span>)(<span class="string">"shsxt"</span>)</span><br><span class="line">        strings.foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        for(i &lt;- a)&#123;</span></span><br><span class="line"><span class="comment">//            println(i)</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        val b = Array(4,5,6)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//        for(i &lt;- b)&#123;</span></span><br><span class="line"><span class="comment">//            println(i)</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        a.foreach(x=&gt; &#123; println(x) &#125; )</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        a.foreach(println(_))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//         a.foreach(println)</span></span><br><span class="line"><span class="comment">//        val c = new Array[Array[Int]](3)</span></span><br><span class="line"><span class="comment">//        c(0) = Array(1,2,3)</span></span><br><span class="line"><span class="comment">//        c(1) = Array(4,5,6)</span></span><br><span class="line"><span class="comment">//        c(2) = Array(7,8,9)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//        c.foreach(x=&gt;&#123;</span></span><br><span class="line"><span class="comment">//            x.foreach(y=&gt;&#123;</span></span><br><span class="line"><span class="comment">//                print(y + "\t")</span></span><br><span class="line"><span class="comment">//            &#125;)</span></span><br><span class="line"><span class="comment">//            println()</span></span><br><span class="line"><span class="comment">//        &#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//        for(i&lt;-c)&#123;</span></span><br><span class="line"><span class="comment">//            for(j &lt;-i)&#123;</span></span><br><span class="line"><span class="comment">//                print(j + "\t")</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//            println()</span></span><br><span class="line"><span class="comment">//        &#125;</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="（4）创建二维数组"><a href="#（4）创建二维数组" class="headerlink" title="（4）创建二维数组"></a>（4）创建二维数组</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 创建二维数组和遍历</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">val</span> arr3 = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">String</span>]](<span class="number">3</span>)</span><br><span class="line">  arr3(<span class="number">0</span>)=<span class="type">Array</span>(<span class="string">"1"</span>,<span class="string">"2"</span>,<span class="string">"3"</span>)</span><br><span class="line">  arr3(<span class="number">1</span>)=<span class="type">Array</span>(<span class="string">"4"</span>,<span class="string">"5"</span>,<span class="string">"6"</span>)</span><br><span class="line">  arr3(<span class="number">2</span>)=<span class="type">Array</span>(<span class="string">"7"</span>,<span class="string">"8"</span>,<span class="string">"9"</span>)</span><br><span class="line">  <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until arr3.length)&#123;</span><br><span class="line">    <span class="keyword">for</span>(j &lt;- <span class="number">0</span> until arr3(i).length)&#123;</span><br><span class="line">      print(arr3(i)(j)+<span class="string">""</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    println()</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span>(arr &lt;- arr3 ;i &lt;- arr)&#123;</span><br><span class="line">    <span class="keyword">if</span>(count%<span class="number">3</span> == <span class="number">0</span>)&#123;</span><br><span class="line">      println()</span><br><span class="line">    &#125;</span><br><span class="line">    print(i+<span class="string">""</span>)</span><br><span class="line">    count +=<span class="number">1</span> </span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  arr3.foreach &#123; arr  =&gt; &#123;</span><br><span class="line">    arr.foreach &#123; println &#125;</span><br><span class="line">  &#125;&#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> arr4 = <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]](<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),<span class="type">Array</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">  arr4.foreach &#123; arr =&gt; &#123;</span><br><span class="line">    arr.foreach(i =&gt; &#123;</span><br><span class="line">      println(i)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;&#125;</span><br><span class="line">  println(<span class="string">"-------"</span>)</span><br><span class="line">  <span class="keyword">for</span>(arr &lt;- arr4;i &lt;- arr)&#123;</span><br><span class="line">    println(i)</span><br></pre></td></tr></table></figure><h2 id="2、list"><a href="#2、list" class="headerlink" title="2、list"></a>2、list</h2><h2 id="（1）创建list"><a href="#（1）创建list" class="headerlink" title="（1）创建list"></a>（1）创建list</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p><code>备注</code>  Nil      长度为0的list</p><h2 id="（2）list遍历"><a href="#（2）list遍历" class="headerlink" title="（2）list遍历"></a>（2）list遍历</h2><blockquote><p>foreach ，for</p></blockquote><h2 id="（3）list操作"><a href="#（3）list操作" class="headerlink" title="（3）list操作"></a>（3）list操作</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">//创建</span></span><br><span class="line">    <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//遍历</span></span><br><span class="line">    list.foreach &#123; x =&gt; println(x)&#125;</span><br><span class="line"><span class="comment">//    list.foreach &#123; println&#125;</span></span><br><span class="line">    <span class="comment">//filter 过滤元素</span></span><br><span class="line">    <span class="keyword">val</span> list1  = list.filter &#123; x =&gt; x&gt;<span class="number">3</span> &#125;</span><br><span class="line">    list1.foreach &#123; println&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//count  计算符合条件的元素个数</span></span><br><span class="line">    <span class="keyword">val</span> value = list1.count &#123; x =&gt; x&gt;<span class="number">3</span> &#125;</span><br><span class="line">    println(value)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//map 对元素操作 如切分</span></span><br><span class="line">    <span class="keyword">val</span> nameList = <span class="type">List</span>(</span><br><span class="line">    <span class="string">"hello shsxt"</span>,</span><br><span class="line">    <span class="string">"hello xasxt"</span>,</span><br><span class="line">    <span class="string">"hello shsxt"</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">val</span> mapResult:<span class="type">List</span>[<span class="type">Array</span>[<span class="type">String</span>]] = nameList.map&#123; </span><br><span class="line">        x =&gt; x.split(<span class="string">" "</span>) </span><br><span class="line">    &#125;</span><br><span class="line">    mapResult.foreach&#123;println&#125;    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">//flatmap 将元素压扁平,先map再flat</span></span><br><span class="line">    <span class="keyword">val</span> flatMapResult : <span class="type">List</span>[<span class="type">String</span>] = nameList.flatMap&#123; x =&gt; x.split(<span class="string">" "</span>) &#125;</span><br><span class="line">    flatMapResult.foreach &#123; println &#125;</span><br></pre></td></tr></table></figure><p><img src="images/map.jpg" alt=""></p><h2 id="3、set"><a href="#3、set" class="headerlink" title="3、set"></a>3、set</h2><p><code>注意：</code>set集合自动去重</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> s = <span class="type">Set</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> s1 = <span class="type">Set</span>(<span class="number">1</span>，<span class="number">2</span>，<span class="number">3</span>，<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历时，自动去重</span></span><br><span class="line">s.foreach(println) </span><br><span class="line"><span class="keyword">for</span>(x&lt;- s)&#123;println(x)&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//交集</span></span><br><span class="line"><span class="keyword">val</span> inse = s.intersect(s1)</span><br><span class="line"><span class="keyword">val</span> ins = s.&amp;(s1)</span><br><span class="line">inse.foreach(println)</span><br><span class="line">inse.foreach&#123;println&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//差集</span></span><br><span class="line"><span class="keyword">val</span> di = s.diff(s1)</span><br><span class="line"><span class="keyword">val</span> de = s.&amp;~(s1)</span><br><span class="line"></span><br><span class="line"><span class="comment">//子集:s是不是s1的子集</span></span><br><span class="line"><span class="keyword">val</span> boo: <span class="type">Boolean</span> = s.subsetof(s1)</span><br><span class="line"></span><br><span class="line"><span class="comment">//max ,min</span></span><br><span class="line"><span class="keyword">val</span> max = s.max</span><br><span class="line"><span class="keyword">val</span> min = s.min</span><br><span class="line">println(max + <span class="string">":"</span> + min)</span><br><span class="line"></span><br><span class="line"><span class="comment">//转成数组，list</span></span><br><span class="line">s.toArray.foreach(println)</span><br><span class="line">s.toList.foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//mkString , 元素以！分隔拼成字符串</span></span><br><span class="line">println(s.mkString)</span><br><span class="line">println(s.mkstring(<span class="string">"!"</span>))</span><br></pre></td></tr></table></figure><h2 id="4、map"><a href="#4、map" class="headerlink" title="4、map"></a>4、map</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//map创建时，若key相同，则都会被后一个key覆盖</span></span><br><span class="line"><span class="keyword">val</span> map =<span class="type">Map</span>（<span class="string">"1"</span>-&gt;<span class="string">"a"</span>,(<span class="string">"3"</span>,<span class="string">"c"</span>),<span class="string">"2"</span>-&gt;<span class="string">"b"</span>）</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取map值</span></span><br><span class="line">println(map.get(<span class="string">"1"</span>))</span><br><span class="line">println(map.get(<span class="string">"1"</span>).get)</span><br><span class="line"><span class="comment">//获取指定key，若没有，就用另一个指定值代替</span></span><br><span class="line">println(map.getOrElse(<span class="string">"5"</span>,<span class="string">"100"</span>))</span><br><span class="line"><span class="keyword">val</span> res = map.get(<span class="string">"8"</span>).getOrElse(<span class="string">"800"</span>)</span><br><span class="line">println(res)</span><br><span class="line"></span><br><span class="line"><span class="comment">//遍历map</span></span><br><span class="line"><span class="comment">//x为map中的一组键值对</span></span><br><span class="line"><span class="keyword">for</span>(x-&gt; map)&#123;</span><br><span class="line">println(<span class="string">"key:"</span>+x._1+<span class="string">",value:"</span>+ x._2) </span><br><span class="line">&#125;</span><br><span class="line">map.foreach(x=&gt;&#123;</span><br><span class="line"> println(<span class="string">"key:"</span>+x._1+<span class="string">",value:"</span>+ x._2)    </span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//遍历key</span></span><br><span class="line"><span class="keyword">val</span> keyIteratable = map.keys</span><br><span class="line">keyIteratable.foreach(k=&gt;&#123;</span><br><span class="line">println(<span class="string">"key"</span>+k+<span class="string">",value"</span>+map.get(k).get)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">//遍历value</span></span><br><span class="line"><span class="keyword">val</span> valueIteratable = map.values</span><br><span class="line">valueIteratable.foreach&#123;v=&gt;&#123;</span><br><span class="line">    println(<span class="string">"value"</span>+v)</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//合并map</span></span><br><span class="line"><span class="keyword">val</span> map1 = <span class="type">Map</span>（(<span class="number">1</span>，<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)）</span><br><span class="line"><span class="keyword">val</span> map2 = <span class="type">Map</span>（(<span class="number">1</span>,<span class="string">"aa"</span>),(<span class="number">2</span>,<span class="string">"bb"</span>),(<span class="number">3</span>,<span class="string">"cc"</span>)）</span><br><span class="line">map1.++(map2).foreach(println)</span><br><span class="line">map1.++:(map2).oreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">//map操作方法</span></span><br><span class="line"><span class="comment">//filter:过滤，留下符合条件的元素</span></span><br><span class="line"><span class="keyword">val</span> result = map.filter(x=&gt;&#123;</span><br><span class="line"><span class="keyword">if</span>(x._1.equals(<span class="string">"1"</span>))</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">&#125;)</span><br><span class="line">result.foreach(println)</span><br><span class="line">map.filter(_._2.equals(<span class="string">"a"</span>)).foreach(println)</span><br><span class="line"><span class="comment">//count：统计符合条件的元素个数</span></span><br><span class="line"><span class="keyword">val</span> countResult = map.count（x=&gt;&#123;</span><br><span class="line">      x._1.equals(<span class="string">"1"</span>)</span><br><span class="line">）</span><br><span class="line">println(countResult)    </span><br><span class="line"><span class="comment">//contains：判断map中是否包含某个key</span></span><br><span class="line">map.contains(<span class="string">"3"</span>)</span><br><span class="line"><span class="comment">//exist：判断符合条件的元素是存在</span></span><br><span class="line"><span class="comment">//若遇到条件返回结果为true，就停止迭代</span></span><br><span class="line">map.exists(x=&gt;&#123;</span><br><span class="line"><span class="keyword">if</span>（x._1.equals(<span class="string">"3"</span>))&#123;</span><br><span class="line">println(<span class="string">"存在喽"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">println(<span class="string">"不存在喽"</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="5、元组Tuple"><a href="#5、元组Tuple" class="headerlink" title="5、元组Tuple"></a>5、元组Tuple</h2><p>同：与列表类似</p><p>不同：元组可以包含不同类型的元素；元组的值是通过将单个值包含在圆括号中构成</p><p><code>注意：</code>Tuple最多支持22个参数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//元素个数取决于Tuple后面的数字</span></span><br><span class="line"><span class="keyword">val</span> t1 = <span class="keyword">new</span> <span class="type">Tuple1</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> t2 = <span class="keyword">new</span> <span class="type">Tuple2</span>(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> t3 = <span class="type">Tuple3</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)  </span><br><span class="line"><span class="keyword">val</span> t5 = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>，<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> t22 = <span class="keyword">new</span> <span class="type">Tuple22</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">22</span>)</span><br><span class="line"><span class="comment">//获取元组中的值 ：用  ._XX  </span></span><br><span class="line">println（t5._4）</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> t = <span class="type">Tuple2</span>((<span class="number">1</span>,<span class="number">2</span>),(<span class="string">"zhangsan"</span>,<span class="string">"lisi"</span>))</span><br><span class="line">println(t._1._2)</span><br><span class="line"></span><br><span class="line"><span class="comment">//元组遍历：元素迭代器</span></span><br><span class="line"><span class="keyword">val</span> interator = t5.productorInterator</span><br><span class="line">interator.foreach(println)</span><br><span class="line"><span class="keyword">while</span>(interator.hasNext)&#123;</span><br><span class="line">    println(intrator.next())</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//元素翻转 swap， 只针对二元数组</span></span><br><span class="line">println(t2.swap)</span><br><span class="line"></span><br><span class="line"><span class="comment">//转换成字符串</span></span><br><span class="line">println(t5.toString())</span><br></pre></td></tr></table></figure><h1 id="七、trait"><a href="#七、trait" class="headerlink" title="七、trait"></a>七、trait</h1><p>Scala：只有extends  ，可以继承多个Trait</p><p>Scala Trait(特征) 相当于 Java 的接口和抽象类的结合</p><p>可以定义属性和方法</p><p><code>注意：</code></p><ul><li>若继承的多个trait中包含同名的属性和方法，就必须在在类中使用<code>override</code> 重新定义</li><li>若重新定义的属性是使用<code>var</code>定义的则会报错，所以必须是使用<code>val</code>定义的属性才可使用override</li><li>trait中不能传参数</li></ul><p><code>举例</code>：trait中带属性带方法实现</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Read</span></span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name = <span class="string">"read"</span></span><br><span class="line">    <span class="keyword">val</span> age = <span class="number">18</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line">        println(<span class="string">"read---"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Write</span></span>&#123;</span><br><span class="line">    <span class="keyword">var</span> name = <span class="string">"write"</span> </span><br><span class="line">    <span class="keyword">val</span> age = <span class="number">19</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line">        println(<span class="string">"write----"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Human</span> <span class="keyword">extends</span> <span class="title">Read</span> <span class="keyword">with</span> <span class="title">Write</span></span>&#123;</span><br><span class="line">  <span class="comment">//前提name必须是用val定义的</span></span><br><span class="line">  <span class="comment">//  override var name  = "person"</span></span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">val</span> age = <span class="number">20</span></span><br><span class="line">    name  = <span class="string">"person"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Trait</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> human = <span class="keyword">new</span> <span class="type">Human</span>() </span><br><span class="line">        println(human.name)</span><br><span class="line">        human.read()</span><br><span class="line">        human.write()</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>举例</code>：trait中带方法不实现</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Equal</span></span>&#123;</span><br><span class="line">    <span class="comment">//抽象方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEqual</span></span>(p:<span class="type">Point</span>):<span class="type">Boolean</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isNoEqual</span></span>(p:<span class="type">Point</span>):<span class="type">Boolean</span>=&#123;</span><br><span class="line">        !isEqual(p)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>(<span class="params">x:<span class="type">Int</span>,y:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Equal</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> xx = x</span><br><span class="line">    <span class="keyword">var</span> yy = y</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isEqual</span> </span>(p:<span class="type">Point</span>) = &#123;</span><br><span class="line">       p.xx == xx &amp; p.yy ==yy </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isEqule</span></span>(p:<span class="type">Any</span>) = &#123;</span><br><span class="line">    p.isInstanceOf[<span class="type">Point</span>] &amp;&amp; p.asInstanceOf[<span class="type">Point</span>].xx==xx</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Trait2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">       <span class="keyword">val</span> p1 = <span class="keyword">new</span> <span class="type">Point</span>(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line">       <span class="keyword">val</span> p2 = <span class="keyword">new</span> <span class="type">Point</span>(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">       ptintln(p1.isNoEqual(p2))</span><br><span class="line">       ptintln(p1.isEqual(p2))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="八、模式匹配match"><a href="#八、模式匹配match" class="headerlink" title="八、模式匹配match"></a>八、模式匹配match</h1><h2 id="1、概念理解"><a href="#1、概念理解" class="headerlink" title="1、概念理解"></a>1、概念理解</h2><ul><li>一个模式匹配包含多个备选项，且每个都以关键字case开始</li><li>每个备选项都包含一个模式和多个表达式，且用箭头符号 =&gt; 隔开了模式和表达式。</li><li>Ø 模式匹配不仅可以匹配值还可以匹配类型</li><li>Ø  从上到下顺序匹配，如果匹配到则不再往下匹配</li><li>Ø  都匹配不上时，会匹配到case_ ,相当于default</li><li>Ø  match 的最外面的”{ }”可以去掉看成一个语句</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Match</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">       <span class="keyword">val</span> p1 =(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>f,<span class="string">"4"</span>,<span class="string">"abc"</span>,<span class="number">55</span>d)</span><br><span class="line">        p1.foreach(x=&gt;&#123;</span><br><span class="line">            println(mymatch(x))</span><br><span class="line">        &#125;)                </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mymatch</span></span>(x:<span class="type">Any</span>)=&#123;     </span><br><span class="line">        x <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"4"</span> =&gt;println(<span class="string">"macth 4--"</span>)</span><br><span class="line">            <span class="keyword">case</span> x:<span class="type">String</span> =&gt; println(<span class="string">"match String"</span>)</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span> =&gt; println(<span class="string">"1--"</span>)</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span> =&gt; println(<span class="string">"2--"</span>)</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>f =&gt; println(<span class="string">"3f--"</span>)</span><br><span class="line">    <span class="comment">//      case x :Double =&gt; println("type is Double")</span></span><br><span class="line">            <span class="keyword">case</span> _ =&gt; println(<span class="string">"no match--"</span>)</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="九、样例类"><a href="#九、样例类" class="headerlink" title="九、样例类"></a>九、样例类</h1><p>1、概念理解</p><ul><li><p>使用了case关键字的类定义就是样例类(case classes)</p></li><li><p>样例类是种特殊的类。实现了类构造参数的getter方法（构造参数默认被声明为val），当构造参数是声明为var类型的，它将帮你实现setter和getter方法。</p><p>Ø  样例类默认帮你实现了toString,equals，copy和hashCode等方法。</p><p>Ø  样例类可以new, 也可以不用new</p></li></ul><p>2、举例：结合模式匹配的代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Match2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> zs = <span class="keyword">new</span> <span class="type">Woman</span>(<span class="number">18</span>,<span class="string">"zs"</span>)</span><br><span class="line">    println(zs.age + <span class="string">" : "</span>+ zs . name) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">val</span> mm = <span class="type">Woman</span>(<span class="number">19</span>,<span class="string">"meimei"</span>)</span><br><span class="line">    <span class="keyword">val</span> nn = <span class="type">Woman</span>(<span class="number">19</span>,<span class="string">"meimei"</span>)</span><br><span class="line">    println(mm.equals(zs))</span><br><span class="line">    println(mm.equals(nn))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> wo = <span class="type">List</span>(ls,mm)</span><br><span class="line">    wo.foreach(x=&gt;&#123;</span><br><span class="line">            x <span class="keyword">match</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> x:<span class="type">Woman</span> =&gt; println(<span class="string">"match Woman--"</span>)</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Woman</span>(<span class="number">18</span>,<span class="string">"ls"</span>) =&gt; println(<span class="string">"ls"</span>)</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Woman</span>(<span class="number">19</span>,<span class="string">"mm"</span>) =&gt; println(<span class="string">"mm"</span>)</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Woman</span>(<span class="number">19</span>,<span class="string">"zs"</span>) =&gt; println(<span class="string">"zs"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Woman</span> (<span class="params">age:<span class="type">Int</span> ,name:<span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure><h1 id="十、Actor-Model"><a href="#十、Actor-Model" class="headerlink" title="十、Actor Model"></a>十、<strong>Actor Model</strong></h1><h2 id="1、概念理解-1"><a href="#1、概念理解-1" class="headerlink" title="1、概念理解"></a>1、概念理解</h2><p>Actor Model是用来编写并行计算或分布式系统的高层次抽象（类似java中的Thread）让程序员不必为多线程模式下共享锁而烦恼,被用在Erlang 语言上, 高可用性99.9999999 % 一年只有31ms 宕机Actors将状态和行为封装在一个轻量的进程/线程中，但是不和其他Actors分享状态，每个Actors有自己的世界观，当需要和其他Actors交互时，通过发送事件和消息，发送是异步的，非堵塞的(fire-andforget)，发送消息后不必等另外Actors回复，也不必暂停，每个Actors有自己的消息队列，进来的消息按先来后到排列，这就有很好的并发策略和可伸缩性，可以建立性能很好的事件驱动系统。</p><h2 id="2、Actor的特征："><a href="#2、Actor的特征：" class="headerlink" title="2、Actor的特征："></a>2、Actor的特征：</h2><p>Ø  ActorModel是消息传递模型,基本特征就是消息传递</p><p>Ø  消息发送是异步的，非阻塞的</p><p>Ø  消息一旦发送成功，不能修改</p><p>Ø  Actor之间传递时，自己决定决定去检查消息，而不是一直等待，是异步非阻塞的</p><h2 id="3、什么是Akka"><a href="#3、什么是Akka" class="headerlink" title="3、什么是Akka"></a>3、什么是Akka</h2><p>Akka 是一个用 Scala 编写的库，用于简化编写容错的、高可伸缩性的 Java 和Scala 的 Actor 模型应用，底层实现就是Actor,Akka是一个开发库和运行环境，可以用于构建高并发、分布式、可容错、事件驱动的基于JVM的应用。使构建高并发的分布式应用更加容易。</p><p>spark1.6版本之前，spark分布式节点之间的消息传递使用的就是Akka，底层也就是actor实现的。1.6之后使用的netty传输。</p><h2 id="4、例：Actor简单例子发送接收消息"><a href="#4、例：Actor简单例子发送接收消息" class="headerlink" title="4、例：Actor简单例子发送接收消息"></a>4、例：Actor简单例子发送接收消息</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.actors.<span class="type">Actor</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyActor</span> <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">act</span></span>():<span class="type">Unit</span>=&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>)&#123; receive &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">"hello"</span>=&gt;println(<span class="string">"hello"</span>)  </span><br><span class="line">        <span class="keyword">case</span> x:<span class="type">String</span> =&gt; println(<span class="string">"save String ="</span>+ x)</span><br><span class="line">        <span class="keyword">case</span> x:<span class="type">Int</span> =&gt; println(<span class="string">"save Int"</span>)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; println(<span class="string">"save default"</span>)</span><br><span class="line"></span><br><span class="line">    &#125;                 </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> actor = <span class="keyword">new</span> <span class="type">MyActor</span>()</span><br><span class="line">        <span class="comment">//启动</span></span><br><span class="line">        actor.start()</span><br><span class="line">        <span class="comment">//发送消息</span></span><br><span class="line">        actor ! <span class="string">"hello"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>## </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Msg</span>(<span class="params">actor</span>)(<span class="params"></span>)</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">WoActor</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">    oevrride <span class="function"><span class="keyword">def</span> <span class="title">act</span></span>():<span class="type">Unit</span> =&#123;</span><br><span class="line">          <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            receive &#123;</span><br><span class="line">                <span class="keyword">case</span> msg:<span class="type">Msg</span> =&gt;&#123;</span><br><span class="line">                    msg.mes <span class="comment">//收到的消息</span></span><br><span class="line">                    msg.actor <span class="comment">//回复的消息</span></span><br><span class="line">                    println(<span class="string">"too"</span>)</span><br><span class="line">                    msg.actor ! <span class="string">"hahaha"</span></span><br><span class="line">                    </span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HuActor</span>(<span class="params">wo:<span class="type">WoActor</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    oevrride <span class="function"><span class="keyword">def</span> <span class="title">act</span></span>():<span class="type">Unit</span> =&#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            receive &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="string">"hello"</span> =&gt;&#123;</span><br><span class="line">                    println(<span class="string">"too"</span>)</span><br><span class="line">                    <span class="keyword">val</span> msg = <span class="type">Msg</span>(<span class="keyword">this</span>,<span class="string">"rrr"</span>)</span><br><span class="line">                    wo ! msg</span><br><span class="line">                &#125;               </span><br><span class="line">                <span class="keyword">case</span> <span class="string">"rrr"</span> =&gt;&#123;</span><br><span class="line">                    println(<span class="string">"uuu"</span>)</span><br><span class="line">                   <span class="keyword">val</span> msg = <span class="type">Msg</span>(<span class="keyword">this</span>,<span class="string">"heheh"</span>)</span><br><span class="line">                    wo ! msg</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Actor2</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>]):<span class="type">Unit</span> = &#123;</span><br><span class="line">     <span class="keyword">val</span> woman = <span class="keyword">new</span> <span class="type">WoActor</span>()</span><br><span class="line">    <span class="keyword">val</span> human = <span class="keyword">new</span> <span class="type">HuActor</span>(woman)</span><br><span class="line">        </span><br><span class="line">        human.start()</span><br><span class="line">        woman.start()</span><br><span class="line">        human ! <span class="string">"hello"</span>        </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="例：Actor与Actor之间通信"><a href="#例：Actor与Actor之间通信" class="headerlink" title="例：Actor与Actor之间通信"></a>例：Actor与Actor之间通信</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Message</span>(<span class="params">actor:<span class="type">Actor</span>,msg:<span class="type">Any</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Actor1</span> <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">act</span></span>()&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">      receive&#123;</span><br><span class="line">        <span class="keyword">case</span>  msg :<span class="type">Message</span> =&gt; &#123;</span><br><span class="line">          println(<span class="string">"i sava msg! = "</span>+ msg.msg)</span><br><span class="line">          </span><br><span class="line">          msg.actor!<span class="string">"i love you too !"</span></span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> msg :<span class="type">String</span> =&gt; println(msg)</span><br><span class="line">        <span class="keyword">case</span>  _ =&gt; println(<span class="string">"default msg!"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor2</span>(<span class="params">actor :<span class="type">Actor</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">  actor ! <span class="type">Message</span>(<span class="keyword">this</span>,<span class="string">"i love you !"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">act</span></span>()&#123;</span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">receive&#123;</span><br><span class="line">  <span class="keyword">case</span> msg :<span class="type">String</span> =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span>(msg.equals(<span class="string">"i love you too !"</span>))&#123;</span><br><span class="line">      println(msg)</span><br><span class="line">     actor! <span class="string">"could we have a date !"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">case</span>  _ =&gt; println(<span class="string">"default msg!"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Lesson_Actor2</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> actor1 = <span class="keyword">new</span> <span class="type">Actor1</span>()</span><br><span class="line">    actor1.start()</span><br><span class="line">    <span class="keyword">val</span> actor2 = <span class="keyword">new</span> <span class="type">Actor2</span>(actor1)</span><br><span class="line">    actor2.start()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="十一、WordCount"><a href="#十一、WordCount" class="headerlink" title="十一、WordCount"></a>十一、WordCount</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在lib文件中添加spark的jar包 ，并addLib</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span>.rddToPairRDDFunctions</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      </span><br><span class="line">    conf.setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"WC"</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)  <span class="comment">//用于了解集群</span></span><br><span class="line">      </span><br><span class="line">    <span class="comment">//RDD:集合 ，抽象的</span></span><br><span class="line">    <span class="keyword">val</span> lines :<span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"./words.txt"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//方式一：</span></span><br><span class="line">    <span class="keyword">val</span> word :<span class="type">RDD</span>[<span class="type">String</span>]  = lines.flatMap&#123;lines =&gt; &#123;</span><br><span class="line">      lines.split(<span class="string">" "</span>)  <span class="comment">//匿名函数</span></span><br><span class="line">    &#125;&#125;</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> pairs:<span class="type">RDD</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = word.map&#123; x =&gt; (x,<span class="number">1</span>) &#125;</span><br><span class="line">      <span class="comment">//map :一进一出    flatmap：一进多出    x：每个单词</span></span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> result:<span class="type">RDD</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = pairs.reduceByKey&#123;(a,b)=&gt; &#123;</span><br><span class="line">        println(<span class="string">"a:"</span>+a+<span class="string">",b:"</span>+b)</span><br><span class="line">        a+b    <span class="comment">//相当于  a = a + b用于下一个统计</span></span><br><span class="line">    &#125;&#125;</span><br><span class="line">      <span class="comment">//优化：排序  false（降序）  第一个_ 代表每个元素</span></span><br><span class="line">    result.sortBy(_._2,<span class="literal">false</span>).foreach(println)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//方式二：简化写法</span></span><br><span class="line">    lines.flatMap &#123; _.split(<span class="string">" "</span>)&#125;.map &#123; (_,<span class="number">1</span>)&#125;.reduceByKey(_+_).foreach(println)</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Scala </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程语言 </tag>
            
            <tag> 静态 </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis学习</title>
      <link href="/2019/02/14/Redis%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/02/14/Redis%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、Redis的简介"><a href="#一、Redis的简介" class="headerlink" title="一、Redis的简介"></a>一、Redis的简介</h1><h2 id="1、前情提要"><a href="#1、前情提要" class="headerlink" title="1、前情提要"></a>1、前情提要</h2><p><strong>磁盘数据库</strong>： </p><p> MySQL（关系型数据库）<br> Hbase</p><p><img src="images/redis1.jpg" alt=""></p><p><strong>内存数据库</strong>：<br>  Redis<br>  memcached<br>  （成本高、性能好、读写速度快、数据安全性低、直接把值放到内存里面，内存数据库就直接把值取到）</p><p><img src="images/redis2.jpg" alt=""></p><h2 id="2、用作数据库、缓存和消息中间件"><a href="#2、用作数据库、缓存和消息中间件" class="headerlink" title="2、用作数据库、缓存和消息中间件"></a>2、用作数据库、缓存和消息中间件</h2><blockquote><p>二八法则：80%是经常被查询，使用内存数据库做缓存中间件，读取性能高</p></blockquote><h1 id="二、Redis的特点"><a href="#二、Redis的特点" class="headerlink" title="二、Redis的特点"></a>二、Redis的特点</h1><h2 id="1、数据结构丰富"><a href="#1、数据结构丰富" class="headerlink" title="1、数据结构丰富"></a>1、数据结构丰富</h2><p>Redis虽然是键值对数据库，但他支持多种类型的数据结构（主要是不同类型的value）</p><blockquote><p>字符串、散列（hashes），列表（lists），集合（sets），有序集合（sorted sets）</p></blockquote><p><img src="images/redis0.jpg" alt=""></p><h2 id="2、数据的持久化"><a href="#2、数据的持久化" class="headerlink" title="2、数据的持久化"></a>2、数据的持久化</h2><p>Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载到内存进行使用</p><h2 id="3、数据的备份"><a href="#3、数据的备份" class="headerlink" title="3、数据的备份"></a>3、数据的备份</h2><p>Redis 支持数据的备份，即 master-slave 模式的数据备份。</p><h1 id="三、Redis的安装"><a href="#三、Redis的安装" class="headerlink" title="三、Redis的安装"></a>三、Redis的安装</h1><h2 id="1、下载安装包"><a href="#1、下载安装包" class="headerlink" title="1、下载安装包"></a>1、下载安装包</h2><p>redis-3.2.9.tar.gz </p><p>网址：<a href="http://www.redis.cn/download.html" target="_blank" rel="noopener">http://www.redis.cn/download.html</a></p><h2 id="2、依赖软件安装"><a href="#2、依赖软件安装" class="headerlink" title="2、依赖软件安装"></a>2、依赖软件安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc tcl -y</span><br></pre></td></tr></table></figure><h2 id="3、解压-redis"><a href="#3、解压-redis" class="headerlink" title="3、解压 redis"></a>3、解压 redis</h2><p>并进入解压目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zvxf redis-3.2.9.tar.gz</span><br></pre></td></tr></table></figure><h2 id="4、-执行-编译命令"><a href="#4、-执行-编译命令" class="headerlink" title="4、 执行 编译命令"></a>4、 执行 编译命令</h2><p>（<code>注意：</code>当前路径下需包含 makefile文件  ： 用于手动编译）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h2 id="5、修改-redis-的配置文件"><a href="#5、修改-redis-的配置文件" class="headerlink" title="5、修改 redis 的配置文件"></a>5、修改 redis 的配置文件</h2><p> redis.config ( 先 备份一个原厂配置文件)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim redis.config</span><br></pre></td></tr></table></figure><ul><li>修改运行模式为后台运行（如果为no，启动redis-server后，控制台就会卡在启动状态）daemonize守护进程</li></ul><blockquote><p>daemonize yes</p></blockquote><ul><li>指定redis数据持久化的路径(在执行redis-cli命令的当前路径，会生成dump.rdb文件)</li></ul><blockquote><p>dir   ./</p></blockquote><ul><li>使用默认的 redis.conf 文件，redis 默认只能通过 127.0.0.1:6379 这个地址访问，这样就只能在本机上操作了</li><li>想要远程操作，需要修改redis.conf 这个配置文件，在配置文件中添加相应的 ip 地址， 在bind 127.0.0.1 后面追加</li></ul><blockquote><p>bind 127.0.0.1 192.168.198.128</p></blockquote><h2 id="6、启动server服务"><a href="#6、启动server服务" class="headerlink" title="6、启动server服务"></a>6、启动server服务</h2><p>（如在redis的解压目录下）</p><blockquote><p>命令： redis-server 配置文件的地址</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server redis-conf</span><br></pre></td></tr></table></figure><p>终止服务：（查询redis进程号，然后，kill该进程）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep redis</span><br><span class="line">kill 进程号</span><br></pre></td></tr></table></figure><h2 id="7、启动客户端服务"><a href="#7、启动客户端服务" class="headerlink" title="7、启动客户端服务"></a>7、启动客户端服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli</span><br></pre></td></tr></table></figure><p>显示：</p><blockquote><p>127.0.0.1:6379&gt; </p></blockquote><h1 id="四、Redis的使用"><a href="#四、Redis的使用" class="headerlink" title="四、Redis的使用"></a>四、Redis的使用</h1><h2 id="0、前情提要"><a href="#0、前情提要" class="headerlink" title="0、前情提要"></a>0、前情提要</h2><p>Redis的key 值是二进制安全的，这意味着可以用任何二进制序列作为 key值。</p><h2 id="1、切换数据库（实例）"><a href="#1、切换数据库（实例）" class="headerlink" title="1、切换数据库（实例）"></a>1、切换数据库（实例）</h2><blockquote><p>select databaseid 默认共有 16 个实例库，</p><p>登录时是 ID 为0 的数据库，总共有 16 个</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select 0</span><br></pre></td></tr></table></figure><h2 id="2、Key操作："><a href="#2、Key操作：" class="headerlink" title="2、Key操作："></a>2、Key操作：</h2><p>（前提：是针对已经存在key）</p><blockquote><p>KEYS pattern<br>查找所有符合给定模式 pattern 的 key 。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> keys * </span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>列出所有Key</p></blockquote><blockquote><p>EXISTS  key<br>检查给定 key 是否存在。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; EXISTS  name</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>若显示0，则不存在；若显示1，则存在</p></blockquote><blockquote><p>EXPIRE key seconds<br>为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; EXPIRE age 1</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>若设置成功，显示1；显示0 ，则为失败，可能是该key不存在</p></blockquote><blockquote><p>TTL key<br>以秒为单位，返回给定 key 的剩余生存时间</p></blockquote><blockquote><p>MOVE key db</p><p>将当前数据库的 key 移动到给定的数据库 db 当中。<br>如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  move name 1</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>name 是数据库0中已存在的key，移动到数据库1中后，0中就不存在该key</p></blockquote><blockquote><p>TYPE key</p><p>返回 key 所储存的值的类型。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; DEL key [key ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>删除给定的一个或多个 key 。不存在的 key 会被忽略</p></blockquote><h2 id="3、String-操作"><a href="#3、String-操作" class="headerlink" title="3、String  操作"></a>3、String  操作</h2><p>字符串是一种最基本的 Redis 值类型。Redis 字符串是二进制安全的，这<br>意味着一个 Redis 字符串能包含任意类型的数据</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SET key value [EX seconds][PX milliseconds] [NX|XX]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p> EX 设置过期时间，秒，等同于 SETEX key seconds value<br> PX 设置过期时间，毫秒，等同于 PSETEX key milliseconds value<br> NX 键不存在，才能设置，等同于 SETNX key value<br> XX 键存在时，才能设置</p></blockquote><p><code>注意</code></p><p>将字符串值 value 关联到 key 。<br>如果 key 已经持有其他值， SET 就覆写旧值，无视类型。<br>对于某个原本带有生存时间（TTL）的键来说， 当 SET 命令成功在这个键上执行时， 这个键原有的 TTL 将被清除。</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; GET key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回 key 所关联的字符串值。如果 key 不存在那么返回特殊值 nil 。<br>假如 key 储存的值不是字符串类型，返回一个错误，因为 GET 只能<br>用于处理字符串值</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; APPEND key value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾。<br>如果 key 不存在， APPEND 就简单地将给定 key 设为 value ，就像执行 SET key value 一样。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; STRLEN key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回 key 所储存的字符串值的长度。<br>当 key 储存的不是字符串值时，返回一个错误。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; INCR key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 key 中储存的数字值增一，并显示。<br>如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCR 操作。<br>如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。</p><p><code>错误：</code></p><p>(error) ERR value is not an integer or out of range</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; INCRBY key increment</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 key 所储存的值加上增量 increment ，并显示。<br>如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCRBY 命令。<br>如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; DECR key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 key 中储存的数字值减一。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; DECRBY key decrement</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 key 所储存的值减去减量 decrement 。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  GETRANGE key start end</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回 key 中字符串值的子字符串，字符串的截取范围由 start 和end 两个偏移量决定(包括 start 和 end 在内)。<br>负数偏移量表示从字符串最后开始计数， -1 表示最后一个字符， -2 表示倒数第二个，以此类推</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SETRANGE key offset value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>用 value 参数覆写(overwrite)给定 key 所储存的字符串值，从偏移量 offset 开始。<br>不存在的 key 当作空白字符串处理。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SETEX key  seconds value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将值 value 关联到 key ，并将 key 的生存时间设为 seconds<br>如果 key 已经存在， SETEX 命令将覆写旧值。<br>这个命令类似于以下两个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; SET key value</span><br><span class="line">&gt; EXPIRE key seconds # 设置生存时间</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><blockquote><p>不同之处是， SETEX 是一个原子性(atomic)操作，关联值和设置生存时间两个动作会在同一时间内完成，该命令在 Redis 用作缓存时，非常实用。</p></blockquote><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> SETNX key value</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 key 的值设为 value ，当且仅当 key 不存在。<br>若给定的 key 已经存在，则 SETNX 不做任何动作。<br>SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; MGET key [key ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回所有(一个或多个)给定 key 的值。<br>如果给定的 key 里面，有某个 key 不存在，那么这个 key 返回特殊值 nil 。因此，该命令永不失败</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; MSET key value [key value ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>同时设置一个或多个 key-value 对。<br>如果某个给定 key 已经存在，那么 MSET 会用新值覆盖原来的旧值，如果这不是你所希望的效果，请考虑使用 MSETNX 命令：它只会在所有给定 key 都不存在的情况下进行设置操作。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; MSETNX key value [key value ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。<br>即使只有一个给定 key 已存在， MSETNX 也会拒绝执行所有给定 key 的设置操作。<br>MSETNX 是原子性的，因此它可以用作设置多个不同 key 表示不同字段(field)的唯一性逻辑对象(unique logic object)，所有字段要么全被设置，要么全不被设置。</p></blockquote><h2 id="4、-List-操作"><a href="#4、-List-操作" class="headerlink" title="4、 List 操作"></a>4、 List 操作</h2><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LPUSH key value [value ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将一个或多个值 value 插入到列表 key 的表头。（从左边插入）<br>如果有多个 value 值，那么各个 value 值按从左到右的顺序依次 插 入 到 表 头 ：</p><p> 比 如 说 ，</p><p> 对 空 列 表 mylist  执 行 命令 LPUSH mylist a b c ，列表的值将是 c b a ，</p><p>这等同于原 子 性 地 执行 LPUSH mylist a 、 LPUSH mylist b 和 LPUSH mylistc 三个命令。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; RPUSH key value [value ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将一个或多个值 value 插入到列表 key 的表尾。（从右边插入）<br>如果有多个 value 值，那么各个 value 值按从左到右的顺序依次 插 入 到 表 尾 ： </p><p>比 如 对 一 个 空 列 表 mylist  执行 RPUSH mylist a b c ，得出的结果列表为 a b c ，</p><p>等同于 执 行 命令 RPUSH mylist a 、 RPUSH mylist b 、 RPUSH mylist c 。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LRANGE key  start stop</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返 回 列 表 key  中 指 定 区 间 内 的 元 素 ， 区 间 以 偏 移量 start 和 stop 指定。（从左至右）<br>下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。-1 表示最后一个元素</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LPOP key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>移除并返回列表 key 的头元素。（从列表key的左边开始弹出元素）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; RPOP key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>移除并返回列表 key 的尾元素。（从列表key的右边开始弹出元素）</p></blockquote><p><code>备注：</code></p><blockquote><p>同向为栈，异向为队列<br>栈（lpush  lpop  ； rpush  rpop）</p><p>队列（lpush  rpop ； rpush   lpop  ）</p><p>ArrayList : 数组（查找快，增删慢）<br>LinkList  : 链表（查找慢，增删快）</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LINDEX key index</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回列表 key 中，下标为 index 的元素。<br>下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。<br>你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LLEN key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回列表 key 的长度。<br>如果 key 不存在，则 key 被解释为一个空列表，返回 0 .</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LREM key count value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>根据参数 count 的值，移除列表中与参数 value 相等的元素。<br>count 的值可以是以下几种：<br>count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元<br>素，数量为 count 。<br>count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元<br>素，数量为 count 的绝对值。<br>count = 0 : 移除表中所有与 value 相等的值。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LTRIM key start stop</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素，</p><p>不在指定区间之内的元素都将被删除。<br>举 个 例 子 ， 执 行 命 令 LTRIM list 0 2 ， 表 示 只 保 留 列表 list 的前三个元素，其余元素全部删除</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; RPOPLPUSH source  destination</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>命令 RPOPLPUSH 在一个原子时间内，执行以下两个动作：<br>将列表 source 中的最后一个元素(尾元素)弹出，并返回给客户端。将 source  弹 出 的 元 素 插 入 到 列 表 destination  ， 作为 destination 列表的的头元素。<br>举 个 例 子 ， </p><p>你 有 两 个 列表 source  和 destination  ， source  列 表 有 元素 a, b, c ， destination 列表有元素 x, y, z ，执行 RPOPLPUSH sourcedestination 之后， source 列表包含元素 a, b ， destination 列表包含元素 c, x, y, z ，并且元素 c 会被返回给客户端。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LSET key index value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将列表 key 下标为 index 的元素的值设置为 value 。<br>当 index 参数超出范围，或对一个空列表( key 不存在)进行 LSET 时，返回一个错误。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LINSERT key BEFORE|AFTER pivot value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将值 value 插入到列表 key 当中，位于值 pivot 之前或之后。<br>当 pivot 不存在于列表 key 时，不执行任何操作。<br>当 key 不存在时， key 被视为空列表，不执行任何操作。</p></blockquote><h2 id="5、Set操作"><a href="#5、Set操作" class="headerlink" title="5、Set操作"></a>5、Set操作</h2><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SADD key member [member ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将一个或多个 member 元素加入到集合 key 当中（无序），已经存在于集合的 member 元素将被忽略。<br>假如 key 不存在，则创建一个只包含 member 元素作成员的集合。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SMEMBERS key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回集合 key 中的所有成员。</p><p>不存在的 key 被视为空集合。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SISMEMBER key member</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>判断 member 元素是否是集合 key 的成员。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SCARD key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回集合 key 的基数(集合中元素的数量)。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SREM key member [member ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>移 除 集 合 key 中 的 一 个 或 多 个 member 元 素 ， 不 存 在的 member 元素会被忽略。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SPOP key （抽奖场景）</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>移除并返回集合中的一个随机元素。<br>如果只想获取一个随机元素，但不想该元素从集合中被移除的话，可以使用 SRANDMEMBER 命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SMOVE source destination member</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 member 元素从 source 集合移动到 destination 集合。<br>SMOVE 是原子性操作。<br>如果 source 集合不存在或不包含指定的 member 元素，<br>则 SMOVE 命令不执行任何操作，仅返回 0 。否则， member 元素从 source 集合中被移除，并添加到 destination 集合中去。<br>当 destination 集合已经包含 member 元素时， SMOVE 命令只是简单地将 source 集合中的 member 元素删除。<br>当 source 或 destination 不是集合类型时，返回一个错误。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SDIFF key [key ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>求差集：从第一个 key 的集合中去除其他集合和自己的交集部分</p><p>sdiff求两个set的差集，有先后顺序，以靠前的为基准，列出前一个set有的，而后一个set没有的元素</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SINTER key [key  ...] （微博求共同关注场景）</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回一个集合的全部成员，该集合是所有给定集合的交集。<br>不存在的 key 被视为空集。<br>当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; SUNION key [key ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回一个集合的全部成员，该集合是所有给定集合的并集。<br>不存在的 key 被视为空集。</p></blockquote><h2 id="6、Sorted-set操作"><a href="#6、Sorted-set操作" class="headerlink" title="6、Sorted set操作"></a>6、Sorted set操作</h2><p>类似 Sets,但是每个字符串元素都关联到一个叫 score 浮动数值。里面的元素总是通过 score 进行着排序，所以不同的是，它是可以检索的一系列元素。</p><p><code>注意：</code></p><blockquote><p>在 set 基础上，加上 score 值，</p><p>之前 set 是key value1 value2….<br>现在 Zset 是 key score1 value1 score2 value2</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZADD key score member [[score member][score member] ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将 一 个 或 多 个 member 元 素 及 其 score 值 加 入 到 有 序集 key 当中。</p><p><code>显示</code></p><p>redis&gt; ZADD page_rank 9 baidu.com 8 bing.com 10 google.com<br>(integer) 2<br>redis&gt; ZRANGE page_rank 0 -1 WITHSCORES</p><p>1) “bing.com”<br>2) “8”<br>3) “baidu.com”<br>4) “9”<br>5) “google.com”<br>6) “10”</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZRANGE key start stop [WITHSCORES]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回有序集 key 中，指定区间内的成员。<br>其中成员的位置按 score 值递增(从小到大)来排序。<br>具有相同 score 值的成员按字典序(lexicographical order )来排列。<br>如果你需要成员按 score 值递减(从大到小)来排列，请使用 ZREVRANGE 命令。<br>下标参数 start 和 stop 都以 0 为底，</p><p>也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。<br>你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。<br>超出范围的下标并不会引起错误。<br>比 如 说 ， 当 start 的 值 比 有 序 集 的 最 大 下 标 还 要 大 ， 或是 start &gt; stop 时， ZRANGE 命令只是简单地返回一个空列表。<br>另一方面，假如 stop 参数的值比有序集的最大下标还要大，那么Redis 将 stop 当作最大下标来处理。<br>可以通过使用 WITHSCORES 选项，来让成员和它的 score 值一并返回，</p><p>返回列表以 value1,score1, …, valueN,scoreN 的格式表示。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZREVRANGE key start stop [WITHSCORES]( ( 音乐排行榜场景) )</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回有序集 key 中，指定区间内的成员。<br>其中成员的位置按 score 值递减(从大到小)来排列。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZREM key member [member ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>移除有序集 key 中的一个或多个成员，不存在的成员将被忽略。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZREMRANGEBYSCORE key min max</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>移除有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZSCORE key member</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回有序集 key 中，成员 member 的 score 值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZCARD key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回有序集 key 的基数（包含的元素的个数）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZCOUNT key min max</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ZRANK key member</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回有序集 key 中，成员 member 的 score 值。<br>ZCARD key<br>返回有序集 key 的基数（包含的元素的个数）。<br>ZCOUNT key min max<br>返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。<br>ZRANK key member</p><p>返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。</p></blockquote><h2 id="7、-Hash-操作-（散列）"><a href="#7、-Hash-操作-（散列）" class="headerlink" title="7、 Hash  操作  （散列）"></a>7、 Hash  操作  （散列）</h2><p>KV 模式不变，但是 V 是一个键值对</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HSET key field value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将哈希表 key 中的域 field 的值设为 value 。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HGET key field</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回哈希表 key 中给定域 field 的值。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HMSET key field value [field value ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>同时将多个 field-value (域-值)对设置到哈希表 key 中。<br>此命令会覆盖哈希表中已存在的域。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HMGET key field [field ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回哈希表 key 中，一个或多个给定域的值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HGETALL key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回哈希表 key 中，所有的域和值。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HKEYS key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回哈希表 key 中的所有域。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HVALS key</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>返回哈希表 key 中所有域的值。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HSETNX key field value</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在。<br>若域 field 已经存在，该操作无效。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HEXISTS key field</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>查看哈希表 key 中，给定域 field 是否存在。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HDEL key field [field ...]</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HINCRBY key field increment</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>为哈希表 key 中的域 field 的值加上增量 increment 。<br>增量也可以为负数，相当于对给定域进行减法操作。</p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; HINCRBYFLOAT key field increment</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>增加浮点数<br>场景：用户维度统计<br>统计数包括：关注数、粉丝数、喜欢商品数、发帖数<br>用户为 Key，不同维度为 Field，Value 为统计数</p></blockquote><h1 id="五、Redis-的持久化"><a href="#五、Redis-的持久化" class="headerlink" title="五、Redis 的持久化"></a>五、Redis 的持久化</h1><h2 id="1、Redis-持久化方式"><a href="#1、Redis-持久化方式" class="headerlink" title="1、Redis  持久化方式"></a>1、Redis  持久化方式</h2><ul><li><p>RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。</p></li><li><p>AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 </p><p>AOF 文件中的命令全部以 Redis协议的格式来保存，新命令会被追加到文件的末尾。 </p><p>Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。</p></li><li><p>Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 </p><p>在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， </p><p>因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。</p></li><li><p>你甚至可以关闭持久化功能，让数据只在服务器运行时存在。</p></li></ul><h2 id="2、Rdb"><a href="#2、Rdb" class="headerlink" title="2、Rdb:"></a>2、Rdb:</h2><p>（1）在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的 snapshot 快照，它恢复时就是将快照文件直接读到内存里。</p><p>Rdb 保存的是 dump.rdb 文件</p><p>（2）如何触发 RDB 快照</p><p>Save：save 时只管保存，其他不管，全部阻塞。<br>Bgsave：redis 会在后台进行快照操作，快照操作的同时还可以响应客户端的请求，可以通过 lastsave 命令获取最后一次成功执行快照的时间。</p><p>（3）如何停止</p><p>静态停止：将配置文件里的 RDB 保存规则改为 save “”<br>动态停止 ：</p><blockquote><p> config set save “ ”</p></blockquote><h2 id="3、AOF"><a href="#3、AOF" class="headerlink" title="3、AOF"></a>3、AOF</h2><p>(Append Only File)</p><p>（1）以日志的形式来记录每个写操作，将 redis 执行过的所有写指令记录下来(读操作不记录)。只许追加文件但不可以改写文件，redis 启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次一完成数据恢复工作。<br>======APPEND ONLY MODE======<br>开启 aof ：appendonly yes (默认是 no)</p><p>（2）Aof  策略</p><p>Appendfsync 参数：</p><p>Always ：同步持久化 每次发生数据变更会被立即记录到磁盘，性能较差<br>但数据完整性较好。<br>Everysec： 出厂默认推荐，异步操作，每秒记录，如果一秒宕机，有数<br>据丢失<br>No：从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选<br>择。</p><p>（3）Rewrite</p><p><code>概念</code>：AOF 采用文件追加方式，文件会越来越来大为避免出现此种情况，<br>新增了重写机制，aof 文件的大小超过所设定的阈值时，redis 就会自动将 aof文件的内容压缩，只保留可以恢复数据的最小指令集，可以使用命令<br>bgrewirteaof。</p><p><code>触发机制</code>：redis 会记录上次重写的 aof 的大小，默认的配置当 aof 文件大小为上次 rewrite 后大小的一倍且文件大于 64M 触发。</p><p><code>重写原理：</code>aof 文件持续增长而大时，会 fork 出一条新进程来将文件重写<br>(也就是先写临时文件最后再 rename)，遍历新进程的内存中的数据，每条记录有一条 set 语句，重写 aof 文件的操作，并没有读取旧的的 aof 文件，而是将整个内存的数据库内容用命令的方式重写了一个新的 aof 文件，这点和快照有点类似。</p><blockquote><p>no-appendfsync-on-rewrite no : </p><p>重写时是否可以运用 Appendfsync 用默认 no 即可，保证数据安全</p><p>auto-aof-rewrite-percentage 倍数  设置基准值<br>auto-aof-rewrite-min-size   设置基准值大小</p></blockquote><p>（4）flushall<br>刷新内存中的数据，即清空数据<br>可通过删除aof文件中该操作的记录来恢复数据<br>rdb文件无法实现数据恢复</p><p>（5）aof 特点</p><p>aof优点：<br>可用于数据恢复<br>缺点：<br>体积大，速度慢</p><p>aof文件体积会越来越大</p><p>（6）aof文件优化<br>重写：当文件大小达到一定阈值，启动压缩文件</p><blockquote><p>netstat -anpt<br>查看所有端口的使用情况</p></blockquote><h2 id="4、备份-Redis-数据"><a href="#4、备份-Redis-数据" class="headerlink" title="4、备份 Redis  数据"></a>4、备份 Redis  数据</h2><p><code>建议：</code><br>创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件<br>夹， 并且每天将一个 RDB 文件备份到另一个文件夹。<br>确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本<br>时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时<br>内的每小时快照， 还可以保留最近一两个月的每日快照。<br>至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你<br>运行 Redis 服务器的物理机器之外。</p><h1 id="六、Redis-主从复制"><a href="#六、Redis-主从复制" class="headerlink" title="六、Redis  主从复制"></a>六、Redis  主从复制</h1><h2 id="0、前情提要-1"><a href="#0、前情提要-1" class="headerlink" title="0、前情提要"></a>0、前情提要</h2><p>Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(masterserver)的精确复制品 。</p><h2 id="1、关于-Redis-复制功能的几个重要方面："><a href="#1、关于-Redis-复制功能的几个重要方面：" class="headerlink" title="1、关于 Redis 复制功能的几个重要方面："></a>1、关于 Redis 复制功能的几个重要方面：</h2><ul><li>Redis 使用异步复制。</li><li>一个主服务器可以有多个从服务器。</li><li>不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器，<br>多个从服务器之间可以构成一个图状结构。</li><li>复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次<br>同步， 主服务器也可以继续处理命令请求。不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。</li><li>复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT 命令可以交给附属节点去运行。</li></ul><h2 id="2、从服务器配置"><a href="#2、从服务器配置" class="headerlink" title="2、从服务器配置"></a>2、从服务器配置</h2><p>方式一：编辑配置文件</p><p>（永久生效）</p><blockquote><p>添加主服务器的IP</p><p>slaveof 192.168.198.128 6379</p></blockquote><p>方式二：调用 SLAVEOF 命令，输入主服务器的 IP 和端口，然后同步就会开始</p><p><code>前提：</code>主从服务器的redis-server均启动了</p><p>（仅适用于当前线程的服务，同步时，会将自己原来的key值数据清空，并且在主服务器页面会显示关于从服务器的日志信息）</p><blockquote><p>127.0.0.1:6379&gt; SLAVEOF 192.168.198.128 6379</p></blockquote><p>若想取消该服务器的主从关系，使用如下命令，且还会保存当前数据</p><blockquote><p>127.0.0.1:6379&gt; SLAVEOF no one</p></blockquote><p>连接远程节点的redis服务（端口默认为6379）</p><blockquote><p>redis-cli -h 192.168.198.128 </p></blockquote><h2 id="3、只读服务器"><a href="#3、只读服务器" class="headerlink" title="3、只读服务器"></a>3、只读服务器</h2><p>从 Redis 2.6 开始， 从服务器支持<code>只读模式</code>， 并且该模式为从服务器的默认模式。<br>  只读模式配置</p><blockquote><p>方式一：由 redis.conf 文件中的 slave-read-only 选项控制</p><p>slave-read-only yes</p><p>方式二：通过 CONFIG SET 命令来开启或关闭这个模式</p></blockquote><p>  只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。</p><p>  另外，对一个从属服务器执行命令 SLAVEOF NO ONE 将使得这个从属服务器关闭复制功能，并从从属服务器转变回主服务器，原来同步所得的数据集不会被丢弃。</p><p>  利用『 SLAVEOF NO ONE 不会丢弃同步所得数据集』这个特性，可以在主服务器失败的时候，将从属服务器用作新的主服务器，从而实现无间断运行。</p><h2 id="4、从服务器相关配置："><a href="#4、从服务器相关配置：" class="headerlink" title="4、从服务器相关配置："></a>4、从服务器相关配置：</h2><p>设置密码：</p><p>​     主服务器：</p><blockquote><p>通过 requirepass 选项设置密码</p></blockquote><p>​     从服务器：</p><p>方式一：</p><blockquote><p>服务器正在运行，使用客户端输入以下命令：<br>config set masterauth <password></password></p></blockquote><p>方式二：</p><blockquote><p>将它加入到配置文件中：<br>masterauth <password></password></p></blockquote><h2 id="5、主服务器配置"><a href="#5、主服务器配置" class="headerlink" title="5、主服务器配置"></a>5、主服务器配置</h2><p>只在有至少 N  个从服务器的情况下，才执行写操作</p><p>从 Redis 2.8 开始， 为了保证数据的安全性，可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。</p><blockquote><p>min-slaves-to-write <number of="" slaves=""><br>min-slaves-max-lag <number of="" seconds=""></number></number></p></blockquote><p>至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag 秒， 那么主服务器就会执行客户端请求的写操作。</p><h1 id="七、Redis-sentinel-哨兵"><a href="#七、Redis-sentinel-哨兵" class="headerlink" title="七、Redis-sentinel( 哨兵)"></a>七、Redis-sentinel( 哨兵)</h1><h2 id="0、前情提要-2"><a href="#0、前情提要-2" class="headerlink" title="0、前情提要"></a>0、前情提要</h2><p>Redis 的 Sentinel 系统用于管理多个 Redis 服务器，该系统执行以下三个任务：</p><ul><li><p>监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。</p></li><li><p>提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。</p></li><li><p>自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时，集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。</p></li><li>Redis Sentinel 是一个分布式系统，你可以在一个架构中运行多个 Sentinel 进程， 这些进程使用流言协议（gossipprotocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移，以及选择哪个从服务器作为新的主服务器。</li></ul><p>​          虽然 Redis Sentinel 是为一个单独的可执行文件 redis-sentinel ， 但实际上它只是一个运行在特殊模式下的 Redis 服务器， 你可以在启动一个普通 Redis 服务器时通过给定 –sentinel 选项来启动 Redis Sentinel 。</p><h2 id="1、启动-Sentinel"><a href="#1、启动-Sentinel" class="headerlink" title="1、启动 Sentinel"></a>1、启动 Sentinel</h2><p>方式一：</p><p>对于 redis-sentinel 程序， 可以用以下命令来启动Sentinel 系统：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-sentinel  /path/to/sentinel.conf</span><br></pre></td></tr></table></figure><p>方式二：</p><p>对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server /path/to/sentinel.conf  -- sentinel</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><blockquote><p>启动 Sentinel 实例必须指定相应的配置文件， 系统会使用配置文件来保存 Sentinel 的当前状态， </p><p>并在 Sentinel 重启时通过载入配置文件来进行状态还原。</p><p>如果启动 Sentinel 时没有指定相应的配置文件， 或者指定的配置文件不可写（not writable）， 那么 Sentinel 会拒绝启动。</p></blockquote><h2 id="2-、配置-Sentinel"><a href="#2-、配置-Sentinel" class="headerlink" title="2  、配置 Sentinel"></a>2  、配置 Sentinel</h2><p>Redis 源码中包含了一个名为 sentinel.conf 的文件，这个文件是一个带有详细注释的 Sentinel 配置文件示例。<br>运行一个 Sentinel 所需的最少配置如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sentinel monitor mymaster  192.168.198.128 6379 2 </span><br><span class="line">sentinel down-after-milliseconds mymaster 30000</span><br><span class="line">sentinel failover-timeout mymaster 180000</span><br><span class="line">sentinel parallel-syncs mymaster 1</span><br><span class="line">protected-mode no / bind  本机  IP</span><br></pre></td></tr></table></figure><p><code>注意：</code>主服务器无密码时，记得在 sentinel 配置里配上bind 本机 ip ，或者关掉保护模式 protected-mode no</p><p><code>配置解释：</code></p><blockquote><p>第一行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个Sentinel 同意 （只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行）。</p></blockquote><blockquote><p>down-after-milliseconds 选项</p><p>指定了 Sentinel 认为服务器已经断线所需的毫秒数。</p><p>如果服务器在给定的毫秒数之内， 没有返回 Sentinel发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线</p></blockquote><blockquote><p>parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。</p><p>你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。</p></blockquote><h1 id="八、Redis集群"><a href="#八、Redis集群" class="headerlink" title="八、Redis集群"></a>八、Redis集群</h1><h2 id="0、前情提要-3"><a href="#0、前情提要-3" class="headerlink" title="0、前情提要"></a>0、前情提要</h2><ul><li>Redis 集群可以在多个 Redis 节点之间进行数据共享</li><li>Redis 集群不支持那些需要同时处理多个键的 Redis 命令</li></ul><blockquote><p><code>原因</code></p><p>执行这些命令可能需要在多个 Redis 节点之间移动数据，在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。</p><p><code>例如</code></p><p>命令 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> lpoplpush key set1 set2</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>两个set可能在两个不同的节点</p></blockquote><ul><li><p>Redis 集群提供了以下两个好处：</p><blockquote><ul><li>将数据自动切分（split）到多个节点的能力。</li><li>当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。</li></ul></blockquote></li></ul><h2 id="1、集群数据共享"><a href="#1、集群数据共享" class="headerlink" title="1、集群数据共享"></a>1、集群数据共享</h2><p>Redis 集群使用数据分片（sharding）而非一致性哈希来实现： </p><p>一个 Redis 集群包含 16384 个哈希槽（hash slot），数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16<br>校验和 。</p><blockquote><p>集群中的每个节点负责处理一部分哈希槽。</p><p><code>举例</code></p><p> 一个集群可以有三个哈希槽， 其中：<br> 节点 A 负责处理 0 号至 5500 号哈希槽。<br> 节点 B 负责处理 5501 号至 11000 号哈希槽。<br> 节点 C 负责处理 11001 号至 16384 号哈希槽。</p></blockquote><p>这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。</p><blockquote><p><code>举例</code><br> 如果用户将新节点 D 添加到集群中， 那么集群只需要将节点A 、B 、 C 中的某些槽移动到节点 D 就可以了。<br> 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。<br>因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。</p></blockquote><h2 id="2、集群的主从复制"><a href="#2、集群的主从复制" class="headerlink" title="2、集群的主从复制"></a>2、集群的主从复制</h2><p>为了使得集群在一部分节点下线或者无法与集群的大多数节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： </p><p>集群中的每个节点都有 1 个至 N 个复制品，其中一个复制品为主节点， 而其余的 N-1 个复制品为从节点。</p><h2 id="3、集群的一致性保证"><a href="#3、集群的一致性保证" class="headerlink" title="3、集群的一致性保证"></a>3、集群的一致性保证</h2><p>Redis 集群不保证数据的强一致性（strong consistency）：</p><blockquote><p>在特定条件下， Redis 集群可能会丢失已经被执行过的写命令。</p></blockquote><p>使用异步复制是 Redis 集群可能会丢失写命令的其中一个原因。 </p><p>考虑以下这个写命令的例子：</p><blockquote><p> 客户端向主节点 B 发送一条写命令。<br> 主节点 B 执行写命令，并向客户端返回命令回复。<br> 主节点 B 将刚刚执行的写命令复制给它的从节点 B1 、 B2 和B3 。</p></blockquote><p>如你所见， 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。</p><p>Redis 集群另外一种可能会丢失命令的情况是， </p><p>集群出现网络分裂（network partition）， 并且一个客户端与至少包括一个主节点在内的少数（minority）实例被孤立。</p><p><code>举例，</code></p><p>假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1六个节点， 其中 A 、B 、C 为主节点， 而 A1 、B1 、C1 分别为三个主节点的从节点， 另外还有一个客户端 Z1 。</p><p>假设集群中发生网络分裂， 那么集群可能会分裂为两方，大多数（majority）的一方包含节点 A 、C 、A1 、B1 和C1 ， 而少数（minority）的一方则包含节点 B 和客户端Z1 。</p><p>在网络分裂期间， 主节点 B 仍然会接受 Z1 发送的写命令：</p><blockquote><p> 如果网络分裂出现的时间很短， 那么集群会继续正常运行；<br> 但是， 如果网络分裂出现的时间足够长， 使得大多数一方将从节点 B1 设置为新的主节点， 并使用 B1 来代替原来的主节点 B ，那么 Z1 发送给主节点 B 的写命令将丢失。</p></blockquote><p>注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项：</p><blockquote><p> 对于大多数一方来说， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么集群会将这个主节点视为下线， 并使用从节点来代替这个主节点继续工作。</p><p> 对于少数一方， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么它将停止处理写命令， 并向客户端报告错误</p></blockquote><h2 id="4、集群搭建"><a href="#4、集群搭建" class="headerlink" title="4、集群搭建"></a>4、集群搭建</h2><p> （0）前情提要：</p><p>使用节点：6个,（3个主节点、3个从节点）</p><p>（1）准备三台机器，计划每台机器运行两个 redis 实例，集群一共运行 6 个实例。</p><p>在每台机器上创建个 redis-cluster 的目录，创建两个以端口号为名字的子目录， 稍后我们在将每个目录中运行一个 Redis 实例。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir redis-cluster</span><br><span class="line">cd  redis-cluster</span><br><span class="line">mkdir 7000 7001</span><br></pre></td></tr></table></figure><p>（2）分别在文件夹7000，7001各创建一个 redis.conf 文件</p><p>配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">port 7000</span><br><span class="line">cluster-enabled yes</span><br><span class="line">cluster-config-file nodes.conf</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line">appendonly yes</span><br><span class="line">daemonize yes</span><br><span class="line">protected-mode no</span><br></pre></td></tr></table></figure><blockquote><p><code>配置解释</code></p><p> port 7000   （与所在文件夹名一致）:redis监听的端口</p><p> cluster-enabled ：用于开实例的集群模式，<br> cluster-conf-file ：设定了保存节点配置文件的路径， 默认值为 nodes.conf.</p></blockquote><p>（3）在每台机器的 7000 和 7001 文件夹下，使用 redis-server redis.conf 启动实例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server redis.conf</span><br></pre></td></tr></table></figure><p>有六个正在运行中的 Redis 实例</p><p>（4）使用这些实例来创建集群， 并为每个节点编写配置文件。</p><blockquote><ul><li><p>使用 Redis 集群命令行工具 redis-trib ， 编写节点配置文件</p></li><li><p>redis-trib 位于 Redis 源码的 src 文件夹中， 它是一个 Ruby 程序</p></li><li>redis-trib 程序通过向实例发送特殊命令来完成创建新集群， 检查集群， 或者对集群进行<br>重新分片（reshared）等工作。</li></ul></blockquote><p>1）安装 ruby 相关的程序：（选取任一节点）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install ruby -y</span><br><span class="line">yum install rubygems -y</span><br><span class="line"></span><br><span class="line">gem install redis-3.2.1.gem   # 可离线安装</span><br></pre></td></tr></table></figure><p>2）安装完之后用执行以下命令来创建集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./redis-trib.rb create --replicas 1 ip1:7000 ip1:7001 ip2:7000 ip2:7001 ip3:7000 ip3:7001</span><br><span class="line"></span><br><span class="line">./redis-trib.rb create --replicas 1 192.168.198.128:7000 192.168.198.128:7001 192.168.198.130:7000 192.168.198.130:7001 192.168.198.131:7000 192.168.198.131:7001</span><br></pre></td></tr></table></figure><blockquote><p><code>命令解释</code></p><p>给定 redis-trib.rb 程序的命令是 create ， 这表示我们希望创建一个新的集群。<br>选项 –replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。<br>之后跟着的其他参数则是实例的地址列表， 我们希望程序使用这些地址所指示的实例来创建新集群。</p><p>简单来说， 以上命令的意思就是</p><p>让 redis-trib 程序创建一个包含三个主节点和三个从节点的集群。</p><p><code>注意</code></p><p>此处的ip不能写别名</p></blockquote><p><code>显示</code>：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt;&gt;&gt; Creating cluster</span><br><span class="line">&gt; &gt;&gt;&gt; Performing hash slots allocation on 6 nodes...</span><br><span class="line">&gt; Using 3 masters:</span><br><span class="line">&gt; 192.168.198.131:7000</span><br><span class="line">&gt; 192.168.198.130:7000</span><br><span class="line">&gt; 192.168.198.128:7000</span><br><span class="line">&gt; Adding replica 192.168.198.130:7001 to 192.168.198.131:7000</span><br><span class="line">&gt; Adding replica 192.168.198.131:7001 to 192.168.198.130:7000</span><br><span class="line">&gt; Adding replica 192.168.198.128:7001 to 192.168.198.128:7000</span><br><span class="line">&gt; M: c08020516b73f8f5989db493b8bd6c4d983c3714 192.168.198.128:7000</span><br><span class="line">&gt;    slots:10923-16383 (5461 slots) master</span><br><span class="line">&gt; S: ca5e075d862471ce4095bae6aac9dadacf70fe50 192.168.198.128:7001</span><br><span class="line">&gt;    replicates c08020516b73f8f5989db493b8bd6c4d983c3714</span><br><span class="line">&gt; M: c0d1f62e8bd107c5dc93d5ae6d0691a4aa2f42f0 192.168.198.130:7000</span><br><span class="line">&gt;    slots:5461-10922 (5462 slots) master</span><br><span class="line">&gt; S: f9f300b4b7b6bbe21432e9b23e5d667e938e80e2 192.168.198.130:7001</span><br><span class="line">&gt;    replicates 8952a0a01a5d4d866e9b205f06e1647f2c9e5547</span><br><span class="line">&gt; M: 8952a0a01a5d4d866e9b205f06e1647f2c9e5547 192.168.198.131:7000</span><br><span class="line">&gt;    slots:0-5460 (5461 slots) master</span><br><span class="line">&gt; S: ded74a7da6d4434053e864d323b59cc5bc853741 192.168.198.131:7001</span><br><span class="line">&gt;    replicates c0d1f62e8bd107c5dc93d5ae6d0691a4aa2f42f0</span><br><span class="line">&gt; Can I set the above configuration? (type &apos;yes&apos; to accept): </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>我上面的配置可以吗？ yes</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt;&gt;&gt; Nodes configuration updated</span><br><span class="line">&gt; &gt;&gt;&gt; Assign a different config epoch to each node</span><br><span class="line">&gt; &gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span><br><span class="line">&gt; Waiting for the cluster to join...</span><br><span class="line">&gt; &gt;&gt;&gt; Performing Cluster Check (using node 192.168.198.128:7000)</span><br><span class="line">&gt; M: c08020516b73f8f5989db493b8bd6c4d983c3714 192.168.198.128:7000</span><br><span class="line">&gt;    slots:10923-16383 (5461 slots) master</span><br><span class="line">&gt;    1 additional replica(s)</span><br><span class="line">&gt; S: ca5e075d862471ce4095bae6aac9dadacf70fe50 192.168.198.128:7001</span><br><span class="line">&gt;    slots: (0 slots) slave</span><br><span class="line">&gt;    replicates c08020516b73f8f5989db493b8bd6c4d983c3714</span><br><span class="line">&gt; M: 8952a0a01a5d4d866e9b205f06e1647f2c9e5547 192.168.198.131:7000</span><br><span class="line">&gt;    slots:0-5460 (5461 slots) master</span><br><span class="line">&gt;    1 additional replica(s)</span><br><span class="line">&gt; S: f9f300b4b7b6bbe21432e9b23e5d667e938e80e2 192.168.198.130:7001</span><br><span class="line">&gt;    slots: (0 slots) slave</span><br><span class="line">&gt;    replicates 8952a0a01a5d4d866e9b205f06e1647f2c9e5547</span><br><span class="line">&gt; M: c0d1f62e8bd107c5dc93d5ae6d0691a4aa2f42f0 192.168.198.130:7000</span><br><span class="line">&gt;    slots:5461-10922 (5462 slots) master</span><br><span class="line">&gt;    1 additional replica(s)</span><br><span class="line">&gt; S: ded74a7da6d4434053e864d323b59cc5bc853741 192.168.198.131:7001</span><br><span class="line">&gt;    slots: (0 slots) slave</span><br><span class="line">&gt;    replicates c0d1f62e8bd107c5dc93d5ae6d0691a4aa2f42f0</span><br><span class="line">&gt; [OK] All nodes agree about slots configuration.</span><br><span class="line">&gt; &gt;&gt;&gt; Check for open slots...</span><br><span class="line">&gt; &gt;&gt;&gt; Check slots coverage...</span><br><span class="line">&gt; [OK] All 16384 slots covered.</span><br><span class="line">&gt; [root@node01 src]# </span><br><span class="line">&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>3）创建完之后用 如下命令来链接集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli -c -p 7000</span><br></pre></td></tr></table></figure><blockquote><p><code>显示</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; [root@node01 src]# redis-cli -c -p 7000 </span><br><span class="line">&gt; 127.0.0.1:7000&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>如果启动客户端时 为 没有使用  <code>-c</code> ,则视为没有启用集群，则添加key时可能会报错：</p><p>我是在节点 192.168.198.130:7000 上启动的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; 127.0.0.1:7000&gt; set xixi 23</span><br><span class="line">&gt; (error) MOVED 15923 192.168.198.128:7000</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>槽点不匹配，需要切换节点才能添加</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 内存数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm学习</title>
      <link href="/2019/01/29/Storm%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/29/Storm%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Storm简介"><a href="#一、Storm简介" class="headerlink" title="一、Storm简介"></a>一、Storm简介</h1><p>官网：<a href="http://storm.apache.org/" target="_blank" rel="noopener">Storm</a></p><h2 id="1、Storm-："><a href="#1、Storm-：" class="headerlink" title="1、Storm ："></a>1、Storm ：</h2><p>流式计算处理框架</p><h2 id="2、-特点："><a href="#2、-特点：" class="headerlink" title="2、 特点："></a>2、 特点：</h2><ul><li><p>实时</p></li><li><p>分布式</p></li><li><p>高容错</p></li><li><p>storm常驻内存（7*24h:意味着永久运行，需人为关闭）</p><ul><li><p>在Web UI 界面点击kill</p></li><li><p>命令kill</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/storm kill &lt;topologyName&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>数据不经过磁盘，在内存中处理</p></li><li>高可靠性：异常处理、消息可靠性保障机制</li><li>可维护性：StormUI图形化监控接口 </li></ul><h2 id="3、应用"><a href="#3、应用" class="headerlink" title="3、应用"></a>3、应用</h2><p>（1）流式处理：（异步）</p><p>​     客户端提交数据进行计算，并不会等待计算结果</p><p>举例：</p><ul><li><p>逐条处理：ETL（用于数据清洗）</p></li><li><p>统计分析</p><ul><li><p>例：计算PV、UV、访问热点 以及 某些数据的聚合、 加和、平均等等</p><ul><li><p>客户端提交数据以后，计算完成结果存储到redis 、 hbase 、 MySQL或其他MQ中</p></li><li><p>客户端并不关心最终结果是多少</p></li></ul></li></ul></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s4bm9fv3j30wj04et90.jpg" alt=""></p><p>（2）实时请求应答服务：（同步）</p><p>   客户端提交数据后，等待取得计算结果并返回给客户端</p><p>Drpc机制</p><p>举例：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s4blxd7vj30tv07pdg6.jpg" alt=""></p><h1 id="二、Storm架构"><a href="#二、Storm架构" class="headerlink" title="二、Storm架构"></a>二、Storm架构</h1><p>（从进程角度）</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s3u2sboxj30gw0cs0tr.jpg" alt=""></p><h2 id="1、nimbus-主"><a href="#1、nimbus-主" class="headerlink" title="1、nimbus : (主)"></a>1、nimbus : (主)</h2><p>接收客户端提交的请求；</p><p>资源调度、任务分配、接收jar包</p><h2 id="2、supervisor：-从"><a href="#2、supervisor：-从" class="headerlink" title="2、supervisor：(从)"></a>2、supervisor：(从)</h2><p>– 接收nimbus分配的任务、启停worker（当前supervisor上worker数量由配置文件设定）</p><p>– 默认配置4个work进程</p><h2 id="3、worker"><a href="#3、worker" class="headerlink" title="3、worker"></a>3、worker</h2><p>– 运行具体处理运算组件的进程（每个Worker对应执行一个Topology的子集）<br>– worker任务类型，即spout任务、bolt任务两种<br>– 启动executor<br>（executor即worker JVM进程中的一个java线程，一般默认每个executor负责执行一个task任务）</p><p>– 与_ack 个数一致</p><h2 id="4、zookeeper"><a href="#4、zookeeper" class="headerlink" title="4、zookeeper"></a>4、zookeeper</h2><p>– 负责主从的通信</p><p>– 存储心跳信息、任务信息</p><p>– 实现主从的解耦，使nimbus对备份要求不高</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s3u2wb3yj30ib0c0400.jpg" alt=""></p><h1 id="三、Storm编程模型"><a href="#三、Storm编程模型" class="headerlink" title="三、Storm编程模型"></a>三、Storm编程模型</h1><p><img src="https://upload-images.jianshu.io/upload_images/2826805-0fdf7e8af9608fce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><img src="images/storm3.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s4bm1hivj30o70e7q3y.jpg" alt=""></p><h2 id="1、Topology"><a href="#1、Topology" class="headerlink" title="1、Topology"></a>1、<strong>Topology</strong></h2><p>（DAG有向无环图的实现）</p><blockquote><p>是对Storm实时计算逻辑的封装，由一系列通过数据流相互关联的Spout、Bolt组成的拓扑结构</p></blockquote><blockquote><p>生命周期：</p><p>此拓扑结构只要启动，就会在集群中一直运行，直到手动将其kill，否则不会终止。</p><p>（与MapReduce中Job的区别，MR中Job在计算完成后就会终止）</p></blockquote><h2 id="2、spout"><a href="#2、spout" class="headerlink" title="2、spout"></a>2、<strong>spout</strong></h2><p>（数据源：发送数据）</p><blockquote><p>一般会从指定的数据源读取元祖（Tuple）发送到拓扑（Topology）中</p><ul><li><p>一个Spout可以发送多个Stream</p><p>（通过OutputFieldDeclare的declare方法声明不同的数据流，发送数据时通过SpoutOutputCollector中的emit方法指定 StreamId将数据发送出去）</p></li><li><p>Spout中的核心方法是nextTuple，该方法会被Storm线程不断调用、主动从数据源拉取数据、再通过emit方法将数据生成元祖（Tuple）发送给之后的Bolt计算</p></li></ul></blockquote><h2 id="3、bolt"><a href="#3、bolt" class="headerlink" title="3、bolt"></a>3、<strong>bolt</strong></h2><p>（数据处理组件：计算数据，个数不限）</p><blockquote><p>单个Bolt可实现简单的任务或数据流转换</p><p>复杂的场景需要多个Bolt分多个步骤完成</p><ul><li><p>一个Bolt可以发送多个Stream</p><p>（通过OutputFieldDeclare的declare方法声明不同的数据流，发送数据时通过SpoutOutputCollector中的emit方法指定 StreamId将数据发送出去）</p></li><li><p>Bolt中的核心方法是execute，该方法通过接收一个元组数据、实现核心业务逻辑</p></li></ul></blockquote><h2 id="4、tuple"><a href="#4、tuple" class="headerlink" title="4、tuple"></a>4、<strong>tuple</strong></h2><p>（Stream中最小的数据组成单位）</p><h2 id="5、Stream"><a href="#5、Stream" class="headerlink" title="5、Stream"></a>5、<strong>Stream</strong></h2><p>（数据流）</p><blockquote><ul><li>从Spout中传递数据给Bolt、上一个Bolt传递数据给下一个Bolt，这样形成的数据通道叫做Stream</li></ul><ul><li>Stream声明时需要给其指定Id（默认为Default，实际开发中多使用单一数据流，无需指定StreamId）</li></ul></blockquote><h2 id="6、Stream-Grouping"><a href="#6、Stream-Grouping" class="headerlink" title="6、Stream Grouping"></a>6、<strong>Stream Grouping</strong></h2><p>– 数据流分组（即数据分发策略）</p><h2 id="7、-数据传输"><a href="#7、-数据传输" class="headerlink" title="7、 数据传输"></a>7、 数据传输</h2><p>– ZMQ<br>​             – ZeroMQ 开源的消息传递框架，并不是一个MessageQueue<br>– Netty<br>​            – Netty是基于NIO的网络框架，更加高效。（之所以Storm 0.9版本之后使用Netty，是因为ZMQ的license和Storm的license不兼容。）</p><h1 id="四、Storm安装部署"><a href="#四、Storm安装部署" class="headerlink" title="四、Storm安装部署"></a>四、Storm安装部署</h1><h2 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h2><p>环境：zookeeper集群（node00，node01，node02）</p><h3 id="1、解压安装"><a href="#1、解压安装" class="headerlink" title="1、解压安装"></a>1、解压安装</h3><p>版本：apache-storm-0.10.0.tar.gz</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zvxf apache-storm-0.10.0.tar.gz</span><br></pre></td></tr></table></figure><h3 id="2、在storm路径中创建logs文件"><a href="#2、在storm路径中创建logs文件" class="headerlink" title="2、在storm路径中创建logs文件"></a>2、在storm路径中创建logs文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir logs</span><br></pre></td></tr></table></figure><h3 id="3、编辑配置文件解压路径-config-storm-yaml"><a href="#3、编辑配置文件解压路径-config-storm-yaml" class="headerlink" title="3、编辑配置文件解压路径/config/storm.yaml"></a>3、编辑配置文件解压路径/config/storm.yaml</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">storm.zookeeper.servers:</span><br><span class="line">- &quot;node00&quot;</span><br><span class="line">- &quot;node01&quot;</span><br><span class="line">- &quot;node02&quot;</span><br><span class="line">storm.local.dir: &quot;/opt/storm&quot;</span><br><span class="line">nimbus.host: “node00&quot;</span><br><span class="line">supervisor.slots.ports:</span><br><span class="line">- 6700</span><br><span class="line">- 6701</span><br><span class="line">- 6702</span><br><span class="line">- 6703</span><br></pre></td></tr></table></figure><p>将安装包发送到其他节点</p><h3 id="4、启动服务"><a href="#4、启动服务" class="headerlink" title="4、启动服务"></a>4、启动服务</h3><h4 id="（1）启动zookeeper‘"><a href="#（1）启动zookeeper‘" class="headerlink" title="（1）启动zookeeper‘"></a>（1）启动zookeeper‘</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h4 id="（2）启动nimbus"><a href="#（2）启动nimbus" class="headerlink" title="（2）启动nimbus"></a>（2）启动nimbus</h4><p>（在node00上：storm安装目录下）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ./bin/storm nimbus &gt;&gt; ./logs/nimbus.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p><code>nohup</code>:防止被操作系统意外挂起</p><p><code>2&gt;&amp;1</code>：标准错误输出重定向</p><p><code>&amp;</code>：后台运行</p><h4 id="（3）启动supervisor"><a href="#（3）启动supervisor" class="headerlink" title="（3）启动supervisor"></a>（3）启动supervisor</h4><p>（在node01 ，node02上：storm安装目录下）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/storm supervisor&gt;&gt; ./logs/supervisor.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h4 id="（4）-启动Logviewer"><a href="#（4）-启动Logviewer" class="headerlink" title="（4） 启动Logviewer"></a>（4） 启动Logviewer</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/storm logviewer &amp;</span><br></pre></td></tr></table></figure><p>UI界面查看：</p><p> 查看日志：<a href="http://node00:8000/log?file=wc-1-1523947796-worker-6703.log" target="_blank" rel="noopener">http://node00:8000/log?file=wc-1-1523947796-worker-6703.log</a></p><h4 id="（5）查看进程-："><a href="#（5）查看进程-：" class="headerlink" title="（5）查看进程 ："></a>（5）查看进程 ：</h4><p>jps</p><p>显示：</p><p>node00：</p><blockquote><p>[root@node00 apache-storm-0.10.0]# jps<br>8869 QuorumPeerMain<br>9093 Jps<br>9083 config_value</p></blockquote><p>node01 、 node02</p><blockquote><p>[root@node01 apache-storm-0.10.0]# jps<br>7280 config_value<br>7290 Jps<br>7230 QuorumPeerMain</p></blockquote><p>且三个节点的logs目录下也相应的生成了指定的日志文件</p><h3 id="5、Storm-UI"><a href="#5、Storm-UI" class="headerlink" title="5、Storm UI"></a>5、Storm UI</h3><p>（从浏览器访问，在前台页面上查看详情）</p><h4 id="（1）启动服务"><a href="#（1）启动服务" class="headerlink" title="（1）启动服务"></a>（1）启动服务</h4><p>（可在node00节点：storm安装目录）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/storm ui &gt;&gt; ./logs/ui.out 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><h4 id="（2）浏览器访问"><a href="#（2）浏览器访问" class="headerlink" title="（2）浏览器访问"></a>（2）浏览器访问</h4><p>（Storm UI在哪台节点启动，就访问哪台节点）</p><blockquote><p><a href="http://node00:8080" target="_blank" rel="noopener">http://node00:8080</a></p></blockquote><h3 id="6、运行jar"><a href="#6、运行jar" class="headerlink" title="6、运行jar"></a>6、运行jar</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>#查看帮助</span><br><span class="line">bin/storm -h</span><br><span class="line"><span class="meta">#</span>#查看指定命令的帮助</span><br><span class="line">bin/storm  help jar</span><br><span class="line"><span class="meta">#</span>#运行jar包</span><br><span class="line">bin/storm jar &lt;jar包所在路径&gt; &lt;包名+类名&gt; &lt;参数：topologyName&gt;</span><br></pre></td></tr></table></figure><h1 id="五、Storm重点概念详解"><a href="#五、Storm重点概念详解" class="headerlink" title="五、Storm重点概念详解"></a>五、Storm重点概念详解</h1><h2 id="1、Storm-Grouping-–-数据流分组（即数据分发策略）"><a href="#1、Storm-Grouping-–-数据流分组（即数据分发策略）" class="headerlink" title="1、Storm Grouping – 数据流分组（即数据分发策略）"></a>1、Storm Grouping – 数据流分组（即数据分发策略）</h2><p>• 1. Shuffle Grouping<br>– 随机分组，随机派发stream里面的tuple，保证每个bolt task接收到的tuple数目大致相同。<br>– 轮询，平均分配</p><p>• 2. Fields Grouping<br>– 按字段分组，比如，按”user-id”这个字段来分组，那么具有同样”user-id”的 tuple 会被分到相同的Bolt里的一个task， 而不同的”user-id”则可能会被分配到不同的task。</p><p>• 3. All Grouping<br>– 广播发送，对于每一个tuple，所有的bolts都会收到</p><p>• 4. Global Grouping<br>– 全局分组，把tuple分配给task id最低的task 。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"></span><br><span class="line">builder.setSpout(<span class="string">"spout"</span>, <span class="keyword">new</span> MySpout(), <span class="number">2</span>);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// shuffleGrouping其实就是随机往下游去发,不自觉的做到了负载均衡</span></span><br><span class="line"><span class="comment">//builder.setBolt("bolt", new MyBolt(),2).shuffleGrouping("spout");</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// fieldsGrouping其实就是MapReduce里面理解的Shuffle,根据fields求hash来取模</span></span><br><span class="line"><span class="comment">//      builder.setBolt("bolt", new MyBolt(), 2).fieldsGrouping("spout", new Fields("session_id"));</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 只往一个里面发,往taskId小的那个里面去发送</span></span><br><span class="line">builder.setBolt(<span class="string">"bolt"</span>, <span class="keyword">new</span> MyBolt(), <span class="number">2</span>).globalGrouping(<span class="string">"spout"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等于shuffleGrouping</span></span><br><span class="line"><span class="comment">//builder.setBolt("bolt", new MyBolt(), 2).noneGrouping("spout");</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 广播下游的所有task都能收到数据</span></span><br><span class="line">builder.setBolt(<span class="string">"bolt"</span>, <span class="keyword">new</span> MyBolt(), <span class="number">5</span>).allGrouping(<span class="string">"spout"</span>);</span><br></pre></td></tr></table></figure><p>———————————————————-以上为常用grouping策略————————————————————–</p><p>• 5. None Grouping<br>– 不分组，这个分组的意思是说stream不关心到底怎样分组。目前这种分组和Shuffle grouping是一样的效果。<br>有一点不同的是storm会把使用none grouping的这个bolt放到这个bolt的订阅者同一个线程里面去执行（未来Storm如果可能的话会这样设计）。</p><p>• 6. Direct Grouping（很少用）<br>– 指向型分组， 这是一种比较特别的分组方法，用这种分组意味着消息（tuple）的发送者指定由消息接收者的那个task处理这个消息。只有被声明为 Direct Stream 的消息流可以声明这种分组方法。而且这种消息tuple必须使用 emitDirect 方法来发射。消息处理者可以通过 TopologyContext 来获取处理它的消息的task的id<br>(OutputCollector.emit方法也会返回task的id)</p><p>• 7. Local or shuffle grouping<br>– 本地或随机分组。如果目标bolt有一个或者多个task与源bolt的task在同一个工作进程中，tuple将会被随机发送给这些同进程中的tasks。否则，和普通的Shuffle Grouping行为一致</p><p>• 8.customGrouping<br>– 自定义，相当于mapreduce那里自己去实现一个partition一样。</p><h2 id="2、并发机制"><a href="#2、并发机制" class="headerlink" title="2、并发机制"></a>2、并发机制</h2><h3 id="1、-Worker-processes"><a href="#1、-Worker-processes" class="headerlink" title="1、 Worker processes"></a>1、 Worker processes</h3><ul><li>Worker – 进程<ul><li>一个Topology拓扑会包含一个或多个Worker（每个Worker进程只能从属于一个特定的Topology）、</li><li>这些Worker进程会并行跑在集群中不同的服务器上，即一个Topology拓扑其实是由并行运行在Storm集群中<br>多台服务器上的进程所组成</li></ul></li></ul><h3 id="2、Executors-threads"><a href="#2、Executors-threads" class="headerlink" title="2、Executors (threads)"></a>2、Executors (threads)</h3><ul><li>Executor – 线程<ul><li>Executor是由Worker进程中生成的一个线程</li><li>每个Worker进程中会运行拓扑当中的一个或多个Executor线程</li><li>一个Executor线程中可以执行一个或多个Task任务（默认每个Executor只执行一个Task任务），但是这些Task任务都是对应着同一个组件（Spout、Bolt）。</li></ul></li></ul><h3 id="3、Task"><a href="#3、Task" class="headerlink" title="3、Task"></a>3、Task</h3><ul><li>Tasks – 任务<ul><li>实际执行数据处理的最小单元</li><li>每个task即为一个Spout或者一个Bolt</li><li>Task数量在整个Topology生命周期中保持不变，Executor数量可以变化或手动调整</li><li>（默认情况下，Task数量和Executor是相同的，即每个Executor线程中默认运行一个Task任务）</li></ul></li></ul><h3 id="4、-设置Worker进程数"><a href="#4、-设置Worker进程数" class="headerlink" title="4、 设置Worker进程数"></a>4、 设置Worker进程数</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Config.setNumWorkers(<span class="keyword">int</span> workers)</span><br></pre></td></tr></table></figure><h3 id="5、设置Executor线程数"><a href="#5、设置Executor线程数" class="headerlink" title="5、设置Executor线程数"></a>5、设置Executor线程数</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TopologyBuilder.setSpout(String id, IRichSpout spout, Number parallelism_hint)</span><br><span class="line">TopologyBuilder.setBolt(String id, IRichBolt bolt, Number parallelism_hint)</span><br><span class="line"><span class="comment">//其中， parallelism_hint即为executor线程数</span></span><br></pre></td></tr></table></figure><h3 id="6、-设置Task数量"><a href="#6、-设置Task数量" class="headerlink" title="6、 设置Task数量"></a>6、 设置Task数量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ComponentConfigurationDeclarer.setNumTasks(Number val)</span><br><span class="line"><span class="comment">// 例：共2 worker ， 3 excutor ， 5  task</span></span><br><span class="line">Config conf = <span class="keyword">new</span> Config() ;</span><br><span class="line">conf.setNumWorkers(<span class="number">2</span>);<span class="comment">//2 个work进程</span></span><br><span class="line">TopologyBuilder topologyBuilder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">topologyBuilder.setSpout(<span class="string">"spout"</span>, <span class="keyword">new</span> MySpout(), <span class="number">1</span>);  <span class="comment">// 1 个excutor线程 ， 1个 task</span></span><br><span class="line">topologyBuilder.setBolt(<span class="string">"green-bolt"</span>, <span class="keyword">new</span> GreenBolt(), <span class="number">2</span>)  <span class="comment">// 2 个excutor线程 ， </span></span><br><span class="line">.setNumTasks(<span class="number">4</span>)   <span class="comment">// 4  个task</span></span><br><span class="line">.shuffleGrouping(<span class="string">"blue-spout);</span></span><br></pre></td></tr></table></figure><h3 id="7、-Rebalance-–-再平衡"><a href="#7、-Rebalance-–-再平衡" class="headerlink" title="7、 Rebalance – 再平衡"></a>7、 Rebalance – 再平衡</h3><p>– 即，动态调整Topology拓扑的Worker进程数量、以及Executor线程数量<br>• 支持两种调整方式：<br>– 1、通过Storm UI<br>– 2、通过Storm CLI</p><h4 id="通过Storm-CLI动态调整："><a href="#通过Storm-CLI动态调整：" class="headerlink" title="通过Storm CLI动态调整："></a>通过Storm CLI动态调整：</h4><p>– 例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10</span><br><span class="line"><span class="meta">#</span>#将mytopology拓扑worker进程数量调整为5个</span><br><span class="line"><span class="meta">#</span>#“ blue-spout ” 所使用的线程数量调整为3个</span><br><span class="line"><span class="meta">#</span>#“ yellow-bolt ”所使用的线程数量调整为10个</span><br></pre></td></tr></table></figure><h2 id="3、通信机制-–-Worker内部的消息传递机制"><a href="#3、通信机制-–-Worker内部的消息传递机制" class="headerlink" title="3、通信机制 – Worker内部的消息传递机制"></a>3、通信机制 – Worker内部的消息传递机制</h2><h4 id="•-Worker进程间的数据通信"><a href="#•-Worker进程间的数据通信" class="headerlink" title="• Worker进程间的数据通信"></a>• Worker进程间的数据通信</h4><p>– ZMQ<br>– ZeroMQ 开源的消息传递框架，并不是一个MessageQueue<br>– Netty<br>– Netty是基于NIO的网络框架，更加高效。（之所以Storm 0.9版本之后使用Netty，是因为ZMQ的license和Storm的license不兼容。）</p><h4 id="•-Worker内部的数据通信"><a href="#•-Worker内部的数据通信" class="headerlink" title="• Worker内部的数据通信"></a>• Worker内部的数据通信</h4><p>– Disruptor<br>– 实现了“队列”的功能。<br>– 可以理解为一种事件监听或者消息处理机制，即在队列当中一边由生产者放入消息数据，另一边消费者并行取出消息数据处理。</p><h2 id="4、容错机制"><a href="#4、容错机制" class="headerlink" title="4、容错机制"></a>4、容错机制</h2><h3 id="1、集群节点宕机"><a href="#1、集群节点宕机" class="headerlink" title="1、集群节点宕机"></a>1、集群节点宕机</h3><p>– Nimbus服务器<br>• 单点故障？<br>– 非Nimbus服务器<br>• 故障时，该节点上所有Task任务都会超时，Nimbus会将这些Task任务重新分配到其他服务器上运行</p><h3 id="2、进程挂掉"><a href="#2、进程挂掉" class="headerlink" title="2、进程挂掉"></a>2、进程挂掉</h3><p>– Worker<br>• 挂掉时，Supervisor会重新启动这个进程。如果启动过程中仍然一直失败，并且无法向Nimbus发送心跳，Nimbus会将该Worker重新分配到其他服务器上</p><p>– Supervisor<br>• 无状态（所有的状态信息都存放在Zookeeper中来管理），不影响已经在运行的worker，但是在当前节点worker如果挂掉就无法重启，可以在另一台supervisor节点重启worker<br>• 快速失败（每当遇到任何异常情况，都会自动毁灭）</p><p>– Nimbus<br>• 无状态（所有的状态信息都存放在Zookeeper中来管理）<br>• 快速失败（每当遇到任何异常情况，都会自动毁灭）</p><h3 id="3、消息的完整性"><a href="#3、消息的完整性" class="headerlink" title="3、消息的完整性"></a>3、消息的完整性</h3><p>– 从Spout中发出的Tuple，以及基于他所产生Tuple（例如上个例子当中Spout发出的句子，以及句子当中单词的tuple等）<br>– 由这些消息就构成了一棵tuple树<br>– 当这棵tuple树发送完成，并且树当中每一条消息都被正确处理，就表明spout发送消息被“完整处理”，即消息的完整性</p><h3 id="4、-Acker-–-消息完整性的实现机制"><a href="#4、-Acker-–-消息完整性的实现机制" class="headerlink" title="4、 Acker – 消息完整性的实现机制"></a>4、 Acker – 消息完整性的实现机制</h3><p>给每条数据添加唯一标记ID ， 对于每条数据的发送或接收，都响应给Acker ，通过异或（同为0 ， 异为1） 从而确认消息是否完整</p><p>– Storm的拓扑当中特殊的一些任务</p><p>– 负责跟踪每个Spout发出的Tuple的DAG（有向无环图）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//核心代码</span></span><br><span class="line"><span class="comment">/*AckTest.class*/</span></span><br><span class="line">        TopologyBuilder topologyBuilder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">        topologyBuilder.setSpout(<span class="string">"ack"</span>,<span class="keyword">new</span> AckSpout());</span><br><span class="line">        topologyBuilder.setBolt(<span class="string">"splitbolt"</span>,</span><br><span class="line">                                <span class="keyword">new</span> AckSplitBolt(),<span class="number">2</span>).shuffleGrouping(<span class="string">"ack"</span>);</span><br><span class="line">        topologyBuilder.setBolt(<span class="string">"countbolt"</span>,</span><br><span class="line">               <span class="keyword">new</span> AckCountBolt(),<span class="number">2</span>).fieldsGrouping(<span class="string">"splitbolt"</span>,<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">        Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">        conf.setMessageTimeoutSecs(<span class="number">3</span>);</span><br><span class="line">        LocalCluster localCluster = <span class="keyword">new</span> LocalCluster();        localCluster.submitTopology(<span class="string">"acktest"</span>,conf,topologyBuilder.createTopology());</span><br><span class="line"></span><br><span class="line"><span class="comment">/*AckSpout.class*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AckSpout</span> <span class="keyword">implements</span> <span class="title">IRichSpout</span> </span>&#123;</span><br><span class="line">    SpoutOutputCollector collector;</span><br><span class="line">    <span class="comment">//缓存map...</span></span><br><span class="line">    Map&lt;Object,String&gt; map =  <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    String[] lines = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">            <span class="string">"i love shsxt"</span>,</span><br><span class="line">            <span class="string">"i hate you"</span>,</span><br><span class="line">            <span class="string">"haha xixi xixi"</span>,</span><br><span class="line">            <span class="string">"xidada is good"</span></span><br><span class="line">    &#125;;</span><br><span class="line">    Random random = <span class="keyword">new</span> Random();</span><br><span class="line"><span class="comment">// id 用以标志发送的每条数据</span></span><br><span class="line">    <span class="keyword">long</span> id = <span class="number">0</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Map conf, TopologyContext context, SpoutOutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String line = lines[random.nextInt(<span class="number">4</span>)];</span><br><span class="line">        collector.emit(<span class="keyword">new</span> Values(line),id);</span><br><span class="line">        map.put(id,line);</span><br><span class="line">        id++;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  如果数据被完整处理，此时调用ack方法，并把msgid穿进去</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msgId</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ack</span><span class="params">(Object msgId)</span> </span>&#123;</span><br><span class="line">        System.out.println(msgId + <span class="string">" 执行成功....并删除缓存"</span>);</span><br><span class="line">        map.remove(msgId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 如果数据未被完整处理，即处理失败，则调用fail方法。</span></span><br><span class="line"><span class="comment">     * 失败的时候从缓存map里重发数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> msgId</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fail</span><span class="params">(Object msgId)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"执行失败，重发...  msgid: "</span> + msgId);</span><br><span class="line">        collector.emit(<span class="keyword">new</span> Values(map.get(msgId)),msgId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"line"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*AckSplitBolt.class*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AckSplitBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    OutputCollector collector ;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map stormConf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple input)</span> </span>&#123;</span><br><span class="line">        String line = input.getStringByField(<span class="string">"line"</span>);</span><br><span class="line">        String[] split = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; split.length-<span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="comment">//继续跟踪数据</span></span><br><span class="line">            collector.emit(input,<span class="keyword">new</span> Values(split[i]));</span><br><span class="line">        &#125;</span><br><span class="line">        collector.ack(input);</span><br><span class="line"><span class="comment">//      collector.fail(input);</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*AckCountBolt.class*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AckCountBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    Map&lt;String,Integer&gt; resultMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    OutputCollector collector;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map stormConf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple input)</span> </span>&#123;</span><br><span class="line">        String word = input.getStringByField(<span class="string">"word"</span>);</span><br><span class="line">        Integer integer = resultMap.get(word);</span><br><span class="line">        <span class="keyword">if</span>(integer==<span class="keyword">null</span>)&#123;</span><br><span class="line">            integer = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            integer++;</span><br><span class="line">        &#125;</span><br><span class="line">        resultMap.put(word,integer);</span><br><span class="line">        System.out.println(word + <span class="string">" : "</span> + integer);</span><br><span class="line">        <span class="comment">//告诉ack 此tuple接受成功</span></span><br><span class="line">        collector.ack(input);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="5、与MapReduce的区别"><a href="#5、与MapReduce的区别" class="headerlink" title="5、与MapReduce的区别"></a>5、与MapReduce的区别</h2><table><thead><tr><th style="text-align:center">Storm</th><th style="text-align:center">MapReduce</th></tr></thead><tbody><tr><td style="text-align:center">流式处理</td><td style="text-align:center">批处理</td></tr><tr><td style="text-align:center">（毫）秒级</td><td style="text-align:center">分钟级</td></tr><tr><td style="text-align:center">DAG模型</td><td style="text-align:center">Map+Reduce模型</td></tr><tr><td style="text-align:center">常驻内存运行</td><td style="text-align:center">反复启停</td></tr><tr><td style="text-align:center">进程、线程常驻内存运行，数据不进入磁盘，数据通过网络传递</td><td style="text-align:center">为TB、PB级别数据设计的批处理计算框架</td></tr></tbody></table><h2 id="6、和Spark-Streaming的区别"><a href="#6、和Spark-Streaming的区别" class="headerlink" title="6、和Spark Streaming的区别"></a>6、和Spark Streaming的区别</h2><table><thead><tr><th style="text-align:center">Storm</th><th style="text-align:center">Spark Streaming</th></tr></thead><tbody><tr><td style="text-align:center">流式处理</td><td style="text-align:center">微批处理</td></tr><tr><td style="text-align:center">（毫）秒级</td><td style="text-align:center">秒级</td></tr><tr><td style="text-align:center">成熟稳定</td><td style="text-align:center">稳定性改进中</td></tr><tr><td style="text-align:center">独立系统，专为流式计算设计</td><td style="text-align:center">Spark核心的一种计算模型<br>能与其他组件很好的结合</td></tr><tr><td style="text-align:center">数据传输模式更为简单，很多地方也更为高效</td><td style="text-align:center">将RDD做的很小来用小的批处理来接近流式处理</td></tr><tr><td style="text-align:center">并不是不能做批处理，<br>它也可以来做微批处理，来提高吞吐</td><td style="text-align:center">基于内存和DAG</td></tr></tbody></table><p><strong>小记</strong></p><p>1、storm源码中包含后缀名为<code>.clj</code>的文件，这是一种Clojure编程语言，它是一种运行在JVM上的Lisp方言。而Lisp是一种以表达性和功能强大著称的编程语言。</p><p>2、阿里巴巴在Storm的基础上使用Java代码并做了相关的改进，开发了JStorm，和Storm一样都是开源的。（反哺行为，包括将Flink→Blink）</p><p>3、   at-least        至少处理一次</p><p>​       exactly-once 有且只有一次</p><h2 id="7、Storm-架构设计与Hadoop架构对比"><a href="#7、Storm-架构设计与Hadoop架构对比" class="headerlink" title="7、Storm 架构设计与Hadoop架构对比"></a>7、Storm 架构设计与Hadoop架构对比</h2><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Storm</th><th style="text-align:center">Hadoop</th></tr></thead><tbody><tr><td style="text-align:center">主节点</td><td style="text-align:center">Nimbus</td><td style="text-align:center">ResourceManager</td></tr><tr><td style="text-align:center">从节点</td><td style="text-align:center">Supervisor</td><td style="text-align:center">NodeManager</td></tr><tr><td style="text-align:center">应用程序</td><td style="text-align:center">Topology</td><td style="text-align:center">Job</td></tr><tr><td style="text-align:center">工作进程</td><td style="text-align:center">Child</td><td style="text-align:center">Worker</td></tr><tr><td style="text-align:center">计算模型</td><td style="text-align:center">Map/Reduce</td><td style="text-align:center">Spout/Bolt</td></tr></tbody></table><h1 id="六、Storm-API-（数据累加）"><a href="#六、Storm-API-（数据累加）" class="headerlink" title="六、Storm API （数据累加）"></a>六、Storm API （数据累加）</h1><p>MyTopology.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.Config;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.LocalCluster;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.generated.StormTopology;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.TopologyBuilder;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTopology</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//数据累加... spout  bolt</span></span><br><span class="line">        TopologyBuilder topologyBuilder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">        topologyBuilder.setSpout(<span class="string">"myspout"</span>,<span class="keyword">new</span> MySpout());</span><br><span class="line">        <span class="comment">//shuffleGrouping（）表示将前一个bolt和后一个spout连接</span></span><br><span class="line">        topologyBuilder.setBolt(<span class="string">"mybolt"</span>,<span class="keyword">new</span> MyBolt()).shuffleGrouping(<span class="string">"myspout"</span>);</span><br><span class="line">        </span><br><span class="line">        StormTopology topology = topologyBuilder.createTopology();</span><br><span class="line">        Config config = <span class="keyword">new</span> Config();</span><br><span class="line">        LocalCluster localCluster = <span class="keyword">new</span> LocalCluster();</span><br><span class="line">        localCluster.submitTopology(<span class="string">"sum"</span>,config,topology);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MySpout.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.spout.SpoutOutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.TopologyContext;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.IRichSpout;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.OutputFieldsDeclarer;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseRichSpout;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Fields;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Values;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySpout</span> <span class="keyword">extends</span> <span class="title">BaseRichSpout</span> </span>&#123;</span><br><span class="line">    SpoutOutputCollector collector;</span><br><span class="line">    <span class="keyword">int</span> i =<span class="number">0</span>;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化方法。。框架在执行任务的时候，会先执行此方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> conf   可以得到spout的配置</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context 上下文环境</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> collector  往下游发送数据...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Map conf, TopologyContext context, SpoutOutputCollector collector)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/***</span></span><br><span class="line"><span class="comment">     * 此方法是spout的核心方法。</span></span><br><span class="line"><span class="comment">     * 框架会一直（无限）调用这个方法，每当调用此方法时，我们应该往下游发送数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *  mysout = new Myspout()     *</span></span><br><span class="line"><span class="comment">     *  mysout.open(conf,context,collector)     *</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *  while(ture)&#123;</span></span><br><span class="line"><span class="comment">     *      mysout.nextTuple()</span></span><br><span class="line"><span class="comment">     *  &#125;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        i++;</span><br><span class="line">       <span class="comment">/*与下面的“number”对应</span></span><br><span class="line"><span class="comment">        *若new Values(i，"zs")</span></span><br><span class="line"><span class="comment">        *那么就是"number","name"</span></span><br><span class="line"><span class="comment">        */</span></span><br><span class="line">        collector.emit(<span class="keyword">new</span> Values(i));</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">"spout 发送.."</span> + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 当需要往下游发送数据时，就要声明字段个数和字段名字。</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> declarer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        </span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"number"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> MyBolt.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.task.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.TopologyContext;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.IRichBolt;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.OutputFieldsDeclarer;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseRichBolt;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Tuple;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> sum;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * bolt 初始化方法。。</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> stormConf</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> collector</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map stormConf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * bolt 中 最核心的方法</span></span><br><span class="line"><span class="comment">     * 框架会一直调用此方法，每次调用就传一个数据进来。</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> input</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple input)</span> </span>&#123;</span><br><span class="line">        Integer integer = input.getInteger(<span class="number">0</span>);</span><br><span class="line"><span class="comment">//        input.getIntegerByField("number");</span></span><br><span class="line">        sum +=integer;</span><br><span class="line">        System.out.println(<span class="string">"excute : "</span> + integer  +  <span class="string">"   sum : "</span> + sum);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s5fu1zxjj31280gfq6k.jpg" alt=""></p><h1 id="七、Storm-API-（单词统计）"><a href="#七、Storm-API-（单词统计）" class="headerlink" title="七、Storm API （单词统计）"></a>七、Storm API （单词统计）</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1s3u24s62j30pw03wmxc.jpg" alt=""></p><p>WordCountToplogy.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.Config;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.LocalCluster;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.StormSubmitter;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.generated.AlreadyAliveException;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.generated.InvalidTopologyException;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.TopologyBuilder;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Fields;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountToplogy</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 一对一 线程与task</span></span><br><span class="line"><span class="comment">     *  thread0 = new Thread(new bolt0)</span></span><br><span class="line"><span class="comment">     *  thread0.start()</span></span><br><span class="line"><span class="comment">     *  run()&#123;</span></span><br><span class="line"><span class="comment">     *      while(true)&#123;</span></span><br><span class="line"><span class="comment">     *           bolt0.excute(tuple)</span></span><br><span class="line"><span class="comment">     *      &#125;</span></span><br><span class="line"><span class="comment">     *  &#125;</span></span><br><span class="line"><span class="comment">     *  thread1 = new Thread ( new bolt1)</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     *  run()&#123;</span></span><br><span class="line"><span class="comment">     *       while(true)&#123;</span></span><br><span class="line"><span class="comment">     *           bolt1.excute(tuple)</span></span><br><span class="line"><span class="comment">     *      &#125;</span></span><br><span class="line"><span class="comment">     *  &#125;</span></span><br><span class="line"><span class="comment">     *  new Thread (new bolt2)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        TopologyBuilder topologyBuilder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"></span><br><span class="line">        topologyBuilder.setSpout(<span class="string">"wcspout"</span>,<span class="keyword">new</span> WordCountSpout());</span><br><span class="line"><span class="comment">//5 在这里指并行度，即task个数</span></span><br><span class="line"><span class="comment">//shuffleGrouping() 将bolt 连接至指定的spout之后</span></span><br><span class="line">        topologyBuilder.setBolt(<span class="string">"splitbolt"</span>,</span><br><span class="line">                                <span class="keyword">new</span> SplitBolt(),<span class="number">5</span>).shuffleGrouping(<span class="string">"wcspout"</span>);</span><br><span class="line"><span class="comment">//并行度 6 </span></span><br><span class="line"><span class="comment">//fieldsGrouping() 将bolt 连接至指定 的bolt之后，按指定字段grouping      </span></span><br><span class="line">        topologyBuilder.setBolt(<span class="string">"countbolt"</span>,</span><br><span class="line">                 <span class="keyword">new</span> CountBolt(),<span class="number">6</span>).fieldsGrouping(<span class="string">"splitbolt"</span>,<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        Config config = <span class="keyword">new</span> Config();</span><br><span class="line">        config.setNumWorkers(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                StormSubmitter.submitTopology(args[<span class="number">0</span>], config, topologyBuilder.createTopology());</span><br><span class="line">            &#125; <span class="keyword">catch</span> (AlreadyAliveException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InvalidTopologyException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LocalCluster localCluster = <span class="keyword">new</span> LocalCluster();</span><br><span class="line">            localCluster.submitTopology(<span class="string">"mytopology"</span>, config, topologyBuilder.createTopology());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>WordCountSpout.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.spout.SpoutOutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.TopologyContext;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.OutputFieldsDeclarer;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseRichSpout;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Fields;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Values;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountSpout</span> <span class="keyword">extends</span> <span class="title">BaseRichSpout</span> </span>&#123;</span><br><span class="line">    SpoutOutputCollector collector;</span><br><span class="line">    String[] lines = <span class="keyword">new</span> String[]&#123;</span><br><span class="line">            <span class="string">"i love learning"</span>,</span><br><span class="line">            <span class="string">"i miss you "</span>,</span><br><span class="line">            <span class="string">"sxt is good"</span>,</span><br><span class="line">            <span class="string">"good good study day day up"</span></span><br><span class="line">    &#125;;</span><br><span class="line">    Random random = <span class="keyword">new</span> Random();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Map conf, TopologyContext context, SpoutOutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> index = random.nextInt(lines.length);</span><br><span class="line">        String line = lines[index];</span><br><span class="line">        collector.emit(<span class="keyword">new</span> Values(line));</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"line"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>SplitBolt.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.task.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.TopologyContext;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.OutputFieldsDeclarer;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseRichBolt;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Fields;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Tuple;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Values;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    OutputCollector collector;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map stormConf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">        System.err.println(<span class="string">"split ----- "</span>  + <span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple input)</span> </span>&#123;</span><br><span class="line">        String line = input.getString(<span class="number">0</span>);</span><br><span class="line">        String[] words = line.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span>(String word : words)&#123;</span><br><span class="line">            collector.emit(<span class="keyword">new</span> Values(word));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CountBolt.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> backtype.storm.task.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.TopologyContext;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.OutputFieldsDeclarer;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseRichBolt;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Tuple;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CountBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    Map&lt;String,Integer&gt; resultMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map stormConf, TopologyContext context, OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        System.err.println(<span class="string">"countbolt ---- "</span> + <span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple input)</span> </span>&#123;</span><br><span class="line">        String word = input.getStringByField(<span class="string">"word"</span>);</span><br><span class="line">        <span class="keyword">if</span>(resultMap.containsKey(word))&#123;</span><br><span class="line">            Integer integer = resultMap.get(word);</span><br><span class="line">            integer++;</span><br><span class="line">            resultMap.put(word,integer);</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            resultMap.put(word,<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        System.err.println(<span class="keyword">this</span> + <span class="string">" --  "</span> + word);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="八、Flume-Kafka-Storm架构设计"><a href="#八、Flume-Kafka-Storm架构设计" class="headerlink" title="八、Flume+Kafka+Storm架构设计"></a>八、Flume+Kafka+Storm架构设计</h1><p>• 采集层：实现日志收集，使用负载均衡策略<br>• 消息队列：作用是解耦及不同速度系统缓冲<br>• 实时处理单元：用Storm来进行数据处理，最终数据流入DB中<br>• 展示单元：数据可视化，使用WEB框架展示</p><p>• 美团Flume架构<br>– <a href="http://tech.meituan.com/mt-log-system-arch.html" target="_blank" rel="noopener">http://tech.meituan.com/mt-log-system-arch.html</a><br>• Flume的负载均衡<br>– <a href="http://flume.apache.org/FlumeUserGuide.html#load-balancing-sink-processor" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html#load-balancing-sink-processor</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.Config;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.LocalCluster;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.spout.SchemeAsMultiScheme;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.OutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.task.TopologyContext;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.BasicOutputCollector;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.OutputFieldsDeclarer;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.TopologyBuilder;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseBasicBolt;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.topology.base.BaseRichBolt;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Fields;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Tuple;</span><br><span class="line"><span class="keyword">import</span> backtype.storm.tuple.Values;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.KafkaSpout;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.SpoutConfig;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.StringScheme;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.ZkHosts;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.bolt.KafkaBolt;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.bolt.mapper.FieldNameBasedTupleToKafkaMapper;</span><br><span class="line"><span class="keyword">import</span> storm.kafka.bolt.selector.DefaultTopicSelector;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * This topology demonstrates Storm's stream groupings and multilang</span></span><br><span class="line"><span class="comment"> * capabilities.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogFilterTopology</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FilterBolt</span> <span class="keyword">extends</span> <span class="title">BaseBasicBolt</span> </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple, BasicOutputCollector collector)</span> </span>&#123;</span><br><span class="line">String line = tuple.getString(<span class="number">0</span>);</span><br><span class="line">System.err.println(<span class="string">"Accept:  "</span> + line);</span><br><span class="line"><span class="comment">// 包含ERROR的行留下</span></span><br><span class="line"><span class="keyword">if</span> (line.contains(<span class="string">"ERROR"</span>)) &#123;</span><br><span class="line">System.err.println(<span class="string">"Filterbolt:  "</span> + line);</span><br><span class="line">collector.emit(<span class="keyword">new</span> Values(line));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 定义message提供给后面FieldNameBasedTupleToKafkaMapper使用</span></span><br><span class="line">declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"message"</span>));</span><br><span class="line">&#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"><span class="comment">// https://github.com/apache/storm/tree/master/external/storm-kafka</span></span><br><span class="line"><span class="comment">// config kafka spout，话题</span></span><br><span class="line">String topic = <span class="string">"kafkatest"</span>;</span><br><span class="line"><span class="comment">//kafka集群所使用的zookeeper集群</span></span><br><span class="line">ZkHosts zkHosts = <span class="keyword">new</span> ZkHosts(<span class="string">"node01:2181,node02:2181,node03:2181"</span>);</span><br><span class="line"><span class="comment">//   /kafka_storm，偏移量offset的根目录，记录队列取到了哪里</span></span><br><span class="line">SpoutConfig spoutConfig = <span class="keyword">new</span> SpoutConfig(zkHosts, topic, <span class="string">"/kafka_storm"</span>, <span class="string">"test_id"</span>);<span class="comment">// 对应一个应用</span></span><br><span class="line"></span><br><span class="line">List&lt;String&gt; zkServers = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">System.out.println(zkHosts.brokerZkStr);</span><br><span class="line"><span class="keyword">for</span> (String host : zkHosts.brokerZkStr.split(<span class="string">","</span>)) &#123;</span><br><span class="line">zkServers.add(host.split(<span class="string">":"</span>)[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">//storm的zookeeper集群地址</span></span><br><span class="line">spoutConfig.zkServers = zkServers;</span><br><span class="line">spoutConfig.zkPort = <span class="number">2181</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否从头开始消费</span></span><br><span class="line">spoutConfig.forceFromStart = <span class="keyword">true</span>;</span><br><span class="line">spoutConfig.socketTimeoutMs = <span class="number">60</span> * <span class="number">1000</span>;</span><br><span class="line"><span class="comment">// StringScheme将字节流转解码成某种编码的字符串</span></span><br><span class="line">spoutConfig.scheme = <span class="keyword">new</span> SchemeAsMultiScheme(<span class="keyword">new</span> StringScheme());</span><br><span class="line"></span><br><span class="line">KafkaSpout kafkaSpout = <span class="keyword">new</span> KafkaSpout(spoutConfig);</span><br><span class="line"></span><br><span class="line"><span class="comment">// set kafka spout</span></span><br><span class="line">builder.setSpout(<span class="string">"kafka_spout"</span>, kafkaSpout, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// set bolt</span></span><br><span class="line">builder.setBolt(<span class="string">"filter"</span>, <span class="keyword">new</span> FilterBolt(), <span class="number">8</span>).shuffleGrouping(<span class="string">"kafka_spout"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数据写出</span></span><br><span class="line"><span class="comment">// set kafka bolt</span></span><br><span class="line"><span class="comment">// withTopicSelector使用缺省的选择器指定写入的topic： LogError</span></span><br><span class="line"><span class="comment">// withTupleToKafkaMapper tuple==&gt;kafka的key和message</span></span><br><span class="line">KafkaBolt kafka_bolt = <span class="keyword">new</span> KafkaBolt().withTopicSelector(<span class="keyword">new</span> DefaultTopicSelector(<span class="string">"Log_error"</span>))</span><br><span class="line">.withTupleToKafkaMapper(<span class="keyword">new</span> FieldNameBasedTupleToKafkaMapper());</span><br><span class="line"></span><br><span class="line">builder.setBolt(<span class="string">"kafka_bolt"</span>, kafka_bolt, <span class="number">2</span>).shuffleGrouping(<span class="string">"filter"</span>);</span><br><span class="line"></span><br><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line"><span class="comment">// set producer properties.</span></span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"metadata.broker.list"</span>, <span class="string">"node01:9092,node02:9092,node03:9092"</span>);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Kafka生产者ACK机制 0 ： 生产者不等待Kafka broker完成确认，继续发送下一条数据</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment"> * 1:生产者等待消息在leader接收成功确认之后，继续发送下一条数据</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * -1 ：生产者等待消息在follower副本接收到数据确认之后，继续发送下一条数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">props.put(<span class="string">"request.required.acks"</span>, <span class="string">"0"</span>);</span><br><span class="line">props.put(<span class="string">"serializer.class"</span>, <span class="string">"kafka.serializer.StringEncoder"</span>);</span><br><span class="line">conf.put(<span class="string">"kafka.broker.properties"</span>, props);</span><br><span class="line"></span><br><span class="line"><span class="comment">//conf.put(Config.STORM_ZOOKEEPER_SERVERS, Arrays.asList(new String[] &#123; "node1", "node2", "node3" &#125;));</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 本地方式运行</span></span><br><span class="line">LocalCluster localCluster = <span class="keyword">new</span> LocalCluster();</span><br><span class="line">localCluster.submitTopology(<span class="string">"mytopology"</span>, conf, builder.createTopology());</span><br><span class="line">System.err.println(<span class="string">"====================haha======================="</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 分布式框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Storm，流式处理框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka学习</title>
      <link href="/2019/01/28/Kafka%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/28/Kafka%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Kafka简介"><a href="#一、Kafka简介" class="headerlink" title="一、Kafka简介"></a>一、Kafka简介</h1><p>Kafka是一个高吞吐量、低延迟分布式的消息队列系统。特点：每秒钟可以处理几十万条消息，他的低延迟最低只有几毫秒。官网：<a href="https://kafka.apache.org/" target="_blank" rel="noopener">https://kafka.apache.org/</a></p><p>底层使用Scala语言实现。</p><p>注意：</p><p>1、A streaming platform has three key capabilities:</p><ul><li>Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.</li><li>Store streams of records in a fault-tolerant durable way.</li><li>Process streams of records as they occur.</li></ul><p>2、Kafka is generally used for two broad classes of applications:</p><ul><li>Building real-time streaming data pipelines that reliably get data between systems or applications</li><li>Building real-time streaming applications that transform or react to the streams of data</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1qq5c9zntj30hf0e7gnl.jpg" alt=""></p><p>3、First a few concepts:</p><ul><li>Kafka is run as a cluster on one or more servers that can span multiple datacenters.</li><li>The Kafka cluster stores streams of <em>records</em> in categories called <em>topics</em>.</li><li>Each record consists of a key, a value, and a timestamp.</li></ul><p>4、Kafka has four core APIs:</p><ul><li>The <a href="https://kafka.apache.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> allows an application to publish a stream of records to one or more Kafka topics.</li><li>The <a href="https://kafka.apache.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a> allows an application to subscribe to one or more topics and process the stream of records produced to them.</li><li>The <a href="https://kafka.apache.org/documentation/streams" target="_blank" rel="noopener">Streams API</a> allows an application to act as a <em>stream processor</em>, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li><li>The <a href="https://kafka.apache.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a> allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.</li></ul><p>5、其他</p><ul><li>Kafka Cluster 中有多个Broker服务器，每个类型的消息被定义为<code></code>topic`</li><li>同一个topic内部的消息按照一定的key 和算法被（partition）分区存储到不同的Broker上</li><li>Producer 和consumer 可以在不同的Broker上生产或消费topic</li></ul><p>6、概念理解</p><ul><li><p>Topics and Logs：</p><ul><li>Topic 即为每条发布到 Kafka 集群的消息都有一个类别，topic在 Kafka 中可以由多个消费者订阅、消费。</li><li>每个 topic 包含一个或多个 partition（分区），partition 数量可以在创建 topic 时指定，每个分区日志中记录了该分区的数据以及索引信息。</li><li>Kafka 只保证一个分区内的消息有序，不能保证一个主题的不同分区之间的消息有序。为一个主题分配一个分区，才能保证所有消息绝对有序。</li><li>分区会给每个消息记录分配一个顺序 ID 号（偏移量）， 能够唯一地标识该分区中的每个记录。Kafka 集群保留所有发布的记录，不管这个记录有没有被消费过，Kafka 提供相应策略通过配置从而对旧数据处理。</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1qr0t4gc1j30e108edgs.jpg" alt=""></p><ul><li>每个消费者唯一保存的元数据信息就是消费者当前消费日志的位移位置。位移位置是由消费者控制，消费者可以通过修改偏移量读取任何位置的数据。</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1qr8l7bkrj30dn07xaan.jpg" alt=""></p></li><li><p>Producers – 生产者<br>指定 topic 来发送消息到 Kafka Broker</p></li><li><p>Consumers – 消费者<br>根据 topic 消费相应的消息</p></li><li><p>Topic – 消息主题（类型）<br> 一个 topic 可以有多个 partition，分布在不同的 broker server 上</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g1qrbi5tyuj30h008aabe.jpg" alt=""></p><p>7、注意：</p><ul><li>consumer自己维护消费消息的offset</li><li>每一个consumer都有对应的group</li><li>group内是queue消费模型<ul><li>每个consumer消费不同的partition</li><li>一个消息被一个group消费一次</li></ul></li><li>group间是publish—subscribe消费模型<ul><li>每个group独立消费，互补影响</li><li>一个消息被各个group消费一次</li></ul></li></ul><p>8、Kafka使用场景（允许数据丢失）</p><ul><li>日志收集：收集各log ， 开放给各个consumer ， 如hbase， hadoop ， solr</li><li><p>消息系统： 群发消息</p></li><li><p>用户活动跟踪： 记录用户行为发布到topic中，提供给consumer做实时监控分析，或装载到hadoop，数仓中做离线分析</p></li><li>运营指标 ： 记录运营监控数据</li><li>流式处理 ： SparkStreaming ， storm</li></ul><h1 id="二、Kafka集群的部署和安装"><a href="#二、Kafka集群的部署和安装" class="headerlink" title="二、Kafka集群的部署和安装"></a>二、Kafka集群的部署和安装</h1><h2 id="1、集群规划："><a href="#1、集群规划：" class="headerlink" title="1、集群规划："></a>1、集群规划：</h2><p>zookeeper ： 三台（Kafka是分布式消息队列 ， 依赖zookeeper）</p><p>kafka  :  三台  node1、node2、node3</p><h2 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h2><p>安装zookeeper （详见zookeeper学习.md）</p><p>安装Kafka </p><p>下载压缩包（官网地址：<a href="http://kafka.apache.org/downloads.html）" target="_blank" rel="noopener">http://kafka.apache.org/downloads.html）</a></p><h3 id="解压："><a href="#解压：" class="headerlink" title="解压："></a>解压：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.10-0.9.0.1.tgz</span><br></pre></td></tr></table></figure><h3 id="修改配置文件："><a href="#修改配置文件：" class="headerlink" title="修改配置文件："></a>修改配置文件：</h3><p>config/server.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## broker.id broker集群中唯一标识id，0、1、2、3 依次增长（broker即 Kafka 集群中的一台服务器）</span><br><span class="line">## 注：当前Kafka 集群共三台节点，分别为：node1、node2、node3。对应的 broker.id 分别为 0、1、2。</span><br><span class="line"> broker.id=0</span><br><span class="line"> ## zookeeper.connect: zk 集群地址列表</span><br><span class="line"> zookeeper.connect=node1:2181、node2:2181、node3:2181</span><br></pre></td></tr></table></figure><p>将当前 node1 服务器上的 Kafka 目录同步到其他 node2、node3 服务器上。</p><h2 id="3、启动kafka集群"><a href="#3、启动kafka集群" class="headerlink" title="3、启动kafka集群"></a>3、启动kafka集群</h2><p>A、启动 Zookeeper 集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p>B、启动 Kafka 集群。<br>分别在三台服务器上执行以下命令启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><h2 id="4、测试"><a href="#4、测试" class="headerlink" title="4、测试"></a>4、测试</h2><p>创建话题（kafka-topics.sh –help 查看帮助手册）</p><p>1、创建 topic：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 2 --partitions 3 --topic test</span><br><span class="line"><span class="meta">#</span>参数说明</span><br><span class="line"><span class="meta">#</span>--replication-factor ：指定每个分区的复制因子个数，默认 1 个</span><br><span class="line"><span class="meta">#</span># 副本有主从之分 ， 且副本分别放在不同的broker节点上</span><br><span class="line"><span class="meta">#</span>--partitions ：指定当前创建的 kafka 分区数量，默认为 1 个</span><br><span class="line"><span class="meta">#</span>--topic ：指定新建 topic 的名称</span><br></pre></td></tr></table></figure><p>2、查看 topic 列表：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list</span><br></pre></td></tr></table></figure><p>3、查看“test”topic 描述：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --describe --topic test</span><br></pre></td></tr></table></figure><p>–Isr （ in_synchronized_replication ）: 代表数据同步的节点</p><p>4、创建消费者：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list node1:9092,node2:9092,node3:9092 --topic test</span><br></pre></td></tr></table></figure><p>然后，在当前节点的控制台输入任何内容，表作为生产的topic</p><p>5、创建消费者：（另选一台节点）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper node1:2181,node2:2181,node3:2181 --from-beginning --topic test</span><br></pre></td></tr></table></figure><p>此时，在控制台会打印出消费的topic</p><p>消费的消息的offset存放在zookeeper中，使用<code>get + 路径</code> 命令 获取对应分区的offset</p><p>注：<br>查看帮助手册：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh help</span><br></pre></td></tr></table></figure><h1 id="三、-Flume-amp-amp-Kafka的结合"><a href="#三、-Flume-amp-amp-Kafka的结合" class="headerlink" title="三、 Flume &amp; &amp; Kafka的结合"></a>三、 Flume &amp; &amp; Kafka的结合</h1><h2 id="1、Flume-安装"><a href="#1、Flume-安装" class="headerlink" title="1、Flume  安装"></a>1、Flume  安装</h2><p>（详见Flume学习.md）</p><h2 id="2、Flume-Kafka"><a href="#2、Flume-Kafka" class="headerlink" title="2、Flume + Kafka"></a>2、Flume + Kafka</h2><p>A、启动 Kafka 集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p>B、配置 Flume 集群，并启动 Flume 集群。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf -f conf/fk.conf -Dflume.root.logger=DEBUG,console</span><br></pre></td></tr></table></figure><p>Flume 配置文件 fk.conf 内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = node1</span><br><span class="line">a1.sources.r1.port = 41414</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.topic = testflume</span><br><span class="line">a1.sinks.k1.brokerList = node1:9092,node2:9092,node3:9092</span><br><span class="line">a1.sinks.k1.requiredAcks = 1</span><br><span class="line">a1.sinks.k1.batchSize = 20</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h2 id="3-、测试"><a href="#3-、测试" class="headerlink" title="3 、测试"></a>3 、测试</h2><ul><li><p>分别启动 Zookeeper、Kafka、Flume 集群。</p></li><li><p>创建 topic：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 2 --partitions 3 --topic testflume</span><br></pre></td></tr></table></figure><ul><li>启动消费者：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper node1:2181,node2:2181,node3:2181 --from-beginning --topic testflume</span><br></pre></td></tr></table></figure><ul><li>运行“RpcClientDemo”代码，通过 rpc 请求发送数据到 Flume 集群。Flume 中 source 类型为 AVRO 类型，此时通过 Java 发送 rpc 请求，测试数据是否传入 Kafka</li><li>其中，Java 发送 Rpc 请求 Flume 代码示例如下：<br>（参考 Flume 官方文档：<a href="http://flume.apache.org/FlumeDeveloperGuide.html）" target="_blank" rel="noopener">http://flume.apache.org/FlumeDeveloperGuide.html）</a></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.api.RpcClient;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.api.RpcClientFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.EventBuilder;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.Charset;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Flume官网案例</span></span><br><span class="line"><span class="comment">* http://flume.apache.org/FlumeDeveloperGuide.html</span></span><br><span class="line"><span class="comment">* <span class="doctag">@author</span> root</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RpcClientDemo</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">MyRpcClientFacade client = <span class="keyword">new</span> MyRpcClientFacade();</span><br><span class="line"><span class="comment">// Initialize client with the remote Flume agent's host and port</span></span><br><span class="line">client.init(<span class="string">"node1"</span>, <span class="number">41414</span>);</span><br><span class="line"><span class="comment">// Send 10 events to the remote Flume agent. That agent should be</span></span><br><span class="line"><span class="comment">// configured to listen with an AvroSource.</span></span><br><span class="line">String sampleData = <span class="string">"Hello Flume!"</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">client.sendDataToFlume(sampleData);</span><br><span class="line">System.out.println(<span class="string">"发送数据："</span> + sampleData);</span><br><span class="line">&#125;</span><br><span class="line">client.cleanUp();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyRpcClientFacade</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> RpcClient client;</span><br><span class="line"><span class="keyword">private</span> String hostname;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> port;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(String hostname, <span class="keyword">int</span> port)</span> </span>&#123;</span><br><span class="line"><span class="comment">// Setup the RPC connection</span></span><br><span class="line"><span class="keyword">this</span>.hostname = hostname;</span><br><span class="line"><span class="keyword">this</span>.port = port;</span><br><span class="line"><span class="keyword">this</span>.client = RpcClientFactory.getDefaultInstance(hostname,port);</span><br><span class="line"><span class="comment">// Use the following method to create a thrift client (instead of the</span></span><br><span class="line"><span class="comment">// above line):</span></span><br><span class="line"><span class="comment">// this.client = RpcClientFactory.getThriftInstance(hostname,port);</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendDataToFlume</span><span class="params">(String data)</span> </span>&#123;</span><br><span class="line"><span class="comment">// Create a Flume Event object that encapsulates the sample data</span></span><br><span class="line">Event event = EventBuilder.withBody(data,Charset.forName(<span class="string">"UTF-8"</span>));</span><br><span class="line"><span class="comment">// Send the event</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">client.append(event);</span><br><span class="line">&#125; <span class="keyword">catch</span> (EventDeliveryException e) &#123;</span><br><span class="line"><span class="comment">// clean up and recreate the client</span></span><br><span class="line">client.close();</span><br><span class="line">client = <span class="keyword">null</span>;</span><br><span class="line">client = RpcClientFactory.getDefaultInstance(hostname,port);</span><br><span class="line"><span class="comment">// Use the following method to create a thrift client (instead of</span></span><br><span class="line"><span class="comment">// the above line):</span></span><br><span class="line"><span class="comment">// this.client =RpcClientFactory.getThriftInstance(hostname, port);</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cleanUp</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="comment">// Close the RPC connection</span></span><br><span class="line">client.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四、Kafka数据丢失问题和重复消费问题"><a href="#四、Kafka数据丢失问题和重复消费问题" class="headerlink" title="四、Kafka数据丢失问题和重复消费问题"></a>四、Kafka数据丢失问题和重复消费问题</h1><h2 id="1、为什么会丢失？"><a href="#1、为什么会丢失？" class="headerlink" title="1、为什么会丢失？"></a>1、为什么会丢失？</h2><p>Kafka ， 高吞吐 ， 一次能处理几十万条数据，</p><p>（1）生产数据时：</p><p>因为服务器（生产者）发送数据给Kafka后，kafka 将数据写入内存后，就直接返回操作成功的消息（ack机制 : 1（默认值）而ack机制 : 0 时，不用管是否操作成功，就发第二条），然后再发第二条，避免的磁盘I/O带来的延迟，可是，这样不安全，万一此时该节点宕机，数据就丢失了。</p><p>而为了解决数据丢失，可以在数据写入内存时，备份到其他节点,再返回操作成功的消息（ack机制 : -1）。</p><p>（2）消费数据时：</p><p>Client消费数据过程中，（频率很短）先更新了消费offset， 再处理数据（如100），结果宕机，那么重启后就会从下一个offset（如101）开始消费消息，那么100这条数据就丢失了。</p><p>解决方案：关闭自动提交  ， 改为 ， 手动提交，保证数据处理完毕后再提交消费offset。但是，解决了数据丢失，提高了性能消耗</p><h2 id="2、数据重复消费问题"><a href="#2、数据重复消费问题" class="headerlink" title="2、数据重复消费问题"></a>2、数据重复消费问题</h2><p>因为Client（消费者）设置定时（频率很长）向zookeeper更新消费消息的offset，（如100 ， 120） 如果在没达到定的时间（如120），client就宕机了，重启后会重新去zookeeper上查询offset， 那么在定的时间之前的消息offset（100到120之间）就不存在，Client就会重新（从100）开始消费，就造成了重复消费问题。</p><p>解决方案：关闭自动定时提交  ， 改为 ， 手动提交，保证数据处理完毕后再提交消费offset。但是，解决重复消费，提高了性能消耗。</p><h2 id="3、注意"><a href="#3、注意" class="headerlink" title="3、注意"></a>3、注意</h2><p>使用解决方案时，要注意业务的要求，是否能允许数据丢失和重复消费问题</p><h2 id="4、API"><a href="#4、API" class="headerlink" title="4、API"></a>4、API</h2><p>​    high level api<br>​    简单，不灵活<br>​     simple api<br>​    复杂，但灵活</p>]]></content>
      
      
      <categories>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式 </tag>
            
            <tag> 消息队列系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch学习</title>
      <link href="/2019/01/25/Elasticsearch%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/25/Elasticsearch%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Elasticsearch是什么"><a href="#一、Elasticsearch是什么" class="headerlink" title="一、Elasticsearch是什么"></a>一、Elasticsearch是什么</h1><p><a href="https://ideas.spkcn.com/software/os/windows/687.html" target="_blank" rel="noopener">https://ideas.spkcn.com/software/os/windows/687.html</a></p><h2 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h2><ul><li><p>一个基于Lucene的、实时的、分布式搜索和分析引擎</p></li><li><p>应用于云计算中。</p></li><li>实时搜索、稳定、可靠、快速</li><li>安装方便</li><li>基于Restful接口</li></ul><h2 id="2、和Lucene的关系"><a href="#2、和Lucene的关系" class="headerlink" title="2、和Lucene的关系"></a>2、和Lucene的关系</h2><ul><li>Lucene 是一个库。必须使用Java开发。工作原理复杂</li><li>Elasticsearch使用Java开发，以Lucene为核心实现索引和搜索功能。通过简单的Restful API隐藏Lucene的复杂性，简化了全文搜索。</li></ul><h2 id="3、和SOLR对比"><a href="#3、和SOLR对比" class="headerlink" title="3、和SOLR对比"></a>3、和SOLR对比</h2><ul><li><p>热度逐渐远高于solr</p></li><li><p>平均查询速度快于solr倍</p></li><li><p>ES的优势：</p><p>a）Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。</p><p>b）Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。</p><p>处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。</p><p>c）Elasticsearch 采用 Gateway 的概念，使得备份更加简单。</p><p>d）各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。</p></li></ul><h2 id="4、与关系型数据库对比"><a href="#4、与关系型数据库对比" class="headerlink" title="4、与关系型数据库对比"></a>4、与关系型数据库对比</h2><ul><li>结构相似</li></ul><table><thead><tr><th style="text-align:center">database（数据库）</th><th style="text-align:center">index（索引库）</th></tr></thead><tbody><tr><td style="text-align:center">table（表）</td><td style="text-align:center">type（类型）</td></tr><tr><td style="text-align:center">row（行）</td><td style="text-align:center">document（文档）</td></tr><tr><td style="text-align:center">column（列）</td><td style="text-align:center">field（字段）</td></tr></tbody></table><ul><li><p>一个ES集群可以有多个索引库。每个索引库包含很多种类型，类型中又包含了很多文档，每个文档又包含很多字段</p></li><li><p>传统数据库为特定列增加一个索引。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的</p></li><li><p>倒排索引：</p><ul><li>源于实际应用中需要根据属性的值来查找记录。</li><li>种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。</li><li>不是由记录来确定属性值，而是由属性值来确定记录的位置</li></ul></li></ul><p>​      </p><h1 id="二、安装与部署"><a href="#二、安装与部署" class="headerlink" title="二、安装与部署"></a>二、安装与部署</h1><p>环境要求：JDK版本为1.7及以上</p><p>下载位置：<a href="https://www.elastic.co/downloads/" target="_blank" rel="noopener">系列产品</a></p><p>1、在安装目录下的config 目录下：编辑elasticsearch.yml文件</p><p>编辑内容： (注意要顶格写，冒号后面要加一个空格)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a)Cluster.name:  shsxt  (同一集群要一样)</span><br><span class="line">b)Node.name：node-1     (同一集群要不一样)</span><br><span class="line">c)Network.Host: 192.168.1.194    (这里不能写127.0.0.1)</span><br><span class="line">d)防止脑裂的配置</span><br><span class="line">discovery.zen.ping.multicast.enabled: false</span><br><span class="line">discovery.zen.ping_timeout: 120s</span><br><span class="line">client.transport.ping_timeout: 60s</span><br><span class="line">discovery.zen.ping.unicast.hosts: [&quot;192.168.1.191&quot;,&quot;192.168.1.192&quot;, &quot;192.168.1.193&quot;]</span><br></pre></td></tr></table></figure><p>然后，将安装包发送到其他节点，再根据所在节点，进行相应的配置</p><p>2、启动 （开几台起几台）</p><p>ES_HOME/bin/elasticsearch     （ctrl + C 结束服务）</p><p>ES_HOME/bin/elasticsearch -d(后台运行)     （前提页面结束服务或者kill 杀死进程）</p><p><code>启动权限问题</code></p><p>1、启动后会报错，说不能在root用户下执行，所以我们需要添加新用户来执行启动</p><p>（分别在3台节点上）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">useradd es</span><br><span class="line">passwd</span><br><span class="line">（设置密码）</span><br><span class="line">chown —R es:es filename(路径/ES的解压文件)</span><br><span class="line">（修改ES安装文件的权限为用户es）</span><br><span class="line">su es</span><br><span class="line">（切换用户为es）</span><br><span class="line">再执行启动命令</span><br></pre></td></tr></table></figure><p>显示：（在启动的所有节点上都会显示出所选举的master节点，此处为node0）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2019-01-26 03:22:40,594cluster.service           detected_master &#123;node0&#125;&#123;GOmO6SISRHexhlbqcddx9w&#125;&#123;192.168.198.128&#125;&#123;192.168.198.128:9300&#125;, added &#123;&#123;node0&#125;&#123;GOmO6SISRHexhlbqcddx9w&#125;&#123;192.168.198.128&#125;&#123;192.168.198.128:9300&#125;,&#123;node2&#125;&#123;blKR0BxaQ92QjmPuMlPaRw&#125;&#123;192.168.198.131&#125;&#123;192.168.198.131:9300&#125;,&#125;, reason: zen-disco-receive(from master [&#123;node0&#125;&#123;GOmO6SISRHexhlbqcddx9w&#125;&#123;192.168.198.128&#125;&#123;192.168.198.128:9300&#125;])</span><br></pre></td></tr></table></figure><p>4.访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问 http://localhost:9200</span><br></pre></td></tr></table></figure><p><code>注意</code></p><blockquote><p>9200  : 是HTTP协议所访问的端口，即从浏览器端访问的port</p><p>9300  ：是Java API访问端口</p></blockquote><h1 id="三、REST风格"><a href="#三、REST风格" class="headerlink" title="三、REST风格"></a>三、REST风格</h1><h2 id="1、简介-1"><a href="#1、简介-1" class="headerlink" title="1、简介"></a>1、简介</h2><p><strong>表现层状态转换</strong>（<a href="https://zh.wikipedia.org/wiki/%E8%8B%B1%E8%AF%AD" target="_blank" rel="noopener">英语</a>：<strong>Representational State Transfer</strong>，<a href="https://zh.wikipedia.org/wiki/%E7%B8%AE%E5%AF%AB" target="_blank" rel="noopener">缩写</a>：<strong>REST</strong>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">一种万维网软件架构风格，</span><br><span class="line">目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。表现层状态转换是根基于超文本传输协议(HTTP)之上而确定的一组约束和属性，是一种设计提供万维网络服务的软件构建风格。</span><br><span class="line">匹配或兼容于这种架构风格(简称为 REST 或 RESTful)的网络服务，允许客户端发出以统一资源标识符访问和操作网络资源的请求，而与预先定义好的无状态操作集一致化。</span><br><span class="line">因此表现层状态转换提供了在互联网络的计算系统之间，彼此资源可交互使用的协作性质(interoperability)。</span><br><span class="line">相对于其它种类的网络服务，例如 SOAP服务则是以本身所定义的操作集，来访问网络上的资源。</span><br></pre></td></tr></table></figure><p>要点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">需要注意的是，</span><br><span class="line">REST是设计风格而不是标准。REST通常基于使用HTTP，URI，和XML以及HTML这些现有的广泛流行的协议和标准。</span><br><span class="line"></span><br><span class="line">* 资源是由URI来指定。</span><br><span class="line">* 对资源的操作包括获取、创建、修改和删除资源，这些操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法。</span><br><span class="line">* 通过操作资源的表现形式来操作资源。</span><br><span class="line">* 资源的表现形式则是XML或者HTML，取决于读者是机器还是人，是消费web服务的客户软件还是web浏览器。当然也可以是任何其他的格式，例如JSON。</span><br></pre></td></tr></table></figure><blockquote><p>URI  ： 统一资源标识符</p><p>URL  ： 全球资源定位器</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzj3dsu1n9j30ha09vn0p.jpg" alt=""></p><h2 id="2、Rest操作"><a href="#2、Rest操作" class="headerlink" title="2、Rest操作"></a>2、Rest操作</h2><p>REST的操作分为以下几种：</p><p>– GET：获取对象的当前状态；</p><p>– PUT：改变对象的状态；</p><p>– POST：创建对象；</p><p>– DELETE：删除对象；</p><p>– HEAD：获取头信息。</p><h2 id="3、ES内置的REST接口"><a href="#3、ES内置的REST接口" class="headerlink" title="3、ES内置的REST接口"></a>3、ES内置的REST接口</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzj3feth0wj30fe08taci.jpg" alt=""></p><h1 id="四、CURL命令"><a href="#四、CURL命令" class="headerlink" title="四、CURL命令"></a>四、CURL命令</h1><p>-X  指定http请求的方法</p><p>-HEAD  GET POST  PUT DELETE</p><p>-d  指定要传输的数据</p><h2 id="1、索引库的创建与删除"><a href="#1、索引库的创建与删除" class="headerlink" title="1、索引库的创建与删除"></a>1、索引库的创建与删除</h2><p>创建索引库：（PUT/POST都可以）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT http://192.168.198.128:9200/sukie/</span><br></pre></td></tr></table></figure><p>显示：（成功了）</p><blockquote><p>[root@node00 ~]# curl -XPUT <a href="http://192.168.198.128:9200/sukie/" target="_blank" rel="noopener">http://192.168.198.128:9200/sukie/</a><br>{“acknowledged”:true}[root@node00 ~]# </p></blockquote><p>删除索引库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XDELETE http://192.168.198.128:9200/sukie/</span><br></pre></td></tr></table></figure><h2 id="2、创建document"><a href="#2、创建document" class="headerlink" title="2、创建document"></a>2、创建document</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(注意格式：JSON  （英文状态下）)</span><br><span class="line">                                               </span><br><span class="line">curl -XPUT http://192.168.198.128:9200/sukie/employee/2?pretty -d &apos;&#123; </span><br><span class="line">&quot;first_name&quot; : &quot;john&quot;, </span><br><span class="line">&quot;last_name&quot; : &quot;smith&quot;, </span><br><span class="line">&quot;age&quot; : 25, </span><br><span class="line">&quot;love&quot; : &quot;I love to go rock climbing&quot;, </span><br><span class="line">&quot;address&quot;: &quot;shanghai&quot;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>==employee==  ： 在此处为type（类型）</p><p>==1==  ： 在此处为id</p><p>==pretty==： 表示以良好格式显示结果</p><p>显示：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt; [root@node00 ~]# curl -XPUT http://192.168.198.128:9200/sukie/employee/2?pretty -d &apos;&#123; </span><br><span class="line">&gt; </span><br><span class="line">&gt; &quot;first_name&quot; : &quot;john&quot;, </span><br><span class="line">&gt; &quot;last_name&quot; : &quot;smith&quot;, </span><br><span class="line">&gt; &quot;age&quot; : 25, </span><br><span class="line">&gt; &quot;love&quot; : &quot;I love to go rock climbing&quot;, </span><br><span class="line">&gt; &quot;address&quot;: &quot;shanghai&quot;</span><br><span class="line">&gt; &#125;&apos;</span><br><span class="line">&gt; </span><br><span class="line">&gt; &#123;</span><br><span class="line">&gt;   &quot;_index&quot; : &quot;sukie&quot;,</span><br><span class="line">&gt;   &quot;_type&quot; : &quot;employee&quot;,</span><br><span class="line">&gt;    &quot;_id&quot; : &quot;2&quot;,</span><br><span class="line">&gt;    &quot;_version&quot; : 1,</span><br><span class="line">&gt;   &quot;_shards&quot; : &#123;</span><br><span class="line">&gt;     &quot;total&quot; : 2,</span><br><span class="line">&gt;     &quot;successful&quot; : 2,</span><br><span class="line">&gt;    &quot;failed&quot; : 0</span><br><span class="line">&gt;   &#125;,</span><br><span class="line">&gt;    &quot;created&quot; : true</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; [root@node00 ~]# </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST http://192.168.198.128:9200/sukie/employee -d &apos;&#123; </span><br><span class="line">&quot;first_name&quot; : &quot;john&quot;, </span><br><span class="line">&quot;last_name&quot; : &quot;smith&quot;, </span><br><span class="line">&quot;age&quot; : 25, </span><br><span class="line">&quot;about&quot; : &quot;I love to go rock climbing&quot;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>==未指定id==</p><p>如：（必须使用POST）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">命令：</span><br><span class="line">curl -XPOST http://localhost:9200/sukie/employee -d &apos;&#123;&quot;first_name&quot; : &quot;John&quot;&#125;&apos;</span><br></pre></td></tr></table></figure><p>若：（使用PUT会报错）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">命令：</span><br><span class="line">curl -XPUT http://localhost:9200/sukie/employee -d &apos;&#123;&quot;first_name&quot; : &quot;John&quot;&#125;&apos; </span><br><span class="line">会报错</span><br></pre></td></tr></table></figure><p><code>创建索引注意事项</code></p><blockquote><ul><li>索引库名称必须要全部<strong><code>小写</code></strong>，<strong><code>不</code></strong>能以下划线开头，也<strong><code>不</code></strong>能包含逗号</li><li>如果没有明确指定索引数据的ID，那么es会自动生成一个随机的ID，这时需要使用POST方式，PUT方式会出错</li></ul></blockquote><h2 id="3、更新document"><a href="#3、更新document" class="headerlink" title="3、更新document"></a>3、更新document</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT http://192.168.198.128:9200/sukie/employee/2 -d &apos;</span><br><span class="line">&#123;</span><br><span class="line"> &quot;first_name&quot; : &quot;god bin&quot;,</span><br><span class="line"> &quot;last_name&quot; : &quot;pang&quot;,</span><br><span class="line"> &quot;age&quot; : 38,</span><br><span class="line"> &quot;about&quot; : &quot;I love to go rock climbing&quot;,</span><br><span class="line"> &quot;address&quot;: &quot;shanghai&quot;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">curl -XPOST http://192.168.198.128:9200/sukie/employee?pretty -d &apos;&#123; </span><br><span class="line">&quot;first_name&quot; : &quot;john&quot;, </span><br><span class="line">&quot;last_name&quot; : &quot;smith&quot;, </span><br><span class="line">&quot;age&quot; : 25, </span><br><span class="line">&quot;about&quot; : &quot;I love to go rock climbing&quot;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 ~]# curl -XPOST http://192.168.198.128:9200/sukie/employee?pretty -d &apos;&#123; </span><br><span class="line">&gt; &quot;first_name&quot; : &quot;john&quot;, </span><br><span class="line">&gt; &quot;last_name&quot; : &quot;smith&quot;, </span><br><span class="line">&gt; &quot;age&quot; : 25, </span><br><span class="line">&gt; &quot;about&quot; : &quot;I love to go rock climbing&quot;</span><br><span class="line">&gt; &#125;&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;sukie&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;employee&quot;,</span><br><span class="line">   &quot;_id&quot; : &quot;AWiFmf347KNgqTe_uJ4-&quot;,    ==自动生成的id==</span><br><span class="line">   &quot;_version&quot; : 1,</span><br><span class="line">  &quot;_shards&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : 2,</span><br><span class="line">     &quot;successful&quot; : 2,</span><br><span class="line">    &quot;failed&quot; : 0</span><br><span class="line">  &#125;,</span><br><span class="line">   &quot;created&quot; : true</span><br><span class="line">&#125;</span><br><span class="line">[root@node00 ~]#</span><br></pre></td></tr></table></figure><p>put  ： 必须指定id   ，若id存在，这更新数据（全局更新）；若id不存在，则新增数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT http://localhost:9200/sukie/employee/1 -d &apos;&#123;&quot;city&quot;:&quot;beijing&quot;,&quot;car&quot;:&quot;BMW&quot;&#125;&apos;</span><br></pre></td></tr></table></figure><p><code>注意;</code>执行更新操作时：</p><p>– ES首先将旧的文档标记为删除状态 </p><p>– 然后添加新的文档 </p><p>– 旧的文档不会立即消失，但是你也无法访问 </p><p>– ES会在你继续添加更多数据的时候在后台清理已经标记为删除状态的文档</p><hr><p>post  ： id若不指定，会自动生成随机的id</p><p>​                id若指定，就实现局部更新操作（添加新字段或更新已有字段）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> curl -XPOST http://localhost:9200/sukie/employee/1/_update -d &apos;</span><br><span class="line">&#123;</span><br><span class="line">&quot;doc&quot;:&#123;</span><br><span class="line">&quot;city&quot;:&quot;beijing&quot;,</span><br><span class="line">  “sex”:”male”</span><br><span class="line">&#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>（同一个索引库，会默认创建5个分片，用以实现分布式搜索；</p><p>   每个分片均另外还有一个副本分布在不同的另一个节点上，用以提高可靠性和查询速率）</p><p>==注意==</p><hr><p>同一个索引库中不同的文档之间若用相同的字段，则这个字段的数据类型必须是一致的；</p><p>字段的数据类型是由第一次推送数据是确定</p><hr><h2 id="4、普通查询索引"><a href="#4、普通查询索引" class="headerlink" title="4、普通查询索引"></a>4、普通查询索引</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">– 根据员工id查询 </span><br><span class="line">curl -XGET http://localhost:9200/sukie/employee/1?pretty</span><br><span class="line">– 在任意的查询字符串中添加pretty参数，es可以得到易于识别的json结果。 </span><br><span class="line">– curl后添加-i 参数，这样你就能得到反馈头文件</span><br><span class="line">curl -i XGET http://localhost:9200/sukie/employee/1?pretty</span><br><span class="line">– 检索文档中的一部分，如果只需要显示指定字段</span><br><span class="line">curl -XGET http://localhost:9200/sukie/employee/1?_source=name,age </span><br><span class="line">如果只需要source的数据 </span><br><span class="line">curl -XGET http://localhost:9200/sukie/employee/1/_source?pretty</span><br><span class="line">– 查询所有</span><br><span class="line">curl -XGET http://localhost:9200/sukie/employee/_search?pretty </span><br><span class="line">– 根据条件进行查询 </span><br><span class="line">curl -XGET http://localhost:9200/sukie/employee/_search?q=last_name:smith</span><br></pre></td></tr></table></figure><h2 id="5-DSL查询"><a href="#5-DSL查询" class="headerlink" title="5.DSL查询"></a>5.DSL查询</h2><p>DSL查询 •Domain Specific Language </p><p>– 领域特定语言 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://localhost:9200/shsxt/employee/_search?pretty -d &apos;&#123;</span><br><span class="line">&quot;query&quot;:</span><br><span class="line">&#123;&quot;match&quot;:</span><br><span class="line">&#123;&quot;last_name&quot;:&quot;smith&quot;&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>#对多个field发起查询：multi_match</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://localhost:9200/shsxt/employee/_search?pretty -d &apos;</span><br><span class="line">&#123;</span><br><span class="line"> &quot;query&quot;:</span><br><span class="line">  &#123;&quot;multi_match&quot;:</span><br><span class="line">   &#123;</span><br><span class="line">    &quot;query&quot;:&quot;bin&quot;,</span><br><span class="line">    &quot;fields&quot;:[&quot;last_name&quot;,&quot;first_name&quot;]</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>复合查询，must，must_not, should </p><p>must： AND   </p><p>must_not：NOT</p><p>should：OR</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://192.168.78.101:9200/shsxt/employee/_search?pretty -d &apos;</span><br><span class="line">&#123;</span><br><span class="line"> &quot;query&quot;:</span><br><span class="line">  &#123;&quot;bool&quot; :</span><br><span class="line">   &#123;</span><br><span class="line">    &quot;must&quot; : </span><br><span class="line">     &#123;&quot;match&quot;:</span><br><span class="line">      &#123;&quot;first_name&quot;:&quot;bin&quot;&#125;</span><br><span class="line">     &#125;,</span><br><span class="line">    &quot;must&quot; : </span><br><span class="line">     &#123;&quot;match&quot;:</span><br><span class="line">      &#123;&quot;age&quot;:37&#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>查询first_name=bin的，并且年龄不在20岁到30岁之间的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://192.168.78.101:9200/shsxt/employee/_search -d &apos;</span><br><span class="line">&#123;</span><br><span class="line"> &quot;query&quot;:</span><br><span class="line">  &#123;&quot;bool&quot; :</span><br><span class="line">   &#123;</span><br><span class="line">   &quot;must&quot; :</span><br><span class="line">    &#123;&quot;term&quot; : </span><br><span class="line">     &#123; &quot;first_name&quot; : &quot;bin&quot; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">   ,</span><br><span class="line">   &quot;must&quot; : </span><br><span class="line">    &#123;&quot;range&quot;:</span><br><span class="line">     &#123;&quot;age&quot; : &#123; &quot;from&quot; : 30, &quot;to&quot; : 40 &#125;</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><h2 id="6-删除索引"><a href="#6-删除索引" class="headerlink" title="6.删除索引"></a>6.删除索引</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XDELETE http://localhost:9200/shsxt/employee/1?pretty</span><br></pre></td></tr></table></figure><p>• 如果文档存在，es会返回200 ok的状态码，found属性值为 true，_version属性的值+1 </p><p>• found属性值为false，但是_version属性的值依然会+1，这个就是内部管理的一部分，它保证了我们在多个节点间的不同操作的顺序都被正确标记了 </p><p>• 注意：删除一个文档也不会立即生效，它只是被标记成已删除。 Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理</p><h1 id="五、Elasticsearch插件安装"><a href="#五、Elasticsearch插件安装" class="headerlink" title="五、Elasticsearch插件安装"></a>五、Elasticsearch插件安装</h1><h2 id="1、head"><a href="#1、head" class="headerlink" title="1、head"></a>1、head</h2><p>（至少一台）</p><p>方式一：</p><p>在bin目录下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./plugin install mobz/elasticsearch-head</span><br></pre></td></tr></table></figure><p>来安装head插件</p><p>方式二：</p><p>使用elasticsearch-head-master.zip文件安装</p><p>在bin目录下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./plugin install file:/usr/soft/elasticsearch-head-master.zip</span><br></pre></td></tr></table></figure><p>来安装head插件</p><p>方式三：</p><p>将elasticsearch-head-master.zip挤压解压安装后的包拷贝到elasticsearch安装目录的plugins目录下</p><p>安装后启动elasticsearch（至少2台）</p><p>访问<a href="http://ip:9200/_plugin/head" target="_blank" rel="noopener">http://ip:9200/_plugin/head</a></p><h2 id="2-Kibana"><a href="#2-Kibana" class="headerlink" title="2.Kibana"></a>2.Kibana</h2><p>（1台）</p><blockquote><p>它是一个基于浏览器页面的ES前端展示工具，是为ES提供日志分析的web接口，可用它对日志进行高效的搜索、可视化、分析等操作。</p></blockquote><p>解压安装,然后修改配置文件config/kibana.yml的elasticsearch.url属性即可</p><h2 id="3、Marvel"><a href="#3、Marvel" class="headerlink" title="3、Marvel"></a>3、Marvel</h2><blockquote><p>Marvel插件可以帮助使用者监控elasticsearch的运行状态，不过这个插件需要license。安装完license后可以安装marvel的agent，agent会收集elasticsearch的运行状态</p></blockquote><p><strong>Step 1:</strong> <strong>Install Marvel into Elasticsearch:(3</strong>台<strong>es</strong>都装)</p><p>Es_home/bin/plugin install license<br> Es_home/bin/plugin install marvel-agent</p><p>（注意：Es_home/plugins 目录的权限问题，是否为当前用户的）</p><p><strong>Step 2:</strong> <strong>Install Marvel into Kibana(</strong>在kibana机器上安)</p><p>Kibana_home/bin/kibana plugin –install elasticsearch/marvel/latest</p><p><strong>Step 3: Start Elasticsearch and Kibana</strong></p><p>bin/elasticsearch</p><p>bin/kibana</p><p><strong>Step 4: 浏览器访问</strong>  <a href="http://node00:5601/app/marvel" target="_blank" rel="noopener">http://node00:5601/app/marvel</a></p><p>==注意：多台节点的时间同步==</p><h2 id="4、分词器安装"><a href="#4、分词器安装" class="headerlink" title="4、分词器安装"></a>4、分词器安装</h2><p>从地址<a href="https://github.com/medcl/elasticsearch-analysis-ik" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-analysis-ik</a></p><p>下载elasticsearch中文分词器</p><p>（1）在安装好的elasticsearch中在plugins目录下新建ik目录</p><p>（2）将此zip包拷贝到ik目录下</p><p>（3）将权限修改为elasticsearch启动用户的权限</p><p>（4）过unzip命令解压缩：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip  elasticsearch-analysis-ik-1.8.0.zip</span><br></pre></td></tr></table></figure><p>（5）每台机器都这样操作，重新启动elasticsearch集群</p><p><code>例子：</code></p><p>a. 创建索引库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT http://localhost:9200/ik</span><br></pre></td></tr></table></figure><p>b. 设置mapping</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST http://localhost:9200/ik/ikType/_mapping -d&apos;&#123;</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;content&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                &quot;index&quot;:&quot;analyzed&quot;,</span><br><span class="line">                &quot;analyzer&quot;: &quot;ik_max_word&quot;,</span><br><span class="line">                &quot;search_analyzer&quot;: &quot;ik_max_word&quot;</span><br><span class="line">            &#125;</span><br><span class="line">       &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p>c.  插入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">curl -XPOST http://localhost:9200/ik/ikType/1 -d&apos;</span><br><span class="line">&#123;&quot;content&quot;:&quot;美国留给伊拉克的是个烂摊子吗&quot;&#125;&apos;</span><br><span class="line"></span><br><span class="line">curl -XPOST http://localhost:9200/ik/ikType/2 -d&apos;</span><br><span class="line">&#123;&quot;content&quot;:&quot;公安部：各地校车将享最高路权&quot;&#125;&apos;</span><br><span class="line"></span><br><span class="line">curl -XPOST http://localhost:9200/ik/ikType/3 -d&apos;</span><br><span class="line">&#123;&quot;content&quot;:&quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot;&#125;&apos;</span><br><span class="line"></span><br><span class="line">curl -XPOST http://localhost:9200/ik/ikType/4 -d&apos;</span><br><span class="line">&#123;&quot;content&quot;:&quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot;&#125;&apos;</span><br></pre></td></tr></table></figure><p>d. 查询</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET http://localhost:9200/ik/ikType/_search?pretty  -d&apos;&#123;</span><br><span class="line">    &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;content&quot; : &quot;中国&quot; &#125;&#125;</span><br><span class="line">   &#125;&apos;</span><br></pre></td></tr></table></figure><p>本地分片</p><p>只在主分片</p><p>优先主分片</p><p>只在某个节点分片</p><p>指定几个节点分片</p><p>优先指定分片</p><p>脑裂：</p><p>集群中出现两个master；1、负载过高时，master所在的节点负责管理和检索，忙不过来了，slaves节点就选出了另一个master；（功能解耦，用两台节点分别负责一个模块；一个放master，一个放data）；</p><p>2、网络波动，节点间的通信出现问题，超时连接，另一台master就又被选出来了;(解决：异地服务器）；</p><p>优化：</p><p>系统最大文件打开数量（默认1024个）</p><p>ES JVM内存大小（256m，1g，最好设为相同值）</p><p>【256m用满了，启动垃圾回收机制，然后扩容；弹性伸缩，因为GC会影响性能。】</p><p>设置mlockall来锁定物理进程true</p><p>【Linux：swap交换区（磁盘空间：存放不用的内存）】</p><p>分片数要合理</p><p>单个分片存储：20g~30g</p><p>个数=数据总量/20g</p><p>再建一个索引库，因为支持多个索引库检索</p><p>副本：数据迁移时，先设置为0，网络和磁盘IO可以降低。</p><p>segment分片存储时的片段设为1 </p><p>删除文档时，添加del标记，到客户端时再过滤。</p><p>hash取余再放到对应的分片</p>]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> 分布式搜索和分析引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume学习</title>
      <link href="/2019/01/18/Flume%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/18/Flume%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="一、理论理解"><a href="#一、理论理解" class="headerlink" title="一、理论理解"></a>一、理论理解</h2><p>1、官网:：<a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p><p>2、概念：</p><p>​    Flume是一个分布式、可扩展、可靠、高可用的海量日志有效聚合及移动的框架。</p><p>​    它通常用于log数据，支持在系统中定制各类数据发送方，用于收集数据。它具有可靠性和容错可调机制和许多故障转移和恢复机制</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1jra7eyl9j30gi0e5jt8.jpg" alt=""></p><p>3、Flume1.0X         —-FlumeNG</p><p>flume1.0x版本中flume只有agent,由3个部分组成</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1jrc5v43yj30me071q39.jpg" alt="FlumeNG"><span class="img-alt">FlumeNG</span></p><p>4、架构解释</p><p>Agent ：将数据源的数据发送给collector ，Agent由source、channel、sink三大组件组成。</p><ul><li>Source：</li></ul><blockquote><p>  从Client收集数据，传递给Channel。可以接收外部源发送过来的数据。</p><p>  不同的 source，可以接受不同的数据格式。</p><p>  比如有目录池(spooling directory)数据源，可以监控指定文件夹中的新文件变化，如果目录中有文件产生，就会立刻读取其内容。</p></blockquote><ul><li>Channel</li></ul><blockquote><p> 是一个存储地，接收source的输出，直到有sink消费掉channel中的数据，Channel中的数据直到进入到下一个channel中或者进入终端才会被删除；</p><p>当sink写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。</p></blockquote><ul><li>Sink</li></ul><blockquote><p>用于数据输出</p></blockquote><p>4、Flume使用原理图</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1jrhaxmslj30mx0dbju2.jpg" alt=""></p><p>Flume使用Agent内部原理图</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1jri44xz1j30l80d00uq.jpg" alt=""></p><h2 id="二、特点"><a href="#二、特点" class="headerlink" title="二、特点"></a>二、特点</h2><h3 id="A、数据可靠性（内部实现）"><a href="#A、数据可靠性（内部实现）" class="headerlink" title="A、数据可靠性（内部实现）"></a>A、数据可靠性（内部实现）</h3><p>​     当节点出现故障时，日志能够被传送到其他节点上而不会丢失。</p><p> Flume提供了三种级别的可靠性保障,从强到弱依次分别为：</p><p>​    1、end-to-end：收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。</p><p>​    2、Store on failure：当数据接收方crash时</p><p>​    3、Best effort：数据发送到接收方后，不会进行确认。(udp)</p><h3 id="B、自身可扩展性"><a href="#B、自身可扩展性" class="headerlink" title="B、自身可扩展性"></a>B、自身可扩展性</h3><ul><li><p>Flume采用了三层架构，分别为agent，collector和storage，每一层均可以水平扩展。所有agent和 collector由master统一管理，使得系统容易监控和维护。master允许有多个（使用ZooKeeper进行管理和负载均衡），避免单点故障问题。</p></li><li><p>【1.0自身agent实现扩展】</p></li></ul><h3 id="C、功能可扩展性"><a href="#C、功能可扩展性" class="headerlink" title="C、功能可扩展性"></a>C、功能可扩展性</h3><p>  用户可以根据需要添加自己的agent。</p><p>   Flume自带了很多组件，包括各种agent（file，syslog，HDFS等）</p><h2 id="三、Flume安装"><a href="#三、Flume安装" class="headerlink" title="三、Flume安装"></a>三、Flume安装</h2><p>​       1)将下载的flume包，解压</p><p>　　2)修改 flume-env.sh 配置文件,主要是JAVA_HOME设置[可选局部环境变量设置]</p><p>​        3)验证是否安装成功 flume-ng version</p><p>telnet 相关安装：</p><p>​     yum list telnet*   查看telnet相关的安装包</p><p>​     直接yum –y install telnet 就OK</p><p>​     yum -y install telnet-server 安装telnet服务</p><p>​     yum -y install telnet-client  安装telnet客户端(大部分系统默认安装)</p><h2 id="四、分类"><a href="#四、分类" class="headerlink" title="四、分类"></a>四、分类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Flume 关于Event的笔记</span><br><span class="line">　　在Flume中使用Event对象来作为传递数据的格式。</span><br><span class="line">　　Sources端在flume-ng-core子项目中的org.apache.flume.serialization包下，有一个名为LineDeserializer的类，这个类负责把数据按行来读取，每一行封装成一个Event（实现方式：按字节读取，当遇到&quot;\n&quot;时封装成Event返回，下一次获取Event时继续获取下一字节并判断）。然后按用户设置的批量传输的值来封装List&lt;Event&gt;</span><br><span class="line"></span><br><span class="line">备注：</span><br><span class="line"></span><br><span class="line">capacity：默认该通道中最大的可以存储的event数量是1000</span><br><span class="line"></span><br><span class="line">trasactionCapacity：每次最大可以source中拿到或者送到sink中的event数量也是100</span><br></pre></td></tr></table></figure><p><code>exec</code>：</p><blockquote><p> Unix等操作系统执行命令行，如tail ，cat 。可监听文件</p></blockquote><p><code>netcat</code></p><blockquote><p>监听一个指定端口，并将接收到的数据的每一行转换为一个event事件</p></blockquote><p><code>avro</code></p><blockquote><p>序列化的一种，实现RPC（一种远程过程调用协议）。</p><p>监听AVRO端口来接收外部AVRO客户端事件流</p></blockquote><h3 id="1、-netcat（监听端口，在本地控制台打印）"><a href="#1、-netcat（监听端口，在本地控制台打印）" class="headerlink" title="1、 netcat（监听端口，在本地控制台打印）"></a>1、 netcat（监听端口，在本地控制台打印）</h3><h4 id="（1）-vim-netcat-logger"><a href="#（1）-vim-netcat-logger" class="headerlink" title="（1） vim netcat_logger"></a>（1） vim netcat_logger</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h4 id="（2）命令操作"><a href="#（2）命令操作" class="headerlink" title="（2）命令操作"></a>（2）命令操作</h4><ul><li>（在会话1端）</li></ul><blockquote><p>在node00节点的控制台输入<code>启动</code>命令：</p><p>(<code>方式一</code>：指定配置文件的路径+文件名)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> flume-ng agent --conf-file /root/flume/netcat_logger --name a1 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>（<code>方式二</code>：配置文件所在当前目录）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> flume-ng agent --conf ./ --conf-file netcat_logger --name a1 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p><code>特别注意</code>：</p><p>#####<code>官网方式</code>#########</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> flume-ng agent --conf conf --conf-file netcat_logger --name a1 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>解释：此命令适用于将配置文件放在flume解压安装目录的conf中（不常用）</p></blockquote><p><code>控制台显示</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">​`````</span><br></pre></td></tr></table></figure><p>19/01/18 12:27:31 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started<br>19/01/18 12:27:31 INFO node.Application: Starting Sink k1<br>19/01/18 12:27:31 INFO node.Application: Starting Source r1<br>19/01/18 12:27:31 INFO source.NetcatSource: Source starting<br>19/01/18 12:27:31 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* (在会话2端)</span><br><span class="line"></span><br><span class="line">&gt; 在node00节点的控制台输入命令：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1、在节点上安装telnet</span><br><span class="line">&gt;</span><br><span class="line">&gt; ```shell</span><br><span class="line">&gt; yum install -y telnet</span><br><span class="line">&gt; yum -y install telnet-server</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p><blockquote><p>2、启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> telnet localhost 44444  </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p><code>注意：</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; a1.sources.r1.bind = localhost</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>前提是/etc/hosts中已经配置</p><p>如果此处配置localhost 那么启动时，localhost  或127.0.0.1都可以，node00就不行</p><p>如果此处配置node00那么启动时，node00或ip都可以，localhost就不行</p><p>3、在会话2控制台输入任何内容;</p><p>都会在会话1端显示，且会话1端（ctrl+c）退出服务，会话2端也自动结束</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum list telnet*   查看telnet相关的安装包</span><br><span class="line"> 直接yum –y install telnet 就OK</span><br><span class="line">   yum -y install telnet-server 安装telnet服务</span><br><span class="line">   yum -y install telnet-client  安装telnet客户端(大部分系统默认安装)</span><br></pre></td></tr></table></figure><h3 id="2、avro（监听远程发送文件，在本地控制台打印）"><a href="#2、avro（监听远程发送文件，在本地控制台打印）" class="headerlink" title="2、avro（监听远程发送文件，在本地控制台打印）"></a>2、avro（监听远程发送文件，在本地控制台打印）</h3><h4 id="（1）vim-avro-logger"><a href="#（1）vim-avro-logger" class="headerlink" title="（1）vim avro_logger"></a>（1）vim avro_logger</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#test avro sources</span><br><span class="line">##使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示</span><br><span class="line">##当前flume节点执行：</span><br><span class="line">#flume-ng agent --conf ./ --conf-file avro_loggers --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line">##其他flume节点执行：</span><br><span class="line">#flume-ng avro-client --conf ./ -H 192.168.198.128 -p 55555 -F ./logs</span><br><span class="line"></span><br><span class="line">a1.sources=r1</span><br><span class="line">a1.channels=c1</span><br><span class="line">a1.sinks=k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = avro  </span><br><span class="line">a1.sources.r1.bind=192.168.198.128</span><br><span class="line">a1.sources.r1.port=55555</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type=logger</span><br><span class="line"></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity=1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.sources.r1.channels=c1</span><br><span class="line">a1.sinks.k1.channel=c1</span><br></pre></td></tr></table></figure><p>实现功能：</p><blockquote><p>使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示</p></blockquote><h4 id="（2）命令操作-1"><a href="#（2）命令操作-1" class="headerlink" title="（2）命令操作"></a>（2）命令操作</h4><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a><code>启动</code></h5><p>（在会话1端）</p><p>在node00上</p><ul><li>当前flume节点执行（配置文件在当前目录）：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf ./ --conf-file avro_logger --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><code>显示：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">19/01/18 13:53:16 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started</span><br><span class="line">19/01/18 13:53:16 INFO node.Application: Starting Sink k1</span><br><span class="line">19/01/18 13:53:16 INFO node.Application: Starting Source r1</span><br><span class="line">19/01/18 13:53:16 INFO source.AvroSource: Starting Avro source r1: &#123; bindAddress: 192.168.198.128, port: 55555 &#125;...</span><br><span class="line">19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.</span><br><span class="line">19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started</span><br><span class="line">19/01/18 13:53:17 INFO source.AvroSource: Avro source r1 started.</span><br></pre></td></tr></table></figure><h5 id="发送"><a href="#发送" class="headerlink" title="发送"></a>发送</h5><p>(在会话2端)</p><p>在node00上发送文件到node00</p><p><code>启动</code></p><p>可在本地和其他flume节点执行（配置文件在当前目录）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng avro-client --conf ./ -H 192.168.198.128 -p 55555 -F ./flume.log</span><br></pre></td></tr></table></figure><p>(在会话1端)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">19/01/18 14:12:57 INFO sink.LoggerSink: </span><br><span class="line">Event: </span><br><span class="line">&#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61          hello bigdata &#125;</span><br></pre></td></tr></table></figure><p>时刻监听传输文件的内容</p><p><code>注意</code></p><blockquote><p>该过程也可应用于不同节点之间</p></blockquote><h3 id="3、exec（监听某一命令，在本地控制台打印）"><a href="#3、exec（监听某一命令，在本地控制台打印）" class="headerlink" title="3、exec（监听某一命令，在本地控制台打印）"></a>3、exec（监听某一命令，在本地控制台打印）</h3><h4 id="（1）vim-exec-logger"><a href="#（1）vim-exec-logger" class="headerlink" title="（1）vim exec_logger"></a>（1）vim exec_logger</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#单节点flume配置</span><br><span class="line"># example.conf: A single-node Flume configuration   </span><br><span class="line"></span><br><span class="line">#给agent三大结构命名</span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">#描述source的配置：类型、命令（监听/root/flume.log文件）</span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /root/flume.log</span><br><span class="line"></span><br><span class="line">#描述sink的配置：类型</span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">#在内存中使用一个channel缓存事件</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#将source和sink绑定到channel上</span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>（xshell会话1：）</p><blockquote><p>在node00上：<code>启动</code></p><p>在exec_logger文件所在的目录下</p><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> flume-ng agent  --conf-file exec_logger --name a1 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>r1 启动</p></blockquote><blockquote><p>（复制会话：会话2）</p><p>在node00上：</p><p>在root目录下</p><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="built_in">echo</span> hello bigdata &gt;&gt;flume.log</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><blockquote><p>之后在会话1上</p><p><code>logger本地控制台打印：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; 19/01/18 12:03:23 INFO sink.LoggerSink: </span><br><span class="line">&gt; Event: </span><br><span class="line">&gt; &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61  hello bigdata &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h3 id="4、netcat–hdfs-监听数据，传到hdfs上"><a href="#4、netcat–hdfs-监听数据，传到hdfs上" class="headerlink" title="4、netcat–hdfs(监听数据，传到hdfs上)"></a>4、netcat–hdfs(监听数据，传到hdfs上)</h3><h4 id="（1）vim-netcat-hdfs"><a href="#（1）vim-netcat-hdfs" class="headerlink" title="（1）vim netcat_hdfs"></a>（1）vim netcat_hdfs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># a1 which ones we want to activate.</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = node00</span><br><span class="line">a1.sources.r1.port = 41414</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://Sunrise/myflume/%y-%m-%d</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp=true</span><br><span class="line"></span><br><span class="line"># Define a memory channel called c1 on a1</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">#默认值，可省</span><br><span class="line">#a1.channels.c1.capacity = 1000</span><br><span class="line">#a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Define an Avro source called r1 on a1 and tell it</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h4 id="2-操作"><a href="#2-操作" class="headerlink" title="(2)操作"></a>(2)操作</h4><p>在node00的会话1上</p><p>启动</p><blockquote><p>在node00上：</p><p>在netcat_hdfs文件所在的目录下</p><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> flume-ng agent  --conf-file netcat_hdfs --name a1 -Dflume.root.logger=INFO,console</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started</span><br><span class="line">19/01/18 14:34:44 INFO node.Application: Starting Sink k1</span><br><span class="line">19/01/18 14:34:44 INFO node.Application: Starting Source r1</span><br><span class="line">19/01/18 14:34:44 INFO source.NetcatSource: Source starting</span><br><span class="line">19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.</span><br><span class="line">19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started</span><br><span class="line">19/01/18 14:34:44 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/192.168.198.128:41414]</span><br></pre></td></tr></table></figure><p>在node00的会话2上</p><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet node00 41414</span><br></pre></td></tr></table></figure><p>显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Trying 192.168.198.128...</span><br><span class="line">Connected to node00.</span><br><span class="line">Escape character is &apos;^]&apos;.</span><br></pre></td></tr></table></figure><p>输入任意内容</p><p>在node00会话1端会显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">19/01/18 14:36:50 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false</span><br><span class="line">19/01/18 14:36:51 INFO hdfs.BucketWriter: Creating hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp</span><br><span class="line">19/01/18 14:37:29 INFO hdfs.BucketWriter: Closing hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp</span><br><span class="line">19/01/18 14:37:29 INFO hdfs.BucketWriter: Renaming hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp to hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259</span><br><span class="line">19/01/18 14:37:29 INFO hdfs.HDFSEventSink: Writer callback called.</span><br></pre></td></tr></table></figure><p>在HDF分布式系统上会显示，生成的文件</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzb4acaz5fj30u10blmzc.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzb4bsv8d2j30wd08k74i.jpg" alt=""></p><p><code>注意：</code></p><p>这种情况会在hdfs上生成很多小文件，</p><p><a href="http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html" target="_blank" rel="noopener">在官网</a></p><p>HDFS Sink：<a href="http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html#hdfs-sink" target="_blank" rel="noopener"><em>文档</em></a></p><p>有很多关于文件生成过程中的配置</p><table><thead><tr><th>Name</th><th>Default</th><th>Description</th></tr></thead><tbody><tr><td><strong>channel</strong></td><td>–</td><td></td></tr><tr><td><strong>type</strong></td><td>–</td><td>The component type name, needs to be <code>hdfs</code></td></tr><tr><td><strong>hdfs.path</strong></td><td>–</td><td>HDFS directory path (eg hdfs://namenode/flume/webdata/)</td></tr><tr><td>hdfs.filePrefix</td><td>FlumeData</td><td>Name prefixed to files created by Flume in hdfs directory</td></tr><tr><td>hdfs.fileSuffix</td><td>–</td><td>Suffix to append to file (eg <code>.avro</code> - <em>NOTE: period is not automatically added</em>)</td></tr><tr><td>hdfs.inUsePrefix</td><td>–</td><td>Prefix that is used for temporal files that flume actively writes into</td></tr><tr><td>hdfs.inUseSuffix</td><td><code>.tmp</code></td><td>Suffix that is used for temporal files that flume actively writes into</td></tr><tr><td>hdfs.rollInterval</td><td>30</td><td>Number of seconds to wait before rolling current file (0 = never roll based on time interval)</td></tr><tr><td>hdfs.rollSize</td><td>1024</td><td>File size to trigger roll, in bytes (0: never roll based on file size)</td></tr><tr><td>hdfs.rollCount</td><td>10</td><td>Number of events written to file before it rolled (0 = never roll based on number of events)</td></tr><tr><td>hdfs.idleTimeout</td><td>0</td><td>Timeout after which inactive files get closed (0 = disable automatic closing of idle files)</td></tr><tr><td>hdfs.batchSize</td><td>100</td><td>number of events written to file before it is flushed to HDFS</td></tr><tr><td>hdfs.codeC</td><td>–</td><td>Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy</td></tr><tr><td>hdfs.fileType</td><td>SequenceFile</td><td>File format: currently <code>SequenceFile</code>, <code>DataStream</code> or <code>CompressedStream</code> (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC</td></tr><tr><td>hdfs.maxOpenFiles</td><td>5000</td><td>Allow only this number of open files. If this number is exceeded, the oldest file is closed.</td></tr><tr><td>hdfs.minBlockReplicas</td><td>–</td><td>Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath.</td></tr><tr><td>hdfs.writeFormat</td><td>Writable</td><td>Format for sequence file records. One of <code>Text</code> or <code>Writable</code>. Set to <code>Text</code> before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive.</td></tr><tr><td>hdfs.callTimeout</td><td>10000</td><td>Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring.</td></tr><tr><td>hdfs.threadsPoolSize</td><td>10</td><td>Number of threads per HDFS sink for HDFS IO ops (open, write, etc.)</td></tr><tr><td>hdfs.rollTimerPoolSize</td><td>1</td><td>Number of threads per HDFS sink for scheduling timed file rolling</td></tr><tr><td>hdfs.kerberosPrincipal</td><td>–</td><td>Kerberos user principal for accessing secure HDFS</td></tr><tr><td>hdfs.kerberosKeytab</td><td>–</td><td>Kerberos keytab for accessing secure HDFS</td></tr><tr><td>hdfs.proxyUser</td><td></td><td></td></tr><tr><td>hdfs.round</td><td>false</td><td>Should the timestamp be rounded down (if true, affects all time based escape sequences except %t)</td></tr><tr><td>hdfs.roundValue</td><td>1</td><td>Rounded down to the highest multiple of this (in the unit configured using <code>hdfs.roundUnit</code>), less than current time.</td></tr><tr><td>hdfs.roundUnit</td><td>second</td><td>The unit of the round down value - <code>second</code>, <code>minute</code> or <code>hour</code>.</td></tr><tr><td>hdfs.timeZone</td><td>Local Time</td><td>Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles.</td></tr><tr><td>hdfs.useLocalTimeStamp</td><td>false</td><td>Use the local time (instead of the timestamp from the event header) while replacing the escape sequences.</td></tr><tr><td>hdfs.closeTries</td><td>0</td><td>Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart.</td></tr><tr><td>hdfs.retryInterval</td><td>180</td><td>Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension.</td></tr><tr><td>serializer</td><td><code>TEXT</code></td><td>Other possible options include <code>avro_event</code> or the fully-qualified class name of an implementation of the <code>EventSerializer.Builder</code> interface.</td></tr></tbody></table><h3 id="avro-hdfs"><a href="#avro-hdfs" class="headerlink" title="avro-hdfs"></a>avro-hdfs</h3><p>（配置方式二）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># a1 which ones we want to activate.</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind=node01</span><br><span class="line">a1.sources.r1.port=55555</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = hdfs://shsxt/hdfsflume</span><br><span class="line"></span><br><span class="line"># Define a memory channel called c1 on a1</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity=1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Define an Avro source called r1 on a1 and tell it</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>##当前flume节点执行：</p><p>#flume-ng agent –conf ./ –conf-file avro_loggers –name a1 -Dflume.root.logger=INFO,console</p><p>##其他flume节点执行：</p><p>#flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./logs</p><h3 id="5、结合版（netcat-avro）"><a href="#5、结合版（netcat-avro）" class="headerlink" title="5、结合版（netcat-avro）"></a>5、结合版（netcat-avro）</h3><h4 id="（1）编辑配置文件"><a href="#（1）编辑配置文件" class="headerlink" title="（1）编辑配置文件"></a>（1）编辑配置文件</h4><p>（node00：vim netcat_avro1）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line">#flume-ng agent --conf ./ --conf-file netcat_avro1 --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line">#flume-ng --conf conf --conf-file /root/flume_test/netcat_hdfs -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line">#telnet 192.168.235.15 44444</span><br><span class="line"># Name the components on this agent</span><br><span class="line"> a1.sources = r1</span><br><span class="line"> a1.sinks = k1</span><br><span class="line"> a1.channels = c1</span><br><span class="line"></span><br><span class="line"> # Describe/configure the source</span><br><span class="line"> a1.sources.r1.type = netcat</span><br><span class="line"> a1.sources.r1.bind = node00</span><br><span class="line"> a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"> # Describe the sink</span><br><span class="line"> a1.sinks.k1.type = avro</span><br><span class="line"> a1.sinks.k1.hostname = node01</span><br><span class="line"> a1.sinks.k1.port = 60000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> # Use a channel which buffers events in memory</span><br><span class="line"> a1.channels.c1.type = memory</span><br><span class="line"> a1.channels.c1.capacity = 1000</span><br><span class="line"> a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"> # Bind the source and sink to the channel</span><br><span class="line"> a1.sources.r1.channels = c1</span><br><span class="line"> a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">#---------------------------</span><br><span class="line">#flume-ng agent --conf-file etect2_logger --name a1 -#Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">#flume-ng agent --conf conf --conf-file netcat_logger --name a1 -#Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>（node01：netcat_avro2）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#flume-ng agent --conf ./ --conf-file avro2 -n a1 </span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = node01</span><br><span class="line">a1.sources.r1.port = 60000</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>(2)操作</p><blockquote><p>先启动后面的flume节点node01  ，在启动node00，最后启动node02</p></blockquote><p>在node01上</p><p><code>启动</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf conf --conf-file netcat_avro1 -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">19/01/18 23:22:27 INFO node.Application: Starting Channel c1</span><br><span class="line">19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.</span><br><span class="line">19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started</span><br><span class="line">19/01/18 23:22:28 INFO node.Application: Starting Sink k1</span><br><span class="line">19/01/18 23:22:28 INFO node.Application: Starting Source r1</span><br><span class="line">19/01/18 23:22:28 INFO source.AvroSource: Starting Avro source r1: &#123; bindAddress: node01, port: 60000 &#125;...</span><br><span class="line">19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.</span><br><span class="line">19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started</span><br><span class="line">19/01/18 23:22:30 INFO source.AvroSource: Avro source r1 started.</span><br></pre></td></tr></table></figure><p>在node00上：</p><p><code>启动</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent --conf ./ --conf-file netcat_avro2 --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>在node02上</p><p><code>启动</code></p><blockquote><p>telnet node00 44444</p></blockquote><p>然后输入数据文件</p><p>最后在</p><p>node01节点上</p><p>显示文件信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">19/01/18 23:33:01 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 0D             hello world. &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDH部署操作</title>
      <link href="/2019/01/18/CDH%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/18/CDH%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="报错："><a href="#报错：" class="headerlink" title="报错："></a><strong>报错：</strong></h1><p>1、<code>Error:JAVA_HOME is not set and Java could not fund.Cloudera Manager requires Java 1.6 or later .NOTE：This script will find Oracle Java whether you install using the binary or the RPM based installer</code></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzc3r48t1wj30f004qq3s.jpg" alt=""></p><p>原因：它运行时会默认到（ /usr/java/default）这个路径下找jdk</p><blockquote><p>在 /usr/java/default目录下创建jdk访问的软连接，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> ln -s /home/jdk1.8.0_191 /usr/java/default</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> CDH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase性能优化</title>
      <link href="/2019/01/17/HBase%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
      <url>/2019/01/17/HBase%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="HBase性能优化方案"><a href="#HBase性能优化方案" class="headerlink" title="HBase性能优化方案"></a>HBase性能优化方案</h2><h1 id="（一）、表的设计"><a href="#（一）、表的设计" class="headerlink" title="（一）、表的设计"></a>（一）、表的设计</h1><p>一、Pre-Creating Regions 预分区</p><p>详情参见：<a href="http://hbase.apache.org/book.html#precreate.regions" target="_blank" rel="noopener">Table Creation: Pre-Creating Regions</a></p><a id="more"></a><blockquote><p>解决海量导入数据时的热点问题</p></blockquote><p><code>背景：</code></p><blockquote><p>在创建HBase表的时候默认一张表只有一个region，</p><p>所有的put操作都会往这一个region中填充数据，</p><p>当这一个region过大时就会进行split。</p><p>如果在创建HBase的时候就进行预分区</p><p>则会减少当数据量猛增时由于region split带来的资源消耗。</p></blockquote><p><code>注意：</code></p><blockquote><p>Hbase表的预分区需要紧密结合业务场景来选择分区的key值。</p><p>每个region都有一个startKey和一个endKey来表示该region存储的rowKey范围。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; create &apos;t1&apos;, &apos;cf&apos;, SPLITS =&gt; [&apos;20150501000000000&apos;, &apos;20150515000000000&apos;, &apos;20150601000000000&apos;]</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; create &apos;t2&apos;, &apos;cf&apos;, SPLITS_FILE =&gt; &apos;/home/hadoop/splitfile.txt&apos; </span><br><span class="line"></span><br><span class="line">/home/hadoop/splitfile.txt中存储内容如下： </span><br><span class="line">20150501000000000</span><br><span class="line">20150515000000000</span><br><span class="line">20150601000000000</span><br></pre></td></tr></table></figure><p>该语句会创建4个region：</p><p>Hbase的Web UI中可以查看到表的分区情况：</p><p>其中，<strong>每个region的命名方式如下：[table],[region start key],[region id]</strong></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1hs6n9iroj30j404uwg2.jpg" alt=""></p><p>二、row key</p><p>1、特性</p><blockquote><ul><li><p>在Hbase中 rowKey 可以是任意字符串，最大长度为64KB ， 一般为10—100bytes ,存储在bytes[ ]字节数组中，一般设计为定长。</p></li><li><p>rowKey是按字典排序</p></li><li><p><strong>Rowkey规则：</strong></p><p>1、 定长 越小越好</p><p>2、 Rowkey的设计是要根据实际业务来</p><p>3、 散列性</p><p>​    a)     反转   001  002  100 200</p><p>​    b)     Hash</p></li></ul></blockquote><p>2、HBase中row key用来检索表中的记录，支持以下三种方式：</p><blockquote><p>· 通过单个row key访问：即按照某个row key键值进行get操作；</p><p>·  通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描；过滤器</p><p>·  全表扫描：即直接扫描整张表中所有行记录。</p></blockquote><p>三、column family</p><p>个数限定在2~3个</p><p>原因：</p><blockquote><p>因为某个column family 在flush会，他临近的column family也会因关联效应被触发flush，最终导致系统会产生更多的I/O。</p></blockquote><p>四、参数设置</p><blockquote><ul><li>In Memory</li></ul><p>创建表时，HColumnDescriptor.setInMemory(true)将表放到RegionServer的缓存中，保证在读取的时候被cache命中。</p><ul><li>Max Version</li></ul><p>创建表时，HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。</p><ul><li>Time To Live</li></ul><p>创建表时，HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 <em> 24 </em> 60 * 60)。</p></blockquote><p>五、Compact &amp; Split</p><p>   在Hbase中，数据更新时 首先写入WAL日志（Hlog）和内存（memstore）中，Memstore中数据是有序的，当Memstore累计数据达到一定阈值后，会创建一个新的Memstore，并且将之前的Memstore添加到flush队列，由单独的线程flush到磁盘中成为一个storefile。同时， 系统会在zookeeper中记录一个redo point，表示这个时刻之前的变更已经持久化了<strong>(minor compact)</strong>。</p><p>StoreFile是只读的，一旦创建后就不可以再修改。因此Hbase的更新其实是不断追加的操作。当一个Store中的StoreFile数量达到一定的阈值后，就会进行一次合并<strong>(major compact)</strong>，将对同一个key的修改合并到一起，形成一个大的StoreFile，当StoreFile的大小达到一定阈值后，又会对 StoreFile进行分割<strong>(split)</strong>，等分为两个StoreFile。</p><p>由于对表的更新是不断追加的，处理读请求时，需要访问Store中全部的StoreFile和MemStore，将它们按照row key进行合并，由于StoreFile和MemStore都是经过排序的，并且StoreFile带有内存中索引，通常合并过程还是比较快的。</p><p>实际应用中，可以考虑必要时手动进行major compact，将同一个row key的修改进行合并形成一个大的StoreFile。同时，可以将StoreFile设置大些，减少split的发生。</p><p>hbase为了防止小文件（被刷到磁盘的menstore）过多，以保证保证查询效率，hbase需要在必要的时候将这些小的store file合并成相对较大的store file，这个过程就称之为compaction。在hbase中，主要存在两种类型的compaction：minor  compaction和major compaction。</p><p>minor compaction:的是较小、很少文件的合并。</p><p>major compaction 的功能是将所有的store file合并成一个</p><p>触发major compaction的可能条件有：</p><blockquote><p>major_compact 命令、</p><p>majorCompact() API、</p><p>region server自动运行</p><p>（相关参数：</p><p>hbase.hregion.majoucompaction 默认为24 小时、</p><p>hbase.hregion.majorcompaction.jetter 默认值为0.2 。防止region server 在同一时间进行major compaction）。</p></blockquote><blockquote><p>hbase.hregion.majorcompaction.jetter参数的作用是：</p><p>对参数hbase.hregion.majoucompaction规定的值起到浮动的作用，假如两个参数都为默认值24和0.2，那么major compact最终使用的数值为：19.2~28.8 这个范围。</p></blockquote><p>minor compaction的运行机制要复杂一些，它由一下几个参数共同决定：</p><p>hbase.hstore.compaction.min </p><p>默认值为 3，表示一次minor compaction中最少选取3个store file.  minor compaction才会启动</p><p>hbase.hstore.compaction.max </p><p>默认值为10，表示一次minor compaction中最多选取10个store file</p><p>hbase.hstore.compaction.min.size </p><p>表示文件大小小于该值的store file 一定会加入到minor compaction的store file中</p><p>hbase.hstore.compaction.max.size</p><p> 表示文件大小大于该值的store file 一定会被minor compaction排除</p><p>hbase.hstore.compaction.ratio </p><p>将store file 按照文件年龄排序（older to younger），minor compaction总是从older store file开始选择</p><p>六、高表和宽表的选择</p><p>资源链接：</p><p><a href="http://www.cnblogs.com/rocky24/p/3372ad2a037a73daf0ff4ed4a9f43625.html" target="_blank" rel="noopener">http://www.cnblogs.com/rocky24/p/3372ad2a037a73daf0ff4ed4a9f43625.html</a></p><p><a href="https://yq.aliyun.com/articles/213705" target="_blank" rel="noopener">https://yq.aliyun.com/articles/213705</a></p><p>1、分类</p><p>Hbase表设计：</p><p>高表：行多列少；</p><p>宽表：行少列多。</p><p>2、根据KeyValue信息的筛选粒度，用户应尽量将需要查询的维度和信息存储在行键中，才能达到更好的数据筛选效率。</p><p>在Hbase中，数据操作具有行级原子性，按行分片。根据用户是否批量修改Value内容来决定高表和宽表的选择，宽表每一行存储的数据信息量多，易超过最大HFile的限制，若用户不存在全局value操作的需求，宽表比较适合。</p><h1 id="（二）、写表操作"><a href="#（二）、写表操作" class="headerlink" title="（二）、写表操作"></a>（二）、写表操作</h1><p>一、多HTable客户端并发写</p><p>创建多个HTable客户端用于写操作，提高写数据的吞吐量。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> Configuration conf = HBaseConfiguration.create();</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> String table_log_name = “user_log”;</span><br><span class="line">wTableLog = <span class="keyword">new</span> HTable[tableN];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; tableN; i++) &#123;</span><br><span class="line">    wTableLog[i] = <span class="keyword">new</span> HTable(conf, table_log_name);</span><br><span class="line">    wTableLog[i].setWriteBufferSize(<span class="number">5</span> * <span class="number">1024</span> * <span class="number">1024</span>); <span class="comment">//5MB</span></span><br><span class="line">    wTableLog[i].setAutoFlush(<span class="keyword">false</span>);</span><br></pre></td></tr></table></figure><p>二、HTable参数设置</p><ul><li>Auto Flush</li></ul><blockquote><p>通过调用HTable.setAutoFlush(false)方法可以将HTable写客户端的自动flush关闭，这样可以批量写入数据到HBase，而不是有一条put就执行一次更新，只有当put填满客户端写缓存时，才实际向HBase服务端发起写请求。默认情况下auto flush是开启的</p></blockquote><ul><li>Write Buffer</li></ul><blockquote><p>通过调用HTable.setWriteBufferSize(writeBufferSize)方法可以设置HTable客户端的写buffer大小，如果新设置的buffer小于当前写buffer中的数据时，buffer将会被flush到服务端。其中，writeBufferSize的单位是byte字节数，可以根据实际写入数据量的多少来设置该值。</p></blockquote><ul><li>WAL Flag</li></ul><blockquote><p>在HBase中，客户端向集群中的RegionServer提交数据时（Put/Delete操作），首先会先写WAL（Write Ahead Log）日志（即HLog，一个RegionServer上的所有Region共享一个HLog），只有当WAL日志写成功后，再接着写MemStore，然后客户端被通知提交数据成功；如果写WAL日志失败，客户端则被通知提交失败。这样做的好处是可以做到RegionServer宕机后的数据恢复。</p><p>因此，对于相对不太重要的数据，可以在Put/Delete操作时，通过调用Put.setWriteToWAL(false)或Delete.setWriteToWAL(false)函数，放弃写WAL日志，从而提高数据写入的性能。</p><p><strong>值得注意的是：谨慎选择关闭**</strong>WAL<strong><strong>日志，因为这样的话，一旦</strong></strong>RegionServer<strong><strong>宕机，</strong></strong>Put/Delete<strong><strong>的数据将会无法根据</strong></strong>WAL<strong>**日志进行恢复。</strong></p></blockquote><p>三、批量写</p><p>通过调用HTable.put(Put)方法可以将一个指定的row key记录写入HBase，同样HBase提供了另一个方法：通过调用HTable.put(List<put>)方法可以将指定的row key列表，批量写入多行记录，这样做的好处是批量执行，只需要一次网络I/O开销，这对于对数据实时性要求高，网络传输RTT高的情景下可能带来明显的性能提升。</put></p><p>四、多线程并发写</p><p>在客户端开启多个HTable写线程，每个写线程负责一个HTable对象的flush操作，这样结合定时flush和写buffer（writeBufferSize），可以既保证在数据量小的时候，数据可以在较短时间内被flush（如1秒内），同时又保证在数据量大的时候，写buffer一满就及时进行flush。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadN; i++) &#123;</span><br><span class="line">    Thread th = <span class="keyword">new</span> Thread() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    sleep(<span class="number">1000</span>); <span class="comment">//1 second</span></span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line"><span class="keyword">synchronized</span> (wTableLog[i]) &#123;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        wTableLog[i].flushCommits();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    th.setDaemon(<span class="keyword">true</span>);</span><br><span class="line">    th.start();</span><br><span class="line">&#125;</span><br><span class="line">MR+HBASE</span><br></pre></td></tr></table></figure><h1 id="（三）、读表操作"><a href="#（三）、读表操作" class="headerlink" title="（三）、读表操作"></a>（三）、读表操作</h1><p><a href="http://kenwublog.com/hbase-performance-tuning" target="_blank" rel="noopener">淘宝Ken Wu同学的博客</a>。</p><p>一、多HTable客户端并发读</p><p>创建多个HTable客户端用于读操作，提高读数据的吞吐量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">static final Configuration conf = HBaseConfiguration.create();</span><br><span class="line">static final String table_log_name = “user_log”;</span><br><span class="line">rTableLog = new HTable[tableN];</span><br><span class="line">for (int i = 0; i &lt; tableN; i++) &#123;</span><br><span class="line">    rTableLog[i] = new HTable(conf, table_log_name);</span><br><span class="line">    rTableLog[i].setScannerCaching(50);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>二、HTable参数设置</p><ul><li>Scanner Caching</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hbase.client.scanner.caching配置项可以设置HBase scanner一次从服务端抓取的数据条数，默认情况下一次一条。通过将其设置成一个合理的值，可以减少scan过程中next()的时间开销，代价是scanner需要通过客户端的内存来维持这些被cache的行记录。</span><br><span class="line"></span><br><span class="line">有三个地方可以进行配置：</span><br><span class="line"></span><br><span class="line">1）在HBase的conf配置文件中进行配置；</span><br><span class="line"></span><br><span class="line">2）通过调用HTable.setScannerCaching(int scannerCaching)进行配置； </span><br><span class="line"></span><br><span class="line">3）通过调用Scan.setCaching(int caching)进行配置。</span><br><span class="line"></span><br><span class="line">三者的优先级越来越高。</span><br></pre></td></tr></table></figure><ul><li>Scan Attribute Selection</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scan时指定需要的Column Family，可以减少网络传输数据量，否则默认scan操作会返回整行所有Column Family的数据。</span><br><span class="line">scan.addColumn(&quot;cf1&quot;.getBytes(), &quot;name&quot;.getBytes());</span><br></pre></td></tr></table></figure><ul><li>Close ResultScanner</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过scan取完数据后，记得要关闭ResultScanner，否则RegionServer可能会出现问题（对应的Server资源无法释放）。</span><br></pre></td></tr></table></figure><p>三、批量读</p><p>四、多线程并发读</p><p>五、缓存查询结果</p><p>六、 Blockcache</p>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase学习</title>
      <link href="/2019/01/15/HBase%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/15/HBase%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>非关系型数据库</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration" target="_blank" rel="noopener">官网</a></p><h2 id="一、对HBase数据库的-基本了解"><a href="#一、对HBase数据库的-基本了解" class="headerlink" title="一、对HBase数据库的 基本了解"></a>一、对HBase数据库的 基本了解</h2><a id="more"></a><h3 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h3><blockquote><p>基于Hadoop 的分布式数据库</p><p>特点：</p><p>1、高可靠性</p><p>2、高性能</p><p>（以上两点：基于分布式的特点）</p><p>3、面向列</p><p>（以（K,V）存储，有唯一标记的rowkey，value包含的是数据库中的列值）</p><p>4、可伸缩</p><p>（搭建在集群上）</p><p>5、实时读写</p><p>（用时间戳唯一标记每一版本的数据记录）</p></blockquote><h3 id="2、工作结构"><a href="#2、工作结构" class="headerlink" title="2、工作结构"></a>2、工作结构</h3><blockquote><p>1,利用Hadoop的HDFS作为其文件存储系统</p><p>2,利用Hadoop的MapReduce来计算处理HBase中的海量数据</p><p>3,利用Zookeeper作为其分布式协同服务</p><p>4,主要用来存储非结构化和半结构化的松散数据（NoSQL非关系型数据库有redis、MongoDB等</p></blockquote><h3 id="3、关系型数据库"><a href="#3、关系型数据库" class="headerlink" title="3、关系型数据库"></a>3、关系型数据库</h3><blockquote><p>1、定义</p><p>关系模型指的就是二维表格模型；</p><p>而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织</p><p>2、三大优点</p><ul><li>容易理解</li><li>使用方便</li><li>易于维护</li></ul><p>3、三大瓶颈</p><ul><li>高并发读写需求</li></ul><p>硬盘I/O是一个很大的瓶颈，并且很难能做到数据的强一致性。</p><ul><li>海量数据的读写性能低</li></ul><p>在一张包含海量数据的表中查询，效率是非常低的。</p><ul><li>​    扩展性和可用性差</li></ul><p>丰富的完整性使得横向扩展把难度加大了</p></blockquote><p><code>ACID特性</code></p><blockquote><p>ACID，指数据库事务正确执行的四个基本要素的缩写;</p><p><code>原子性</code>（Atomicity）:<strong>事务不可再分割</strong></p><p>整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。</p><p><code>一致性</code>（Consistency）:<strong>事务前后数据保持一致</strong></p><p>事务必须始终保持系统处于一致的状态，不管在任何给定的时间并发事务有多少。如果事务是并发多个，系统也必须如同串行事务一样操作。</p><p><code>隔离性</code>（Isolation）：<strong>串行化</strong>，使得同一时间仅有一个请求作用于同一数据。</p><p>事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。</p><p><code>持久性</code>（Durability）：</p><p>在事务完成以后，该事务对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。</p></blockquote><h3 id="4、非关系型数据库"><a href="#4、非关系型数据库" class="headerlink" title="4、非关系型数据库"></a>4、非关系型数据库</h3><blockquote><p>1、存储格式：key value键值对，文档，图片等等，结构不固定 </p><p>2、可以减少一些时间和空间的开销，仅需要根据id取出相应的value就可以完成查询。</p><p>3、一般不支持ACID特性，无需经过SQL解析，读写性能高</p><p>4、不提供where字段条件过滤</p><p>5、难以体现设计的完整性，只适合存储一些较为简单的数据</p></blockquote><h2 id="二、对HBase的基本里了解"><a href="#二、对HBase的基本里了解" class="headerlink" title="二、对HBase的基本里了解"></a>二、对HBase的基本里了解</h2><h3 id="1、数据结构组成"><a href="#1、数据结构组成" class="headerlink" title="1、数据结构组成"></a>1、数据结构组成</h3><p>（ 1）Row key  :</p><blockquote><p>唯一标记决定一行数据<br>按照字典排序<br>最大只能存储64KB的字节数据<br>设计非常关键</p></blockquote><p>（2）Column Family列族 &amp; qualifier列</p><blockquote><p><code>列族</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            必须作为表模式(schema)定义的一部分预先给出，</p><p>表中的每个列都归属于某个列族；</p><p>权限控制、存储以及调优都是在列族层面进行的；</p><p><code>列名</code></p><p>以列族作为前缀，每个“列族”都可以有多个列成员(column)； </p><p>新的列可以随后按需、动态加入；</p></blockquote><p>（3）Cell单元格</p><blockquote><p> 由行和列的坐标交叉决定； 单元格是有版本的（有时间戳决定）；</p><p> 单元格的内容是未解析的字节数组；cell中的数据是没有类型的，全部是字节码形式存贮。</p> <figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;  由&#123;rowkey， column( =&lt;family&gt; +&lt;qualifier&gt;)， version&#125; 唯一确定的单元。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>（4）Timestamp时间戳</p><blockquote><p>在HBase每个cell存储单元对同一份数据有多个版本，</p><p>根据唯一的时间戳来区分每个版本之间的差异，</p><p>不同版本的数据按照时间倒序排序，最新的数据版本排在最前面</p><p>时间戳的类型是64位整型。</p><p>时间戳可以由HBase(在数据写入时自动)赋值，精确到毫秒</p><p>时间戳也可以由客户显式赋值，但必须唯一性</p></blockquote><p>（5）HLog(WAL log)</p><blockquote><ul><li>HLog文件就是一个普通的Hadoop SequenceFile</li><li>HLog Sequence File的Key是HLogKey对象<ul><li>HLogKey中记录了写入数据的归属信息，包括table和region名字，sequence number（起始值为0或是最近一次存入文件系统中sequence  number）和timestamp（写入时间）</li></ul></li><li>HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue<ul><li>存储hbase表的操作记录，(K，V)数据信息   </li></ul></li></ul></blockquote><p>2、体系架构</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1gla9rm43j30lo0c3wjo.jpg" alt=""></p><p>（1）Client</p><blockquote><p>包含访问HBase的接口并在缓存中维护着已经访问过的Region位置信息来加快对HBase的访问。</p></blockquote><p>（2）Zookeeper</p><blockquote><ul><li><p>保证任何时候，集群中只有一个master；</p></li><li><p>存贮所有Region的寻址入口。</p></li><li><p>实时监控Region server的上线和下线信息，并实时通知Master</p></li><li><p>存储HBase的schema和table元数据</p></li></ul></blockquote><p>Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等</p><p>（3）Master</p><blockquote><p>为Region server分配region；</p><ul><li>负责Region server的负载均衡；</li><li>发现失效的Region server并将其上的region重新分配；</li><li>在Region分裂或合并后，负责重新调整Region的分布</li><li>管理用户对table的增删改操作；</li></ul></blockquote><p>（4）RegionServer    </p><blockquote><ul><li><p>维护region，处理对这些region的IO请求</p></li><li><p>负责切分在运行过程中变得过大的region</p></li></ul></blockquote><p>（5）Region</p><blockquote><ul><li><p>保存一个表里面某段连续的数据，每个表一开始只有一个region；</p></li><li><p>随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）</p><p>（HBase自动把表水平划分成多个区域(region)）</p></li><li><p>当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上</p></li></ul></blockquote><p>（6）Memstore与storefile</p><blockquote><ul><li>一个region由2-3store组成，一个store对应一个CF（列族）</li><li>store包括位于内存中的memstore和位于磁盘的storefile。</li><li>写操作先写入memstore，当memstore中的数据达到某个阈值（默认64M），hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile；</li><li>当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major，compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile</li><li>当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡</li><li>客户端检索数据，先在memstore找，找不到再找storefile</li><li>HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。</li><li>每个Strore又由一个memStore和0至多个StoreFile组成,StoreFile以HFile格式保存在HDFS上(HFile)。</li></ul></blockquote><h2 id="三、Hbase-安装部署"><a href="#三、Hbase-安装部署" class="headerlink" title="三、Hbase 安装部署"></a>三、Hbase 安装部署</h2><p>完全分布式搭建</p><p>1、安装包准备</p><p><a href="http://hbase.apache.org/downloads.html" target="_blank" rel="noopener">Hbase</a>（本文使用0.98版本）</p><p>将tar上传至Linux系统，进行解压安装</p><p>2、修改配置文件hbase-env.sh（在Hbase的解压目录的conf目录中）</p><p>修添加JAVA_HOME环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line"># export JAVA_HOME=/usr/java/jdk1.6.0/</span><br><span class="line">export JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br></pre></td></tr></table></figure><p>不使用HBase的默认zookeeper配置，（使用自己的）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not.</span><br><span class="line"> export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure><p>3、修改配置hbase-site.xml（在Hbase的解压目录的conf目录中）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Hdfs配置时的集群名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://Sunrise:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--zookeeper的三台节点--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node00,node01,node02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置http访问的port---&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.info.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>60010<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>注意：（会出bug的地方）</code>：</p><p>1、问题描述：</p><blockquote><p>HBase启动时，警告：<br>Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 </p></blockquote><p>解决方案：</p><p><code>原因：</code>由于使用了JDK8 ，需要在HBase的配置文件中hbase-env.sh，注释掉两行。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+</span></span><br><span class="line"><span class="comment">#export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"</span></span><br><span class="line"><span class="comment">#export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"</span></span><br><span class="line"><span class="comment">##-XX:PermSize=128m -XX:MaxPermSize=128m 设置JVM的新生代内存大小，最大值用于保护</span></span><br></pre></td></tr></table></figure><p>重新启动HBase。</p><p>2、问题描述：</p><blockquote><p>配置好HBase后，各项服务正常，但想从浏览器通过端口60010看下节点情况，但是提示拒绝访问</p></blockquote><p><code>检测：</code>在服务器上netstat -natl|grep 60010 发现并没有60010端口</p><p><code>原因：</code>HBase 1.0 之后的版本都需要在hbase-site.xml中配置端口，如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.info.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60010<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>重新启动HBase,在浏览器再次访问，就ok了</p><p>4、添加配置regionservers 文件（在Hbase的解压目录的conf目录中）</p><p>添加配置的regionservers 的主机名</p><p>regionservers</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node00</span><br><span class="line">node01</span><br><span class="line">node02</span><br></pre></td></tr></table></figure><p>5、添加配置backup-masters</p><p>添加配置的master备份的主机名（在Hbase的解压目录的conf目录中）</p><p>backup-masters</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br></pre></td></tr></table></figure><p>6、将Hadoop安装解压目录/etc/hadoop目录下的hdfs-site.xml文件 拷贝到Hbase的解压目录的conf目录中</p><p>7、配置环境变量 ~/.bash_profile</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line">HADOOP_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line">ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br><span class="line">HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line">SQOOP_HOME=/usr/soft/sqoop-1.4.6</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin</span><br><span class="line">HBASE_HOME=/usr/soft/hbase-1.2.9</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HBASE_HOME</span>/bin</span><br></pre></td></tr></table></figure><blockquote><p>source ~/.bash_profile</p></blockquote><p>8、将如上配置远程发送至其他节点（Hbase 、 ./bash_profile）</p><p>9、各个节点注意要做时间同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate cn.ntp.org.cn</span><br></pre></td></tr></table></figure><p>10、启动HDFS集群：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line">start-hdfs.sh</span><br></pre></td></tr></table></figure><p>11、启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><p>显示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node00.out</span><br><span class="line">node02: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node02.out</span><br><span class="line">node01: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node01.out</span><br><span class="line">node00: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node00.out</span><br><span class="line">node02: starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node02.out</span><br></pre></td></tr></table></figure><p>12、查看进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>13、浏览器访问：node00:60010</p><p>14、关闭：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-hbase.sh</span><br></pre></td></tr></table></figure><h2 id="四、通过hbase-shell命令进入HBase-命令行接口"><a href="#四、通过hbase-shell命令进入HBase-命令行接口" class="headerlink" title="四、通过hbase shell命令进入HBase 命令行接口"></a>四、通过hbase shell命令进入HBase 命令行接口</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure><p>进入hbase交互式界面。</p><p>通过<code>help</code>可查看所有命令的支持以及帮助手册</p><blockquote><p>帮助创建</p><p>hbase(main):007:0&gt;  help create</p></blockquote><table><thead><tr><th style="text-align:center">名称</th><th>Shell命令</th><th>举例</th></tr></thead><tbody><tr><td style="text-align:center">创建表</td><td>create ‘表名’, ‘列族名1’[,…]</td><td>create ‘t1’，‘cf1’</td></tr><tr><td style="text-align:center">列出所有表</td><td>list</td><td>list</td></tr><tr><td style="text-align:center">添加记录</td><td>put ‘表名’, ‘RowKey’, ‘列族名称:列名’, ‘值’</td><td>put        ‘t1’,‘rk_00101’,‘cf1:name’,‘zs’</td></tr><tr><td style="text-align:center">查看记录</td><td>get ‘表名’, ‘RowKey’, ‘列族名称:列名’</td><td>get ‘t1’,‘rk_00101’                         get ‘t1’,‘rk_00101’,‘cf1:name’</td></tr><tr><td style="text-align:center">查看所有记录</td><td>count  ‘表名’</td><td>count ‘t1’</td></tr><tr><td style="text-align:center">删除记录</td><td>delete  ‘表名’   , ‘RowKey’,   ‘列族名称:列名’</td><td>delete ‘t1’,‘rk_00101’,‘cf1:name’</td></tr><tr><td style="text-align:center">删除一张表</td><td>先要屏蔽该表，才能对该表进行删除。 <br>第一步 disable   ‘表名称’ <br>第二步 drop   ‘表名称’</td><td>disable ‘t1’                                                      drop ‘t1’</td></tr><tr><td style="text-align:center">查看所有记录</td><td>scan   ‘表名 ‘</td><td>scan ‘t1’</td></tr><tr><td style="text-align:center"></td><td>create ‘t2’, {NAME =&gt; ‘cf1’, VERSIONS =&gt; 2}, METADATA =&gt; { ‘mykey’ =&gt; ‘myvalue’ }</td><td></td></tr><tr><td style="text-align:center">查看未加工数据中指定版本记录</td><td>scan ‘t1’, {RAW =&gt; true, VERSIONS =&gt; 3}                           raw  未加工的</td><td>3</td></tr><tr><td style="text-align:center">查看保存版本记录</td><td>scan ‘t1’, {VERSIONS =&gt; 2}</td><td>2</td></tr><tr><td style="text-align:center"></td><td></td></tr></tbody></table><p><img src="https://img-blog.csdn.net/20151113203232681" alt=""></p><p><img src="https://img-blog.csdn.net/20151113203253457" alt=""></p><h2 id="五、HBase优化"><a href="#五、HBase优化" class="headerlink" title="五、HBase优化"></a>五、HBase优化</h2><p><code>详见HBase性能优化文档</code></p><h2 id="六、Hive和Hbase的整合"><a href="#六、Hive和Hbase的整合" class="headerlink" title="六、Hive和Hbase的整合"></a>六、Hive和Hbase的整合</h2><p>hive和hbase同步<br><a href="https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration</a></p><h3 id="1、拷贝jar包"><a href="#1、拷贝jar包" class="headerlink" title="1、拷贝jar包"></a>1、拷贝jar包</h3><p>把hive-hbase-handler-1.2.1.jar  cp到hbase/lib 下<br>​    同时把hbase中的所有的jar，cp到hive/lib</p><p><code>注意</code>：</p><blockquote><p>hive-hbase-handler-1.2.1.jar在Hive的lib目录下</p></blockquote><h3 id="2、在hive的配置文件增加属性："><a href="#2、在hive的配置文件增加属性：" class="headerlink" title="2、在hive的配置文件增加属性："></a>2、在hive的配置文件增加属性：</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01,node02,node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3、在hive中创建临时表"><a href="#3、在hive中创建临时表" class="headerlink" title="3、在hive中创建临时表"></a>3、在hive中创建临时表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(注意：需要先在Hbase中创建t_order表，列族为order：<span class="keyword">create</span> <span class="string">'t_order'</span> <span class="string">'order'</span>)</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> tmp_tbl</span><br><span class="line">(<span class="keyword">key</span> <span class="keyword">string</span>, <span class="keyword">id</span> <span class="keyword">string</span>, user_id <span class="keyword">string</span>)  </span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span>  </span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,order:order_id,order:user_id"</span>)  </span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"t_tbl"</span>，<span class="string">"hbase.mapred.output.outputtable"</span> = <span class="string">"t_tbl"</span>);</span><br><span class="line"></span><br><span class="line">（确保xyz没有在Hbase中存在）</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hbasetbl(<span class="keyword">key</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, age <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key,cf1:name,cf1:age"</span>)</span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"xyz"</span>, <span class="string">"hbase.mapred.output.outputtable"</span> = <span class="string">"xyz"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop学习</title>
      <link href="/2019/01/13/Sqoop%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/13/Sqoop%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="一、Sqoop简介"><a href="#一、Sqoop简介" class="headerlink" title="一、Sqoop简介"></a>一、Sqoop简介</h1><p><a href="http://sqoop.apache.org/" target="_blank" rel="noopener">官网</a></p><blockquote><ul><li><p>是将关系数据库（oracle、mysql、postgresql等）数据与hadoop数据进行<code>转换的工具</code>。</p></li><li><p>可以将一个关系型数据库(例如MySQL、Oracle)中的数据导入到Hadoop(例如HDFS、Hive、Hbase)中，也可以将Hadoop(例如HDFS、Hive、Hbase)中的数据导入到关系型数据库(例如Mysql、Oracle)中。</p></li></ul></blockquote><p>版本：(两个版本完全不兼容)</p><ul><li><p>sqoop1：1.4.x    （推荐）</p></li><li><p>sqoop2：1.99.x</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1gjqrd7n8j30se0c9glv.jpg" alt=""></p><h1 id="二、sqoop-架构"><a href="#二、sqoop-架构" class="headerlink" title="二、sqoop 架构"></a>二、sqoop 架构</h1><p>hadoop生态系统的架构最简单的框架。</p><blockquote><p>sqoop1由client端直接接入hadoop，任务通过解析生成对应的mapreduce执行</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1gjqvsixcj30pk0fsmxf.jpg" alt=""></p><h1 id="三、Sqoop安装"><a href="#三、Sqoop安装" class="headerlink" title="三、Sqoop安装"></a>三、Sqoop安装</h1><h2 id="1、安装包解压："><a href="#1、安装包解压：" class="headerlink" title="1、安装包解压："></a>1、安装包解压：</h2><p>Sqoop1  : <a href="http://mirrors.shu.edu.cn/apache/sqoop/1.99.7" target="_blank" rel="noopener">1.4.7</a> (推荐) </p><p>Sqoop2 :  <a href="http://mirrors.shu.edu.cn/apache/sqoop/1.99.7" target="_blank" rel="noopener">1.99.7</a></p><h2 id="2、配置环境变量（追加）"><a href="#2、配置环境变量（追加）" class="headerlink" title="2、配置环境变量（追加）"></a>2、配置环境变量（追加）</h2><blockquote><p>vim  ~/.bash_profile</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">HADOOP_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">SQOOP_HOME=/usr/soft/sqoop-1.4.6</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br></pre></td></tr></table></figure><p>(编辑结束后，保存并退出，然后在控制台输入下面的命令，从而是环境变量生效)</p><p>链接资源：</p><blockquote><p>source /etc/profile</p></blockquote><h2 id="3、添加数据库驱动包"><a href="#3、添加数据库驱动包" class="headerlink" title="3、添加数据库驱动包"></a>3、添加数据库驱动包</h2><blockquote><p>在Sqoop的安装解压目录下的lib目录下添加jar包</p><p>mysql-connector-java-5.1.10.jar</p><p>用以连接Mysql</p></blockquote><h2 id="4、重命名配置文件"><a href="#4、重命名配置文件" class="headerlink" title="4、重命名配置文件"></a>4、重命名配置文件</h2><p>在Sqoop的解压安装目录下的conf目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure><p>编辑sqoop-env.sh</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Set path to where bin/hadoop is available</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#Set path to where hadoop-*-core.jar is available</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line"></span><br><span class="line"><span class="comment">#set the path to where bin/hbase is available</span></span><br><span class="line"><span class="built_in">export</span> HBASE_HOME=/usr/soft/hbase-1.2.9</span><br><span class="line"></span><br><span class="line"><span class="comment">#Set the path to where bin/hive is available</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin</span><br><span class="line"></span><br><span class="line"><span class="comment">#Set the path for where zookeper config dir is</span></span><br><span class="line"><span class="comment">#export ZOOCFGDIR=/usr/soft/zookeeper-3.4.13</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br></pre></td></tr></table></figure><h2 id="5、修改配置configure-sqoop"><a href="#5、修改配置configure-sqoop" class="headerlink" title="5、修改配置configure-sqoop"></a>5、修改配置configure-sqoop</h2><p>在Sqoop的解压安装目录的bin目录下</p><blockquote><p>注释掉未安装服务相关内容</p><p>例如（HBase、HCatalog、Accumulo）：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then</span><br><span class="line">#    HCAT_HOME=/usr/lib/hive-hcatalog</span><br><span class="line">#  elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then</span><br><span class="line">#    HCAT_HOME=/usr/lib/hcatalog</span><br><span class="line">#  else</span><br><span class="line">#    HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog</span><br><span class="line">#    if [ ! -d $&#123;HCAT_HOME&#125; ]; then</span><br><span class="line">#       HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog</span><br><span class="line">#    fi</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br><span class="line">#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then</span><br><span class="line">#  if [ -d &quot;/usr/lib/accumulo&quot; ]; then</span><br><span class="line">#    ACCUMULO_HOME=/usr/lib/accumulo</span><br><span class="line">#  else</span><br><span class="line">#    ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo</span><br><span class="line">#  fi</span><br><span class="line">#fi</span><br></pre></td></tr></table></figure><h2 id="6、运行sqoop"><a href="#6、运行sqoop" class="headerlink" title="6、运行sqoop"></a>6、运行sqoop</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop version</span><br></pre></td></tr></table></figure><p>前提:</p><p>MySQL运行正常，且服务启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysqld start</span><br></pre></td></tr></table></figure><p>启动sqoop连接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop list–databases --connect jdbc:mysql://node00:3306/ -username root -password 123456</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">sqoop list-tables --connect jdbc:mysql://192.168.198.128:3306/mysql --username root --password 123456</span><br></pre></td></tr></table></figure><p><code>警告：</code></p><p>关于zookeeper环境变量配置的问题：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node00 conf]# sqoop version</span><br><span class="line">Warning: /usr/soft/sqoop-1.4.6/bin/../../zookeeper does not exist! Accumulo imports will fail.</span><br><span class="line">Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.</span><br><span class="line">19/01/18 17:02:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">Sqoop 1.4.6</span><br><span class="line">git commit id c0c5a81723759fa575844a0a1eae8f510fa32c25</span><br><span class="line">Compiled by root on Mon Apr 27 14:38:36 CST 2015</span><br></pre></td></tr></table></figure><p><code>解决方案：</code></p><p>在sqoop解压安装目录的conf目录下，在sqoop-env.sh文件中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>添加本地ZOOKEEPER_HOME的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br></pre></td></tr></table></figure><h1 id="四、Sqoop导入导出选项"><a href="#四、Sqoop导入导出选项" class="headerlink" title="四、Sqoop导入导出选项"></a>四、Sqoop导入导出选项</h1><h2 id="1、导入工具import："><a href="#1、导入工具import：" class="headerlink" title="1、导入工具import："></a>1、导入工具import：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">   选项                                 含义说明</span><br><span class="line">   </span><br><span class="line">--append将数据追加到HDFS上一个已存在的数据集上</span><br><span class="line"></span><br><span class="line">--as-avrodatafile将数据导入到Avro数据文件</span><br><span class="line"></span><br><span class="line">--as-sequencefile将数据导入到SequenceFile</span><br><span class="line"></span><br><span class="line">--as-textfile将数据导入到普通文本文件（默认）</span><br><span class="line"></span><br><span class="line">--boundary-query &lt;statement&gt;边界查询，用于创建分片（InputSplit）</span><br><span class="line"></span><br><span class="line">--columns &lt;col,col,col…&gt;从表中导出指定的一组列的数据</span><br><span class="line"></span><br><span class="line">--delete-target-dir    如果指定目录存在，则先删除掉</span><br><span class="line"></span><br><span class="line">--direct    使用直接导入模式（优化导入速度）</span><br><span class="line"></span><br><span class="line">--direct-split-size &lt;n&gt;    分割输入stream的字节大小（在直接导入模式下）</span><br><span class="line"></span><br><span class="line">--fetch-size &lt;n&gt;        从数据库中批量读取记录数</span><br><span class="line"></span><br><span class="line">--inline-lob-limit &lt;n&gt;        设置内联的LOB对象的大小</span><br><span class="line"></span><br><span class="line">-m,--num-mappers &lt;n&gt;    使用n个map任务并行导入数据</span><br><span class="line"></span><br><span class="line">-e,--query &lt;statement&gt;     导入的查询语句</span><br><span class="line"></span><br><span class="line">--split-by &lt;column-name&gt;指定按照哪个列去分割数据</span><br><span class="line"></span><br><span class="line">--table &lt;table-name&gt;    导入的源表表名</span><br><span class="line"></span><br><span class="line">--target-dir &lt;dir&gt;        导入HDFS的目标路径</span><br><span class="line"></span><br><span class="line">--warehouse-dir &lt;dir&gt;    HDFS存放表的根路径</span><br><span class="line"></span><br><span class="line">--where &lt;where clause&gt;    指定导出时所使用的查询条件</span><br><span class="line"></span><br><span class="line">-z,--compress    启用压缩</span><br><span class="line"></span><br><span class="line">--compression-codec &lt;c&gt;    指定Hadoop的codec方式（默认gzip）</span><br><span class="line"></span><br><span class="line">--null-string &lt;null-string&gt;        如果指定列为字符串类型，使用指定字符串替换值为null的该类                                     列的值</span><br><span class="line"></span><br><span class="line">--null-non-string &lt;null-string&gt;     如果指定列为非字符串类型，使用指定字符串替换值为null的该                                     类列的值</span><br></pre></td></tr></table></figure><h2 id="2、导出工具export："><a href="#2、导出工具export：" class="headerlink" title="2、导出工具export："></a>2、导出工具export：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">   选项                                         含义说明</span><br><span class="line"></span><br><span class="line">--validate &lt;class-name&gt;         启用数据副本验证功能，仅支持单表拷贝，可以指定验证使用的实现类</span><br><span class="line"></span><br><span class="line">--validation-threshold &lt;class-name&gt;  指定验证门限所使用的类</span><br><span class="line"></span><br><span class="line">--direct                          使用直接导出模式（优化速度）</span><br><span class="line"></span><br><span class="line">--export-dir &lt;dir&gt;                  导出过程中HDFS源路径</span><br><span class="line"></span><br><span class="line">--m,--num-mappers &lt;n&gt;              使用n个map任务并行导出</span><br><span class="line"></span><br><span class="line">--table &lt;table-name&gt;              导出的目的表名称</span><br><span class="line"></span><br><span class="line">--call &lt;stored-proc-name&gt;          导出数据调用的指定存储过程名</span><br><span class="line"></span><br><span class="line">--update-key &lt;col-name&gt;              更新参考的列名称，多个列名使用逗号分隔</span><br><span class="line"></span><br><span class="line">--update-mode &lt;mode&gt;                指定更新策略，包括：updateonly（默认）、allowinsert</span><br><span class="line"></span><br><span class="line">--input-null-string &lt;null-string&gt;  使用指定字符串，替换字符串类型值为null的列</span><br><span class="line"></span><br><span class="line">--input-null-non-string &lt;null-string&gt;使用指定字符串，替换非字符串类型值为null的列</span><br><span class="line"></span><br><span class="line">--staging-table &lt;staging-table-name&gt;在数据导出到数据库之前，数据临时存放的表名称</span><br><span class="line"></span><br><span class="line">--clear-staging-table              清除工作区中临时存放的数据</span><br><span class="line"></span><br><span class="line">--batch                              使用批量模式导出</span><br></pre></td></tr></table></figure><h1 id="四、Sqoop导入导出操作"><a href="#四、Sqoop导入导出操作" class="headerlink" title="四、Sqoop导入导出操作"></a>四、Sqoop导入导出操作</h1><h2 id="1、导入"><a href="#1、导入" class="headerlink" title="1、导入"></a>1、导入</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1gje9uswwj30rp0gxmzv.jpg" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop     ##sqoop命令</span><br><span class="line">import    ##表示导入</span><br><span class="line">--connect jdbc:mysql://ip:3306/sqoop    ##告诉jdbc，连接mysql的url</span><br><span class="line">--username root              ##连接mysql的用户名</span><br><span class="line">--password 123456            ##连接mysql的密码</span><br><span class="line">--table myuser               ##从mysql到出的表名</span><br><span class="line">-m 1                         ##使用1个map任务进行导出</span><br><span class="line">--hive-import                ##把mysql表数据导入到hive中，如果不适用该选项意味着导入到hdfs中</span><br><span class="line">--target-dir &lt;dir&gt;           ##HDFS destination dir</span><br></pre></td></tr></table></figure><h3 id="1、将MySQL中的数据导入到HDFS-Hive-Hbase"><a href="#1、将MySQL中的数据导入到HDFS-Hive-Hbase" class="headerlink" title="1、将MySQL中的数据导入到HDFS/Hive/Hbase"></a>1、将MySQL中的数据导入到HDFS/Hive/Hbase</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">MySQL--&gt;</span><span class="bash"> HDFS：</span></span><br><span class="line">sqoop import --connect jdbc:mysql://node00:3306/test --username root --password 123456 --table myuser -m 1 -target-dir hdfs://Sunrise/my02</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">MySQL--&gt;</span><span class="bash"> Hive：</span></span><br><span class="line">sqoop import --connect jdbc:mysql://node00:3306/test --username root --password root --table myuser --hive-import -m 1</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#由于使用Sqoop从MySQL导入数据到Hive需要指定target-dir，因此导入的是普通表而不能为外部表。</span></span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">MySQL--&gt;</span><span class="bash"> HBase:</span></span><br><span class="line">sqoop import --connect jdbc:mysql://node00:3306/test --username root --password 1234 --table mysqoop --hbase-create-table --hbase-table sukie  --hbase-row-key name --column-family cf1 -m 1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#选项解释</span></span></span><br><span class="line">--column-family        ##指定列族名</span><br><span class="line">--hbase-row-key        ##指定rowkey对应的mysql中的键</span><br><span class="line">--hbase-create-table   ##自动在Hbase中创建表</span><br></pre></td></tr></table></figure><h2 id="2、导出"><a href="#2、导出" class="headerlink" title="2、导出"></a>2、导出</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDly1g1gjeoi0pej30tu0hqq5y.jpg" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop</span><br><span class="line">export                                 ##表示如hive数据导出到mysql</span><br><span class="line">--connect jdbc:mysql://ip:3306/test </span><br><span class="line">--username root </span><br><span class="line">--password 123 </span><br><span class="line">--table mysqoop                       ##mysql中的表（必须已存在）</span><br><span class="line">--export-dir /root/hive               ## hive中导出的文件目录</span><br><span class="line">--fields-terminated-by '\t'           ##表示如hive导出文件中的行的字段分隔符</span><br></pre></td></tr></table></figure><h3 id="2、使用Sqoop将HDFS-Hive-HBase中的数据导出到MySQL"><a href="#2、使用Sqoop将HDFS-Hive-HBase中的数据导出到MySQL" class="headerlink" title="2、使用Sqoop将HDFS/Hive/HBase中的数据导出到MySQL"></a>2、使用Sqoop将HDFS/Hive/HBase中的数据导出到MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">HDFS--&gt;</span><span class="bash">MySQL:</span></span><br><span class="line">sqoop export --connect jdbc:mysql://192.168.198.128:3306/test --username root --password 123 --table my --export-dir /root/my</span><br></pre></td></tr></table></figure><blockquote><p>将HDFS/Hive/HBase中的数据导出到MySQL操作都基本大同小异</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">Hive--&gt;</span><span class="bash">MySQL:</span></span><br><span class="line">sqoop export --connect jdbc:mysql://192.168.198.128:3306/test --username root --password 123 --table testa --export-dir /user/hive/warehouse/testa --input-fields-terminated-by '\001’</span><br></pre></td></tr></table></figure><p>HBase–&gt;mysql:</p><blockquote><p> 目前没有直接的命令将HBase的数据导出到mysql，但可以先将数据导出到hdfs，再导出到mysql </p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sqoop export --connect jdbc:mysql://192.168.198.128:3306/mysql --username root --password 123456 --table bb --export-dir  '/mysql_data/part-m-00000'</span><br></pre></td></tr></table></figure><blockquote><p>也可以通过Hive建立2个表，一个外部表是基于这个Hbase表的，另一个是单纯的基于hdfs的hive原生表，然后把外部表的数据导入到原生表（临时），然后通过hive将临时表里面的数据导出到mysql</p></blockquote><p>1、mysql建立空表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span></span><br><span class="line"><span class="keyword">TABLE</span>  <span class="string">`employee`</span> (</span><br><span class="line">  <span class="string">`rowkey`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,   </span><br><span class="line">  PRIMARY <span class="keyword">KEY</span>  (<span class="string">`id`</span>)   </span><br><span class="line">) <span class="keyword">ENGINE</span>=MyISAM  <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br></pre></td></tr></table></figure><p>2、Hbase建立employee表,并加载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="string">'employee'</span>,<span class="string">'info'</span></span><br><span class="line"></span><br><span class="line">put <span class="string">'employee'</span>,<span class="number">1</span>,<span class="string">'info:id'</span>,<span class="number">1</span></span><br><span class="line"></span><br><span class="line">put <span class="string">'employee'</span>,<span class="number">1</span>,<span class="string">'info:name'</span>,<span class="string">'peter'</span></span><br><span class="line"></span><br><span class="line">put <span class="string">'employee'</span>,<span class="number">2</span>,<span class="string">'info:id'</span>,<span class="number">2</span></span><br><span class="line"></span><br><span class="line">put <span class="string">'employee'</span>,<span class="number">2</span>,<span class="string">'info:name'</span>,<span class="string">'paul'</span></span><br></pre></td></tr></table></figure><p>3、建立Hive外部表</p><p>hive 有分为原生表和外部表，原生表是以简单文件方式存储在hdfs里面，外部表依赖别的框架，比如Hbase，我们现在建立一个依赖于我们刚刚建立的employee hbase表的hive 外部表 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> h_employee(<span class="keyword">key</span> <span class="built_in">int</span>, <span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'org.apache.hadoop.hive.hbase.HBaseStorageHandler'</span></span><br><span class="line"><span class="keyword">WITH</span> SERDEPROPERTIES (<span class="string">"hbase.columns.mapping"</span> = <span class="string">":key, info:id,info:name"</span>)</span><br><span class="line">TBLPROPERTIES (<span class="string">"hbase.table.name"</span> = <span class="string">"employee"</span>);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from h_employee;</span><br><span class="line">OK</span><br><span class="line">1   1   peter</span><br><span class="line">2   2   paul</span><br></pre></td></tr></table></figure><p>4、建立Hive原生表</p><p>这个hive原生表只是用于导出的时候临时使用的，所以取名叫 h_employee_export，字段之间的分隔符用逗号</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> h_employee_export(</span><br><span class="line">    <span class="keyword">key</span> <span class="built_in">INT</span>, </span><br><span class="line">    <span class="keyword">id</span> <span class="built_in">INT</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">STRING</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\054'</span>;</span><br></pre></td></tr></table></figure><p>5、源Hive表导入数据到临时表</p><p>将数据从 h_employee(基于Hbase的外部表)导入到 h_employee_export(原生Hive表) </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert overwrite table h_employee_export select * from h_employee;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from h_employee_export;</span><br><span class="line">OK</span><br><span class="line">1   1   peter</span><br><span class="line">2   2   paul</span><br></pre></td></tr></table></figure><blockquote><p>我们去看下实际存储的文本文件是什么样子的 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> $ hdfs dfs -cat /user/hive/warehouse/h_employee_export/000000_0</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 1,1,peter</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> 2,2,paul</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>6、从Hive导出数据到mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sqoop <span class="built_in">export</span> --connect jdbc:mysql://localhost:3306/sqoop_test --username root --password root --table employee -m 1 --<span class="built_in">export</span>-dir /user/hive/warehouse/h_employee_export/</span></span><br></pre></td></tr></table></figure><p>7、注意</p><p>在这段日志中有这样一句话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`14/12/05 08:49:46 INFO mapreduce.Job: The url ``to` `track the job: https://hadoop01:8088/proxy/application_1406097234796_0037/`</span><br></pre></td></tr></table></figure><p>意思是你可以用  浏览器 访问这个地址去看下任务的执行情况，如果你的任务长时间卡主没结束就是出错了，可以去这个地址查看详细的错误日志 </p><p>8、查询结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from employee;</span><br><span class="line">+--------+----+-------+</span><br><span class="line">| rowkey | id | name  |</span><br><span class="line">+--------+----+-------+</span><br><span class="line">|      1 |  1 | peter |</span><br><span class="line">|      2 |  2 | paul  |</span><br><span class="line">+--------+----+-------+</span><br><span class="line">2 rows in set (0.00 sec) </span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">1、Sqoop增量导入</span><br><span class="line">sqoop import </span><br><span class="line">-D sqoop.hbase.add.row.key=true </span><br><span class="line">--connect jdbc:mysql://node00:3306/spider </span><br><span class="line">--username root --password root </span><br><span class="line">--table TEST_GOODS </span><br><span class="line">--columns ID,GOODS_NAME,GOODS_PRICE </span><br><span class="line">--hbase-create-table </span><br><span class="line">--hbase-table t_goods </span><br><span class="line">--column-family cf </span><br><span class="line">--hbase-row-key ID </span><br><span class="line">--incremental lastmodified </span><br><span class="line">--check-column U_DATE </span><br><span class="line">--last-value '2017-06-27' </span><br><span class="line">--split-by U_DATE</span><br><span class="line"></span><br><span class="line">--incremental lastmodified 增量导入支持两种模式 append 递增的列；lastmodified时间戳。</span><br><span class="line">--check-column 增量导入时参考的列</span><br><span class="line">--last-value 最小值，这个例子中表示导入2017-06-27到今天的值</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">2、Sqoop job：</span><br><span class="line">   sqoop job </span><br><span class="line">   --create testjob01 </span><br><span class="line">   --import </span><br><span class="line">   --connect jdbc:mysql://node00:3306/spider </span><br><span class="line">   --username root --password root </span><br><span class="line">   --table TEST_GOODS </span><br><span class="line">   --columns ID,GOODS_NAME,GOODS_PRICE </span><br><span class="line">   --hbase-create-table </span><br><span class="line">   --hbase-table t_goods </span><br><span class="line">   --column-family cf </span><br><span class="line">   --hbase-row-key ID </span><br><span class="line">   -m 1</span><br><span class="line">   </span><br><span class="line">   设置定时执行以上sqoop job</span><br><span class="line">   使用linux定时器：crontab -e</span><br><span class="line">   例如每天执行</span><br><span class="line">   0 0 * * * /opt/local/sqoop-1.4.6/bin/sqoop job ….</span><br><span class="line">   --exec testjob01</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive优化</title>
      <link href="/2019/01/12/Hive%E4%BC%98%E5%8C%96/"/>
      <url>/2019/01/12/Hive%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="一、核心思想："><a href="#一、核心思想：" class="headerlink" title="一、核心思想："></a>一、核心思想：</h2><blockquote><p>把Hive SQL 当做MapReduce程序进行优化</p></blockquote><p><code>注意：</code>以下不能HQL转化为Mapreduce任务运行</p><p>—select 仅查询本表字段</p><p>—where 仅对本表字段做条件过滤</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--比如</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span>；</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="二、explain"><a href="#二、explain" class="headerlink" title="二、explain"></a>二、explain</h2><blockquote><p>用以显示任务执行计划</p><p>格式：</p><p>EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query</p></blockquote><p><code>语法解释</code></p><blockquote><p>从语法组成可以看出来是一个“explain ”+三个可选参数+查询语句。大家可以积极尝试一下，后面两个显示内容很简单的，我介绍一下第一个 extended 这个可以显示hql语句的语法树</p><p>其次，执行计划一共有三个部分：</p><ul><li>这个语句的抽象语法树</li><li><p>这个计划不同阶段之间的依赖关系</p></li><li><p>对于每个阶段的详细描述</p></li></ul></blockquote><p><code>例子：</code></p><blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; hive&gt; explain select * from log;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p><code>拓展</code>课下查询MySQl的执行计划。</p><h2 id="三、Hive运行方式"><a href="#三、Hive运行方式" class="headerlink" title="三、Hive运行方式"></a>三、Hive运行方式</h2><h3 id="集群模式："><a href="#集群模式：" class="headerlink" title="集群模式："></a>集群模式：</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行hql：</span><br><span class="line">hive&gt; select count(*) from log;</span><br></pre></td></tr></table></figure><p><code>结论：</code></p><blockquote><p>函数（如count）是在reduce阶段进行<br>默认提交到yarn所在的节点上运行，</p></blockquote><hr><h3 id="优化一"><a href="#优化一" class="headerlink" title="优化一:"></a>优化一:</h3><p>设置  本地模式（运行速度加快。但对加载文件有限制）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.mode.local.auto=true;</span><br><span class="line"></span><br><span class="line">查看：</span><br><span class="line">hive&gt;set hive.exec.mode.local.auto</span><br></pre></td></tr></table></figure><p><code>但是</code>如果加载文件的最大值大于配置（默认配置为100M），仍会使用集群模式运行（在yarn所在的节点）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看最大加载文件</span><br><span class="line">hive&gt; set hive.exec.mode.local.auto.inputbytes.max;</span><br><span class="line"></span><br><span class="line">显示：</span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max=134217728</span><br></pre></td></tr></table></figure><hr><h3 id="优化二："><a href="#优化二：" class="headerlink" title="优化二："></a>优化二：</h3><p>设置 严格模式:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通过设置以下参数开启严格模式[防止误操作]：</span><br><span class="line">hive&gt; set hive.mapred.mode=strict;</span><br><span class="line">（默认为：nonstrict非严格模式）</span><br></pre></td></tr></table></figure><p><code>但是</code>存在查询限制:</p><p>​          可以防止用户执行那些可能产生不好的效果的查询。即某些查询在严格模式下无法执行。</p><blockquote><p>1、对分区表查询时，必须添加where对于分区字段的条件过滤；</p><p>就是用户不允许扫描所有的分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。如果没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; hive&gt; select * from day_table where dt='2019-01-13';</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>2、order by语句必须包含limit输出限制；</p><p>因为orderby为了执行排序过程会讲所有的结果分发到同一个reducer中进行处理，强烈要求用户增加这个limit语句可以防止reducer额外执行很长一段时间。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; hive&gt; select * from log order by id limit 1;</span><br><span class="line">&gt; 这里的1， 表示显示前多少条记录，只能设一个数字</span><br><span class="line">&gt; 和Mysql（可以从0 开始）不同的是，它只能从1开始</span><br><span class="line">&gt; mysql可以有两个数字，表示从第几条开始，显示几条</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>3、限制执行笛卡尔积的查询；</p><p>因为在关系型数据库中，可以使用where充当on，但是在hive数据仓库中，必须使用on，否则，查询会出此案不可控的情况。</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzhry9ni0xj30m10cjdgp.jpg" alt="limit"><span class="img-alt">limit</span></p><h3 id="优化三："><a href="#优化三：" class="headerlink" title="优化三："></a>优化三：</h3><p>设置并行计算:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--通过设置一下参数设置并行模式</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span></span><br><span class="line"><span class="comment">--通过以下设置一次SQL计算中允许并行执行的job个数的最大值</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number</span><br></pre></td></tr></table></figure><p>执行sql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> t1.cf1,t2.cf2 <span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">as</span> cf1 <span class="keyword">from</span> <span class="keyword">table</span>) t1,</span><br><span class="line">(<span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">as</span> cf2 <span class="keyword">from</span> <span class="keyword">table</span>) t2;</span><br></pre></td></tr></table></figure><h2 id="四、Hive排序"><a href="#四、Hive排序" class="headerlink" title="四、Hive排序"></a>四、Hive排序</h2><h3 id="1、Order-By"><a href="#1、Order-By" class="headerlink" title="1、Order By"></a>1、Order By</h3><p>— 对于查询结果做<code>全局</code>排序，只允许有<code>一个</code>reduce处理<br>（当数据量较大时，reduce数量有限，应慎用。</p><p>​     严格模式下，必须结合limit来使用）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">limit</span> <span class="number">9</span>;    （结果有序）</span><br></pre></td></tr></table></figure><p>显示：</p><blockquote><p>Time taken: 102.065 seconds, Fetched: 7 row(s)</p></blockquote><h3 id="2、Sort-By"><a href="#2、Sort-By" class="headerlink" title="2、Sort By"></a>2、Sort By</h3><p>– 对于<code>单个</code>reduce的数据进行排序</p><p>–局部（单个reduce）有序，全局无序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以通过设置mapred.reduce.tasks的值来控制reduce的数，然后对reduce输出的结果做二次排序</span><br></pre></td></tr></table></figure><p><code>案例</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">sort</span> <span class="keyword">by</span> <span class="keyword">id</span>;       (结果无序)</span><br></pre></td></tr></table></figure><p><code>显示</code></p><blockquote><p>Time taken: 147.077 seconds, Fetched: 7 row(s)</p></blockquote><h3 id="3、Distribute-By"><a href="#3、Distribute-By" class="headerlink" title="3、Distribute By"></a>3、Distribute By</h3><p>– 分区排序，经常和 Sort By 结合使用 全局有序，局部无序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">id</span>;     （结果无序）</span><br></pre></td></tr></table></figure><blockquote><p>Time taken: 144.708 seconds, Fetched: 7 row(s)</p></blockquote><p><code>注意：</code>hive要求DISTRIBUTE BY语句出现在SORT BY语句之前</p><blockquote><p>Distribute By可以将Map阶段输出的数据按指定的字段划分到不同的reduce文件中，然后，sort by 对reduce阶段的输出数据做排序。</p></blockquote><p>情况一、(无序)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> distrubute <span class="keyword">by</span> <span class="keyword">class</span>  <span class="keyword">sort</span> <span class="keyword">by</span> acore;</span><br></pre></td></tr></table></figure><p>情况二、（？？）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">id</span> ) t2 <span class="keyword">sort</span> <span class="keyword">by</span> t2.id <span class="keyword">asc</span>;</span><br></pre></td></tr></table></figure><h3 id="4、Cluster-By"><a href="#4、Cluster-By" class="headerlink" title="4、Cluster By"></a>4、Cluster By</h3><p>– 相当于 Sort By + Distribute By<br>（Cluster By不能通过asc、desc的方式指定排序规则；<br>可通过 distribute by column sort by column asc|desc 的方式）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.* <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> cluster <span class="keyword">by</span> <span class="keyword">id</span> ) a <span class="keyword">order</span> <span class="keyword">by</span> a.id <span class="keyword">limit</span> <span class="number">9</span> ; (结果有序)</span><br><span class="line"></span><br><span class="line">9 在这里是表中数据记录的总条数</span><br></pre></td></tr></table></figure><p>显示：</p><blockquote><p> Time taken: 234.593 seconds, Fetched: 7 row(s)</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> cluster <span class="keyword">by</span> <span class="keyword">id</span>) a；</span><br></pre></td></tr></table></figure><h2 id="五、-Hive-Join-（重难点）"><a href="#五、-Hive-Join-（重难点）" class="headerlink" title="五、==Hive Join  （重难点）=="></a>五、==Hive Join  （重难点）==</h2><h3 id="1、Join-连接时，将小表（驱动表）放在join的左边"><a href="#1、Join-连接时，将小表（驱动表）放在join的左边" class="headerlink" title="1、Join 连接时，将小表（驱动表）放在join的左边"></a>1、Join 连接时，将小表（驱动表）放在join的左边</h3><h3 id="2、Map-Join-："><a href="#2、Map-Join-：" class="headerlink" title="2、Map Join ："></a>2、Map Join ：</h3><blockquote><p>因为Map Join 是在Map端且在内存中进行的，所以不需要启动Reduce任务，也没有shuffle阶段，从而在一定程度上节省资源，提高Join效率。</p></blockquote><h3 id="方式：（两种）"><a href="#方式：（两种）" class="headerlink" title="方式：（两种）"></a>方式：（两种）</h3><h4 id="1、SQL方式："><a href="#1、SQL方式：" class="headerlink" title="1、SQL方式："></a>1、SQL方式：</h4><p>​     在HQl语句中添加MapJoin标记（mapjoin）(将小表加入到内存，注意小表的大小)</p><p>​     语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(smallTable) */</span>  smallTable.key,  bigTable.value </span><br><span class="line">              <span class="keyword">FROM</span>  smallTable  <span class="keyword">JOIN</span>  bigTable  <span class="keyword">ON</span>  smallTable.key  = bigTable.key;</span><br></pre></td></tr></table></figure><p><code>案例：</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(log1) */</span>  log.id,log1.name </span><br><span class="line">             <span class="keyword">FROM</span>  <span class="keyword">log</span>  <span class="keyword">JOIN</span>  log1  <span class="keyword">ON</span>  log.id  = log1.id;</span><br></pre></td></tr></table></figure><h4 id="2、自动的MapJoin"><a href="#2、自动的MapJoin" class="headerlink" title="2、自动的MapJoin"></a>2、自动的MapJoin</h4><p>​           通过修改以下配置启用自动的mapjoin：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.auto.convert.join = true;</span><br></pre></td></tr></table></figure><p>​    （  该参数为true时，Hive自动对左边的表统计数据量，如果是小表就加入内存，即对小表使用Map join）</p><p>其他相关配置参数：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.mapjoin.smalltable.filesize;</span><br></pre></td></tr></table></figure><p>（默认：大表小表判断的阈值25MB左右，如果表的大小小于该值则会被加载到内存中运行，可自定义）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.ignore.mapjoin.hint;</span><br></pre></td></tr></table></figure><p>（默认值：true；是否忽略mapjoin hint 即mapjoin标记；如果为false，这则需要添加-MapJoin标记，mapjoin（smalltable））</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.auto.convert.join.noconditionaltask;</span><br></pre></td></tr></table></figure><p>（默认值：true；将普通的join转化为普通的mapjoin时，是否将多个mapjoin转化为一个mapjoin）</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.auto.convert.join.noconditionaltask.size;</span><br></pre></td></tr></table></figure><p>（默认：10M；将多个mapjoin转化为一个mapjoin时，其表的最大值为10M，可自定义）</p><h2 id="六、Map-Side聚合"><a href="#六、Map-Side聚合" class="headerlink" title="六、Map-Side聚合"></a>六、Map-Side聚合</h2><blockquote><p>相当于聚合函数：count（）</p></blockquote><p>设置参数，开启在Map端的聚合(相当于combiner)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>相关配置参数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval；</span><br></pre></td></tr></table></figure><p>（默认为：100000，表示 map端group by执行聚合时处理的多少行数据）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction；</span><br></pre></td></tr></table></figure><p>（默认为：0.5，进行聚合的最小比例，预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.percentmemory;</span><br></pre></td></tr></table></figure><p>（默认： 0.5 ，map端聚合使用的内存的最大值）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold;</span><br></pre></td></tr></table></figure><p>（默认为：0.9，map端做聚合操作是hash表的最大可用内容，大于该值则会触发flush</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata；</span><br></pre></td></tr></table></figure><p>（默认为：false，是否对GroupBy产生的数据倾斜做优化）</p><p><code>附加：</code></p><blockquote><ul><li>数据倾斜问题解决：多种方式（使用MapJoin、使用MapSide）</li></ul></blockquote><p><code>参考</code></p><p><a href="http://www.sohu.com/a/224276626_543508" target="_blank" rel="noopener">http://www.sohu.com/a/224276626_543508</a></p><h2 id="七、控制Hive中Map和Reduce的数量"><a href="#七、控制Hive中Map和Reduce的数量" class="headerlink" title="七、控制Hive中Map和Reduce的数量"></a>七、控制Hive中Map和Reduce的数量</h2><h3 id="1、Map数量相关的参数"><a href="#1、Map数量相关的参数" class="headerlink" title="1、Map数量相关的参数"></a>1、Map数量相关的参数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size;</span><br></pre></td></tr></table></figure><p>（默认为：256M，一个split的最大值，即每个map处理文件的最大值）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node;</span><br></pre></td></tr></table></figure><p>(一个节点上最小split数：1个)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack;</span><br></pre></td></tr></table></figure><p>(一个机架上最小split数：1个)</p><h3 id="2、Reduce数量相关的参数"><a href="#2、Reduce数量相关的参数" class="headerlink" title="2、Reduce数量相关的参数"></a>2、Reduce数量相关的参数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure><p>(默认为：-1，强制指定reduce任务的数量。-1，是未定义，不发挥作用。如果指定了，就会按指定的数量执行)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;</span><br></pre></td></tr></table></figure><p>（默认为：256M ，每个reduce任务处理的数据量）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;</span><br></pre></td></tr></table></figure><p>（默认为：1009个，每个任务最大的reduce数 [Map数量 &gt;= Reduce数量 ]）</p><h2 id="八、Hive-JVM重用"><a href="#八、Hive-JVM重用" class="headerlink" title="八、Hive - JVM重用"></a>八、Hive - JVM重用</h2><p><code>适用场景：</code><br>1、小文件个数过多<br>2、task个数过多</p><p><code>原理：</code></p><p>hadoop默认配置是使用派生JVM来执行map和reduce任务的，JVM重用可以使得JVM实例在同一个JOB中重新使用N次</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.reuse.jvm.num.tasks;</span><br></pre></td></tr></table></figure><p>(默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM)</p><p><code>缺点：</code></p><p>设置开启之后，task插槽会一直占用资源，不论是否有task运行，<br>直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！</p>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive中常用的UDF函数总结</title>
      <link href="/2019/01/12/Hive%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84UDF%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/"/>
      <url>/2019/01/12/Hive%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84UDF%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>一、网络资源</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">1、类型转换</span><br><span class="line"></span><br><span class="line">cast(expr as &lt;type&gt;)  </span><br><span class="line"></span><br><span class="line">如： cast('1' as BIGINT) 字符串转换为数字</span><br><span class="line"></span><br><span class="line">2、if语句</span><br><span class="line"></span><br><span class="line">if(boolean testCondition, T valueTrue, T valueFalseOrNull)</span><br><span class="line"></span><br><span class="line">如果 testCondition 为 true 返回 valueTrue， 否则返回 valueFalse 或 Null</span><br><span class="line"></span><br><span class="line">如： if(1 == 1, 1, 2) 结果为1</span><br><span class="line"></span><br><span class="line">3、case语句</span><br><span class="line"></span><br><span class="line">CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END</span><br><span class="line"></span><br><span class="line">如：case when a == b then b when a == c then c else d <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="number">4</span>、字符串连接</span><br><span class="line"></span><br><span class="line"><span class="keyword">concat</span>(string1, string2, ...)</span><br><span class="line"></span><br><span class="line">如：<span class="keyword">concat</span>(<span class="string">'hello'</span>, <span class="string">' word'</span>) 结果为 hello word</span><br><span class="line"></span><br><span class="line"><span class="number">5</span>、计算字符串长度</span><br><span class="line"></span><br><span class="line"><span class="keyword">length</span>(<span class="keyword">string</span>)</span><br><span class="line"></span><br><span class="line">如：<span class="keyword">length</span>(<span class="string">'hello'</span>) 结果为<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="number">6</span>、查找子串的位置</span><br><span class="line"></span><br><span class="line"><span class="keyword">locate</span>(<span class="keyword">string</span> <span class="keyword">substr</span>, <span class="keyword">string</span> <span class="keyword">str</span>[, <span class="built_in">int</span> pos])</span><br><span class="line"></span><br><span class="line">如：<span class="keyword">locate</span>(<span class="string">'%'</span>, <span class="string">'100%'</span>) 返回<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="number">7</span>、聚合某一列数据</span><br><span class="line"></span><br><span class="line">collect_set(<span class="keyword">col</span>)    会去重</span><br><span class="line"></span><br><span class="line">collect_list(<span class="keyword">col</span>)    不会去重</span><br><span class="line"></span><br><span class="line">这两个函数均会返回一个索引数组</span><br><span class="line"></span><br><span class="line">将数组转换为分割符分割的字符串，如下</span><br><span class="line"></span><br><span class="line"><span class="keyword">concat_ws</span>(<span class="string">' '</span>, collect_set(tblsecondtagmap.tag_name)) </span><br><span class="line"><span class="number">8</span>、将数组或者<span class="keyword">map</span>类型的数据分成多行</span><br><span class="line"></span><br><span class="line">explode(<span class="built_in">ARRAY</span>&lt;T&gt; a)</span><br><span class="line"></span><br><span class="line">explode(<span class="keyword">MAP</span>&lt;Tkey,Tvalue&gt; m)</span><br><span class="line"></span><br><span class="line">如：</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> explode(<span class="built_in">array</span>(<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>));   </span><br><span class="line"></span><br><span class="line">对应abc三行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">A</span><br><span class="line">B</span><br><span class="line">C</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> explode(<span class="keyword">map</span>(<span class="string">'A'</span>,<span class="number">10</span>,<span class="string">'B'</span>,<span class="number">20</span>,<span class="string">'C'</span>,<span class="number">30</span>));</span><br><span class="line"></span><br><span class="line">对应键值对三行</span><br><span class="line"></span><br><span class="line">A10</span><br><span class="line">B20</span><br><span class="line">C30</span><br><span class="line">9、解析json数据</span><br><span class="line"></span><br><span class="line">get_json_object(string json_string, string path)</span><br><span class="line"></span><br><span class="line">path在不同的hive版本中支持情况不同</span><br><span class="line"></span><br><span class="line">$ : json对象的根</span><br><span class="line"></span><br><span class="line">. : 子对象的操作符</span><br><span class="line"></span><br><span class="line">[] : 数组类型的下标形式</span><br><span class="line"></span><br><span class="line">* :  通配符，结合 [] 一起使用</span><br><span class="line"></span><br><span class="line">如：get_json_object('&#123;"name":"bob"&#125;', '$.name')  返回bob</span><br><span class="line"></span><br><span class="line">       get_json_object('&#123;"name":["own","one"]&#125;','$.name[]') 返回 ["own","one"]</span><br><span class="line"></span><br><span class="line">       get_json_object('&#123;"name":["own","one"]&#125;','$.name[0]') 返回 own</span><br><span class="line"></span><br><span class="line">10、支持的复杂数据类型</span><br><span class="line"></span><br><span class="line">array  数组类型，类比索引数组</span><br><span class="line"></span><br><span class="line">map   map类型， 类比关联数组</span><br><span class="line"></span><br><span class="line">11、支持rlike语句</span><br><span class="line"></span><br><span class="line">rlike支持正则表达式。如：</span><br><span class="line"></span><br><span class="line">title rlike '^.*?医.*?(公司|院|网|中心|会|联盟|所|门诊|店|厂|门户|集团|美容|整型).*?$'</span><br><span class="line"></span><br><span class="line">12、字母大小写转换</span><br><span class="line"></span><br><span class="line">upper(string A)   ucase(string A)    将字符串转换为大写字母</span><br><span class="line"></span><br><span class="line">lower(string A)    lcase(string A)     将字符串转换为小写字母</span><br><span class="line"></span><br><span class="line">13、时间戳与时间的转换</span><br><span class="line"></span><br><span class="line">from_unixtime(bigint unixtime[, string format])    将时间戳转换为时间，形如“2008-10-07 03:28:54”这种的形式</span><br><span class="line"></span><br><span class="line">unix_timestamp(string date)                                   将时间转换为时间戳，将形如“2008-10-07 03:28:54”这种形式的时间转换为时间戳</span><br><span class="line"></span><br><span class="line">14、获取时间或者日期</span><br><span class="line"></span><br><span class="line">year(string date)         年    </span><br><span class="line"></span><br><span class="line">month(string date)      月</span><br><span class="line"></span><br><span class="line">day(string date)          日</span><br><span class="line"></span><br><span class="line">hour(string date)         小时</span><br><span class="line"></span><br><span class="line">minute(string date)      分钟</span><br><span class="line"></span><br><span class="line">second(string date)     秒</span><br><span class="line"></span><br><span class="line"><span class="comment">--PS</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--前三个函数支持‘2008-10-07 03:28:54’ ‘2008-10-07’ 这两种形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--后三个函数支持‘2008-10-07 03:28:54’ ‘03:28:54’ 这两种形式</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习</title>
      <link href="/2019/01/11/Hive%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/11/Hive%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Hive是什么？"><a href="#一、Hive是什么？" class="headerlink" title="一、Hive是什么？"></a>一、Hive是什么？</h2><h3 id="1、基于-Hadoop-的一个数据仓库工具"><a href="#1、基于-Hadoop-的一个数据仓库工具" class="headerlink" title="1、基于 Hadoop 的一个数据仓库工具"></a>1、基于 Hadoop 的一个<code>数据仓库工具</code></h3><ul><li>可以将<code>结构化</code>的数据文件映射为一张<code>hive数据库表</code>；</li><li>这张Hive数据库表保存不了metadata元数据信息，而是将metadata存储在本地磁盘上的MySQL（关系型数据库）中</li><li>并提供简单的 sql 查询功能；</li><li>可以将 sql 语句转换为 MapReduce 任务进行运行。</li></ul><h3 id="2、快速实现简单的MapReduce-统计的工具"><a href="#2、快速实现简单的MapReduce-统计的工具" class="headerlink" title="2、快速实现简单的MapReduce 统计的工具"></a>2、快速实现简单的MapReduce 统计的工具</h3><ul><li>方便非Java编程者对HDFS的数据做mapreduce操作；</li><li>学习成本低，十分适合数据仓库的统计分析。</li></ul><h3 id="3、什么是数据仓库？"><a href="#3、什么是数据仓库？" class="headerlink" title="3、什么是数据仓库？"></a>3、什么是数据仓库？</h3><ul><li>Data Warehouse(DW 或DWH）是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。</li><li>单个数据存储，出于分析性报告和决策支持目的而创建。</li><li>为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制.</li><li><strong>数据仓库</strong>是用来做<strong>查询分析的数据库</strong>，<strong>基本不用来做插入，修改，删除操作</strong>。</li></ul><h3 id="4、数据处理的两大分类"><a href="#4、数据处理的两大分类" class="headerlink" title="4、数据处理的两大分类"></a>4、数据处理的两大分类</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz2wawi0ujj30h106c0tg.jpg" alt="oltp+olap"><span class="img-alt">oltp+olap</span></p><ul><li><h4 id="联机事务处理OLTP（on-line-transaction-processing）"><a href="#联机事务处理OLTP（on-line-transaction-processing）" class="headerlink" title="联机事务处理OLTP（on-line transaction processing）"></a>联机事务处理OLTP（on-line transaction processing）</h4></li></ul><blockquote><p>OLTP是传统的关系型<a href="http://lib.csdn.net/base/mysql" target="_blank" rel="noopener">数据库</a>的主要应用，主要是基本的、日常的事务处理，例如银行交易。</p><p>OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作；</p></blockquote><ul><li><h4 id="联机分析处理OLAP（On-Line-Analytical-Processing）"><a href="#联机分析处理OLAP（On-Line-Analytical-Processing）" class="headerlink" title="联机分析处理OLAP（On-Line Analytical Processing）"></a>联机分析处理OLAP（On-Line Analytical Processing）</h4></li></ul><blockquote><p>OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。</p><p>OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。</p></blockquote><ul><li>数据文件按结构的分类</li></ul><blockquote><p>结构化数据：关系型</p><p>半结构化数据：K-V</p><p>松散型：</p></blockquote><p>原理：</p><p>Hive包括：解释器、编译器、优化器</p><p>其中，编译器将一个HiveSQL 转换为操作符，<code>操作符</code>是Hive的最小的处理单位，每一个操作符代表HDFS的一个操作或一个MapReduce作业。</p><h2 id="二、Hive架构原理"><a href="#二、Hive架构原理" class="headerlink" title="二、Hive架构原理"></a>二、Hive架构原理</h2><p><img src="https://img-blog.csdnimg.cn/20181113225516701.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d3d3p5ZGNvbQ==,size_16,color_FFFFFF,t_70" alt="Hive架构图"><span class="img-alt">Hive架构图</span></p><p>1、架构图解释：</p><blockquote><p>Hive通过用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口</p></blockquote><p>2、用户接口</p><blockquote><p> 主要有三个：Client CLI(hive shell 命令行)，JDBC/ODBC(java访问hive)，WEBUI(浏览器访问hive)</p><p>​         其中最常用的是<strong>CLI命令行</strong>，Cli启动的时候，会同时启动一个Hive副本；<strong>Client</strong>是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。</p></blockquote><p>3、元数据:Metastore </p><blockquote><p>元数据包括:</p><p>表名,表所属数据库(默认是default) ,表的拥有者,列/分区字段,表的类型(是否是外部表),表的数据所在目录等；</p><p>默认存储在自带的derby数据库中,推荐使用MySQL存储Metastore</p></blockquote><p>4、任务运行</p><blockquote><p>Hive 使用HDFS进行存储,使用MapReduce进行计算</p><p>(0)驱动器:Driver</p><p>(1)解析器(SQL Parser):将SQL字符转换成抽象语法树AST,这一步一般使用都是第三方工具库完成,比如antlr,对AST进行语法分析,比如表是否存在,字段是否存在,SQL语句是否有误</p><p>(2)编译器(Physical Plan):将AST编译生成逻辑执行计划 </p><p>(3)优化器(Query Optimizer):对逻辑执行计划进行优化</p><p>(4)执行器(Execution):把逻辑执行计划转换成可以运行的物理计划,对于Hive来说,就是MR/Spark</p><p>  其中(select *  不会产生MR任务)</p></blockquote><h2 id="三、Hive搭建及三种模式"><a href="#三、Hive搭建及三种模式" class="headerlink" title="三、Hive搭建及三种模式"></a>三、Hive搭建及三种模式</h2><h3 id="1、Hive的安装配置："><a href="#1、Hive的安装配置：" class="headerlink" title="1、Hive的安装配置："></a>1、Hive的安装配置：</h3><h4 id="（1）基本环境：Hadoop集群环境（至少3个节点）"><a href="#（1）基本环境：Hadoop集群环境（至少3个节点）" class="headerlink" title="（1）基本环境：Hadoop集群环境（至少3个节点）"></a>（1）基本环境：Hadoop集群环境（至少3个节点）</h4><blockquote><p><strong>Hive</strong>是依赖于hadoop系统的，因此在运行Hive之前需要保证已经搭建好hadoop集群环境。</p></blockquote><h4 id="（2）安装一个关系型数据mysql"><a href="#（2）安装一个关系型数据mysql" class="headerlink" title="（2）安装一个关系型数据mysql"></a>（2）安装一个关系型数据mysql</h4><blockquote><p>因为Hive数据仓库的元数据信息是存放在本地磁盘的关系数据库上的</p></blockquote><p><code>安装步骤</code>：详见  “Linux系统数据库MySQL安装.md”</p><h4 id="（3）解压安装（按需在指定节点上）"><a href="#（3）解压安装（按需在指定节点上）" class="headerlink" title="（3）解压安装（按需在指定节点上）"></a>（3）解压安装（按需在指定节点上）</h4><blockquote><p>解压apache-hive-1.2.1-bin.tar.gz</p></blockquote><h4 id="（4）追加配置环境变量"><a href="#（4）追加配置环境变量" class="headerlink" title="（4）追加配置环境变量"></a>（4）追加配置环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HIVE_HOME=Hive的解压路径</span><br><span class="line"></span><br><span class="line">HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><h4 id="（5）替换和添加相关jar包"><a href="#（5）替换和添加相关jar包" class="headerlink" title="（5）替换和添加相关jar包"></a>（5）替换和添加相关jar包</h4><blockquote><ul><li><p>修改HADOOP_HOME/share/hadoop/yarn/lib目录下的jline-*.jar </p><p>将其替换成HIVE_HOME/lib下的<code>jline-2.12.jar</code>。 </p></li></ul></blockquote><blockquote><ul><li><p>–将如下(<code>hive连接mysql)</code>的jar包拷贝到hive解压目录的lib目录下</p><p><code>mysql-connector-java-5.1.32-bin.jar</code></p></li></ul></blockquote><h4 id="（6）修改配置文件（选择3种模式里哪一种）"><a href="#（6）修改配置文件（选择3种模式里哪一种）" class="headerlink" title="（6）修改配置文件（选择3种模式里哪一种）"></a>（6）修改配置文件（选择3种模式里哪一种）</h4><p><code>见三种安装模式</code></p><h4 id="（7）启动"><a href="#（7）启动" class="headerlink" title="（7）启动"></a>（7）启动</h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> hive </span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>启动hive交互式界面</p></blockquote><h3 id="2、三种模式"><a href="#2、三种模式" class="headerlink" title="2、三种模式"></a>2、三种模式</h3><table><thead><tr><th>三种模式</th></tr></thead><tbody><tr><td>A、内嵌模式（元数据保存在内嵌的derby中，允许一个会话链接，尝试多个会话链接时会报错）【了解】                                                                                                        B、本地模式（本地安装mysql 替代derby存储元数据）【重要】                                                                                  C、远程模式（远程安装mysql 替代derby存储元数据）【重要】</td></tr></tbody></table><h4 id="（1）内嵌Derby单用户模式（了解）"><a href="#（1）内嵌Derby单用户模式（了解）" class="headerlink" title="（1）内嵌Derby单用户模式（了解）"></a>（1）内嵌Derby单用户模式（了解）</h4><ul><li>元数据是内嵌在Derby数据库中的，只能允许一个会话连接，数据会存放到HDFS上。</li><li>存储方式简单，只需要hive-site.xml </li><li>注：使用 derby<br>存储方式时，运行 hive 会在当前目录生成一个 derby 文件和一个metastore_db</li></ul><p>hive-site.xml ：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:derby:;databaseName=metastore_db;create=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.derby.jdbc.EmbeddedDriver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="（2）本地用户模式（重要，多用于本地开发测试）"><a href="#（2）本地用户模式（重要，多用于本地开发测试）" class="headerlink" title="（2）本地用户模式（重要，多用于本地开发测试）"></a>（2）本地用户模式（<code>重要</code>，多用于本地开发测试）</h4><table><thead><tr><th>与嵌入式的区别</th></tr></thead><tbody><tr><td><em> 不再使用内嵌的Derby作为元数据的存储介质，而是使用其他数据库比如MySQL来存储元数据且是一个多用户的模式，运行多个用户client连接到一个数据库中。这种方式一般作为公司内部同时使用Hive。                                                                                                                                                                               </em> 这里有一个前提，每一个用户必须要有对MySQL的访问权利，即每一个客户端使用者需要知道MySQL的用户名和密码才行。</td></tr></tbody></table><ul><li>需要在本地运行一个 mysql 服务器</li><li>在node00上（与MySQL在同一个节点上）解压安装Hive</li></ul><blockquote><p>MySQL， Hive      :  node00</p></blockquote><ul><li>需要将连接mysql 的 jar 包（mysql-connector-java-5.1.32-bin.jar）拷贝到$HIVE_HOME/lib 目录下</li></ul><p>hive-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive_local/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node00:3306/hive_local?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>注意</code>：需要实现在mysql中创建数据库：hive_local</p><h4 id="（3）远程模式（重要）"><a href="#（3）远程模式（重要）" class="headerlink" title="（3）远程模式（重要）"></a>（3）远程模式（重要）</h4><ul><li><h5 id="remote-一体"><a href="#remote-一体" class="headerlink" title="remote 一体"></a>remote 一体</h5></li></ul><blockquote><p>将Hive解压安装与MySQL不同的节点上</p><p>MySQL  ：node00</p><p>Hive  ： node02</p><p>需要在 Hive服务器启动 meta服务</p><p>Hive的服务端和客户端在同一台节点</p></blockquote><p>配置hive-site.xml（在 node02节点上）</p><p>(hadoop 2.6.5)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive_remote/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node02:3306/hive_remote?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果在hadoop2.5.X环境下还需要添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node02:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>注</code><strong>：</strong>这里把hive的服务端和客户端都放在同一台服务器上了。服务端和客户端可以拆开</p><ul><li><h5 id="Remote-分开-公司企业经常用"><a href="#Remote-分开-公司企业经常用" class="headerlink" title="Remote 分开(公司企业经常用)"></a>Remote 分开(公司企业经常用)</h5></li></ul><p>将 hive-site.xml 配置文件拆为如下两部分（此时不与MySQL在同一台节点上）</p><blockquote><p>MySql  ：   node00</p><p>服务端 ：   node02</p><p>客户端 ：   node01</p></blockquote><p><strong>1</strong>）、服务端配置文件（node02）</p><p>配置hive-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive2/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node00:3306/hive2?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2</strong>）、客户端配置文件（node01）</p><p> 配置hive-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive2/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--注意这里的路径要和服务端一致---&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node02:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>启动 hive 服务端程序</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore &amp;</span><br></pre></td></tr></table></figure><p><strong>客户端启动</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>Hive常见问题总汇：</p><p><a href="http://blog.csdn.net/freedomboy319/article/details/44828337" target="_blank" rel="noopener">http://blog.csdn.net/freedomboy319/article/details/44828337</a></p><p><a href="https://gengqi88.iteye.com/blog/1983492" target="_blank" rel="noopener">https://gengqi88.iteye.com/blog/1983492</a></p><blockquote><p>如果报错：</p><p>org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083.</p></blockquote><p>查看进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>将启动命令的节点上所以Runjar  进程执行如下kill 命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -9 pid</span><br></pre></td></tr></table></figure><h2 id="四、HQL详解"><a href="#四、HQL详解" class="headerlink" title="四、HQL详解"></a>四、HQL详解</h2><p><code>Hql 就是HiveSQl语句</code></p><h3 id="1、DDL语句（数据库定义语言）"><a href="#1、DDL语句（数据库定义语言）" class="headerlink" title="1、DDL语句（数据库定义语言）"></a>1、DDL语句（数据库定义语言）</h3><h4 id="（1）具体参见：https-cwiki-apache-org-confluence-display-Hive-LanguageManual-DDL"><a href="#（1）具体参见：https-cwiki-apache-org-confluence-display-Hive-LanguageManual-DDL" class="headerlink" title="（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL"></a>（1）具体参见：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</a></h4><p><code>Hive的数据定义语言</code> （<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener">LanguageManual DDL</a>;)）</p><p><strong><code>重点</code> hive 的建表语句和分区。</strong></p><h4 id="（2）创建-删除-修改-使用数据库"><a href="#（2）创建-删除-修改-使用数据库" class="headerlink" title="（2）创建/删除/修改/使用数据库"></a>（2）创建/删除/修改/使用数据库</h4><ul><li><h5 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h5></li></ul><p>（Hive搭建完毕后，会创建一个默认的数据库）</p><blockquote><p>查看    show databases；</p></blockquote><blockquote><p>创建    </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment];</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>举例：</p><p>create database attribute;</p><p>create database attr;</p></blockquote><p><code>注意：</code>创建数据库时，数据库名不要和系统关键字冲突，否则会报错；</p><p>如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">命令：</span><br><span class="line">hive&gt; create database out;</span><br><span class="line"></span><br><span class="line">报错：</span><br><span class="line">FAILED: ParseException line 1:16 Failed to recognize predicate 'out'. Failed rule: 'identifier' in create database statement</span><br><span class="line"></span><br><span class="line">原因：</span><br><span class="line">在Hive1.2.0版本开始增加了如下配置选项，默认值为true：</span><br><span class="line"></span><br><span class="line">hive.support.sql11.reserved.keywords</span><br><span class="line"></span><br><span class="line">该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。</span><br><span class="line"></span><br><span class="line">解决：</span><br><span class="line">法一：弃用这个关键字，换个名字</span><br><span class="line">法二：弃用对保留关键字的支持</span><br><span class="line">在conf下的hive-site.xml配置文件中修改配置选项：</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.sql11.reserved.keywords<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> DROP (DATABASE|SCHEMA) [IF EXISTS] database_name;</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>举例：</p><p>drop database attribute;</p></blockquote></li></ul><ul><li>修改数据库(了解)</li></ul><blockquote><p>ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …);</p><p>ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; </p></blockquote><ul><li><h5 id="使用数据库-（进入某一数据库。如果没有这步操作，会进入默认default数据库）"><a href="#使用数据库-（进入某一数据库。如果没有这步操作，会进入默认default数据库）" class="headerlink" title="使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库）"></a>使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库）</h5></li></ul><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> USE database_name;</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>举例：</p><p>use attr；</p></blockquote><h4 id="（3）创建-删除-表（重点）"><a href="#（3）创建-删除-表（重点）" class="headerlink" title="（3）创建/删除/表（重点）"></a>（3）创建/删除/表（重点）</h4><ul><li><h5 id="创建表（重要！）"><a href="#创建表（重要！）" class="headerlink" title="==创建表（重要！）=="></a>==创建表（重要！）==</h5></li></ul><p>数据类型：</p><blockquote><p>data_type<br>  : <code>primitive_type  原始数据类型</code><br>  | <code>array_type        数组</code><br>  | <code>map_type        map</code><br>  | struct_type<br>  | union_type  – (Note: Available in Hive 0.7.0 and later)</p><p><em>primitive_type</em><br>  : TINYINT<br>  | SMALLINT<br>  | <code>INT</code><br>  | <code>BIGINT</code><br>  | BOOLEAN<br>  | FLOAT<br>  | <code>DOUBLE</code><br>  | DOUBLE PRECISION<br>  | <strong>STRING  基本可以搞定一切</strong><br>  | BINARY<br>  | TIMESTAMP<br>  | DECIMAL<br>  | DECIMAL(precision, scale)<br>  | <code>DATE</code><br>  | VARCHAR<br>  | CHAR  </p><p><em>array_type</em><br>  : <code>ARRAY &lt; data_type &gt;</code></p><p><em>map_type</em><br>  : <code>MAP &lt; primitive_type, data_type &gt;</code></p><p><em>struct_type</em><br>  : STRUCT &lt; col_name : data_type [COMMENT col_comment], …&gt;</p><p><em>union_type</em><br>  : UNIONTYPE &lt; data_type, data_type, … &gt;  </p></blockquote><ul><li><h5 id="1、准备数据"><a href="#1、准备数据" class="headerlink" title="1、准备数据"></a>1、准备数据</h5></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai</span><br><span class="line">2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai</span><br><span class="line">3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing</span><br></pre></td></tr></table></figure><ul><li><h5 id="2、创建表"><a href="#2、创建表" class="headerlink" title="2、创建表"></a>2、创建表</h5></li></ul><p>(如果没有指定进入某一数据库，就会在默认数据库中创建)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">log</span>(</span><br><span class="line"> <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"> <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line"> age <span class="built_in">int</span>,</span><br><span class="line"> likes <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line"> address <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;</span><br><span class="line"> )</span><br><span class="line"> <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line"> COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">by</span> <span class="string">'-'</span></span><br><span class="line"> <span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"> <span class="keyword">lines</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\n'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>导入数据</strong>（属于DML但是为了演示需要在此应用）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br><span class="line"></span><br><span class="line"> [LOCAL]:从本地  |  若无，则为从HDFS</span><br><span class="line"> [OVERWRITE]  ： 会覆盖Hive表中的数据   | 若无则会追加</span><br><span class="line"> </span><br><span class="line"> [PARTITION....] ： 创建分区</span><br></pre></td></tr></table></figure><blockquote><p>将log1文件中的数据加载到log表中</p><p>（log1中数据的格式要和log表格式保持一致，否则会乱；若文件已存在，则会自动重命名）</p><ul><li>本地加载（相当于复制）数据到Hive的制定表中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> LOAD DATA LOCAL INPATH '/root/su/log1' INTO TABLE log;</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><ul><li>HDFS加载（相当于剪切）数据到Hive的制定表中</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> LOAD DATA INPATH '/root/su/log1' INTO TABLE log ;</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><ul><li>查看表中数据</li></ul><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> 对本表查询不会产生MapReduce任务</span><br><span class="line"><span class="meta">&gt;</span> hive&gt; select * from log;</span><br><span class="line"><span class="meta">&gt;</span> 使用函数查询会产生MapReduce任务</span><br><span class="line"><span class="meta">&gt;</span> hive&gt; select count(*) from log;</span><br><span class="line"><span class="meta">&gt;</span> 查询表的字段信息：描述字段类型</span><br><span class="line"><span class="meta">&gt;</span> hive&gt; desc log;</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><p>第一个查询结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1zshang18[&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;]&#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;</span><br><span class="line">1zhaoliu18[&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;]&#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;</span><br><span class="line">2lishi16[&quot;shop&quot;,&quot;boy&quot;,&quot;book&quot;]&#123;&quot;stu_addr&quot;:&quot;hunan&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;</span><br><span class="line">3wang2mazi20[&quot;fangniu&quot;,&quot;eat&quot;]&#123;&quot;stu_addr&quot;:&quot;shanghai&quot;,&quot;work_addr&quot;:&quot;tianjing&quot;&#125;</span><br></pre></td></tr></table></figure><p>第二个查询结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure><p><code>附加题</code></p><blockquote><p>查询表中likes字段中有girl的人</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name from log2 where likes[1]=&quot;girl&quot;;</span><br></pre></td></tr></table></figure><blockquote><p>查询表中address字段有stu_addr为beijing的人</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  select name from log2 where address[&quot;stu_addr&quot;]=&quot;beijing&quot;;</span><br></pre></td></tr></table></figure><ul><li><h5 id="3、删除表"><a href="#3、删除表" class="headerlink" title="3、删除表"></a>3、删除表</h5></li></ul><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; DROP TABLE [IF EXISTS] table_name [PURGE];</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>举例：</p><p>（用drop命令删除表，会将表中数据一并删除，其对应在MySQl中的表的元数据信息也会随之删除；</p><p>​    用hdfs命令删除表对应的文件目录，表中数据也一并删除，但其元数据信息依然保存在My  SQL上，</p><p>​     再load数据，可恢复该表）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </p></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; drop table log1；</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; hdfs dfs -rmr /user/hive_local/warehouse/attr.db/log1</span><br><span class="line">&gt; </span><br><span class="line">&gt; hive&gt; use attr;</span><br><span class="line">&gt; hive&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;</span><br><span class="line">&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><ul><li><h5 id="创建外部表（重要）"><a href="#创建外部表（重要）" class="headerlink" title="创建外部表（重要）"></a>创建外部表（重要）</h5></li></ul><blockquote><p>外部关键字EXTERNAL允许您创建一个表,并提供一个位置,以便hive不使用这个表的默认位置。<strong>这方便如果你已经生成了数据，当删除一个外部表</strong>,<strong>表中的数据不会从文件系统中删除</strong>。外部表指向任何HDFS的存储位置,而不是存储在配置属性指定的文件夹<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.metastore.warehouse.dir&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener"> hive.metastore.warehouse.dir</a>;)中</p></blockquote><p>创建表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create EXTERNAL table log1(</span><br><span class="line"> id int,</span><br><span class="line"> name string,</span><br><span class="line"> age int,</span><br><span class="line"> likes array&lt;string&gt;,</span><br><span class="line"> address map&lt;string,string&gt;</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by &apos;,&apos;</span><br><span class="line"> COLLECTION ITEMS TERMINATED by &apos;-&apos;</span><br><span class="line"> map keys terminated by &apos;:&apos;</span><br><span class="line"> lines terminated by &apos;\n&apos;;</span><br></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;</span><br></pre></td></tr></table></figure><p>删除外部表（<code>相当于删除的是表的元数据信息，而表中的数据还保存</code>）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table log1；</span><br></pre></td></tr></table></figure><p>结果：</p><blockquote><p>hive&gt; show tables;</p><p>无log1</p><p>MySQl中也无此表元数据信息</p><p>但是，</p><p>在HDFS文件系统中，此表数据依然存在</p><p>也就是说，此表还可以恢复</p></blockquote><p>恢复表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">重新创建log1表，该表即可恢复</span><br></pre></td></tr></table></figure><h4 id="（4）修改表-更新，删除数据-这些很少用"><a href="#（4）修改表-更新，删除数据-这些很少用" class="headerlink" title="（4）修改表,更新，删除数据(这些很少用)"></a>（4）修改表,更新，删除数据(这些很少用)</h4><p>重命名表</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; ALTER TABLE table_name RENAME TO new_table_name;</span><br><span class="line">&gt; </span><br><span class="line">&gt; Eg: alter table meninem rename to jacke;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>更新数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE tablename SET column = value [, column = value ...][WHERE expression]</span><br></pre></td></tr></table></figure><p>删除数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE FROM tablename [WHERE expression]</span><br></pre></td></tr></table></figure><h3 id="2、DML语句（数据库管理语言）"><a href="#2、DML语句（数据库管理语言）" class="headerlink" title="2、DML语句（数据库管理语言）"></a>2、DML语句（数据库管理语言）</h3><h4 id="（1）具体参见："><a href="#（1）具体参见：" class="headerlink" title="（1）具体参见："></a>（1）具体参见：</h4><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML</a></p><p>   <strong>重点是数据加载和查询插入语法</strong></p><p>Hive数据操作语言（<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener">LanguageManual DML</a>;)）</p><h4 id="（2）四种插入-导入数据-重要"><a href="#（2）四种插入-导入数据-重要" class="headerlink" title="（2）四种插入/导入数据(重要)"></a>（2）四种插入/导入数据(重要)</h4><blockquote><p>Hive不能很好的支持用<code>insert</code>语句一条一条的进行插入操作，不支持<code>update</code>操作。数据是以<code>load</code>的方式加载到建立好的表中。数据一旦导入就不可以修改。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table log3(</span><br><span class="line"> id int,</span><br><span class="line"> name string,</span><br><span class="line"> age int</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by &apos;,&apos;</span><br><span class="line"> lines terminated by &apos;\n&apos;;</span><br></pre></td></tr></table></figure><h5 id="1-直接加载数据"><a href="#1-直接加载数据" class="headerlink" title="1.直接加载数据"></a>1.直接加载数据</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br><span class="line">load data local inpath '/root/su/log1' into table log1;</span><br></pre></td></tr></table></figure><h5 id="2-将表1查询结果插入表2"><a href="#2-将表1查询结果插入表2" class="headerlink" title="2.将表1查询结果插入表2"></a>2.将表1查询结果插入表2</h5><p><code>注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建person2表，然后从表person1查询数据导入：</span><br><span class="line">覆盖：</span><br><span class="line">INSERT OVERWRITE TABLE person2 [PARTITION(dt=&apos;2008-06-08&apos;, country)]</span><br><span class="line">       SELECT id,name, age From ppt;</span><br><span class="line">追加：</span><br><span class="line">INSERT INTO TABLE log3 </span><br><span class="line">       SELECT id,name, age From log;</span><br></pre></td></tr></table></figure><h5 id="3-将表1、表2查询结果插入表3、表4"><a href="#3-将表1、表2查询结果插入表3、表4" class="headerlink" title="3.将表1、表2查询结果插入表3、表4"></a>3.将表1、表2查询结果插入表3、表4</h5><p><code>注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FROM person t1</span><br><span class="line">INSERT OVERWRITE | INTO TABLE person1 [PARTITION(dt=&apos;2008-06-08&apos;, country)]</span><br><span class="line">       SELECT t1.id, t1.name, t1.age ;</span><br><span class="line">       </span><br><span class="line">FROM log t1,log1 t2 </span><br><span class="line">INSERT OVERWRITE TABLE log4  </span><br><span class="line"> SELECT t1.id,t1.name,t2.age ;</span><br><span class="line"> </span><br><span class="line"> 是否存在笛卡尔积：？？？？存在。</span><br><span class="line"> </span><br><span class="line"> 为了防止笛卡尔积：</span><br><span class="line"> FROM log t1,log1 t2 </span><br><span class="line">INSERT OVERWRITE TABLE log4  </span><br><span class="line"> SELECT t1.id,t1.name,t2.age where t1.id =t2.id;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">【from放前面好处就是后面可以插入多条语句 】</span><br><span class="line">FROM abc t1,sun t2 </span><br><span class="line">INSERT OVERWRITE TABLE qiniu  </span><br><span class="line"> SELECT t1.id,t1.name,t1.age,t2.likes,t2.address ;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM abc t1,sun t2 </span><br><span class="line">INSERT OVERWRITE TABLE qiniu  </span><br><span class="line"> SELECT t1.id,t1.name,t1.age,t1.likes,t1.address where…</span><br><span class="line">INSERT OVERWRITE TABLE wbb</span><br><span class="line"> SELECT t2.id,t2.name,t2.age,t2.likes,t2.address where…;</span><br></pre></td></tr></table></figure><h5 id="4-直接列出数据插入表中（大量数据时不推荐）"><a href="#4-直接列出数据插入表中（大量数据时不推荐）" class="headerlink" title="4.直接列出数据插入表中（大量数据时不推荐）"></a>4.直接列出数据插入表中（大量数据时不推荐）</h5><p><code>注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO TABLE students</span><br><span class="line">   VALUES (1,&apos;zs&apos;,18,&apos;boy&apos;,&apos;beijng&apos;),(2,&apos;wh&apos;,&apos;girl&apos;,&apos;stu_addr&apos;:shanghai&apos;);</span><br></pre></td></tr></table></figure><blockquote><p>本地load数据和从HDFS上load加载数据的过程有什么<code>区别</code>？</p><ul><li><p>本地： local 会自动复制到HDFS上的hive的**目录下</p></li><li><p>Hdfs导入 后移动到hive的**目录下</p></li></ul></blockquote><h4 id="（3）查询数据并保存"><a href="#（3）查询数据并保存" class="headerlink" title="（3）查询数据并保存"></a>（3）查询数据并保存</h4><ol><li><ul><li><h5 id="保存数据到本地："><a href="#保存数据到本地：" class="headerlink" title="保存数据到本地："></a>保存数据到本地：</h5></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory '/opt/datas/hive_exp_emp2'</span><br><span class="line">     ROW FORMAT DELIMITED FIELDS TERMINATED BY ','</span><br><span class="line">         select * from db_1128.emp ;</span><br><span class="line">留意两种的区别：保存的数据格式</span><br><span class="line"></span><br><span class="line">insert overwrite local directory '/sun/temp/hive_save1'</span><br><span class="line">     row format delimited fields terminated by ','</span><br><span class="line">      COLLECTION ITEMS TERMINATED by '-'</span><br><span class="line">      map keys terminated by ':'      </span><br><span class="line">          select * from log2 ;</span><br><span class="line">          </span><br><span class="line">这里如果将 overwrite  改为into 会报错。</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//查看数据</span><br><span class="line">!cat /sun/temp/hive_save1/000000_0;</span><br></pre></td></tr></table></figure><ol start="2"><li><ul><li><h5 id="保存数据到HDFS上："><a href="#保存数据到HDFS上：" class="headerlink" title="保存数据到HDFS上："></a>保存数据到HDFS上：</h5></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/user/beifeng/hive/hive_exp_emp&apos;</span><br><span class="line">     select * from db_1128.emp ;</span><br><span class="line"></span><br><span class="line">insert overwrite directory &apos;/sun/hive/temp/hive_save1&apos;</span><br><span class="line">      row format delimited fields terminated by &apos;,&apos;</span><br><span class="line">      COLLECTION ITEMS TERMINATED by &apos;-&apos;</span><br><span class="line">      map keys terminated by &apos;:&apos;</span><br><span class="line">      select * from log2 ;</span><br><span class="line">     </span><br><span class="line">这里如果将 overwrite  改为into 会报错。</span><br></pre></td></tr></table></figure></li><li><ul><li><h5 id="在外部shell中将数据重定向到文件中："><a href="#在外部shell中将数据重定向到文件中：" class="headerlink" title="在外部shell中将数据重定向到文件中："></a>在外部shell中将数据重定向到文件中：</h5></li></ul></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(注意：需要指明是哪个数据库的表)</span><br><span class="line"># hive -e &quot;select * from attr.log;&quot; &gt; /sun/hive/temp/hive_save2</span><br><span class="line"># cat /sun/hive/temp/hive_save2</span><br></pre></td></tr></table></figure><h4 id="（4）备份数据或还原数据（在HDFS上）"><a href="#（4）备份数据或还原数据（在HDFS上）" class="headerlink" title="（4）备份数据或还原数据（在HDFS上）"></a>（4）备份数据或还原数据（在HDFS上）</h4><ol><li><ul><li>备份数据（包括表的元数据和表中的数据）：</li></ul></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPORT TABLE log to &apos;/sun/hive/datas/export/cp1&apos;</span><br></pre></td></tr></table></figure><ol start="2"><li><ul><li>删除再还原数据：</li></ul></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">先删除表。</span><br><span class="line">drop table log;</span><br><span class="line">show tables ;</span><br><span class="line">再还原数据：</span><br><span class="line">IMPORT FROM &apos;/sun/hive/datas/export/cp1&apos; ;</span><br></pre></td></tr></table></figure><h4 id="（5）其他Hql操作"><a href="#（5）其他Hql操作" class="headerlink" title="（5）其他Hql操作"></a>（5）其他Hql操作</h4><h5 id="Hive的group-by-join-left-join-right-join等-having-sort-by-order-by等操作和MySQL没有什么大的区别："><a href="#Hive的group-by-join-left-join-right-join等-having-sort-by-order-by等操作和MySQL没有什么大的区别：" class="headerlink" title="Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别："></a>Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别：</h5><p><a href="http://www.2cto.com/kf/201609/545560.html" target="_blank" rel="noopener">http://www.2cto.com/kf/201609/545560.html</a></p><h3 id="3、Hive-SerDe（序列化、反序列化）"><a href="#3、Hive-SerDe（序列化、反序列化）" class="headerlink" title="3、Hive SerDe（序列化、反序列化）"></a>3、Hive SerDe（序列化、反序列化）</h3><h4 id="1-定义"><a href="#1-定义" class="headerlink" title="(1)定义"></a>(1)定义</h4><p><strong><code>Hive SerDe</code></strong> - Serializer and Deserializer  SerDe 用于做序列化和反序列化。</p><p>构建在数据存储和执行引擎之间，对两者实现解耦。</p><p>对数据实现序列化，清洗数据，使之成为有效数据并加载。</p><p>Hive通过ROW FORMAT DELIMITED以及SERDE进行内容的读写。</p><h4 id="（2）实现"><a href="#（2）实现" class="headerlink" title="（2）实现"></a>（2）实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">row_format</span><br><span class="line">: DELIMITED </span><br><span class="line">          [FIELDS TERMINATED BY char [ESCAPED BY char]] </span><br><span class="line">          [COLLECTION ITEMS TERMINATED BY char] </span><br><span class="line">          [MAP KEYS TERMINATED BY char] </span><br><span class="line">          [LINES TERMINATED BY char] </span><br><span class="line">: SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Hive正则匹配（实现数据清洗）</span><br><span class="line">创建表 logtbl：</span><br><span class="line"></span><br><span class="line"> CREATE TABLE logtbl (</span><br><span class="line">    host STRING,</span><br><span class="line">    identity STRING,</span><br><span class="line">    t_user STRING,</span><br><span class="line">    time STRING,</span><br><span class="line">    request STRING,</span><br><span class="line">    referer STRING,</span><br><span class="line">    agent STRING)</span><br><span class="line">  ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos;</span><br><span class="line">  WITH SERDEPROPERTIES (</span><br><span class="line"> &quot;input.regex&quot;=&quot;([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \&quot;(.*)\&quot; (-|[0-9]*) (-|[0-9]*)&quot;) </span><br><span class="line">  STORED AS TEXTFILE;</span><br><span class="line">  </span><br><span class="line">加载数据:</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/root/su/localhost_access_log.2016-02-29&apos; into table logtbl;</span><br><span class="line"></span><br><span class="line">查看数据：</span><br><span class="line"></span><br><span class="line">select * from logtbl;</span><br><span class="line"></span><br><span class="line">显示：</span><br><span class="line">//192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br><span class="line"></span><br><span class="line">192.168.57.4--29/Feb/2016:18:14:35 +0800GET /bg-upper.png HTTP/1.1304    -</span><br><span class="line">192.168.57.4--29/Feb/2016:18:14:35 +0800GET /bg-nav.png HTTP/1.1304    -</span><br><span class="line">192.168.57.4--29/Feb/2016:18:14:35 +0800GET /asf-logo.png HTTP/1.1304    -</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">(省略。。。)</span><br></pre></td></tr></table></figure><p><code>表数据--见数据文件：localhost_access_log.2016-02-29.txt</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br></pre></td></tr></table></figure><h2 id="五、Beeline和Hiveserver2（Hive的升级）"><a href="#五、Beeline和Hiveserver2（Hive的升级）" class="headerlink" title="五、Beeline和Hiveserver2（Hive的升级）"></a>五、Beeline和Hiveserver2（Hive的升级）</h2><h4 id="1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）"><a href="#1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）" class="headerlink" title="1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）"></a>1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./hiveserver2</span><br></pre></td></tr></table></figure><p>若已经配置环境变量则启动方式为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hivesever2</span><br></pre></td></tr></table></figure><h4 id="2、启动-beeline（可在服务端-客户端启动，相当于客户端）"><a href="#2、启动-beeline（可在服务端-客户端启动，相当于客户端）" class="headerlink" title="2、启动 beeline（可在服务端|客户端启动，相当于客户端）"></a>2、启动 beeline（可在服务端|客户端启动，相当于客户端）</h4><blockquote><p>因为beeline是在Hive安装目录的/bin下，所以只要有hive包都可以启动</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># ./beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2://node00:10000 root 123456</span><br><span class="line">显示：</span><br><span class="line">Connecting to jdbc:hive2://node00:10000</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://node00:10000&gt;</span><br><span class="line">使用：列出数据库</span><br><span class="line">0: jdbc:hive2://node00:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| attr           |</span><br><span class="line">| attribute      |</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">3 rows selected (7.493 seconds)</span><br><span class="line">0: jdbc:hive2://node00:10000&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">而在服务端：</span><br><span class="line">显示：</span><br><span class="line"></span><br><span class="line">[root@node00 ~]# hiveserver2</span><br><span class="line">19/01/07 08:52:09 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not exist</span><br><span class="line">OK</span><br><span class="line">OK</span><br><span class="line">OK</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">退出：</span><br><span class="line">服务端：ctrl + c</span><br><span class="line">客户端：！quit；  或 ctrl + c</span><br><span class="line"></span><br><span class="line">作用：</span><br><span class="line">对操作结果添加了美化。不过不太常用，耗内存，数据大的时候，还影响页面。</span><br></pre></td></tr></table></figure><h2 id="六、Hive的JDBC"><a href="#六、Hive的JDBC" class="headerlink" title="六、Hive的JDBC"></a>六、Hive的JDBC</h2><blockquote><p>一般是平台使用展示或接口，服务端启动hiveserver2后，在java代码中通过调用hive的jdbc访问默认端口10000进行连接、访问</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HivejdbcClient</span> </span>&#123;    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String driverName = <span class="string">"org.apache.hive.jdbc.HiveDriver"</span>;   </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;        </span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (ClassNotFoundException)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.exit(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">// repalace "hive" here with the name of user the queries should run as</span></span><br><span class="line">        Connection con = DriverManager.getConnection(<span class="string">"jdbc:hive2://node00:10000/default"</span>,<span class="string">"root"</span>,<span class="string">"123456"</span>);</span><br><span class="line">        Statement stmt = con.createStatement();</span><br><span class="line">        String sql = <span class="string">"select * from log limit 0"</span>;</span><br><span class="line">        ResultSet rs = stmt.executeQuery(sql);</span><br><span class="line">        <span class="keyword">while</span>(rs.next())&#123;</span><br><span class="line">            System.out.println(rs.getInt(<span class="number">1</span>)+<span class="string">"-"</span>+rs.getString(<span class="string">"name"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="七、-Hive分区与自定义函数UDF-UDAF-UDTF"><a href="#七、-Hive分区与自定义函数UDF-UDAF-UDTF" class="headerlink" title="七、==Hive分区与自定义函数UDF  UDAF UDTF=="></a>七、==Hive分区与自定义函数UDF  UDAF UDTF==</h2><h3 id="1、-Hive的分区partition（重要）"><a href="#1、-Hive的分区partition（重要）" class="headerlink" title="1、==Hive的分区partition（重要）=="></a>1、==Hive的分区partition（重要）==</h3><blockquote><p><code>功能：</code></p><p>为了方便海量数据的管理和查询，可以对数据建立分区（可按日期、部门、类型等具体业务）。进行分门别类的管理。</p></blockquote><blockquote><p><code>注意：</code></p><ul><li><p>必须在定义表的时候创建partition分区</p></li><li><p>存储数据时，添加分区字段的数据，直接将数据按分区进行存储。<br>   添加分区时：</p><p>   ​             时间的格式：/   ： 存储时会乱码，用   -  不会。<br>   ​             需要指定分区<br>   ​             多个分区时，存在父子目录关系，按顺序对应，对应父子<br>   ​             创建表时，已经指定分区个数，就只能填加指定个数的字段数据</p><pre><code>删除分区时：</code></pre><p>   ​            若该分区是父分区的最后一个子区，则父分区也会被删除<br>   ​            若删除父分区，其所有子分区也都会被删除<br>   ​            若删除的分区，分别在多个不同父分区中存在，则都会被删除</p><pre><code>重命名分区时：</code></pre><p>   ​            修改之后的名字不能是已经存在的</p></li><li><strong>注意：在创建 删除多分区等操作时一定要注意分区的先后顺序，他们是父子节点的关系。分区字段不要和表字段相同</strong></li></ul></blockquote><blockquote><p><code>类别：</code></p><ul><li>单分区和多分区</li><li>静态分区和动态分区</li></ul></blockquote><h4 id="（1）创建分区"><a href="#（1）创建分区" class="headerlink" title="（1）创建分区"></a>（1）创建分区</h4><ul><li><h5 id="单分区建表"><a href="#单分区建表" class="headerlink" title="单分区建表"></a>单分区建表</h5></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> day_table(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, </span><br><span class="line"><span class="keyword">content</span> <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line">partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p><code>注意：</code>【单分区表，按天分区，在表结构中存在id，content，dt三列；以dt为文件夹区分】</p><ul><li><h5 id="双分区建表"><a href="#双分区建表" class="headerlink" title="双分区建表"></a>双分区建表</h5></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> day_hour_table (</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">content</span> <span class="keyword">string</span></span><br><span class="line">) </span><br><span class="line">partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>, <span class="keyword">hour</span> <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><p>【双分区表，按天和小时分区，在表结构中新增加了dt和hour两列；先以dt为文件夹，再以hour子文件夹区分】</p><h4 id="（2）添加分区表的分区"><a href="#（2）添加分区表的分区" class="headerlink" title="（2）添加分区表的分区"></a>（2）添加分区表的分区</h4><p>（表已创建，在此基础上添加分区：按什么分区）：</p><p><code>注意：报错</code>：此时添加，要注意分区的个数相对应，否则会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FAILED: ValidationFailureSemanticException Partition spec &#123;dt=2008-08-08, hour=08&#125; contains non-partition columns</span><br></pre></td></tr></table></figure><p><code>注意：报错</code>此时添加，要注意分区的字段名要对应添加，否则会报如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FAILED: ValidationFailureSemanticException Partition spec &#123;d=2008-08-08&#125; contains non-partition columns</span><br></pre></td></tr></table></figure><p><code>注意：</code>一定是存在分区，才可添加</p><p> 添加分区：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name</span><br><span class="line"><span class="keyword">ADD</span> partition_spec [ LOCATION <span class="string">'location1'</span> ] partition_spec [ LOCATION <span class="string">'location2'</span> ] ...</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例： </span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> day_table <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (dt=<span class="string">'2028-08-08'</span>, <span class="keyword">hour</span>=<span class="string">'08'</span>);</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> day_table <span class="keyword">ADD</span> <span class="keyword">PARTITION</span> (dt=<span class="string">'2028-08-08'</span>);</span><br></pre></td></tr></table></figure><h4 id="（3）删除分区"><a href="#（3）删除分区" class="headerlink" title="（3）删除分区"></a>（3）删除分区</h4><p>语法：（– 用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。）</p><p>删除如双分区中的子级分区时，如果仅剩一个子分区，那么父级分区也会被删除。（连坐）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name DROP partition_spec, partition_spec,...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line">ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);</span><br><span class="line"></span><br><span class="line">ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;);</span><br></pre></td></tr></table></figure><h4 id="（4）数据加载进分区表中"><a href="#（4）数据加载进分区表中" class="headerlink" title="（4）数据加载进分区表中"></a>（4）数据加载进分区表中</h4><p>语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> [<span class="keyword">LOCAL</span>] INPATH <span class="string">'filepath'</span> [OVERWRITE] <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1=val1,partcol2=val2 ...)]</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HDFS：</span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> INPATH <span class="string">'/user/pv.txt'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> day_hour_table <span class="keyword">PARTITION</span>(dt=<span class="string">'2008-08-08'</span>, <span class="keyword">hour</span>=<span class="string">'08'</span>);</span><br><span class="line"></span><br><span class="line">本地：</span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">local</span> INPATH <span class="string">'/user/hua/*'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> day_hour <span class="keyword">partition</span>(dt=<span class="string">'2010-07-07'</span>);</span><br></pre></td></tr></table></figure><h4 id="（5）查看表的所有分区"><a href="#（5）查看表的所有分区" class="headerlink" title="（5）查看表的所有分区"></a>（5）查看表的所有分区</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions day_hour_table;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> day_table;</span><br></pre></td></tr></table></figure><h4 id="（6）重命名分区"><a href="#（6）重命名分区" class="headerlink" title="（6）重命名分区"></a>（6）重命名分区</h4><p>语法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">PARTITION</span> partition_spec <span class="keyword">RENAME</span> <span class="keyword">TO</span> <span class="keyword">PARTITION</span> partition_spec1;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> day_table </span><br><span class="line">              <span class="keyword">PARTITION</span> (tian=<span class="string">'2018-05-01'</span>) <span class="keyword">RENAME</span> <span class="keyword">TO</span> <span class="keyword">PARTITION</span> (tain=<span class="string">'2018-06-01'</span>);</span><br></pre></td></tr></table></figure><h4 id="（7）-动态分区-重要-–注意外部表"><a href="#（7）-动态分区-重要-–注意外部表" class="headerlink" title="（7）==动态分区(重要)–注意外部表=="></a>（7）==动态分区(重要)–注意外部表==</h4><ol><li><p>在本地文件/home/grid/a  中写入以下4行数据</p><blockquote><p> aaa,US,CA<br> aaa,US,CB<br> bbb,CA,BB<br> bbb,CA,BC</p></blockquote></li><li><p>建立非分区表并加载数据 </p></li></ol><p><strong>创建表</strong>   info1</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span>  (</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">STRING</span>, </span><br><span class="line">      cty <span class="keyword">STRING</span>, </span><br><span class="line">      st <span class="keyword">STRING</span></span><br><span class="line">) </span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p><strong>加载数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">'/root/su/a'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> info1;</span><br></pre></td></tr></table></figure><p><strong>查看</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> info1;</span><br></pre></td></tr></table></figure><ol start="3"><li>建立外部分区表  info2   , 并动态加载数据  （注意删除外部表的相关事项）</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> info2 (</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">STRING</span></span><br><span class="line">) </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (country <span class="keyword">STRING</span>, state <span class="keyword">STRING</span>);</span><br></pre></td></tr></table></figure><p><strong>实现动态分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode=nonstrict;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode=<span class="number">1000</span>;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> info2 <span class="keyword">PARTITION</span> (country, state) <span class="keyword">SELECT</span> <span class="keyword">name</span>, cty, st <span class="keyword">FROM</span> info1; </span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> info2 <span class="keyword">PARTITION</span> (country, state) <span class="keyword">SELECT</span> <span class="keyword">name</span>, cty, st <span class="keyword">FROM</span> info1; </span><br><span class="line"><span class="comment">--两次插入数据，会有两份相同的数据</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> info2;</span><br></pre></td></tr></table></figure><p><code>注意：</code>添加分区数据时，分区字段按顺序放在查询的最后。</p><ol start="4"><li>使用动态分区需要注意设定以下参数：</li></ol><blockquote><p> <strong>hive.exec.dynamic.partition</strong></p><p>默认值：false</p><p>是否开启动态分区功能，默认false关闭。</p><p>使用动态分区时候，该参数必须设置成true;</p></blockquote><blockquote><p><strong>hive.exec.dynamic.partition.mode</strong></p><p>默认值：strict</p><p>动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。</p><p>一般需要设置为nonstrict</p></blockquote><blockquote><p> <strong>hive.exec.max.dynamic.partitions.pernode</strong></p><p>默认值：100</p><p>在每个执行MR的节点上，最大可以创建多少个动态分区。</p><p>该参数需要根据实际的数据来设定。</p><p>比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p></blockquote><blockquote><p><strong>hive.exec.max.dynamic.partitions</strong></p><p>默认值：1000</p><p>在所有执行MR的节点上，最大一共可以创建多少个动态分区。</p><p>同上参数解释。</p></blockquote><blockquote><p> <strong>hive.exec.max.created.files</strong></p><p>默认值：100000</p><p>整个MR Job中，最大可以创建多少个HDFS文件。</p><p>一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。</p></blockquote><blockquote><p> <strong>hive.error.on.empty.partition</strong></p><p>默认值：false</p><p>当有空分区生成时，是否抛出异常。</p><p>一般不需要设置。</p></blockquote><h3 id="2、自定义函数UDF-UDAF-UDTF"><a href="#2、自定义函数UDF-UDAF-UDTF" class="headerlink" title="2、自定义函数UDF  UDAF UDTF"></a>2、自定义函数UDF  UDAF UDTF</h3><blockquote><p>自定义函数包括三种 UDF、UDAF、UDTF</p><p>UDF：一进一出</p><p>UDAF：聚集函数，多进一出。如：Count/max/min</p><p>UDTF：一进多出，如 lateralview  explore()，（类似于mysql中的视图）</p><p><strong>使用方式</strong> ：在HIVE会话中add自定义函数的jar 文件，然后创建 function 继而使用函数</p></blockquote><h4 id="（1）UDF-开发（用的多一点）"><a href="#（1）UDF-开发（用的多一点）" class="headerlink" title="（1）UDF 开发（用的多一点）"></a>（1）UDF 开发（用的多一点）</h4><p>Hive的函数课参考官网，用时查阅即可： <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</a></p><p><strong>1、</strong>UDF函数可以直接应用于 select 语句，对查询结构做格式化处理后，再输出内容。</p><p><strong>2、</strong>编写 UDF 函数的时候需要注意一下几点：</p><p>  a）自定义 UDF 需要<code>继承</code> org.apache.hadoop.hive.ql.<code>UDF</code>。</p><p>  b）需要<code>实现</code> <code>evaluate</code> 函数，evaluate 函数支持重载。</p><p><strong>3、</strong>步骤</p><p>  a）把程序打包放到目标机器上去；</p><p>（需要hive和hadoop，jdk 的相关jar包）</p><p>函数一：脱敏处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.hive.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TuoMing</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> Text res = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(String string)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 校验参数是否为空</span></span><br><span class="line"><span class="keyword">if</span>(string==<span class="keyword">null</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">        <span class="comment">// 若为单个字符        </span></span><br><span class="line"><span class="keyword">if</span>(string.length()==<span class="number">1</span>)&#123;</span><br><span class="line">res.set(<span class="string">"*"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  String str1 = string.substring(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">  String str2 = string.substring(string.length()-<span class="number">1</span>,string.length());</span><br><span class="line">  res.set(str1+<span class="string">"***"</span>+str2);</span><br><span class="line">  <span class="keyword">return</span> res;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数二：add函数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.hive.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Add</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> Text res = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(String num1,String num2)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 校验参数是否为空</span></span><br><span class="line"><span class="keyword">if</span>(num1==<span class="keyword">null</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(num2==<span class="keyword">null</span>)&#123;</span><br><span class="line">res.set(num1);</span><br><span class="line"><span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> n = Integer.parseInt(num1)+Integer.parseInt(num2);</span><br><span class="line">    String str =n+<span class="string">""</span>;</span><br><span class="line">    res.set(str);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  b）进入 hive 客户端，添加 jar 包</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;add jar /root/su/TuoMing.jar;</span><br><span class="line">(相当于添加到环境变量中)</span><br><span class="line">(清除缓存时记得删除jar包： <span class="keyword">delete</span> jar <span class="comment">/*)</span></span><br><span class="line"><span class="comment">delete jar /jar/udf_test.jar;</span></span><br></pre></td></tr></table></figure><p>  c）创建临时函数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;CREATE TEMPORARY FUNCTION add_example AS 'hive.udf.add';</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TEMPORARY</span> <span class="keyword">FUNCTION</span> tm_example <span class="keyword">AS</span> <span class="string">'com.bigdata.hive.udf.TuoMing'</span>;</span><br><span class="line">（as 后面添加的是：包名+类名）</span><br></pre></td></tr></table></figure><p>  d）查询 HQL 语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span>  add_example(<span class="number">8</span>,<span class="number">9</span>)  <span class="keyword">FROM</span>  scores;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span>  add_example(scores.math,scores.art)  <span class="keyword">FROM</span>  scores;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span>  tm_example(<span class="keyword">id</span>)  <span class="keyword">FROM</span>  <span class="keyword">log</span>;</span><br></pre></td></tr></table></figure><p>  e）销毁临时函数：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  DROP  TEMPORARY  FUNCTION  tm_example;</span><br></pre></td></tr></table></figure><h4 id="（2）UDAF自定义聚集函数-用的少"><a href="#（2）UDAF自定义聚集函数-用的少" class="headerlink" title="（2）UDAF自定义聚集函数(用的少)"></a>（2）UDAF自定义聚集函数(用的少)</h4><blockquote><p>多行进一行出，如 sum()、min()，用在 group  by 时</p></blockquote><p><strong>1.</strong>必须<code>继承</code>org.apache.hadoop.hive.ql.exec.<code>UDAF</code>(函数类继承)</p><p>org.apache.hadoop.hive.ql.exec.<code>UDAFEvaluator</code>(内部类 Evaluator 实现 UDAFEvaluator 接口)</p><p><strong>2.</strong>Evaluator 需要实现 <code>init、iterate、terminatePartial、merge、terminate</code> 这几个函数</p><blockquote><ul><li><p>init():类似于构造函数，用于 UDAF 的初始化</p></li><li><p>iterate():接收传入的参数，并进行内部的轮转，返回 boolean</p></li><li><p>terminatePartial():无参数，其为 iterate 函数轮转结束后，返回轮转数据，</p></li></ul><p>类似于 hadoop 的Combinermerge()：接收 terminatePartial 的返回结果，进行数据 merge 操作，</p><p>​                                                                  其返回类型为 boolean </p><ul><li>terminate():返回最终的聚集函数结果</li></ul></blockquote><p>开发一个功能同：</p><blockquote><p>Oracle 的 wm_concat()函数</p><p>Mysql 的 group_concat()</p></blockquote><blockquote><p>Hive  UDF 的数据类型：</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz53mse6n1j30fe0c3jxm.jpg" alt="Hive  UDF 的数据类型："><span class="img-alt">Hive  UDF 的数据类型：</span></p><h4 id="（3）UDTF（用的少一点）"><a href="#（3）UDTF（用的少一点）" class="headerlink" title="（3）UDTF（用的少一点）"></a>（3）UDTF（用的少一点）</h4><p>UDTF：一进多出，如 lateral  view  explode( )  返回一个数组表</p><blockquote><p><strong>Hive Lateral View</strong>   视图</p><p>Lateral View用于和UDTF函数（explode、split）结合来使用。</p><p>首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。</p><p><code>主要解决</code></p><p>在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题</p></blockquote><blockquote><p><code>语法：</code></p><p>LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias)</p></blockquote><blockquote><p><code>例：</code></p><p>统计人员表中共有多少种爱好、多少个城市?</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; select count(distinct(myCol1)), count(distinct(myCol2))，count(distinct(myCol3))from log2 </span><br><span class="line">&gt;       LATERAL VIEW explode(likes) myTable1 AS myCol1 </span><br><span class="line">&gt;       LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select myCol1, myCol2 from log2 </span><br><span class="line">      LATERAL VIEW explode(likes) myTable1 AS myCol1 </span><br><span class="line">      LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;</span><br></pre></td></tr></table></figure><blockquote><p>distinct(myCol1) 表示去重</p></blockquote><blockquote><p>LATERAL VIEW explode(likes) myTable1   AS myCol1   </p><p>将likes查询结果放到mytable1表中，作为字段myCol1     </p></blockquote><blockquote><p> LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;</p><p>将address查询结果放到myTable2 表中，作为字段myCol2，myCol3，因为address是包含K-V的（两个）</p></blockquote><h2 id="八、Hive索引-知道"><a href="#八、Hive索引-知道" class="headerlink" title="八、Hive索引(知道)"></a>八、Hive索引(知道)</h2><blockquote><p>一个表上创建索引：</p><p>使用给定的列表的列作为键创建一个索引。</p><p>详见创建<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/IndexDev#IndexDev-CREATEINDEX&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener">索引</a>;)设计文档。</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">INDEX</span> index_name</span><br><span class="line">  <span class="keyword">ON</span> <span class="keyword">TABLE</span> base_table_name (col_name, ...)</span><br><span class="line">  <span class="keyword">AS</span> index_type</span><br><span class="line">  [<span class="keyword">WITH</span> <span class="keyword">DEFERRED</span> <span class="keyword">REBUILD</span>]</span><br><span class="line">  [IDXPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">  [<span class="keyword">IN</span> <span class="keyword">TABLE</span> index_table_name]</span><br><span class="line">  [</span><br><span class="line">     [ <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> ...] <span class="keyword">STORED</span> <span class="keyword">AS</span> ...</span><br><span class="line">     | <span class="keyword">STORED</span> <span class="keyword">BY</span> ...</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (...)]</span><br><span class="line">  [<span class="keyword">COMMENT</span> <span class="string">"index comment"</span>];</span><br></pre></td></tr></table></figure><h2 id="九、案例实践"><a href="#九、案例实践" class="headerlink" title="九、案例实践"></a>九、案例实践</h2><h3 id="案例一：-基站掉话率"><a href="#案例一：-基站掉话率" class="headerlink" title="案例一：(基站掉话率)"></a>案例一：(基站掉话率)</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz53p74gyrj30fe07sab1.jpg" alt="基站掉话率"><span class="img-alt">基站掉话率</span></p><h4 id="1、创建表"><a href="#1、创建表" class="headerlink" title="1、创建表"></a>1、创建表</h4><p>cell_monitor表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table cell_monitor(</span><br><span class="line">        record_time string,</span><br><span class="line">        imei string,</span><br><span class="line">        cell string,</span><br><span class="line">        ph_num int,</span><br><span class="line">        call_num int,</span><br><span class="line">        drop_num int,</span><br><span class="line">        duration int,</span><br><span class="line">        drop_rate DOUBLE,</span><br><span class="line">        net_type string,</span><br><span class="line">        erl string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;</span><br><span class="line">STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure><p> 结果表cell_drop_monitor</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table cell_drop_monitor(</span><br><span class="line">imei string,</span><br><span class="line">total_call_num int,</span><br><span class="line">total_drop_num int,</span><br><span class="line">d_rate DOUBLE</span><br><span class="line">) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure><h4 id="2、load数据"><a href="#2、load数据" class="headerlink" title="2、load数据"></a><strong>2</strong>、load<strong>数据</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;/root/su/cdr_summ_imei_cell_info.csv&apos; OVERWRITE INTO TABLE cell_monitor;</span><br></pre></td></tr></table></figure><h4 id="3、找出掉线率最高的基站"><a href="#3、找出掉线率最高的基站" class="headerlink" title="3、找出掉线率最高的基站"></a><strong>3</strong>、找出掉线率最高的基站</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from cell_monitor cm </span><br><span class="line">insert overwrite table cell_drop_monitor  </span><br><span class="line">select cm.imei ,sum(cm.drop_num),sum(cm.duration),sum(cm.drop_num)/sum(cm.duration) d_rate </span><br><span class="line">group by cm.imei </span><br><span class="line">sort by d_rate desc;</span><br></pre></td></tr></table></figure><h3 id="案例二：（单词统计）"><a href="#案例二：（单词统计）" class="headerlink" title="案例二：（单词统计）"></a>案例二：（单词统计）</h3><h4 id="1、建表"><a href="#1、建表" class="headerlink" title="1、建表"></a><strong>1</strong>、建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table docs(line string);</span><br><span class="line">create table wc(word string, totalword int);</span><br></pre></td></tr></table></figure><h4 id="2、加载数据"><a href="#2、加载数据" class="headerlink" title="2、加载数据"></a>2、加载数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/wc&apos; into table docs;</span><br></pre></td></tr></table></figure><h4 id="3、统计"><a href="#3、统计" class="headerlink" title="3、统计"></a>3、统计</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from (select explode(split(line, &apos; &apos;)) as word from docs) w </span><br><span class="line">insert into table wc </span><br><span class="line">  select word, count(1) as totalword </span><br><span class="line">  group by word </span><br><span class="line">  order by word;</span><br></pre></td></tr></table></figure><h4 id="4、查询结果"><a href="#4、查询结果" class="headerlink" title="4、查询结果"></a><strong>4</strong>、查询结果</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from wc;</span><br></pre></td></tr></table></figure><h2 id="十、-分桶（重要）"><a href="#十、-分桶（重要）" class="headerlink" title="十、==分桶（重要）=="></a>十、==分桶（重要）==</h2><h3 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h3><blockquote><ul><li><p>主要应用于<code>数据抽样</code>。</p></li><li><p>通过对<code>列值取哈希</code>值的方式，将不同数据放到不同的文件中存储。</p></li><li><p>对Hive中每个<code>表</code>、<code>分区</code>都可以进行分桶。</p></li><li><p>列的哈希值 /桶的个数→<code>决定</code>每条数据划分到哪个桶中</p></li></ul></blockquote><h3 id="2、开启支持分桶"><a href="#2、开启支持分桶" class="headerlink" title="2、开启支持分桶"></a>2、开启支持分桶</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.enforce.bucketing=true;</span><br></pre></td></tr></table></figure><blockquote><p>默认：false；</p><p>设置为true之后，mr运行时会根据bucket的个数自动分配reduce task个数。</p><p>（用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用）</p><p><strong>一次作业产生的桶数 = reducde task数</strong></p></blockquote><h3 id="3、往分桶表中加载数据"><a href="#3、往分桶表中加载数据" class="headerlink" title="3、往分桶表中加载数据"></a>3、往分桶表中加载数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> bucket_table <span class="keyword">select</span> <span class="keyword">columns</span> <span class="keyword">from</span> tbl;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> bucket_table <span class="keyword">select</span> <span class="keyword">columns</span> <span class="keyword">from</span> tbl;</span><br></pre></td></tr></table></figure><h3 id="4、桶表"><a href="#4、桶表" class="headerlink" title="4、桶表"></a>4、桶表</h3><h3 id="抽样查询"><a href="#抽样查询" class="headerlink" title="抽样查询"></a>抽样查询</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> bucket_table tablesample(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">columns</span>);</span><br></pre></td></tr></table></figure><blockquote><p>TABLESAMPLE语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; TABLESAMPLE(BUCKET x OUT OF y)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>x：表示从哪个bucket开始抽取数据，<code>x&lt;=y</code></p><p>y：必须为该表总bucket数的<code>倍数</code>或<code>因子</code></p><p>理解：</p><p>分桶表已经按age分为4桶，然后，有y个人去抽，从第(x 取模 桶数)桶中抽</p></blockquote><h3 id="5、实战"><a href="#5、实战" class="headerlink" title="5、实战"></a>5、实战</h3><p>创建普通表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> mm( </span><br><span class="line"><span class="keyword">id</span> <span class="built_in">INT</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">STRING</span>, </span><br><span class="line">age <span class="built_in">INT</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p>测试数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1,tom,11</span><br><span class="line">2,cat,22</span><br><span class="line">3,dog,33</span><br><span class="line">4,hive,44</span><br><span class="line">5,hbase,55</span><br><span class="line">6,mr,66</span><br><span class="line">7,alice,77</span><br><span class="line">8,scala,88</span><br></pre></td></tr></table></figure><p>加载数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath '/root/su/mm' into table mm;</span><br></pre></td></tr></table></figure><p><strong>创建分桶表</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> psnbucket( </span><br><span class="line"><span class="keyword">id</span> <span class="built_in">INT</span>, </span><br><span class="line"><span class="keyword">name</span> <span class="keyword">STRING</span>, </span><br><span class="line">age <span class="built_in">INT</span></span><br><span class="line">)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (age) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS </span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><p><strong>加载数据：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> psnbucket <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span>, age <span class="keyword">from</span> mm;</span><br></pre></td></tr></table></figure><p><strong>抽样</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span>, age <span class="keyword">from</span> psnbucket tablesample(bucket <span class="number">2</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> age);</span><br></pre></td></tr></table></figure><p><code>注意：</code></p><blockquote><p> hive&gt; select id, name, age from psnbucket tablesample(bucket 4 out of 2 on age);<br>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table psnbucket</p><p>denominator  : 分母</p></blockquote><h2 id="十一、运行方式"><a href="#十一、运行方式" class="headerlink" title="十一、运行方式"></a>十一、运行方式</h2><h3 id="1、Hive运行模式"><a href="#1、Hive运行模式" class="headerlink" title="1、Hive运行模式"></a>1、Hive运行模式</h3><blockquote><h4 id="–-命令行方式cli：控制台模式"><a href="#–-命令行方式cli：控制台模式" class="headerlink" title="– 命令行方式cli：控制台模式"></a>– 命令行方式cli：控制台模式</h4></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--与hdfs交互</span><br><span class="line">  * 执行dfs命令</span><br><span class="line">  * 例 ：hive&gt; dfs -ls /</span><br><span class="line">  </span><br><span class="line">--与Linux交互</span><br><span class="line">   *  ！ 开头</span><br><span class="line">   *  hive&gt; !pwd</span><br></pre></td></tr></table></figure><blockquote><h4 id="–脚本运行方式：（生产中常用）"><a href="#–脚本运行方式：（生产中常用）" class="headerlink" title="–脚本运行方式：（生产中常用）"></a>–脚本运行方式：（生产中常用）</h4></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">在外部shell中执行,指定数据库,分号可加可不加</span><br><span class="line"># hive -e &quot;select * from attr.log &quot;</span><br><span class="line"># hive -e &quot;select * from attr.log；select * from default.log2&quot;</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">将执行结果重定向到指定文件：</span><br><span class="line"># hive -e &quot;select * from attr.log &quot; &gt;&gt;log1</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">静默模式执行，不打印log日志</span><br><span class="line"># hive -S -e &quot;select * from attr.log &quot; &gt;&gt;log1</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">脚本执行</span><br><span class="line">先编辑脚本问价</span><br><span class="line"># vim file1</span><br><span class="line">编辑内容</span><br><span class="line">select * from attr.log where id = 1;</span><br><span class="line">select * from attr.log where id &lt; 3;</span><br><span class="line">执行脚本</span><br><span class="line"># hive -f file1</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">?? 使用命令文件执行hive-init.sql</span><br><span class="line">?? # hive -i /home/hive-init.sql</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">在hive cli中执行脚本文件</span><br><span class="line">hive&gt; source file1</span><br></pre></td></tr></table></figure><h3 id="？未解决？"><a href="#？未解决？" class="headerlink" title="？未解决？"></a>？未解决？</h3><blockquote><p>?? 使用命令文件执行hive-init.sql<br>?? # hive -i /home/hive-init.sql</p></blockquote><h2 id="十二、hive的GUI接口（web页面）"><a href="#十二、hive的GUI接口（web页面）" class="headerlink" title="十二、hive的GUI接口（web页面）"></a>十二、hive的GUI接口（web页面）</h2><p>Hive Web GUI接口</p><h3 id="web界面安装："><a href="#web界面安装：" class="headerlink" title="web界面安装："></a>web界面安装：</h3><p><strong>1、</strong>下载源码包apache-hive-1.2.1-src.tar.gz,</p><p><strong>2、</strong>在本地Windows系统中解压</p><p>并将\apache-hive-1.2.1-src\hwi\web路径中所有的文件打成war包</p><p><code>制作方法：</code></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5b7di85fj30rn0eodhm.jpg" alt="war包"><span class="img-alt">war包</span></p><blockquote><p>1、到\apache-hive-1.2.1-src\hwi\web路径下</p><p>2、在路径栏输入命令：jar -cvf hive-hwi.war *</p><p>3、即可生成文件：hive-hwi.war</p></blockquote><p><strong>3、</strong>将hwi-war包放在$HIVE_HOME/lib/中（Linux系统）</p><p><strong>4、</strong>复制tools.jar(在jdk的lib目录下)到$HIVE_HOME/lib下</p><p><strong>5、</strong>修改hive-site.xml</p><p>路径：/usr/soft/apache-hive-1.2.1-bin/conf/hive-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.listen.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.listen.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>9999<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.war.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lib/hive-hwi.war<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>6、</strong>启动hwi服务(端口号9999)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hwi</span><br></pre></td></tr></table></figure><p><strong>7、</strong>浏览器通过以下链接来访问</p><p><a href="http://node00:9999/hwi/" target="_blank" rel="noopener">http://node00:9999/hwi/</a></p><p><strong>8、</strong>登录页面：</p><p>USER:</p><p>GROUPS:</p><p>自已定义</p><h2 id="十三、权限管理"><a href="#十三、权限管理" class="headerlink" title="十三、权限管理"></a>十三、权限管理</h2><p>Hive - SQL Standards Based Authorization in  HiveServer2</p><h3 id="（1）三种授权模型"><a href="#（1）三种授权模型" class="headerlink" title="（1）三种授权模型"></a>（1）三种授权模型</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5bus4ks8j30kb08o3zp.jpg" alt=""></p><h3 id="（2）常用：基于SQL标准的完全兼容SQL的授权模型"><a href="#（2）常用：基于SQL标准的完全兼容SQL的授权模型" class="headerlink" title="（2）常用：基于SQL标准的完全兼容SQL的授权模型"></a>（2）常用：基于SQL标准的完全兼容SQL的授权模型</h3><p>特点：</p><ul><li><p>支持对于用户的授权认证</p></li><li><p>支持角色role的授权认证</p></li><li><p>role可理解为是一组权限的集合，通过role为用户授权</p><p> 一个用户可以具有一个或多个角色</p><p>​    默认包含俩种角色：public、admin</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5c0cf550j30k406tjtk.jpg" alt="限制"><span class="img-alt">限制</span></p><h3 id="（3）操作"><a href="#（3）操作" class="headerlink" title="（3）操作"></a>（3）操作</h3><p>在<code>hive服务端</code>修改配置文件hive-site.xml添加以下配置内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.users.in.admin.role<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authenticator.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>服务端启动hiveserver2；客户端通过beeline进行连接</strong></p><p>角色的添加、删除、查看、设置：</p><p>第一次操作无权限：</p><p>需要：CREATE ROLE  admin；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE ROLE role_name;   -- 创建角色</span><br><span class="line">DROP ROLE role_name;   -- 删除角色</span><br><span class="line">SET ROLE (role_name|ALL|NONE);  -- 设置角色</span><br><span class="line">SHOW CURRENT ROLES;     -- 查看当前具有的角色</span><br><span class="line">SHOW ROLES;             -- 查看所有存在的角色</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5c6wynhnj30hs07kgmv.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5c8bvsjkj30im077wft.jpg" alt=""></p><p>【官网：权限】</p><table><thead><tr><th>Action</th><th>Select</th><th>Insert</th><th>Update</th><th>Delete</th><th>Owership</th><th>Admin</th><th>URL Privilege(RWX   Permission + Ownership)</th></tr></thead><tbody><tr><td>ALTER DATABASE</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>ALTER INDEX PROPERTIES</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>ALTER INDEX REBUILD</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>ALTER PARTITION LOCATION</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td>Y (for new partition   location)</td></tr><tr><td>ALTER TABLE (all of them   except the ones above)</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>ALTER TABLE ADD PARTITION</td><td></td><td>Y</td><td></td><td></td><td></td><td></td><td>Y (for partition location)</td></tr><tr><td>ALTER TABLE DROP PARTITION</td><td></td><td></td><td></td><td>Y</td><td></td><td></td><td></td></tr><tr><td>ALTER TABLE LOCATION</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td>Y (for new location)</td></tr><tr><td>ALTER VIEW PROPERTIES</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>ALTER VIEW RENAME</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>ANALYZE TABLE</td><td>Y</td><td>Y</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CREATE DATABASE</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Y (if custom location   specified)</td></tr><tr><td>CREATE FUNCTION</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>CREATE INDEX</td><td></td><td></td><td></td><td></td><td>Y (of table)</td><td></td><td></td></tr><tr><td>CREATE MACRO</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>CREATE TABLE</td><td></td><td></td><td></td><td></td><td>Y (of database)</td><td></td><td>Y  (for create   external table – the location)</td></tr><tr><td>CREATE TABLE AS SELECT</td><td>Y (of input)</td><td></td><td></td><td></td><td>Y (of database)</td><td></td><td></td></tr><tr><td>CREATE VIEW</td><td>Y + G</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DELETE</td><td></td><td></td><td></td><td>Y</td><td></td><td></td><td></td></tr><tr><td>DESCRIBE TABLE</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DROP DATABASE</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>DROP FUNCTION</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>DROP INDEX</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>DROP MACRO</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>DROP TABLE</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>DROP VIEW</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>DROP VIEW PROPERTIES</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>EXPLAIN</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>INSERT</td><td></td><td>Y</td><td></td><td>Y (for OVERWRITE)</td><td></td><td></td><td></td></tr><tr><td>LOAD</td><td></td><td>Y (output)</td><td></td><td>Y (output)</td><td></td><td></td><td>Y (input location)</td></tr><tr><td>MSCK (metastore check)</td><td></td><td></td><td></td><td></td><td></td><td>Y</td><td></td></tr><tr><td>SELECT</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SHOW COLUMNS</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SHOW CREATE TABLE</td><td>Y+G</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SHOW PARTITIONS</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SHOW TABLE PROPERTIES</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SHOW TABLE STATUS</td><td>Y</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>TRUNCATE TABLE</td><td></td><td></td><td></td><td></td><td>Y</td><td></td><td></td></tr><tr><td>UPDATE</td><td></td><td></td><td>Y</td><td></td><td></td><td></td></tr></tbody></table><h2 id="十四、-Hive优化（重点）"><a href="#十四、-Hive优化（重点）" class="headerlink" title="十四、==Hive优化（重点）=="></a>十四、==Hive优化（重点）==</h2><p><code>详见Hive优化文档</code></p><h1 id="hive-参数与变量"><a href="#hive-参数与变量" class="headerlink" title="hive 参数与变量"></a>hive 参数与变量</h1><h2 id="1、hive当中的参数、变量"><a href="#1、hive当中的参数、变量" class="headerlink" title="1、hive当中的参数、变量"></a>1、hive当中的参数、变量</h2><p>hive当中的参数、变量，都是以命名空间开头</p><table><thead><tr><th style="text-align:center">命名空间</th><th style="text-align:center">读写权限</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">hiveconf</td><td style="text-align:center">可读写</td><td style="text-align:center">hive-site.xml当中的各配置变量<br>例：hive –hiveconf hive.cli.print.header=true</td></tr><tr><td style="text-align:center">system</td><td style="text-align:center">可读写</td><td style="text-align:center">系统变量，包含JVM运行参数<br>例：system:user.name=root</td></tr><tr><td style="text-align:center">env</td><td style="text-align:center">只读</td><td style="text-align:center">环境变量<br>例：env:JAVA_HOME</td></tr><tr><td style="text-align:center">hivevar</td><td style="text-align:center">可读写</td><td style="text-align:center">例：hive -d val=key</td></tr></tbody></table><p>通过${}方式进行引用，其中system、env下的变量必须以前缀开头</p><h2 id="2、hive-参数设置方式"><a href="#2、hive-参数设置方式" class="headerlink" title="2、hive 参数设置方式"></a>2、hive 参数设置方式</h2><p>1、修改配置文件 ${HIVE_HOME}/conf/hive-site.xml</p><p>2、启动hive cli时，通过–hiveconf key=value的方式进行设置</p><p>例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --hiveconf hive.cli.print.header=true</span><br></pre></td></tr></table></figure><p>3、进入cli之后，通过使用set命令设置</p><h2 id="3、hive-set命令"><a href="#3、hive-set命令" class="headerlink" title="3、hive set命令"></a>3、hive set命令</h2><p>-    在hive CLI控制台可以通过set对hive中的参数进行查询、设置</p><p>-    set设置：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cli.print.header=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>-     set查看</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cli.print.header</span><br></pre></td></tr></table></figure><p>-     hive参数初始化配置</p><p>当前用户家目录下的.hiverc文件</p><p>如:  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/.hiverc</span><br></pre></td></tr></table></figure><p>如果没有，可直接创建该文件，将需要设置的参数写到该文件中，hive启动运行时，会加载改文件中的配置。</p><p>-     hive历史操作命令集</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/.hivehistory</span><br></pre></td></tr></table></figure><p>Hive常用函数：</p><p><a href="https://www.cnblogs.com/kimbo/p/6288516.html" target="_blank" rel="noopener">https://www.cnblogs.com/kimbo/p/6288516.html</a></p><p><a href="https://www.iteblog.com/archives/2258.html#3_avg" target="_blank" rel="noopener">https://www.iteblog.com/archives/2258.html#3_avg</a></p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions</a></p><p>MapReducde底层源码：</p><ol><li><a href="http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD</a></li><li><a href="http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1</a></li><li><a href="http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3</a></li><li><a href="http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce源码分析</title>
      <link href="/2019/01/08/MapReduce%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
      <url>/2019/01/08/MapReduce%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> MapReduce </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS </tag>
            
            <tag> 源码分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce案例实践</title>
      <link href="/2019/01/07/MapReduce%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/"/>
      <url>/2019/01/07/MapReduce%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>端口统计：</p><table><thead><tr><th style="text-align:center">Port</th><th style="text-align:center">使用框架</th></tr></thead><tbody><tr><td style="text-align:center">50070</td><td style="text-align:center">hdfs的http端口</td></tr><tr><td style="text-align:center">9000</td><td style="text-align:center">hadoop1.X的rpc端口</td></tr><tr><td style="text-align:center">8020</td><td style="text-align:center">hadoop2.X的rpc端口</td></tr><tr><td style="text-align:center">8088</td><td style="text-align:center">YARN的http端口</td></tr><tr><td style="text-align:center"></td></tr></tbody></table><h1 id="一、单词字数统计"><a href="#一、单词字数统计" class="headerlink" title="一、单词字数统计"></a>一、单词字数统计</h1><p>Job类</p><blockquote><p>新建Configuration ：</p><p>​     设置hdfs和yarn的配置</p><p>获取Job ：</p><p>​     设置Job类和JobName</p><p>​     设置Map端和Reduce端的类</p><p>​    设置Map端输出的key和value的类</p><p>调用FileInputFormat类添加输入的文件</p><p>调用FileOutputFormat类添加计算结果存放的路径</p></blockquote><p>Map类</p><blockquote><p>将输入的（K，V）转换成新的（K,V）：每个单词计数为1</p><p>通过Context写出</p></blockquote><p>Reduce类</p><blockquote><p>遍历获取的value-list ， 实现累加</p><p>通过Context写出</p></blockquote><h1 id="二、二度人脉推荐"><a href="#二、二度人脉推荐" class="headerlink" title="二、二度人脉推荐"></a>二、二度人脉推荐</h1><p>Job类</p><blockquote><p>新建Configuration ：</p><p>​     设置hdfs和yarn的配置</p><p>获取Job ：</p><p>​     设置Job类和JobName</p><p>​     设置Map端和Reduce端的类</p><p>​    设置Map端输出的key和value的类</p><p>调用FileInputFormat类添加输入的文件（在hdfs上）</p><p>调用FileOutputFormat类添加计算结果存放的路径（在hdfs上）</p><p><em>如上配置两个Job任务。</em></p></blockquote><p>Map01类</p><blockquote><p>根据一度好友，建立排序后的某一用户与好友对应关系，作为key ，用0作为value</p><p>同样  ，建立排序后某一用户好友的好友之间的对应关系，作为key ， 用 1 作为value</p><p>用context写出</p></blockquote><p>Reduce01类</p><blockquote><p>排除好友对应关系中value为 0 的 key ， 统计好友关系中value 为非0 的key的个数 ， 并将该好友关系拼接个数，作为key，用context写出，null为value</p></blockquote><p>Map02类</p><blockquote><p>切分输入的key ， 分别获取好友关系以及个数，写出时，根据个数排序，</p></blockquote><p>Reduce02类</p><blockquote><p>再次合并，输出二度好友关系，按热度排序</p></blockquote><h1 id="三、天气统计每月Top"><a href="#三、天气统计每月Top" class="headerlink" title="三、天气统计每月Top"></a>三、天气统计每月Top</h1><p>存在二次排序：需要定义两个比较器</p><p>分组—排序</p><p>排序–再按温度</p><p>Map类</p><blockquote><p>将输入的数据的格式，转换成所需的文件格式对象，并以（K,V）格式写出</p><p>在写出之前，已经按月分组，并按温度排序</p></blockquote><p>Reduce类</p><blockquote><p>将对象转换成字符串，再以新的（K,V）格式写出</p><p>写出之前只取前两个温度最高的</p></blockquote><h1 id="四、tf-idf-：微博热词重要性搜索"><a href="#四、tf-idf-：微博热词重要性搜索" class="headerlink" title="四、tf-idf ：微博热词重要性搜索"></a>四、tf-idf ：微博热词重要性搜索</h1><p>分成三个Job</p><p>Job1</p><blockquote><p>基本配置以及指定输入输出文件路径</p></blockquote><p>Map1</p><blockquote><p>计算词频 ， 分词器ik  ， 得到的单词拼接微博Id ，作为key ， 以1 为value</p><p>以count为key ， 以 1 为value ， 用来对微博计数</p></blockquote><p>Reduce1</p><blockquote><p>对分词累加 ， 对微博数累加 ， 按分区以新的（K,V） 写出</p></blockquote><p>Job 2 </p><blockquote><p>以Job1 的输出作为输入文件，再指定输出文件路径</p></blockquote><p>Map2</p><blockquote><p>获取所有输入的split ， 对所有单词 ， 以单词为key ， 以  1 为value</p></blockquote><p>Reduce2 </p><blockquote><p>对所有单词进行统计，</p></blockquote><p>Job3</p><blockquote><p>以Job1 的输出文件作为输入文件</p></blockquote><p>Map3 </p><blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> 计算框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MapReduce </tag>
            
            <tag> 分布式离线计算框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MapReduce学习</title>
      <link href="/2019/01/05/MapReduce%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/05/MapReduce%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="一、MapReduce是什么"><a href="#一、MapReduce是什么" class="headerlink" title="一、MapReduce是什么"></a>一、MapReduce是什么</h2><p>1、概念</p><p>MapReduce是一种<code>分布式离线计算框架</code>，是一种编程模型，用于在分布式系统上大规模数据集(大于1TB)的并行运算。</p><p>分布式编程：</p><blockquote><p>借助一个集群，通过多台机器去并行处理大规模数据集，从而获得海量计算能力。</p></blockquote><p>2、理解</p><p><code>Map</code>(映射)</p><p><code>Reduce</code>(归约)</p><blockquote><p>指定一个Map(映射)函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce(归约)函数，用来保证所有映射的键值对中的每一个共享相同的键组。</p></blockquote><h2 id="二、MapReduce设计理念"><a href="#二、MapReduce设计理念" class="headerlink" title="二、MapReduce设计理念"></a>二、MapReduce设计理念</h2><p>1、分布式计算</p><blockquote><p>分布式计算将该应用分解成许多小的部分，分配给多台计算机节点进行处理。这样可以节约整体计算时间，大大提高计算效率。</p></blockquote><p><strong>分而治之</strong>策略：</p><blockquote><p>一个存储在分布式文件系统中的大规模数据集，</p><p>会被切分成许多独立的分片（split），</p><p>这些分片可以被</p><p>多个Map任务并行处理</p></blockquote><p>2、移动计算，而非移动数据</p><blockquote><p>将计算程序应用移动到具有数据的集群计算机节点之上进行计算操作；</p><p>将有用、准确、及时的信息提供给任何时间、任何地点的任何客户。</p></blockquote><p>3、Master/Slave架构</p><blockquote><p>包括一个Master和若干个Slave。<br>Master上运行JobTracker，Slave上运行TaskTracker</p></blockquote><h2 id="三、MapReduce计算框架的组成"><a href="#三、MapReduce计算框架的组成" class="headerlink" title="三、MapReduce计算框架的组成"></a>三、MapReduce计算框架的组成</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6guaswsqj30fd06pwgh.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6hucmi5pj30v90exn7e.jpg" alt="MR"><span class="img-alt">MR</span></p><p>1、 Mapper负责“<strong>分</strong>”，即把得到的复杂的任务分解为若干个“简单的任务”执行。</p><p>​        “简单的任务”：</p><ul><li><p>数据或计算规模相对于原任务要大大缩小；</p></li><li><p>就近计算，即会被分配到存放了所需数据的节点进行计算；</p></li><li>每个map任务之间可以并行计算，不产生任何通信。</li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6hw158klj30f308eq7e.jpg" alt="split"><span class="img-alt">split</span></p><p>2、Split规则：（取三者的中间值）</p><p>–  max.split(100M)</p><p>–  min.split(10M)</p><p>–  block(64M)</p><p><strong>max(min.split,min(max.split,block))</strong></p><p><strong>split实际大小=block大小</strong>（2.X：128M）</p><p>Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6hz81714j30ea07vmzd.jpg" alt=""></p><p>3、Reduce详解（总·重要）</p><p>–  Reduce的任务是对map阶段的结果进行“<strong>汇总</strong>”并输出。</p><p>Reducer的数目由mapred-site.xml配置文件里的项目mapred.reduce.tasks决定。缺省值为1，用户可自定义。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6i3l3zo1j30fe09mq62.jpg" alt=""></p><p>4、Shuffle详解（总·核心）</p><p>– 在mapper和reducer中间的一个步骤</p><p>   可以把mapper的输出按照某种key值重新切分和组合成n份，把key值符合某种范围的输出送到特定的reducer那里去处理。</p><p>–  可以简化reducer过程</p><p>Partitoner ： hash(key) mod R</p><h2 id="四、MapReduce架构"><a href="#四、MapReduce架构" class="headerlink" title="四、MapReduce架构"></a>四、MapReduce架构</h2><h3 id="1、非共享式架构"><a href="#1、非共享式架构" class="headerlink" title="1、非共享式架构"></a>1、非共享式架构</h3><p>每个节点都有自己的内存，容错性比较好。</p><h3 id="2、一主多从架构"><a href="#2、一主多从架构" class="headerlink" title="2、一主多从架构"></a>2、一主多从架构</h3><p>可扩展性好，硬件要求易达到。</p><p>–  主 JobTracker:（ResourceManager资源管理）</p><blockquote><p>负责调度分配每一个子任务task运行于TaskTracker上，</p><p>如果发现有失败的task就重新分配其任务到其他节点。</p><p>每一个hadoop集群中只一个 JobTracker, 一般它运行在Master节点上。</p></blockquote><p>–  从TaskTracker:（NodeManager）</p><blockquote><p>TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务，</p><p>为了减少网络带宽TaskTracker最好运行在HDFS的DataNode上。</p></blockquote><h1 id="MapReduce的体系结构"><a href="#MapReduce的体系结构" class="headerlink" title="MapReduce的体系结构"></a>MapReduce的体系结构</h1><p>MapReduce主要有以下4个部分组成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1 ）Client</span><br><span class="line">•用户编写的MapReduce程序通过Client提交到JobTracker端</span><br><span class="line">•用户可通过Client提供的一些接口查看作业运行状态</span><br><span class="line">2 ）JobTracker</span><br><span class="line">•JobTracker负责资源监控和作业调度</span><br><span class="line">•JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点</span><br><span class="line">•JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器</span><br><span class="line">（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源</span><br><span class="line">3 ）TaskTracker</span><br><span class="line">•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报</span><br><span class="line">给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、</span><br><span class="line">杀死任务等）</span><br><span class="line">•TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到</span><br><span class="line">一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分</span><br><span class="line">配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask和Reduce Task使用</span><br><span class="line">（所以最好放在DataNode上）</span><br><span class="line">4 ）Task</span><br><span class="line">Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6i591yd9j30tp0fqq4x.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6ivspvrcj30y50dujum.jpg" alt=""></p><h2 id="五、MapReduce搭建"><a href="#五、MapReduce搭建" class="headerlink" title="五、MapReduce搭建"></a>五、MapReduce搭建</h2><h3 id="1、节点分布情况"><a href="#1、节点分布情况" class="headerlink" title="1、节点分布情况"></a>1、节点分布情况</h3><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">NN</th><th style="text-align:center">DN</th><th style="text-align:center">JN</th><th style="text-align:center">ZK</th><th style="text-align:center">ZKFC</th><th style="text-align:center">RM</th><th style="text-align:center">NM</th></tr></thead><tbody><tr><td style="text-align:center">node00</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center"></td><td style="text-align:center">√</td></tr><tr><td style="text-align:center">node01</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr><tr><td style="text-align:center">node02</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr></tbody></table><h3 id="2、配置文件"><a href="#2、配置文件" class="headerlink" title="2、配置文件"></a>2、配置文件</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6j2l1t72j30fe0begoy.jpg" alt=""></p><p>修改配置文件</p><p>(1)<strong>mapred-site.xml:</strong>（配置mapreudce需要的框架环境）</p><p>路径：F:\hadoop-2.6.5\etc\hadoop\mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（2）<strong>yarn-site.xml:</strong>（配置yarn的任务调度的计算框架）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>因为<strong>ResourceManager</strong> <strong>和NodeManager</strong>主从结构，RM存在单点故障，要对它做HA（通过ZK）</p><p>修改yarn-site.xml配置文件,完整的内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:2181,node02:2181,node03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="六、个人理解"><a href="#六、个人理解" class="headerlink" title="六、个人理解"></a>六、个人理解</h2><blockquote><p>基于源码，对mapreduce的工作流程的描述：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">一个应用程序要进行大规模数据处理分析</span><br><span class="line"></span><br><span class="line">数据文件保存在HDFS中，分块存储在分布式节点上</span><br><span class="line"></span><br><span class="line">首先是将数据文件切分成许多split切片</span><br><span class="line"></span><br><span class="line">每一个split切片单独启动一个map任务，所以会启动多个map任务</span><br><span class="line"></span><br><span class="line">map阶段的输入是诸多(key,value),输出是新的（key,value）,然后被拉去到不同的reduce上并行处理操作</span><br><span class="line"></span><br><span class="line">所以每个map的输出阶段都执行分区操作，并决定reduce任务的个数</span><br><span class="line"></span><br><span class="line">然后对map输出结果进行分区、排序、归并、合并，这个过程叫map阶段的shuffle</span><br><span class="line"></span><br><span class="line">shuffle结束后，将相应的结果分发给reduce，让reduce完成后续的工作 </span><br><span class="line"></span><br><span class="line">结束后，将结果输出给HDFS。</span><br><span class="line"></span><br><span class="line">不同的map之间并行计算，不会通信；不同的reduce也不会通信，整个过程对用户透明。</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6zgfpvqnj30tw0fxace.jpg" alt="shuffle"><span class="img-alt">shuffle</span></p><blockquote><p>MapReduce执行的各个阶段：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1、从HDFS中加载文件，加载读取由InputFormat模块来完成，对输入负责格式验证，同时，对数据进行逻辑上切分成split</span><br><span class="line"></span><br><span class="line">2、由record read具体根据分片的位置长度信息去找各个block，以（key，value）输出，作为map的输入，</span><br><span class="line"></span><br><span class="line">3、map中有用户自定义的map函数就可以进行相应的数据处理，并输出一堆（key，value），作为中间结果</span><br><span class="line"></span><br><span class="line">4、之后，是shuffle（洗牌）过程对这中间结果进行分区、排序、合并，并溢写到磁盘，</span><br><span class="line"></span><br><span class="line">5、相应的reduce任务就会来fetch对应的分区（key，value-list）</span><br><span class="line"></span><br><span class="line">6、reduce中有用户自定义的reduce函数就可以完成对数据的分析，结果以新的（key，value）输出</span><br><span class="line"></span><br><span class="line">7、输出结果借助OutputFormat模块对输出格式进行检查，以及相关目录是否存在等，最后写入到HDFS中。</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6zikp8mbj30vh0fldle.jpg" alt="split"><span class="img-alt">split</span></p><blockquote><p>关于split的切分的理解：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1、InputFormat将大的数据文件分成很多split</span><br><span class="line">2、文件在HDFS中是以很多个物理块block分布式存储不同的节点上</span><br><span class="line">3、切片是用户自定义的逻辑分片</span><br><span class="line">4、split的数量决定map任务的数量</span><br><span class="line">5、切片过多会导致map任务启动过多，map任务之间切换的时候就会耗费相关的管理资源，所以切片过多会影响执行效率</span><br><span class="line">6、 切片过少又会影响任务执行的并行度，所以理想情况用block块的大小作为切片的大小。</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6ztmqg0oj30s00dg42p.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz85ufnyj9j30ud0b3mz0.jpg" alt=""></p><blockquote><p>关于shuffle的理解</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">map端shuffle</span><br><span class="line">1、从HDFS输入数据和执行map任务，在map任务执行之前，RecordReader阅读器还将数据变成满足Map函数所需的（K，V）形式，然后InputFormat会将其切分成若干切片（一堆（K，V））。</span><br><span class="line">2、每个切片会分配一个map任务，每个map任务会分配一个默认的缓存，一般默认缓存为100M.map的输出键值对作为中间结果先写入到缓存（直接写入磁盘会增加寻址开销，所以集中写入磁盘一次寻址就可以完成批量写入，就可以将寻址开销分摊到大量数据中，这就是缓存的作用）。</span><br><span class="line">3、当写入的内容达到缓存空间的一定比例后（溢写比，一般为0.8，就是80M的时候，为了不影响map任务的继续执行），会启动溢写进程，把缓存中相关数据写入磁盘。</span><br><span class="line">4、在溢写过程中，会执行分区（partition）、排序（sort，按照key值）和可能的合并（combine，为了减少溢写到磁盘的数据量，慎用）操作，写入磁盘，生成磁盘的溢写文件。</span><br><span class="line">5、在map任务运行结束前，系统会对溢写文件进行归并（merge），形成大文件（里面的键值对是分区，排序的）,文件格式为（key,value&lt;list&gt;），归并时如果溢写文件大于预定值（默认为3），会再次合并</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reduce端shuffle</span><br><span class="line">1、reduce任务会询问JobTracker，去拉取map机器上的属于自己的分区，对来自不同机器的数据进行归并、合并，然后输入到reduce函数中进行数据的处理分析，再写入磁盘</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz87b2w9k0j30vg0fijvo.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz87chuik1j30y50g00wu.jpg" alt=""></p><p>我</p><h1 id="MapReduce应用程序执行过程"><a href="#MapReduce应用程序执行过程" class="headerlink" title="MapReduce应用程序执行过程"></a>MapReduce应用程序执行过程</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz87d53czuj30vp0fltap.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper学习</title>
      <link href="/2019/01/04/Zookeeper%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/04/Zookeeper%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>动物园管理员</p><p><code>推荐图书：</code>《从Paxo到Zookeeper》</p><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h2 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h2><blockquote><p>开源的、分布式应用程序，提供<strong><code>一致性</code></strong>服务，是Haoop （实现HA）和Hbase（和zookeeper是强依赖关系）的重要组件</p></blockquote><p>提供的功能：</p><ul><li>配置维护</li><li>域名维护</li><li>分布式的同步</li><li>组服务</li></ul><p>Zookeeper→提供通用分布式锁服务，用以协调分布式应用</p><p>Keepalived→实现节点健康检查，采用优先级监控，没有协同工作，功能单一，可扩展性差。</p><h2 id="2、Zookeep的角色"><a href="#2、Zookeep的角色" class="headerlink" title="2、Zookeep的角色"></a>2、Zookeep的角色</h2><p> <img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd81ntj7tj30fd06pgn1.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd828tp44j30fo07lwgl.jpg" alt=""></p><p>（一般很少配置Observer，因为用的少，而且配置的节点一般为奇数）</p><blockquote><p>Zookeeper需保证高可用和强一致性；</p><p>​    为了支持更多的客户端，需要增加更多Server；</p><p>​    Server增多，投票阶段延迟增大，影响性能；</p><p>​    权衡伸缩性和高吞吐率，引入Observer</p><p>​    Observer不参与投票；</p><p>​    Observers接受客户端的连接，并将写请求转发给leader节点；</p><p>​    加入更多Observer节点，提高伸缩性，同时不影响吞吐率。</p></blockquote><h2 id="3、Zookeeper特点"><a href="#3、Zookeeper特点" class="headerlink" title="3、Zookeeper特点"></a>3、Zookeeper特点</h2><table><thead><tr><th style="text-align:center"><strong>特点</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td style="text-align:center"><strong><em>最终</em>一致性</strong></td><td>为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能（与强一致性相对）</td></tr><tr><td style="text-align:center"><strong>可靠性</strong></td><td>如果消息被到一台服务器接受，那么它将被所有的服务器接受.</td></tr><tr><td style="text-align:center"><strong>实时性</strong></td><td>Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。</td></tr><tr><td style="text-align:center"><strong>独立性</strong></td><td>各个Client之间互不干预</td></tr><tr><td style="text-align:center"><strong>原子性</strong></td><td>更新只能成功或者失败，没有中间状态。</td></tr><tr><td style="text-align:center"><strong>顺序性</strong></td><td>所有Server，同一消息发布顺序一致。</td></tr></tbody></table><h3 id="4、安装部署："><a href="#4、安装部署：" class="headerlink" title="4、安装部署："></a>4、安装部署：</h3><p><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">官网：</a></p><p><a href="https://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">下载：</a></p><p>（1）<code>修改</code>配置文件：</p><p>在Zokeeper的安装目录中的conf目录下，将zoo_sample.cfg文件改名为zoo.cfg</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> mv zoo_sample.cfg zoo.cfg</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p><code>编辑：</code></p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> vim /usr/soft/zookeeper-3.4.13/conf/zoo.cfg</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#发送心跳的间隔时间，单位：毫秒 ; 2秒</span><br><span class="line">tickTime=2000  </span><br><span class="line">dataDir=/usr/soft/zookeeper-3.4.13/data</span><br><span class="line">dataLogDir=/usr/soft/zookeeper-3.4.13/logs</span><br><span class="line">dataLogDir=/Users/zdandljb/zookeeper/dataLog</span><br><span class="line">#客户端连接 Zookeeper 服务器的端口，</span><br><span class="line">clientPort=2181    </span><br><span class="line">#Zookeeper 会监听这个端口，接受客户端的访问请求。</span><br><span class="line">initLimit=5</span><br><span class="line">syncLimit=2</span><br><span class="line">server.1=node01:2888:3888</span><br><span class="line">server.2=node02:2888:3888</span><br><span class="line">server.3=node03:2888:3888</span><br></pre></td></tr></table></figure><p><code>配置解释:</code></p><blockquote><p><code>initLimit</code>： 这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5 个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒</p><p>syncLimit：这个配置项标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的心跳时间长度，总的时间长度就是 2*2000=4 秒</p><p>server.A=B：C：D：其 中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号</p></blockquote><p>(2)<strong>创建myid文件</strong>（在上面配置文件中配置dataDir  的目录下）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server1机器的内容为：1，</span><br><span class="line">server2机器的内容为：2，</span><br><span class="line">server3机器的内容为：3</span><br></pre></td></tr></table></figure><p>（3）将zookeeper包发到各个节点上</p><h1 id="Paxo算法"><a href="#Paxo算法" class="headerlink" title="Paxo算法"></a>Paxo算法</h1><p><a href="http://zh.wikipedia.org/zh-cn/Paxos" target="_blank" rel="noopener">官网</a></p><h2 id="1、简介-1"><a href="#1、简介-1" class="headerlink" title="1、简介"></a>1、简介</h2><p>一种基于消息传递且具有高度容错特性的一致性算法，广泛应用于分布式计算中，是到目前为止唯一的分布式一致性算法。</p><p><code>前提：</code></p><p>Paxos 有一个前提：没有拜占庭将军问题。就是说 Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。</p><h2 id="2、结合故事的对应理解"><a href="#2、结合故事的对应理解" class="headerlink" title="2、结合故事的对应理解"></a>2、结合故事的对应理解</h2><blockquote><p>小岛(Island)——ZK Server Cluster<br>议员(Senator)——ZK Server<br>提议(Proposal)——ZNode Change(Create/Delete/SetData…)<br>提议编号(PID)——Zxid(ZooKeeper Transaction Id)<br>正式法令——所有 ZNode 及其数据</p><p>总统——ZK Server Leader</p></blockquote><h1 id="zookeeper的节点及工作原理"><a href="#zookeeper的节点及工作原理" class="headerlink" title="zookeeper的节点及工作原理"></a>zookeeper的节点及工作原理</h1><h2 id="1、工作原理"><a href="#1、工作原理" class="headerlink" title="1、工作原理"></a>1、<strong>工作原理</strong></h2><blockquote><p>1.每个Server在内存中存储了一份数据；</p><p>2.Zookeeper启动时，将从实例中选举一个leader（Paxos协议）</p><p>3.Leader负责处理数据更新等操作</p><p>4.一个更新操作成功，当且仅当大多数Server在内存中成功修改数据。</p></blockquote><p>Zookeeper的核心是<strong>原子广播</strong>，这个机制保证了各个server之间的同步。实现这个机制的协议叫做<strong>Zab协议</strong>。</p><p>Zab协议有两种模式，它们分别是<strong>恢复模式</strong>和<strong>广播模式</strong>。</p><blockquote><p>当服务启动或者在领导者崩溃后，Zab就进入了<strong>恢复模式</strong>，当领导者被选举出来，且大多数server的完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和server具有相同的系统状态。一旦leader已经和多数的follower进行了状态同步后，他就可以开始广播消息了，即进入<strong>广播状态</strong>。这时候当一个server加入zookeeper服务中，它会在恢复模式下启动，发现leader，并和leader进行状态同步。待到同步结束，它也参与消息广播。Zookeeper服务一直维持在Broadcast状态，直到leader崩溃了或者leader失去了大部分的followers支持.</p><p>广播模式需要保证proposal被按顺序处理，因此zk采用了<strong>递增的事务id号(zxid)</strong>来保证。所有的提议(proposal)都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch。低32位是个递增计数。</p><p>当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的server都恢复到一个正确的状态。</p></blockquote><h2 id="2、Znode节点"><a href="#2、Znode节点" class="headerlink" title="2、Znode节点"></a>2、Znode节点</h2><p>（1）Znode有两种类型，<strong>短暂的（ephemeral）和持久的（persistent）</strong></p><p> Znode的类型在创建时确定并且之后不能再修改。</p><ul><li><p>短暂znode的客户端会话结束时，zookeeper会将该短暂znode删除，<strong>短暂znode不可以有子节点</strong></p></li><li><p>持久znode不依赖于客户端会话，只有当客户端明确要删除该持久znode时才会被删除</p></li></ul><p>（2）Znode有四种形式的目录节点</p><ul><li><p>PERSISTENT、持久的</p></li><li><p>EPHEMERAL、短暂的</p></li><li><p>PERSISTENT_SEQUENTIAL、持久且有序的</p></li><li><p>EPHEMERAL_SEQUENTIAL   短暂且有序的</p></li></ul><h2 id="3、shell操作"><a href="#3、shell操作" class="headerlink" title="3、shell操作"></a>3、shell操作</h2><p>启动服务端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh start</span><br></pre></td></tr></table></figure><p>停止服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh stop</span><br></pre></td></tr></table></figure><p>启动客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkCli.sh -server 127.0.0.1 : 2081</span><br></pre></td></tr></table></figure><p>​                                             （localhost | node01）</p><p>​                                             （也可连接其他节点）</p><p>​                      (port默认2081,可省；ip也可省)</p><p>退出客户端：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quit</span><br></pre></td></tr></table></figure><p>操作指南：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help</span><br></pre></td></tr></table></figure><p>查看根目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll /</span><br></pre></td></tr></table></figure><p>​     （ll  +路径） </p><p>获取具体服务内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">get /</span><br><span class="line">(get +路径+服务)可查看注册zookeeper服务的节点信息</span><br></pre></td></tr></table></figure><p>（如果作为leader的namenode挂了，最新文件会相应的更换数据信息，如果没有nn，那么就没有相应的最新文件，只会有记录上一个阶段数据的文件）</p><p>创建服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create /sun aabbcc</span><br></pre></td></tr></table></figure><p>​               (create +路径 + 数据内容) </p><p>在其他节点也可启动客户端，创建服务</p><p>删除服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmr /sun</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g18fy424pbj30fd091wg3.jpg" alt=""></p><h2 id="4、API操作"><a href="#4、API操作" class="headerlink" title="4、API操作"></a>4、API操作</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzdcgvx18uj30fe0akwie.jpg" alt=""></p><p><code>见代码testzookeeper</code></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote><p>Zookeeper 作为 Hadoop 项目中的一个子项目，是Hadoop 集群管理的一个必不可少的模块，它<strong>主要用来控制集群中的数据</strong>，如它管理 Hadoop 集群中的NameNode，还有 Hbase 中 Master、 Server 之间状态同步等。</p><p>​    Zoopkeeper 提供了一套很好的分布式集群管理的机制，就是它这种基于层次型的目录树的数据结构，并对树中的节点进行有效管理，从而可以设计出多种多样的分布式的数据管理模型。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YARN的入门学习</title>
      <link href="/2019/01/04/Yarn%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/04/Yarn%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h1><h2 id="yarn（资源管理器）"><a href="#yarn（资源管理器）" class="headerlink" title="yarn（资源管理器）"></a>yarn（资源管理器）</h2><h3 id="（1）存在背景："><a href="#（1）存在背景：" class="headerlink" title="（1）存在背景："></a>（1）存在背景：</h3><p>MR1.0存在缺陷：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzczt4wiysj30x90ftq5v.jpg" alt=""></p><ul><li>单点故障：</li></ul><p>仅有一个JobTracker负责整个作业的调度、管理、监控、资源调度</p><p>（一个作业拿到后会分解多个任务去执行mapduce，JobTracker把任务分配给TaskTracker来具体负责执行相关map或reduce任务）</p><ul><li>JobTracker‘大包大揽’，管理事项过多</li></ul><p>（上限4000个节点）</p><ul><li><p>容易出现内存溢出</p></li><li><p>资源划分不合理</p><p>（强行划分slot，map资源和reduce资源不能互用，导致忙的忙死，闲的闲死）</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">既是一个计算框架，也是一个资源管理框架</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzczujcfrmj30n50ee75d.jpg" alt=""></p><h3 id="（2）yarn产生"><a href="#（2）yarn产生" class="headerlink" title="（2）yarn产生"></a>（2）yarn产生</h3><ul><li>对JobTracker进行功能分解，将资源管理功能分给ResourceManager，将任务调度和任务监控分给ApplicationMaster，将TaskTracker的任务交给NodeManager</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">纯粹的资源管理框架</span><br><span class="line">被剥离资源管理调度功能的MapReduce就变成了MR2.0，他就是一个运行在YARN上的一个纯粹的计算框架，由YARN为其提供资源管理调度服务</span><br></pre></td></tr></table></figure><p><code>什么叫纯粹的计算框架？？</code></p><p>它提供一些计算基类，使用时，编写map类和reduce类的子类，去继承它。然后计算框架去做后台自动分片，shuffle过程。</p><p><code>资源管理框架？？</code></p><p>它专门管理CPU内存资源的分配</p><h1 id="二、YARN设计思路"><a href="#二、YARN设计思路" class="headerlink" title="二、YARN设计思路"></a>二、YARN设计思路</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzczzrit8qj30wm0e6wie.jpg" alt=""></p><p>YARN资源管理，任务调度流程：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g18ecu679nj30cj088jso.jpg" alt=""></p><p>流程大致如下：</p><p>·         client客户端向yarn集群(resourcemanager)提交任务</p><p>·         resourcemanager选择一个node创建appmaster</p><p>·         appmaster根据任务向rm申请资源</p><p>·         rm返回资源申请的结果</p><p>·         appmaster去对应的node上创建任务需要的资源（container形式，包括内存和CPU）</p><p>·         appmaster负责与nodemanager进行沟通，监控任务运行</p><p>·         最后任务运行成功，汇总结果。</p><p>其中Resourcemanager里面一个很重要的东西，就是调度器Scheduler，调度规则可以使用官方提供的，也可以自定义。</p><h1 id="三、YARN体系结构"><a href="#三、YARN体系结构" class="headerlink" title="三、YARN体系结构"></a>三、YARN体系结构</h1><p>三大核心：</p><h2 id="1、RecourceManager（RM）"><a href="#1、RecourceManager（RM）" class="headerlink" title="1、RecourceManager（RM）"></a>1、RecourceManager（RM）</h2><blockquote><ul><li><p>ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主<br>要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager）</p></li><li><p>调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式<br>分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行<br>就近选择，从而实现“计算向数据靠拢”</p></li><li><p>容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、<br>磁盘等资源，从而限定每个应用程序可以使用的资源量</p></li><li><p>调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也<br>允许用户根据自己的需求重新设计调度器</p></li><li><p>应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括<br>应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状<br>态并在失败时重新启动等</p></li></ul></blockquote><h2 id="2、ApplicationMaster"><a href="#2、ApplicationMaster" class="headerlink" title="2、ApplicationMaster"></a>2、ApplicationMaster</h2><blockquote><p>ResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集<br>来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMaster<br>ApplicationMaster的主要功能是：<br>（1）当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，<br>ResourceManager会以容器的形式为ApplicationMaster分配资源；</p><p>（2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源<br>的“二次分配”；</p><p>（3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请<br>到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行<br>失败恢复（即重新申请资源重启任务）；</p><p>（4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信<br>息；</p><p>（5）当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。</p></blockquote><h2 id="3、NodeManager"><a href="#3、NodeManager" class="headerlink" title="3、NodeManager"></a>3、NodeManager</h2><blockquote><p>NodeManager是驻留在一个YARN集群中的每个节点上的代理，有所需数据的节点，主要负责：</p><ul><li><p>容器生命周期管理</p></li><li><p>监控每个容器的资源（CPU、内存等）使用情况</p></li><li><p>跟踪节点健康状况</p></li><li><p>以“心跳”的方式与ResourceManager保持通信</p></li><li><p>向ResourceManager汇报作业的资源使用情况和每个容器的运行状态</p></li><li><p>接收来自ApplicationMaster的启动/停止容器的各种请求</p></li></ul><p>需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事<br>情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这<br>些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与<br>NodeManager通信来掌握各个任务的执行状态</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd1ueqsbmj30xm0fjwgg.jpg" alt=""></p><h1 id="四、YARN-工作流程"><a href="#四、YARN-工作流程" class="headerlink" title="四、YARN 工作流程"></a>四、YARN 工作流程</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd1x7ksg6j30yl0fqjxh.jpg" alt=""></p><h1 id="五、YARN框架与MapReduce1-0框架"><a href="#五、YARN框架与MapReduce1-0框架" class="headerlink" title="五、YARN框架与MapReduce1.0框架"></a>五、YARN框架与MapReduce1.0框架</h1><h2 id="1、对比分析"><a href="#1、对比分析" class="headerlink" title="1、对比分析"></a>1、对比分析</h2><blockquote><ul><li>从MapReduce1.0框架发展到YARN框架，客户端并没有发生变化，其大部分调用API及<br>接口都保持兼容，因此，原来针对Hadoop1.0开发的代码不用做大的改动，就可以直接放<br>到Hadoop2.0平台上运行</li></ul></blockquote><blockquote><ul><li><p>总体而言，YARN相对于MapReduce1.0来说具有以下优势：</p><p>大大减少了承担中心服务功能的ResourceManager的资源消耗</p><ul><li>ApplicationMaster来完成需要大量资源消耗的任务调度和监控</li><li>多个作业对应多个ApplicationMaster，实现了监控分布化</li></ul></li></ul></blockquote><blockquote><ul><li>MapReduce1.0既是一个计算框架，又是一个资源管理调度框架，但是，只能支持<br>MapReduce编程模型。而YARN则是一个纯粹的资源调度管理框架，在它上面可以运行包<br>括MapReduce在内的不同类型的计算框架，只要编程实现相应的ApplicationMaster</li></ul></blockquote><blockquote><ul><li>YARN中的资源管理比MapReduce1.0更加高效<ul><li>以容器为单位，而不是以slot为单位</li></ul></li></ul></blockquote><h2 id="2、-MapReduce-On-YARN：MRv2"><a href="#2、-MapReduce-On-YARN：MRv2" class="headerlink" title="2、 MapReduce On YARN：MRv2"></a>2、 MapReduce On YARN：MRv2</h2><ul><li><p>将MapReduce作业直接运行在YARN上，而不是由JobTracker和TaskTracker构建的MRv1系统中</p></li><li><p>基本功能模块</p></li></ul><p>​    YARN：负责资源管理和调度</p><p>​    MRAppMaster：负责任务切分、任务调度、任务监控和容错等</p><p>​    MapTask/ReduceTask：任务驱动引擎，与MRv1一致</p><ul><li>每个MapRduce作业对应一个MRAppMaster任务调度</li></ul><p>​    YARN将资源分配给MRAppMaster</p><p>​    MRAppMaster进一步将资源分配给内部的任务</p><ul><li>MRAppMaster容错</li></ul><p>​    失败后，由YARN重新启动</p><p>​    任务失败后，MRAppMaster重新申请资源</p><h1 id="六、YARN-的发展目标"><a href="#六、YARN-的发展目标" class="headerlink" title="六、YARN 的发展目标"></a>六、YARN 的发展目标</h1><p><strong>YARN 的目标就是实现“一个集群多个框架”？ ，为什么？</strong></p><ul><li><p>一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架</p><ul><li><p>MapReduce实现离线批处理</p></li><li><p>使用Impala实现实时交互式查询分析</p></li><li><p>使用Storm实现流式数据实时分析</p></li><li><p>使用Spark实现迭代计算</p></li></ul></li></ul><ul><li><p>这些产品通常来自不同的开发团队，具有各自的资源调度管理机制</p></li><li><p>为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分<br>别安装运行不同的计算框架，即“一个框架一个集群”</p></li></ul><ul><li><p>导致问题</p><ul><li><p>集群资源利用率低</p></li><li><p>数据无法共享</p></li><li><p>维护代价高</p></li></ul></li></ul><ul><li><p>YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源<br>调度管理框架YARN，在YARN之上可以部署其他各种计算框架</p></li><li><p>由YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架<br>的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩</p></li><li><p>可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率</p></li><li><p>不同计算框架可以共享底层存储，避免了数据集跨集群移动</p></li></ul><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd5qv2p73j30r808tdgv.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> yarn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop2.X</title>
      <link href="/2019/01/03/Hadoop2.X/"/>
      <url>/2019/01/03/Hadoop2.X/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="一、Hadoop-2-x产生背景"><a href="#一、Hadoop-2-x产生背景" class="headerlink" title="一、Hadoop 2.x产生背景"></a>一、Hadoop 2.x产生背景</h2><h3 id="1、Hadoop-1-0存在的问题"><a href="#1、Hadoop-1-0存在的问题" class="headerlink" title="1、Hadoop 1.0存在的问题"></a>1、Hadoop 1.0存在的问题</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g18dlu5j8uj30fe08mq6t.jpg" alt=""></p><h4 id="（1）HDFS存在的问题"><a href="#（1）HDFS存在的问题" class="headerlink" title="（1）HDFS存在的问题"></a>（1）HDFS存在的问题</h4><ul><li>NameNode单点故障，难以应用于在线场景</li><li>NameNode（一个）压力过大，内存受限，影响系统扩展性</li></ul><h4 id="（2）MapReduce存在的问题"><a href="#（2）MapReduce存在的问题" class="headerlink" title="（2）MapReduce存在的问题"></a>（2）MapReduce存在的问题</h4><ul><li>JobTracker访问压力大，影响系统扩展性</li><li>难以支持MapReduce以外的计算框架，比如Spark、Storm</li></ul><h3 id="2、Hadoop-2-0分支"><a href="#2、Hadoop-2-0分支" class="headerlink" title="2、Hadoop 2.0分支"></a>2、Hadoop 2.0分支</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g18dmrv448j30ft05i76r.jpg" alt=""></p><p>HDFS：分布式文件存储系统<br>MapReduce：计算框架<br>YARN：资源管理系统</p><h3 id="3、特点"><a href="#3、特点" class="headerlink" title="3、特点"></a>3、特点</h3><p> 1）. 解决单点故障：HDFS HA（高可用）</p><blockquote><p>  通过主备NameNode解决，如果主NameNode发生故障，就切换到备NameNode上   |</p></blockquote><p> 2).解决内存受限问题：HDFS Federation（联邦制）、HA</p><blockquote><p> HA：两个NameNode<br> (3.0就实现了一组多从：水平扩展，支持多个NameNode；每个NameNode分管一部分目录；所有NameNode共享所有DataNode资源)</p></blockquote><p> 3).仅架构上发生变化使用方式不变</p><h2 id="二、HDFS-HA结构及功能"><a href="#二、HDFS-HA结构及功能" class="headerlink" title="二、HDFS HA结构及功能"></a>二、HDFS HA结构及功能</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g18dn0946qj30hp09rdjp.jpg" alt=""></p><h3 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h3><p>DN：DataNode（数据节点）</p><blockquote><p>存放数据block块；遵循心跳机制向NN Active和NN Standby汇报block块信息，但只执行active的命令         </p></blockquote><p>主备NN：NameNode Active 和 NameNode Standby （主备名称节点）</p><blockquote><p>主NN对外提供读写服务，备NN同步主NN元数据，以待切换，所有的DN同时向两个NN汇报数据块信息</p><p>元数据信息加载到主NN，并写入JN（至少写两台：过半原则）；</p><p>备NN可以从JN中同步元数据信息；</p><p>解决单点故障；</p><p>———两种切换方式：</p><p>手动：通过命令实现主备切换</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> hdfs haadmin -transitionToActive nn2</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>​           这时nn1如果还存活则变成不可写状态，需要重启，重启后自动成为standby nn</p><p>自动：基于Zookeeper实现（详情见搭建步骤）</p></blockquote><p>JN：JournalNode（至少3台）</p><blockquote><p>存储主NN元数据信息，实现主备NN间数据共享；</p><p>（遵循过半原则：至少有过半的数量参与投票）</p></blockquote><p>ZKFC：FailoverController（竞争锁）</p><blockquote><p>谁拿到了这个所，谁就是active NN</p><p>心跳机制监控主备NN状态，一旦出现一台挂机，就会释放锁，另一个NN就会立即启动竞争锁，成为active NN</p></blockquote><p>ZK：Zookeeper（至少3台）</p><blockquote><p>（实现主备NN切换）</p></blockquote><h3 id="联邦"><a href="#联邦" class="headerlink" title="联邦"></a>联邦</h3><blockquote><p>通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使到namenode/namespace可以通过增加机器来进行水平扩展</p></blockquote><blockquote><p>通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中。</p></blockquote><h2 id="三、-！！YARN-资源管理-！！"><a href="#三、-！！YARN-资源管理-！！" class="headerlink" title="三、==！！YARN(资源管理)！！=="></a>三、==！！YARN(资源管理)！！==</h2><p><code>详见Yarn学习.md</code></p><p>1、核心思想：ResourceManager（资源管理）+ApplicationMaster（任务调度）</p><p>2、yarn的引入使得多个计算框架可以应用到一个集群中</p><p>ResourceManager：  负责整个集群的资源管理和`调度。</p><p>ApplicationMaster：  负责应用程序相关的事务，比如任务调度、任务监控和容错等。</p><p>​                                      每个应用程序对应一个ApplicationMaster（应用程序控制-主人）</p><p>  目前多个计算框架可以运行在YARN上，比如MapReduce、Spark、Storm等。</p><h2 id="四、-！！Zookeeper工作原理！！"><a href="#四、-！！Zookeeper工作原理！！" class="headerlink" title="四、==！！Zookeeper工作原理！！=="></a>四、==！！Zookeeper工作原理！！==</h2><p><code>详见Zookeeper学习.md</code></p><h2 id="五、Hadoop2-X-集群搭建"><a href="#五、Hadoop2-X-集群搭建" class="headerlink" title="五、Hadoop2.X 集群搭建"></a>五、Hadoop2.X 集群搭建</h2><h3 id="1、linux环境下搭建"><a href="#1、linux环境下搭建" class="headerlink" title="1、linux环境下搭建"></a>1、linux环境下搭建</h3><table><thead><tr><th></th><th style="text-align:center">NN</th><th style="text-align:center">DN</th><th style="text-align:center">JN</th><th style="text-align:center">ZKFC</th><th style="text-align:center">ZK</th><th style="text-align:center">RM</th><th style="text-align:center">NM</th></tr></thead><tbody><tr><td>node00</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr><tr><td>node01</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center">√</td></tr><tr><td>node02</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center">√</td><td style="text-align:center"></td><td style="text-align:center">√</td><td style="text-align:center"></td><td style="text-align:center">√</td></tr></tbody></table><p>Zookeeper Failover Controller：</p><blockquote><p> <code>Failover</code> ：失效备援（为系统备援能力的一种，当系统中其中一项设备失效而无法运作时，另一项设备即可自动接手原失效系统所执行的工作）</p></blockquote><p>监控NameNode健康状态，并向Zookeeper注册NameNode，NameNode挂掉后，ZKFC为NameNode竞争锁，获得ZKFC锁的NameNode变为active。</p><h4 id="0-gt-在搭建环境之前的准备"><a href="#0-gt-在搭建环境之前的准备" class="headerlink" title="0&gt;在搭建环境之前的准备"></a>0&gt;在搭建环境之前的准备</h4><p>三台虚拟机：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">关闭防火墙</span><br><span class="line">安装jdk</span><br><span class="line">编辑/etc/hosts/给各个节点服务器起别名</span><br><span class="line">时间服务器：ntpdate</span><br><span class="line">     安装：yum install ntpdate -y</span><br><span class="line">     生成：ntpdate cn.ntp.org.cn</span><br><span class="line">免密登录环境准备</span><br></pre></td></tr></table></figure><p>在hadoop安装目录下hadoop-2.6.5/etc/hadoop/</p><h4 id="1-gt-编辑hadoop-env-sh"><a href="#1-gt-编辑hadoop-env-sh" class="headerlink" title="1&gt;编辑hadoop-env.sh"></a>1&gt;编辑hadoop-env.sh</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br></pre></td></tr></table></figure><h4 id="2-gt-编辑core-site-xml"><a href="#2-gt-编辑core-site-xml" class="headerlink" title="2&gt;编辑core-site.xml"></a>2&gt;编辑core-site.xml</h4><p><code>注意：</code></p><blockquote><p>fs.defaultFS 默认的服务端口是 NameNode URI</p><p>hadoop.tmp.dir是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果在hdfs-site.xml中没有配置namenode 和datanode的存放位置，默认及存放在这个路径中。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="comment">&lt;!--配置集群的名字--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:2181,node01:2181,node02:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置zookeeper：三个节点--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="comment">&lt;!--配置hadoop基础配置存放的路径--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-gt-编辑hdfs-site-xml"><a href="#3-gt-编辑hdfs-site-xml" class="headerlink" title="3&gt;编辑hdfs-site.xml"></a>3&gt;编辑hdfs-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.Sunrise<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.Sunrise.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.Sunrise.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.Sunrise.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.Sunrise.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://node00:8485;node01:8485;node02:8485/Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.Sunrise<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 配置隔离机制为ssh 防止脑裂：保证activeNN仅有一台--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定秘钥的位置 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_dsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--免密登录是生成的文件，有的是id_rsa--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 指定journalnode日志文件存储的路径 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启自动故障转移 --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="4-gt-配置hadoop中的slaves"><a href="#4-gt-配置hadoop中的slaves" class="headerlink" title="4&gt;配置hadoop中的slaves"></a>4&gt;配置hadoop中的slaves</h4><p>（主从架构：datanode）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node00</span><br><span class="line">node01</span><br><span class="line">node02</span><br></pre></td></tr></table></figure><h4 id="5-gt-准备zookeeper"><a href="#5-gt-准备zookeeper" class="headerlink" title="5&gt;准备zookeeper"></a>5&gt;准备zookeeper</h4><ul><li><p>三台zookeeper：node00，node01，node02</p></li><li><p>编辑zookeeper-3.4.13/conf/zoo.cfg</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/usr/soft/zookeeper-3.4.13/data</span><br><span class="line">dataLogDir=/usr/soft/zookeeper-3.4.13/logs</span><br><span class="line">clientPort=2181</span><br><span class="line">#用于投票选举</span><br><span class="line">server.1=node00:2888:3888</span><br><span class="line">server.2=node01:2888:3888</span><br><span class="line">server.3=node02:2888:3888</span><br></pre></td></tr></table></figure></li><li><p>在dataDir目录中创建文件myid，三台节点的文件内容分别为1，2，3</p></li></ul><h4 id="6-gt-配置环境变量"><a href="#6-gt-配置环境变量" class="headerlink" title="6&gt;配置环境变量"></a>6&gt;配置环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bash_profile</span><br></pre></td></tr></table></figure><p>编辑内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">HADOOP_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> source ~/.bash_profile</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>使其成为资源文件，发送到其他节点后，也需要此操作</p></blockquote><h4 id="7-gt-将以上配置文件远程发送至其他节点服务器"><a href="#7-gt-将以上配置文件远程发送至其他节点服务器" class="headerlink" title="7&gt;将以上配置文件远程发送至其他节点服务器"></a>7&gt;将以上配置文件远程发送至其他节点服务器</h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> scp -r filename nodename:`pwd`</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="8-gt-命令操作："><a href="#8-gt-命令操作：" class="headerlink" title="8&gt;命令操作："></a>8&gt;命令操作：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">1. 启动三个zookeeper：./zkServer.sh start</span><br><span class="line">2. 启动三个JournalNode：./hadoop-daemon.sh start journalnode</span><br><span class="line">3. （生成fsimage文件）在其中一个namenode上格式化：</span><br><span class="line">    hdfs namenode -format</span><br><span class="line">4. 把刚刚格式化之后的元数据拷贝到另外一个namenode上</span><br><span class="line">   a)启动刚刚格式化的namenode : </span><br><span class="line">        hadoop-daemon.sh start namenode</span><br><span class="line">   b)（同步fsimage文件）在另一个（没有格式化的）namenode上执行：</span><br><span class="line">        hdfs namenode -bootstrapStandby</span><br><span class="line">   c)启动没格式化的namenode：   </span><br><span class="line">        hadoop-daemon.sh start namenode</span><br><span class="line">5. （初始化竞争锁zookeeper）在其中一个namenode上初始化zkfc：</span><br><span class="line">    hdfs zkfc -formatZK</span><br><span class="line">6. 停止上面节点：</span><br><span class="line">    stop-dfs.sh</span><br><span class="line">7. 全面启动（三个节点）：</span><br><span class="line">    start-dfs.sh</span><br><span class="line">8. 启动yarn资源管理器</span><br><span class="line">   yarn-daemon.sh start resourcemanager </span><br><span class="line">   (yarn resourcemanager  )</span><br></pre></td></tr></table></figure><h3 id="2、使用"><a href="#2、使用" class="headerlink" title="2、使用"></a>2、使用</h3><p>（启动步骤）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(1)关闭防火墙：service iptables stop        （3台）</span><br><span class="line">(2)启动zookeeper:zkServer.sh start          （3台）</span><br><span class="line">(3)启动集群：start-dfs.sh |（start-all.sh   :  同时启动hdfs和yarn)</span><br><span class="line">注意：启动了所在节点的resourcemanager</span><br><span class="line">(4)启动yarn：yarn-daemon.sh start resourcemanager （启动另一台resourcemanager节点）</span><br></pre></td></tr></table></figure><p>（关闭步骤）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)关闭yarn：yarn-daemon.sh stop resourcemanager  （开几台关几台）</span><br><span class="line">(2)关闭集群：stop-dfs.sh   |（stop-all.sh    :同时关闭hdfs和yarn） （3台）</span><br><span class="line">(3)关闭zookeeper：zkServer.sh stop                 （3台）</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">有可能会出错的地方</span><br><span class="line">1，确认每台机器防火墙均关掉</span><br><span class="line">2，确认每台机器的时间是一致的</span><br><span class="line">3，确认配置文件无误，并且确认每台机器上面的配置文件一样</span><br><span class="line">4，如果还有问题想重新格式化，那么先把所有节点的进程关掉</span><br><span class="line">5，删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录，所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录</span><br><span class="line">6，接上面的第6步又可以重新格式化已经启动了</span><br><span class="line">7，最终Active Namenode停掉的时候，StandBy可以自动接管！</span><br></pre></td></tr></table></figure><h3 id="3、关于脑裂"><a href="#3、关于脑裂" class="headerlink" title="3、关于脑裂"></a>3、关于脑裂</h3><p><a href="http://en.wikipedia.org/wiki/Split-brain_(computing" target="_blank" rel="noopener">脑裂（brain-split）</a>)</p><p>1&gt;定义：</p><p>是指在主备切换时，由于切换不彻底或其他原因，导致客户端和slave误以为出现两个active master，最终使得整个集群处于混乱状态。</p><p>2&gt;解决脑裂问题：</p><p>通常采用<a href="http://en.wikipedia.org/wiki/Fencing_(computing" target="_blank" rel="noopener">隔离(Fencing)</a>)机制</p><p>包括三个方面：</p><ul><li><p>共享fencing ： 确保只有一个Master 往共享存储中写数据；</p></li><li><p>客户端fencing ：确保只有一个Master可以相应客户端的请求；</p></li><li>Slave fencing ： 确保只有一个Master可以向Slave下发命令</li></ul><blockquote><p>​      Hadoop公共库中对外提供了两种fenching实现，分别是sshfence和shellfence（缺省实现），其中sshfence是指通过ssh登陆目标Master节点上，使用命令fuser将进程杀死（通过tcp端口号定位进程pid，该方法比jps命令更准确），shellfence是指执行一个用户事先定义的shell命令（脚本）完成隔离。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS学习</title>
      <link href="/2019/01/03/HDFS%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/01/03/HDFS%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>==hadoop理解==</p><p>狭义：</p><p>hadoop1 = hdfs1 + MR1</p><p>hadoop2 = hdfs2 + MR2 + YARN</p><p>广义：</p><p>Hadoop生态</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g16vvlbzawj30ex09nq6e.jpg" alt=""></p><p>用于解决海量数据的处理和数据存储，数据级别为GB，TB，PB。</p><h2 id="一、分布式文件存储系统HDFS"><a href="#一、分布式文件存储系统HDFS" class="headerlink" title="一、分布式文件存储系统HDFS"></a>一、分布式文件存储系统HDFS</h2><h3 id="1、什么是分布式？"><a href="#1、什么是分布式？" class="headerlink" title="1、什么是分布式？"></a>1、什么是分布式？</h3><blockquote><p>定义：将海量的数据，复杂的业务分发到不同的计算机节点和服务器上分开处理和计算。</p><p>特点：</p><ul><li>多副本，提高服务的容错率、安全性、高可靠性</li><li>适合批处理，提高服务的效率和速度，</li><li>减轻单台服务器的压力</li><li>具有很好的可扩展性</li><li>计算向数据靠拢，安全，高效</li></ul></blockquote><p><code>大数据三驾马车：GFS、MapReduce、Bigtable</code>====HDFS、MR、HBASE</p><h3 id="2、什么是HDFS？"><a href="#2、什么是HDFS？" class="headerlink" title="2、什么是HDFS？"></a>2、什么是HDFS？</h3><h4 id="（1）HDFS为什么会出现？"><a href="#（1）HDFS为什么会出现？" class="headerlink" title="（1）HDFS为什么会出现？"></a>（1）HDFS为什么会出现？</h4><blockquote><p>主要解决大量【pb级以上】的大数据的分布式存储问题</p></blockquote><h4 id="（2）-HDFS的特点"><a href="#（2）-HDFS的特点" class="headerlink" title="（2）==HDFS的特点=="></a>（2）==HDFS的特点==</h4><blockquote><p>$$ 分布式特性：</p><ul><li>适合大数据处理：GB、TB、PB以上的数据</li><li>百万规模以上的文件数量:10K+ 节点</li><li>适合批处理：移动计算而非数据(MR),数据位置暴露给计算框架</li></ul><p>$$ 自身特性：</p><ul><li>可构建在廉价机器上</li><li>高可靠性：通过多副本实现</li><li>高容错性：数据自动保存多个副本；副本丢失后，自动恢复,提供了恢复机制</li></ul><p>$$ 缺点：</p><p>—–低延迟高数据吞吐访问问题（不适合低延迟数据访问，Hbase适合）</p><ul><li>不支持毫秒级</li><li>吞吐量大但有限制于其延迟（瓶颈：低延迟无法突破）</li></ul><p>—–小文件存取占用NameNode大量内存(寻道时间超过读取时间,约占99%)</p><p>——-不支持多用户写入及任意修改文件</p><ul><li>不支持文件修改：一个文件只能有一个写者</li><li>文件仅支持append不支持修改</li><li>（其实本身是支持的，主要为了用空间换时间，节约成本）</li></ul><p>$$ 实现目标：</p><ul><li>兼容廉价的硬件设施</li><li>实现流数据读写</li><li>支持大数据集</li><li>支持简单的文件模型</li><li>强大的跨平台兼容性</li></ul></blockquote><h4 id="（3）-HDFS架构图"><a href="#（3）-HDFS架构图" class="headerlink" title="（3）==HDFS架构图=="></a>（3）==HDFS架构图==</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0lg96gkqj30hz0dsgmf.jpg" alt="HDFS架构图"><span class="img-alt">HDFS架构图</span></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0lq2gmmtj30f009vwfj.jpg" alt="HDFS架构图"><span class="img-alt">HDFS架构图</span></p><blockquote><p><code>关系型数据库：</code>安全，存储在磁盘中；如MySql、Oracle、SQlServer</p><p><code>非关系型数据库：</code>不安全，存储在内存中；如Redis、MemcacheDB、mongoDB、Hbase</p></blockquote><h3 id="3、-HDFS的功能模块及原理详解"><a href="#3、-HDFS的功能模块及原理详解" class="headerlink" title="3、==HDFS的功能模块及原理详解=="></a>3、==HDFS的功能模块及原理详解==</h3><h4 id="HDFS数据存储模型（block）"><a href="#HDFS数据存储模型（block）" class="headerlink" title=" HDFS数据存储模型（block）"></a><1> HDFS数据存储模型（block）</1></h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0n3j6xvnj30fe09vdh0.jpg" alt="block"><span class="img-alt">block</span></p><h5 id="（1）文件被线性切分固定大小的数据块：block"><a href="#（1）文件被线性切分固定大小的数据块：block" class="headerlink" title="（1）文件被线性切分固定大小的数据块：block"></a>（1）文件被<code>线性切分</code>固定大小的数据块：block</h5><ul><li><p>通过偏移量offset（单位：byte）标记</p></li><li><p>默认数据块大小为64MB (hadoop1.x，hadoop2.x默认为128M）)，可自定义配置</p></li><li><p>若文件大小不到64MB ，则单独存成一个block</p></li></ul><h5 id="（2）一个文件存储方式"><a href="#（2）一个文件存储方式" class="headerlink" title="（2）一个文件存储方式"></a>（2）一个文件存储方式</h5><ul><li><p>按大小被切分成若干个block ，存储到<code>不同节点上</code></p></li><li><p>默认情况下每个block共有3个副本</p></li><li><p>副本数不大于节点数</p></li></ul><h5 id="（3）Block大小和副本数通过Client端上传文件时设置，"><a href="#（3）Block大小和副本数通过Client端上传文件时设置，" class="headerlink" title="（3）Block大小和副本数通过Client端上传文件时设置，"></a>（3）Block大小和副本数通过Client端上传文件时设置，</h5><blockquote><p> 文件上传成功后副本数可以变更，Block Size大小不可变更</p><p>块的大小远远大于普通文件系统，可以最小化寻址开销</p></blockquote><h4 id="NameNode（简称NN）"><a href="#NameNode（简称NN）" class="headerlink" title="NameNode（简称NN）"></a><2>NameNode（简称NN）</2></h4><blockquote><ul><li>存储<code>元数据</code>；</li><li>元数据保存在<code>内存中</code>；</li><li>保存<code>文件</code>、<code>block块</code>、<code>datanode</code>之间的映射关系</li></ul></blockquote><h5 id="1-gt-NN主要功能："><a href="#1-gt-NN主要功能：" class="headerlink" title="1&gt; NN主要功能："></a>1&gt; NN主要功能：</h5><blockquote><p>接收客户端的读写服务；接收DN汇报block位置关系</p></blockquote><h5 id="2-gt-NN保存metadate元信息"><a href="#2-gt-NN保存metadate元信息" class="headerlink" title="2&gt; NN保存metadate元信息"></a>2&gt; NN保存metadate元信息</h5><blockquote><p>基于<code>内存</code>存储，<code>不会</code>和磁盘发生交换</p></blockquote><p>​        <code>metadata</code>元数据信息包括以下</p><blockquote><ul><li><p>文件的归属（ownership）和权限（permission）</p></li><li><p>文件大小和写入时间</p></li><li><p>block列表【偏移量】：即一个完整文件有哪些block（b0+b1+b2+..=file）</p></li><li><p>位置信息（<code>动态</code>的）：Block每个副本保存在哪个DataNode中</p><p><code>*注意*</code>：位置信息是由DN启动时上报给NN ，因为它会随时变化，所以不会保存在内存和磁盘中</p></li></ul></blockquote><h5 id="3-gt-NameNode的metadate信息在启动后会加载到内存"><a href="#3-gt-NameNode的metadate信息在启动后会加载到内存" class="headerlink" title="3&gt; NameNode的metadate信息在启动后会加载到内存"></a>3&gt; NameNode的metadate信息在启动后会加载到内存</h5><blockquote><p>同时：</p><p>metadata信息也会保存fsimage文件中（fsimage文件是位于磁盘上的镜像文件）</p><p>对metadata的操作日志也会记录在edits 文件中（edits文件是位于磁盘上的日志文件）</p></blockquote><h4 id="SecondaryNameNode（简称SNN）"><a href="#SecondaryNameNode（简称SNN）" class="headerlink" title="SecondaryNameNode（简称SNN）"></a><3>SecondaryNameNode（简称SNN）</3></h4><h5 id="1-gt-SNN主要功能"><a href="#1-gt-SNN主要功能" class="headerlink" title="1&gt;SNN主要功能"></a>1&gt;SNN主要功能</h5><blockquote><p>帮助NameNode合并edits和fsimage文件，减少NN启动时间；</p><p>SecondaryNameNode一般是单独运行在一台机器上；</p><p>它不是NN的备份（但可以做备份)。</p></blockquote><h5 id="2-gt-合并流程"><a href="#2-gt-合并流程" class="headerlink" title="2&gt;合并流程"></a>2&gt;合并流程</h5><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0p1lys2zj30f009vwf2.jpg" alt="SNN合并"><span class="img-alt">SNN合并</span></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SecondaryNameNode的工作情况：</span><br><span class="line">（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，</span><br><span class="line">     暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，</span><br><span class="line">     上层写日志的函数完全感觉不到差别；</span><br><span class="line">（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文</span><br><span class="line">    件，并下载到本地的相应目录下；</span><br><span class="line">（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；</span><br><span class="line">    这个过程就是EditLog和FsImage文件合并；</span><br><span class="line">（4）SecondaryNameNode执行完（3）操作之后，</span><br><span class="line">     会通过post方式将新的FsImage文件发送到NameNode节点上</span><br><span class="line">（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，</span><br><span class="line">     同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了</span><br></pre></td></tr></table></figure><h5 id="3-gt-合并机制"><a href="#3-gt-合并机制" class="headerlink" title="3&gt;合并机制"></a>3&gt;合并机制</h5><blockquote><p> ——-SNN执行合并时间和机制</p><ul><li>A、根据配置文件设置指定连续两次检查点的最大时间间隔 <code>fs.checkpoint.period</code> 默认3600秒（1小时）</li><li>B、根据配置文件设置edits log文件大小 <code>fs.checkpoint.size</code> 默认最大值64M</li><li>配置文件：core-site.xml</li></ul></blockquote><h4 id="DataNode（简称DN）"><a href="#DataNode（简称DN）" class="headerlink" title="DataNode（简称DN）"></a><4>DataNode（简称DN）</4></h4><h5 id="1-gt-DN主要功能"><a href="#1-gt-DN主要功能" class="headerlink" title="1&gt;  DN主要功能"></a>1&gt;  DN主要功能</h5><blockquote><ul><li>存储<code>文件内容</code>（block）；</li><li>文件内容保存在<code>磁盘</code>；</li><li>维护了<code>block id</code> 到<code>datanode本地文件</code>的映射关系</li><li>启动DN线程的时候会向NameNode汇报block位置信息</li></ul></blockquote><h5 id="2-gt-DN工作机制"><a href="#2-gt-DN工作机制" class="headerlink" title="2&gt;   DN工作机制"></a>2&gt;   DN工作机制</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•    数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，</span><br><span class="line">•    会根据客户端或者是名称节点的调度来进行数据的存储和检索，</span><br><span class="line">•    并且通过心跳机制向名称节点定期发送自己所存储的块的列表，保持与其联系（3秒一次）</span><br><span class="line">    （如果NN 10分钟没有收到DN的心跳，则认为其已经lost，并copy其上的block到其它DN）</span><br><span class="line">•    每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中</span><br></pre></td></tr></table></figure><h5 id="3-gt-block的副本放置策略"><a href="#3-gt-block的副本放置策略" class="headerlink" title="3&gt; block的副本放置策略"></a>3&gt; block的副本放置策略</h5><blockquote><p>–  第一个副本：放置在上传文件的DN（集群内提交）；</p><p>​                           如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。</p><p>–  第二个副本：放置在于第一个副本不同的机架的节点上。</p><p>–  第三个副本：与第二个副本相同机架的不同节点。</p><p>–  更多副本：随机节点</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ny1tfrjj30f00akaaf.jpg" alt="block块存放位置"><span class="img-alt">block块存放位置</span></p><h3 id="4、HDFS读写流程"><a href="#4、HDFS读写流程" class="headerlink" title="4、HDFS读写流程"></a>4、HDFS读写流程</h3><h4 id="读文件过程"><a href="#读文件过程" class="headerlink" title=" 读文件过程"></a><1> 读文件过程</1></h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1o4z4nmwj30gm09vgmg.jpg" alt="read"><span class="img-alt">read</span></p><blockquote><p>1、首先<code>client端</code>调用DistributedFileSystem对象（<code>DFS</code>）的<code>open方法</code>，（DFS：一个DistributedFileSystem的实例）。<br>2、DistributedFileSystem通过<code>rpc</code>协议从NameNode（<code>NN</code>）获得文件的第一批block的<code>locations</code>，（同一个block按副本数会返回多个locations，因为同一文件的block<code>分布式存储</code>在不同节点上），这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面（<code>就近选择</code>）。</p><p>3、前两步会返回一个FSDataInputStream对象，该对象会被封装DFSInputStream对象，DFSInputStream可以方便的管理DN和NN的数据流。客户端调用<code>read方法</code>，DFSInputStream会连接离客户端最近的DN，数据从DN源源不断的流向客户端（对客户端是透明的，只能看到一个读入的Input流）。</p><p>4、如果第一批block都读完了， DFSInputStream就会去NN拿下一批block的locations，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ptkw9krj30q40gegpp.jpg" alt="读"><span class="img-alt">读</span></p><p><code>注意：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">   如果在读数据的时候,DFSInputStream和DN的通讯发生异常，就会尝试连接正在读的block的排序第二近的DN,并且会记录哪个DN发生错误，剩余的blocks读的时候就会直接跳过该DN。</span><br><span class="line">   DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到NN，然后DFSInputStream在其他的DN上读该block的镜像。</span><br><span class="line">   该设计就是客户端直接连接DN来检索数据，并且NN来负责为每一个block提供最优的DN，NN仅仅处理block location的请求，这些信息都加载在NN的内存中，hdfs通过DN集群可以承受大量客户端的并发访问。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   * RPC *（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。</span><br><span class="line">RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。</span><br></pre></td></tr></table></figure><h4 id="写文件过程"><a href="#写文件过程" class="headerlink" title="写文件过程"></a><2>写文件过程</2></h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1o6wamv3j30ga0aawfd.jpg" alt="write"><span class="img-alt">write</span>3</p><blockquote><p><strong>1.</strong>客户端通过调用DistributedFileSystem的<code>create方法</code>创建新文件。</p><p><strong>2.</strong>DistributedFileSystem通过<code>RPC</code>调用NN去创建一个没有blocks关联的新文件，创建前，NN会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NN就会记录下新文件，否则就会抛出IO异常。</p><p><strong>3.</strong>前两步结束后，会返回FSDataOutputStream的对象，封装在DFSOutputStream，客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的<code>packet</code>，然后排成队列dataQuene。</p><p><strong>4.</strong>NN会给这个新的block分配最适合存储的几个datanode，DFSOutputStream把packet包排成一个<code>管道pipeline</code>输出。先按队列输出到管道的第一个datanode中，并将该Packet从dataQueue队列中移到ackQueue队列中，第一个datanode又把packet输出到第二个datanode中，以此类推。</p><p><strong>5.</strong>DFSOutputStream中的<code>ackQuene</code>，也是由packet组成，等待DN的收到响应，当pipeline中的DN都表示已经收到数据的时候，这时ackQuene才会把对应的packet包移除掉。 如果在写的过程中某个DN发生错误，会采取以下几步：</p><p>​      1) pipeline被关闭掉；  </p><p>​      2)为了防止丢包，ackQuene里的packet会<code>同步</code>到dataQuene里;新建pipeline管道接到其他正常DN上</p><p>​     4)剩下的部分被写到剩下的正常的datanode中； </p><p>​     5)NN找到另外的DN去创建这个块的复制。（对客户端透明）</p><p><strong>6.</strong>客户端完成写数据后调用<code>close方法</code>关闭写入流</p></blockquote><p><code>注意：</code>客户端执行write操作后，写完的block才是可见的，正在写的block对客户端是不可见的</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ptp2xlej30on0gf78r.jpg" alt="写"><span class="img-alt">写</span></p><h3 id="5-HDFS文件权限和安全模式"><a href="#5-HDFS文件权限和安全模式" class="headerlink" title="5.HDFS文件权限和安全模式"></a>5.HDFS文件权限和安全模式</h3><h4 id="？？HDFS文件权限？？"><a href="#？？HDFS文件权限？？" class="headerlink" title="？？HDFS文件权限？？"></a><1>？？HDFS文件权限？？</1></h4><p>– 与Linux文件权限类似 </p><blockquote><p>   • r: read;    w:write;    x:execute，权限x对于文件忽略，对于文件夹表示是否允许访问其内容 </p></blockquote><p>– 如果Linux系统用户zs使用hadoop命令创建一个文件，那么这个 文件在HDFS中owner就是zs。 </p><p>– HDFS的权限目的：阻止好人做错事，而不是阻止坏人做坏事。</p><h4 id="？？安全模式？？"><a href="#？？安全模式？？" class="headerlink" title="？？安全模式？？"></a><2>？？安全模式？？</2></h4><blockquote><ul><li>NN启动的时候，首先将映像文件(fsimage)载入内存，并执行编辑日志(edits)中的各项操作。 </li></ul></blockquote><blockquote><ul><li>一旦在内存中成功建立文件系统元数据的映射，则创建一个新的fsimage文件(这个操作不需要SecondaryNameNode)和一个空的编辑日志。 </li></ul></blockquote><blockquote><ul><li>此刻namenode运行在安全模式。即namenode的文件系统对于客服端来说是只读的。(显示目录，显示文件内容等。写、删除、重命名都会失败)。 </li></ul></blockquote><blockquote><ul><li>在此阶段Namenode收集各个datanode的报告，当数据块达到最小副本数以上时，会被认为是“安全”的， 在一定比例（可设置）的数据块被确定为“安全”后，再过若干时间，安全模式结束 </li></ul></blockquote><blockquote><ul><li>当检测到副本数不足的数据块时，该块会被复制直到达到最小副本数，系统中数据块的位置并不是由namenode维护的，而是以块列表形式存储在datanode中。</li></ul></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1s32tmr2j30hj04ggmr.jpg" alt="异常"><span class="img-alt">异常</span></p><p>手动退出安全模式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -safemode leave</span><br></pre></td></tr></table></figure><h2 id="二、完全分布式搭建及eclipse插件"><a href="#二、完全分布式搭建及eclipse插件" class="headerlink" title="二、完全分布式搭建及eclipse插件"></a>二、完全分布式搭建及eclipse插件</h2><h3 id="1、-完全分布式搭建（必备）"><a href="#1、-完全分布式搭建（必备）" class="headerlink" title="1、==完全分布式搭建（必备）=="></a>1、==完全分布式搭建（必备）==</h3><h4 id="（1）环境的准备"><a href="#（1）环境的准备" class="headerlink" title="（1）环境的准备"></a>（1）环境的准备</h4><blockquote><ul><li>Linux (前面已经安装好了)</li></ul><ul><li><p>JDK（前面已经安装好了）</p></li><li><p>准备至少3台机器（通过克隆虚拟机；)</p></li><li><p>(网络配置、JDK搭建、hosts配置，保证节点间能互ping通）</p></li><li><p>时间同步</p><p>（推荐）ntpdate cn.ntp.org.cn </p><p> (ntpdate time.nist.gov)</p></li><li><p>ssh免秘钥登录   (两两互通免秘钥)</p></li></ul></blockquote><h4 id="（2）完全分布式搭建步骤"><a href="#（2）完全分布式搭建步骤" class="headerlink" title="（2）完全分布式搭建步骤"></a>（2）完全分布式搭建步骤</h4><h5 id="Hadoop-1-X"><a href="#Hadoop-1-X" class="headerlink" title="Hadoop 1.X"></a><strong>Hadoop 1.X</strong></h5><h6 id="1、下载解压缩Hadoop"><a href="#1、下载解压缩Hadoop" class="headerlink" title="1、下载解压缩Hadoop"></a>1、下载解压缩Hadoop</h6><h6 id="2、配置hadoop-hadoop-env-sh"><a href="#2、配置hadoop-hadoop-env-sh" class="headerlink" title="2、配置hadoop/hadoop-env.sh"></a>2、配置hadoop/hadoop-env.sh</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/latest</span><br></pre></td></tr></table></figure><h6 id="3、配置core-site-xml"><a href="#3、配置core-site-xml" class="headerlink" title="3、配置core-site.xml:"></a>3、配置core-site.xml:</h6><blockquote><p>fs.defaultFS 默认的服务端口NameNode URI</p><p>hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这个路径中</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop-2.6.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h6 id="4、配置hdfs-site-xml"><a href="#4、配置hdfs-site-xml" class="headerlink" title="4、配置hdfs-site.xml:"></a>4、配置hdfs-site.xml:</h6><p>dfs.datanode.https.address   https服务的端口，浏览器访问端口</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--默认为3个副本，若指定，则以指定的为准--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="comment">&lt;!--http访问：SecondaryNameNode的服务器的ip地址别名,端口号50090--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--https访问：SecondaryNameNode的服务器的ip地址别名,端口号50090--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.https-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node02:50091<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h6 id="5、Masters-master-可以做主备的SNN"><a href="#5、Masters-master-可以做主备的SNN" class="headerlink" title="5、Masters:master 可以做主备的SNN"></a>5、Masters:<strong>master</strong> 可以做主备的SNN</h6><p>在/home/hadoop-2.6.5/etc/hadoop/新建masters文件 写上<strong>SNN</strong>节点名： node02（IP地址别名） </p><h6 id="6、Slaves-slave-奴隶-苦干；拼命工作"><a href="#6、Slaves-slave-奴隶-苦干；拼命工作" class="headerlink" title="6、Slaves: slave 奴隶 苦干；拼命工作"></a>6、Slaves: <strong>slave</strong> 奴隶 苦干；拼命工作</h6><p>在/home/hadoop-2.5.1/etc/hadoop/slaves文件中填写<strong>DN</strong> 节点名：node02 node03 [注意：每行写一个 写成3行]</p><h6 id="7、最后将配置好的Hadoop通过SCP命令发送都其他节点"><a href="#7、最后将配置好的Hadoop通过SCP命令发送都其他节点" class="headerlink" title="7、最后将配置好的Hadoop通过SCP命令发送都其他节点"></a>7、最后将配置好的Hadoop通过SCP命令发送都其他节点</h6><p>   配置Hadoop的环境变量</p><p>8、vim ~/.bash_profile  (最好手敲输入 粘贴有时候会出错)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME/home/hadoop-2.6.5</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure><p> 9、记得一定要  source ~/.bash_profile<br>10、回到跟目录下对NN进行格式化  hdfs namenode -format</p><p>11、启动HDFS： start-dfs.sh</p><p>12、关闭防火墙：service iptables stop<br>在浏览器输入 node01:50070 </p><h5 id="Hadoop-2-X"><a href="#Hadoop-2-X" class="headerlink" title="==!!Hadoop 2.X!!=="></a>==!!<strong>Hadoop 2.X</strong>!!==</h5><p><code>详情见Hadoop2.X.md文件</code></p><h3 id="2、HDFS命令"><a href="#2、HDFS命令" class="headerlink" title="2、HDFS命令"></a>2、HDFS命令</h3><h4 id="0-命令-：hdfs-dfs"><a href="#0-命令-：hdfs-dfs" class="headerlink" title="(0)  命令 ：hdfs dfs"></a>(0)  命令 ：hdfs dfs</h4><h4 id="1-上传文件到HDFS："><a href="#1-上传文件到HDFS：" class="headerlink" title="(1)上传文件到HDFS："></a>(1)上传文件到HDFS：</h4><blockquote><p> <strong>hdfs dfs -put 本地路径/fileName </strong>PATH [hdfs的文件路径]</p></blockquote><blockquote><p>上传本地文件/root/install.log到/myhdfs目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> hdfs dfs -put /root/install.log /myhdfs</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>​                                      （文件路径)                (上传目录）    </p></blockquote><h4 id="2-创建文件夹"><a href="#2-创建文件夹" class="headerlink" title="(2)创建文件夹"></a>(2)创建文件夹</h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> hdfs dfs -mkdir[-p] &lt;paths&gt;</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p> <strong>-p</strong>   穿透，用于创建多级文件夹</p></blockquote><h4 id="3-删除文件或文件夹"><a href="#3-删除文件或文件夹" class="headerlink" title="(3)删除文件或文件夹"></a>(3)<strong>删除文件或文件夹</strong></h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> hdfs dfs -rm -r /myhadoop1.0 </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>删除多个文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> hdfs dfs -rm -r /input /logs</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p><strong>-r</strong>   递归，用于删除文件夹以及下级文件和文件夹</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -du [-s] [-h] URI[URI ...] 显示文件(夹)大小. </span><br><span class="line"></span><br><span class="line">hdfs dfs -cp [-f] [-p] URI[URI...]&lt;dest&gt;    复制文件(夹)，可以覆盖，可以保留原有权限信息</span><br><span class="line"></span><br><span class="line">hdfs dfs -count [-q] [-h] &lt;paths&gt;列出文件夹数量、文件数量、内容大小.</span><br><span class="line"></span><br><span class="line">hdfs dfs -chown [-R] [OWNER] [:[GROUP]] URI[URI] 修改所有者.</span><br><span class="line"></span><br><span class="line">hdfs dfs -chmod [-R]&lt;MODE[,MODE]...|OCTALMODE&gt; URI[URI ...] 修改权限.</span><br></pre></td></tr></table></figure><h4 id="（4）指定block大小"><a href="#（4）指定block大小" class="headerlink" title="（4）指定block大小"></a>（4）指定block大小</h4><p>其中副本数是在在配置文件中配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">产生100000条数据：</span><br><span class="line"></span><br><span class="line">for i in `seq 100000`;do  echo "hello sxt $i" &gt;&gt; test.txt;done</span><br><span class="line"></span><br><span class="line">上传文件test.txt到指定的Java22目录下，并指定block块的大小1M：</span><br><span class="line"></span><br><span class="line">hdfs dfs -D dfs.blocksize=1048576 -put test.txt /java22</span><br><span class="line"></span><br><span class="line">-D   ----设置属性</span><br></pre></td></tr></table></figure><h3 id="3、eclipse插件安装配置"><a href="#3、eclipse插件安装配置" class="headerlink" title="3、eclipse插件安装配置"></a>3、eclipse插件安装配置</h3><h4 id="（1）、导入插件"><a href="#（1）、导入插件" class="headerlink" title="（1）、导入插件"></a>（1）、导入插件</h4><blockquote><p>将以下jar包放入eclipse的plugins文件夹中</p><p>​         hadoop-eclipse-plugin-2.6.0.jar</p></blockquote><p>启动eclipse：出现界面如下：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1uhqzg9oj30fe09nt9c.jpg" alt="插件应用"><span class="img-alt">插件应用</span></p><h4 id="（2）配置环境变量"><a href="#（2）配置环境变量" class="headerlink" title="（2）配置环境变量"></a>（2）配置环境变量</h4><p><strong>Eclipse</strong>插件安装完后修改windows下的用户名，然后重启Eclipse：</p><p><strong>【注意：改成Windows下用户的用户名root（重启生效）或改Linux文件的用户】</strong></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1uk49nbrj30fe0770tj.jpg" alt="环境变量"><span class="img-alt">环境变量</span></p><h4 id="（3）新建Java项目"><a href="#（3）新建Java项目" class="headerlink" title="（3）新建Java项目"></a>（3）新建Java项目</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ut3z3r2j30et0ah0tw.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g182idgjuuj30fe0a6my9.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1utb5r5tj30fe0brgmt.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1uw9a82yj30g508qmxh.jpg" alt=""></p><h2 id="三、网盘"><a href="#三、网盘" class="headerlink" title="三、网盘"></a>三、网盘</h2><p><strong>1、代码编写</strong></p><p><strong>新建Java项目，导入所需要的jar包</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop中的share\hadoop\hdfs</span><br><span class="line"></span><br><span class="line">hadoop中的share\hadoop\hdfs\lib</span><br><span class="line"></span><br><span class="line">hadoop中的share\hadoop\common</span><br><span class="line"></span><br><span class="line">hadoop中的share\hadoop\common\lib下的jar包。</span><br></pre></td></tr></table></figure><p><strong>block</strong>底层—offset偏移量来读取字节数组</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">blk</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Path ifile = <span class="keyword">new</span> Path(<span class="string">""</span>);</span><br><span class="line">FileStatus file = fs.getFileStatus(ifile );</span><br><span class="line"><span class="comment">//      获取block的location信息HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容</span></span><br><span class="line">BlockLocation[] blk = fs.getFileBlockLocations(file,<span class="number">0</span>, file.getLen());</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (BlockLocation bb : blk) &#123;</span><br><span class="line">System.out.println(bb);</span><br><span class="line">&#125;</span><br><span class="line">FSDataInputStream input = fs.open(ifile);</span><br><span class="line">System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line">System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line"><span class="comment">//指定从哪个offset的位置偏移量来读</span></span><br><span class="line">input.seek(<span class="number">1048576</span>);</span><br><span class="line">System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line">input.seek(<span class="number">1048576</span>);</span><br><span class="line">System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>HDFS读取合并的小文件</strong></p>]]></content>
      
      
      <categories>
          
          <category> HDFS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HDFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx入门学习（二）</title>
      <link href="/2019/01/02/Nginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A02/"/>
      <url>/2019/01/02/Nginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A02/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="一、虚拟主机"><a href="#一、虚拟主机" class="headerlink" title="一、虚拟主机"></a>一、虚拟主机</h2><h3 id="1、什么是虚拟主机？"><a href="#1、什么是虚拟主机？" class="headerlink" title="1、什么是虚拟主机？"></a>1、什么是虚拟主机？</h3><p>（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。</p><p>（2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。</p><h3 id="2、虚拟主有啥特点？"><a href="#2、虚拟主有啥特点？" class="headerlink" title="2、虚拟主有啥特点？"></a>2、虚拟主有啥特点？</h3><p>（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用</p><p>（2）也大大简化了服务器管理的复杂性；</p><h3 id="3、虚拟主机有哪些类别？"><a href="#3、虚拟主机有哪些类别？" class="headerlink" title="3、虚拟主机有哪些类别？"></a>3、虚拟主机有哪些类别？</h3><p>（1）基于域名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">     &#125; </span><br><span class="line">upstream bjsxt&#123; </span><br><span class="line">        server node03; </span><br><span class="line">     &#125; </span><br><span class="line">     </span><br><span class="line">     server &#123;    </span><br><span class="line">            listen 80; </span><br><span class="line">            //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里</span><br><span class="line">            server_name  sxt2.com;</span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://bjsxt;</span><br><span class="line">            &#125;</span><br><span class="line">      &#125; </span><br><span class="line">      server &#123; </span><br><span class="line">            listen 80; </span><br><span class="line">           //访问sxt1.com的时候，会把请求导到shsxt的服务器组里</span><br><span class="line">            server_name  sxt1.com; </span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://shsxt;</span><br><span class="line">            &#125;</span><br><span class="line">      &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p></blockquote><blockquote><p>（1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。</p><p>（C:\Windows\System32\drivers\etc\hosts     给IP取别名）</p><p>如：192.168.198.130   sxt1.com</p></blockquote><blockquote><p>（2）每台服务器的Tomcat的端口不与配置的listen一致，那么windows系统浏览器访问时，需要加上TOmcat的端口，（192.168.198.128：8080）</p><p>​         如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80</p></blockquote><p>（2）基于端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">     &#125; </span><br><span class="line">upstream bjsxt&#123; </span><br><span class="line">        server node03</span><br><span class="line">    &#125; </span><br><span class="line"> server &#123; </span><br><span class="line">       //当访问nginx的80端口时，将请求导给bjsxt组</span><br><span class="line">        listen 8080; </span><br><span class="line">        server_name 192.168.198.128;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://bjsxt;</span><br><span class="line">        &#125;</span><br><span class="line">&#125; </span><br><span class="line">  server &#123; </span><br><span class="line">           //当访问nginx的81端口时，将请求导给shsxt组</span><br><span class="line">            listen 81; </span><br><span class="line">            server_name 192.168.198.128;  //nginx服务器的IP</span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://shsxt;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（3）基于IP  ：（不常用）</p><h2 id="二、正向代理和反向代理"><a href="#二、正向代理和反向代理" class="headerlink" title="二、正向代理和反向代理"></a>二、正向代理和反向代理</h2><h3 id="1、正向代理"><a href="#1、正向代理" class="headerlink" title="1、正向代理"></a>1、正向代理</h3><p>理解：</p><blockquote><p>代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见）</p></blockquote><p>举例：</p><blockquote><p>国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙）</p><p>但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口</p></blockquote><h3 id="2、反向代理"><a href="#2、反向代理" class="headerlink" title="2、反向代理"></a>2、反向代理</h3><p>理解：</p><blockquote><p>代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器</p></blockquote><p>举例：</p><blockquote><p>如我们访问<a href="http://www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。" target="_blank" rel="noopener">www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。</a></p></blockquote><p>Nginx就是性能很好的反向代理服务器，用来作负载均衡。</p><h2 id="三、Nginx的session一致性问题"><a href="#三、Nginx的session一致性问题" class="headerlink" title="三、Nginx的session一致性问题"></a>三、Nginx的session一致性问题</h2><h3 id="1、背景："><a href="#1、背景：" class="headerlink" title="1、背景："></a>1、背景：</h3><p>http协议是无状态的，多次访问如果是不同服务器响应请求，就会出现上次访问留下的session或cookie失效。这就引发了session共享的问题。</p><h3 id="2、Session一致性解决方案"><a href="#2、Session一致性解决方案" class="headerlink" title="2、Session一致性解决方案"></a>2、Session一致性解决方案</h3><p>（1）–session复制<br>   tomcat 本身带有复制session的功能。</p><p>（2）-共享session</p><p>  需要专门管理session的软件，<br>   memcached 缓存服务，可以和tomcat整合，帮助tomcat共享管理session。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0gr7u5zoj31740hb42u.jpg" alt=""></p><h3 id="3、搭建memcached"><a href="#3、搭建memcached" class="headerlink" title="3、搭建memcached"></a>3、搭建memcached</h3><p>memcached （同redis一样）是基于<code>内存</code>的数据库</p><h4 id="1、安装memcached"><a href="#1、安装memcached" class="headerlink" title="1、安装memcached"></a>1、安装memcached</h4><blockquote>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">   yum –y install memcache</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>验证本机11211端口是否可用:</p><blockquote>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    telnet localhost 11211</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><h4 id="2、启动memcached"><a href="#2、启动memcached" class="headerlink" title="2、启动memcached"></a>2、启动memcached</h4><p>(IP地址为memcached安装的节点的IP地址)</p><blockquote>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    memcached -d -m 128m -p 11211 -l 192.168.198.128 -u root -P /tmp/</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>3、拷贝memcached所需jar包</p><p>将web服务器连接memcached的jar包拷贝到tomcat的lib目录下</p><p><code>访问Tomcat服务器期间产生的session通过相关jar包，才能写入到memcached数据库中</code></p><blockquote><p>asm-3.2.jar</p><p>kryo-1.04.jar</p><p>kryo-serializers-0.11.jar</p><p>memcached-session-manager-1.7.0.jar</p><p>memcached-session-manager-tc7-1.8.1.jar</p><p>minlog-1.2.jar</p><p>msm-kryo-serializer-1.7.0.jar</p><p>reflectasm-1.01.jar</p><p>spymemcached-2.7.3.jar</p></blockquote><p>4、配置tomcat的conf目录下的context.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Manager</span> <span class="attr">className</span>=<span class="string">"de.javakaffee.web.msm.MemcachedBackupSessionManager"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">memcachedNodes</span>=<span class="string">"n1:192.168.198.128:11211"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">sticky</span>=<span class="string">"true"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">lockingMode</span>=<span class="string">"auto"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">sessionBackupAsync</span>=<span class="string">"false"</span></span></span><br><span class="line"><span class="tag">   <span class="attr">requestUriIgnorePattern</span>=<span class="string">".*\.(ico|png|gif|jpg|css|js)$"</span></span></span><br><span class="line"><span class="tag"><span class="attr">sessionBackupTimeout</span>=<span class="string">"1000"</span> <span class="attr">transcoderFactoryClass</span>=<span class="string">"de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory"</span> /&gt;</span></span><br></pre></td></tr></table></figure><p>配置memcachedNodes属性，</p><blockquote><p>配置memcached数据库的ip和端口，默认11211，多个的话用逗号隔开.</p><p>目的是为了让tomcat服务器从memcached缓存里面拿session或者是放session</p></blockquote><p>5、将配置完成的context.xml发送到其他虚拟机器上</p><blockquote><p>scp -r context.xml root@node01:<code>pwd</code></p><p>或</p><p>scp -r context.xml node01:<code>pwd</code></p><p>或</p><p>scp -r context.xml <a href="mailto:root@192.168.198.130" target="_blank" rel="noopener">root@192.168.198.130</a>:<code>pwd</code></p></blockquote><p>6、修改tomcat安装目录中的webapps/ROOT下的 index.jsp，取sessionid看一看</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;%@ page language=<span class="string">"java"</span> contentType=<span class="string">"text/html; charset=UTF-8"</span>  pageEncoding=<span class="string">"UTF-8"</span>%&gt;</span><br><span class="line">&lt;html lang=<span class="string">"en"</span>&gt;</span><br><span class="line">SessionID:&lt;%=session.getId()%&gt;</span><br><span class="line">&lt;/br&gt;</span><br><span class="line">SessionIP:&lt;%=request.getServerName()%&gt;</span><br><span class="line">&lt;/br&gt;</span><br><span class="line">&lt;h1&gt;tomcat1&lt;/h1&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>7、在浏览器段访问服务器，默认端口 ： 80 ，对此测验，就会发现sessionID不会改变</p>]]></content>
      
      
      <categories>
          
          <category> 负载均衡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx入门学习（一）</title>
      <link href="/2019/01/02/Nginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A01/"/>
      <url>/2019/01/02/Nginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A01/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="大型网站高并发运行处理"><a href="#大型网站高并发运行处理" class="headerlink" title="大型网站高并发运行处理"></a>大型网站高并发运行处理</h1><h2 id="一、Nginx使用背景"><a href="#一、Nginx使用背景" class="headerlink" title="一、Nginx使用背景"></a>一、Nginx使用背景</h2><p>[^ 开发者]: 由俄罗斯的程序设计师Igor Sysoev所开发</p><h3 id="1、背景"><a href="#1、背景" class="headerlink" title="1、背景"></a>1、背景</h3><p>1）高并发（海量数据，复杂业务，大量线程）集中访问服务器</p><p>2)单台服务器资源和能力有限</p><p>引发服务器宕机，无法提供服务</p><h3 id="2、概念理解"><a href="#2、概念理解" class="headerlink" title="2、概念理解"></a>2、概念理解</h3><p>1)高并发</p><blockquote><p>海量数据请求访问（高），多个线程或者多个进程同时处理（并发）不同操作</p></blockquote><p>2）负载均衡（Load Balance）</p><blockquote><p>均匀分配请求|数据到不同操作单元上</p><p>其中，【均匀】是分布式系统架构设计中必须考虑的关键因素之一</p></blockquote><p>3）常见互联网分布式架构</p><blockquote><p>客户端层→反向代理层→站点层→服务层→数据层</p><p>只需要实现“将请求/数据均匀分摊到多个操作单元上执行”，就能实现负载均衡</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0gr7u5zoj31740hb42u.jpg" alt=""></p><h2 id="二、Nginx入门"><a href="#二、Nginx入门" class="headerlink" title="二、Nginx入门"></a>二、Nginx入门</h2><h3 id="1、了解nginx是什么"><a href="#1、了解nginx是什么" class="headerlink" title="1、了解nginx是什么"></a>1、了解nginx是什么</h3><blockquote><p>nginx是一款轻量级（开发方便，配置简捷）的Web 服务器/<strong>反向代理</strong>服务器及电子邮件（IMAP/POP3）代理服务器</p></blockquote><h3 id="2、Nginx特点"><a href="#2、Nginx特点" class="headerlink" title="2、Nginx特点"></a>2、Nginx特点</h3><blockquote><ul><li>占有内存少，CPU、内存等资源消耗也少；</li><li>运行稳定，并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。</li></ul><p>（底层使用C语言编写）</p><p>Tomcat的最高并发量为250个</p></blockquote><h3 id="3、Nginx-VS-Apache"><a href="#3、Nginx-VS-Apache" class="headerlink" title="3、Nginx   ==VS==  Apache"></a>3、Nginx   ==VS==  Apache</h3><h4 id="（1）nginx相对于apache的优点："><a href="#（1）nginx相对于apache的优点：" class="headerlink" title="（1）nginx相对于apache的优点："></a>（1）nginx相对于apache的优点：</h4><blockquote><ul><li>轻量级，同样起web 服务，比apache 占用更少的内存及资源</li><li>nginx 处理请求是异步非阻塞（如前端ajax）的，而apache 则是阻塞型的</li><li>在高并发下nginx能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单</li><li>Nginx 配置简洁, Apache 复杂</li></ul></blockquote><h4 id="（2）apache-相对于nginx-的优点："><a href="#（2）apache-相对于nginx-的优点：" class="headerlink" title="（2）apache 相对于nginx 的优点："></a>（2）apache 相对于nginx 的优点：</h4><blockquote><ul><li>Rewrite重写 ，比nginx 的rewrite 强大模块超多，基本想到的都可以找到</li><li>少bug ，nginx 的bug 相对较多。（出身好起步高）</li></ul></blockquote><h3 id="4、配置搭建Nginx"><a href="#4、配置搭建Nginx" class="headerlink" title="4、配置搭建Nginx"></a>4、配置搭建Nginx</h3><p>（Linux系统环境下）</p><p><code>资源</code>：</p><p>Tengine（推荐）：<a href="http://tengine.taobao.org/download/tengine-2.2.3.tar.gz" target="_blank" rel="noopener">Tengine-2.2.3.tar.gz</a> </p><p>​                                 <a href="http://tengine.taobao.org/download.html" target="_blank" rel="noopener">其他版本</a></p><p>nginx：<a href="http://nginx.org/download/nginx-1.8.1.zip" target="_blank" rel="noopener">nginx/Windows-1.8.1</a></p><h4 id="1）安装依赖"><a href="#1）安装依赖" class="headerlink" title="1）安装依赖"></a>1）安装依赖</h4><blockquote><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> yum -y install gcc openssl-devel pcre-devel zlib-devel</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="2）解压tar包"><a href="#2）解压tar包" class="headerlink" title="2）解压tar包"></a>2）解压tar包</h4><blockquote><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> tar -zxvf Tengine-2.2.3.tar.gz</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="3）configure配置："><a href="#3）configure配置：" class="headerlink" title="3）configure配置："></a>3）configure配置：</h4><p>在解压后的源码目录中</p><p>两种方案：</p><blockquote><ul><li><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    ./configure</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></li></ul></blockquote><blockquote><p>默认配置/usr/soft/nginx</p></blockquote><blockquote><ul><li><p>命令 :</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">    ./configure –profix==/usr/soft/nginx</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></li></ul></blockquote><blockquote><p>配置在指定路径</p></blockquote><h4 id="4）编译并安装"><a href="#4）编译并安装" class="headerlink" title="4）编译并安装"></a>4）编译并安装</h4><p>(默认会在/usr/local下生成nginx目录)</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> make &amp;&amp; make install</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="5）配置nginx服务"><a href="#5）配置nginx服务" class="headerlink" title="5）配置nginx服务"></a>5）配置nginx服务</h4><p>在/etc/rc.d/init.d/目录中建立文本文件nginx</p><p>在文件中粘贴下面的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">#</span><br><span class="line"># nginx - this script starts and stops the nginx daemon</span><br><span class="line">#</span><br><span class="line"># chkconfig:   - 85 15 </span><br><span class="line"># description:  Nginx is an HTTP(S) server, HTTP(S) reverse \</span><br><span class="line">#               proxy and IMAP/POP3 proxy server</span><br><span class="line"># processname: nginx</span><br><span class="line"># config:      /etc/nginx/nginx.conf</span><br><span class="line"># config:      /etc/sysconfig/nginx</span><br><span class="line"># pidfile:     /var/run/nginx.pid</span><br><span class="line"> </span><br><span class="line"># Source function library.</span><br><span class="line">. /etc/rc.d/init.d/functions</span><br><span class="line"> </span><br><span class="line"># Source networking configuration.</span><br><span class="line">. /etc/sysconfig/network</span><br><span class="line"> </span><br><span class="line"># Check that networking is up.</span><br><span class="line">[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0</span><br><span class="line"> </span><br><span class="line">nginx=&quot;/usr/local/nginx/sbin/nginx&quot;</span><br><span class="line">prog=$(basename $nginx)</span><br><span class="line"> </span><br><span class="line">NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot;</span><br><span class="line"> </span><br><span class="line">[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx</span><br><span class="line"> </span><br><span class="line">lockfile=/var/lock/subsys/nginx</span><br><span class="line"> </span><br><span class="line">make_dirs() &#123;</span><br><span class="line">   # make required directories</span><br><span class="line">   user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -`</span><br><span class="line">   options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;`</span><br><span class="line">   for opt in $options; do</span><br><span class="line">       if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then</span><br><span class="line">           value=`echo $opt | cut -d &quot;=&quot; -f 2`</span><br><span class="line">           if [ ! -d &quot;$value&quot; ]; then</span><br><span class="line">               # echo &quot;creating&quot; $value</span><br><span class="line">               mkdir -p $value &amp;&amp; chown -R $user $value</span><br><span class="line">           fi</span><br><span class="line">       fi</span><br><span class="line">   done</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">start() &#123;</span><br><span class="line">    [ -x $nginx ] || exit 5</span><br><span class="line">    [ -f $NGINX_CONF_FILE ] || exit 6</span><br><span class="line">    make_dirs</span><br><span class="line">    echo -n $&quot;Starting $prog: &quot;</span><br><span class="line">    daemon $nginx -c $NGINX_CONF_FILE</span><br><span class="line">    retval=$?</span><br><span class="line">    echo</span><br><span class="line">    [ $retval -eq 0 ] &amp;&amp; touch $lockfile</span><br><span class="line">    return $retval</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">stop() &#123;</span><br><span class="line">    echo -n $&quot;Stopping $prog: &quot;</span><br><span class="line">    killproc $prog -QUIT</span><br><span class="line">    retval=$?</span><br><span class="line">    echo</span><br><span class="line">    [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile</span><br><span class="line">    return $retval</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">restart() &#123;</span><br><span class="line">    configtest || return $?</span><br><span class="line">    stop</span><br><span class="line">    sleep 1</span><br><span class="line">    start</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">reload() &#123;</span><br><span class="line">    configtest || return $?</span><br><span class="line">    echo -n $&quot;Reloading $prog: &quot;</span><br><span class="line">    killproc $nginx -HUP</span><br><span class="line">    RETVAL=$?</span><br><span class="line">    echo</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">force_reload() &#123;</span><br><span class="line">    restart</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">configtest() &#123;</span><br><span class="line">  $nginx -t -c $NGINX_CONF_FILE</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">rh_status() &#123;</span><br><span class="line">    status $prog</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">rh_status_q() &#123;</span><br><span class="line">    rh_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">    start)</span><br><span class="line">        rh_status_q &amp;&amp; exit 0</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    stop)</span><br><span class="line">        rh_status_q || exit 0</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    restart|configtest)</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    reload)</span><br><span class="line">        rh_status_q || exit 7</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    force-reload)</span><br><span class="line">        force_reload</span><br><span class="line">        ;;</span><br><span class="line">    status)</span><br><span class="line">        rh_status</span><br><span class="line">        ;;</span><br><span class="line">    condrestart|try-restart)</span><br><span class="line">        rh_status_q || exit 0</span><br><span class="line">            ;;</span><br><span class="line">    *)</span><br><span class="line">        echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot;</span><br><span class="line">        exit 2</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><h4 id="6）修改nginx文件的执行权限"><a href="#6）修改nginx文件的执行权限" class="headerlink" title="6）修改nginx文件的执行权限"></a>6）修改nginx文件的执行权限</h4><blockquote><p>命令 ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">  chmod +x nginx</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="7）添加该文件到系统服务中去"><a href="#7）添加该文件到系统服务中去" class="headerlink" title="7）添加该文件到系统服务中去"></a>7）添加该文件到系统服务中去</h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> chkconfig --add nginx</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="8-查看是否添加成功"><a href="#8-查看是否添加成功" class="headerlink" title="8)查看是否添加成功"></a>8)查看是否添加成功</h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> chkconfig --list nginx</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="9-启动，停止，重新装载"><a href="#9-启动，停止，重新装载" class="headerlink" title="9)启动，停止，重新装载"></a>9)启动，停止，重新装载</h4><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> service nginx start|stop</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h2 id="三、Nginx配置"><a href="#三、Nginx配置" class="headerlink" title="三、Nginx配置"></a>三、Nginx配置</h2><h3 id="1、查看配置"><a href="#1、查看配置" class="headerlink" title="1、查看配置"></a>1、查看配置</h3><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> <span class="built_in">cd</span>   /usr/<span class="built_in">local</span>/nginx/conf</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> vim   nginx.conf</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h3 id="2、配置解析"><a href="#2、配置解析" class="headerlink" title="2、配置解析"></a>2、配置解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#进程数，建议设置和CPU个数一样或2倍</span><br><span class="line">worker_processes  2;</span><br><span class="line"></span><br><span class="line">#日志级别</span><br><span class="line">error_log  logs/error.log  warning;(默认error级别)</span><br><span class="line"></span><br><span class="line"># nginx 启动后的pid 存放位置</span><br><span class="line">#pid        logs/nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">#配置每个进程的连接数，总的连接数= worker_processes * worker_connections</span><br><span class="line">    #默认1024</span><br><span class="line">    worker_connections  10240;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">#连接超时时间，单位秒</span><br><span class="line">keepalive_timeout  65;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost                 </span><br><span class="line">        #默认请求</span><br><span class="line">  location / &#123;</span><br><span class="line">     root  html;   #定义服务器的默认网站根目录位置</span><br><span class="line">     index  index.php index.html index.htm;  #定义首页索引文件的名称</span><br><span class="line">        &#125;</span><br><span class="line">    #定义错误提示页面</span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h3 id="3、负载均衡配置"><a href="#3、负载均衡配置" class="headerlink" title="3、负载均衡配置"></a>3、负载均衡配置</h3><p>1）安装Tomcat，参考 <code>Tomcat配置</code></p><h4 id="2）多负载均执行一下操作："><a href="#2）多负载均执行一下操作：" class="headerlink" title="2）多负载均执行一下操作："></a>2）多负载均执行一下操作：</h4><p> 多负载的情况下，打开指定虚拟机器</p><blockquote><p>open  node01    </p><p>node01  为指定虚拟机器的别名，在hosts文件中配置的</p></blockquote><p>启动Tomcat</p><blockquote><p>在Tomcat解压的目录下       ./startup.sh  </p></blockquote><p>注意： 记得关闭虚拟机器的防火墙</p><blockquote><p>service  iptables  stop</p></blockquote><p>浏览器访问</p><blockquote><p>虚拟机器IP地址：8080</p></blockquote><p><strong>默认负载均衡配置</strong>（轮询）</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt; http &#123;</span><br><span class="line">&gt;     upstream shsxt&#123;   </span><br><span class="line">&gt;     # 以下均为实际执行服务的服务器</span><br><span class="line">&gt;     #只有当hosts文件中给ip地址配置了别名，这里server后面才能用别名，</span><br><span class="line">&gt;     #否则跟IP地址</span><br><span class="line">&gt;         server node01; </span><br><span class="line">&gt;         server node02; </span><br><span class="line">&gt;     &#125; </span><br><span class="line">&gt; </span><br><span class="line">&gt;     server &#123; </span><br><span class="line">&gt;     #指定访问端口为80 ，那么Tomcat服务器端的port也要改为80</span><br><span class="line">&gt;         listen 80;   </span><br><span class="line">&gt;     server_name  localhost;</span><br><span class="line">&gt;         location / &#123;</span><br><span class="line">&gt;             proxy_pass http://shsxt;    </span><br><span class="line">&gt;             # shsxt  是指定的代理服务器</span><br><span class="line">&gt;         &#125;</span><br><span class="line">&gt;     &#125; </span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>查看使用 80端口的程序</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> netstat -anp |grep 80</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p>配置文件编辑结束后，启动nginx服务</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> service  nginx  start</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="（1）轮询负载均衡（默认）"><a href="#（1）轮询负载均衡（默认）" class="headerlink" title="（1）轮询负载均衡（默认）"></a>（1）轮询负载均衡（默认）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 对应用程序服务器的请求以循环方式分发</span><br></pre></td></tr></table></figure><h4 id="（2）加权负载均衡"><a href="#（2）加权负载均衡" class="headerlink" title="（2）加权负载均衡"></a>（2）加权负载均衡</h4><blockquote><p>通过使用服务器权重，还可以进一步影响nginx负载均衡算法，</p><p>谁的权重越大，分发到的请求就越多。</p><p>权重总数：10</p></blockquote><p>在nginx.conf文件中修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream shsxt &#123;</span><br><span class="line">       server node01 weight=3;//域名为在/etc/hosts文件中取的别名</span><br><span class="line">       server node02;</span><br><span class="line">       server node03;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>配置修改之后，重启</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> service  nginx  restart</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><h4 id="（3）最少连接负载平衡"><a href="#（3）最少连接负载平衡" class="headerlink" title="（3）最少连接负载平衡"></a>（3）最少连接负载平衡</h4><blockquote><p>在连接负载最少的情况下，nginx会尽量避免将过多的请求分发给繁忙的应用程序服务器，</p><p>而是将新请求分发给不太繁忙的服务器，避免服务器过载。</p></blockquote><p>在nginx.conf文件中修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">upstream shsxt &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server node00;</span><br><span class="line">        server node01;</span><br><span class="line">        server node02;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="（4）保持会话持久性——ip-hash负载平衡机制"><a href="#（4）保持会话持久性——ip-hash负载平衡机制" class="headerlink" title="（4）保持会话持久性——ip-hash负载平衡机制"></a>（4）保持会话持久性——ip-hash负载平衡机制</h4><p><code>特点</code>：保证相同的客户端总是定向到相同的服务;</p><p>(此方法可确保来自同一客户端的请求将始终定向到同一台服务器，除非此服务器不可用。)</p><p>在nginx.conf文件中修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">upstream shsxt&#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server （IP地址|别名）;</span><br><span class="line">    server （IP地址|别名）;</span><br><span class="line">    server （IP地址|别名）;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="（5）Nginx的访问控制"><a href="#（5）Nginx的访问控制" class="headerlink" title="（5）Nginx的访问控制"></a>（5）Nginx的访问控制</h4><blockquote><p>Nginx还可以对IP的访问进行控制，allow代表允许，deny代表禁止.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">deny 192.168.2.180;</span><br><span class="line">allow 192.168.78.0/24;</span><br><span class="line">allow 10.1.1.0/16;</span><br><span class="line">allow 192.168.1.0/32;</span><br><span class="line">deny all;</span><br><span class="line">proxy_pass http://shsxt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">从上到下的顺序，匹配到了便跳出。</span><br><span class="line">如上的例子先禁止了1个，</span><br><span class="line">接下来允许了3个网段，</span><br><span class="line">其中包含了一个ipv6，</span><br><span class="line">最后未匹配的IP全部禁止访问</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 负载均衡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据思想</title>
      <link href="/2018/12/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%80%9D%E6%83%B3/"/>
      <url>/2018/12/30/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%80%9D%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h3 id="1、大数据核心问题："><a href="#1、大数据核心问题：" class="headerlink" title="1、大数据核心问题："></a>1、大数据核心问题：</h3><p>==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）==</p><h3 id="2、大数据思维"><a href="#2、大数据思维" class="headerlink" title="2、大数据思维"></a>2、大数据思维</h3><p><strong>分而治之</strong></p><blockquote><p>把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q）</p></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9d7pkpij30pr0eut99.jpg" alt="enter description here"><span class="img-alt">enter description here</span></p><h3 id="3、业务场景"><a href="#3、业务场景" class="headerlink" title="3、业务场景"></a>3、业务场景</h3><p>仓储、数牌</p><h4 id="业务一：找-重复行-chongfuhang"><a href="#业务一：找-重复行-chongfuhang" class="headerlink" title="业务一：找{重复行}(chongfuhang)"></a><strong>业务一：找{重复行}(chongfuhang)</strong></h4><p><em>++现有1TB的TXT文件 ;<br>格式：数字+字符 ；<br>网速：500M/s ；<br>服务器内存大小：128M ；<br>条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++</em></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9cu9psdj30ht0a1wf1.jpg" alt="enter description here"><span class="img-alt">enter description here</span></p><p><strong>==方法==</strong></p><p> 答：共需要2次IO：2*30min=1h</p><p>==第一次IO==：</p><ol><li>给<em><code>每一行内容加上唯一标记（hashcode（内容），value（行号））</code></em>。<br>对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。</li><li><em>`对每一行的hash值进行取模运算，并放置于归类分区的小文件中</em>`。<br>由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。</li></ol><p>==第二次IO==：</p><ol start="3"><li>在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。</li></ol><h4 id="业务二：快-排序-paixu"><a href="#业务二：快-排序-paixu" class="headerlink" title="业务二：快{排序}(paixu)"></a><strong>业务二：快{排序}(paixu)</strong></h4><p><em>++现有1TB的TXT文件 ;<br>格式：数字；<br>网速：500M/s ；<br>服务器内存大小：128M ；<br>条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++</em></p><p>两次IO，2 * 30分钟 = 1小时</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9dek3xtj30pr0eu758.jpg" alt="enter description here"><span class="img-alt">enter description here</span></p><p><strong>==方法一：先全局有序后局部有序==</strong></p><p> 1.对全局按分区排序（由大到小）。<br>​    用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················）<br> 2.对局部进行排序（由大到小）。<br>​    对每个分区进行排序。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9dj82b6j30pr0ewdgm.jpg" alt="enter description here"><span class="img-alt">enter description here</span></p><p><strong>==方法二：先局部有序后全局有序==</strong></p><ol><li>先实现局部有序(小–&gt;大)。<br>将文件划分为N个分区，在每个分区内部进行排序</li><li>使用归并实现全局有序。<br>每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。</li></ol><p><img src="./images/坚持_1.jpg" alt="知否"><span class="img-alt">知否</span></p>]]></content>
      
      
      <categories>
          
          <category> 头脑风暴 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分而治之 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用Linux命令的学习（三）</title>
      <link href="/2018/12/29/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A03/"/>
      <url>/2018/12/29/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A03/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="一、服务操作"><a href="#一、服务操作" class="headerlink" title="一、服务操作"></a>一、服务操作</h2><table><thead><tr><th>列出  所有  服务</th><th>chkconfig   <br>查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。</th></tr></thead><tbody><tr><td>服务  操作</td><td>仅适用于当前    ：service 服务名 start\</td><td>stop\</td><td>status\</td><td>restart   <br>防火墙的服务名为：iptables   <br>service iptables  status\</td><td>start\</td><td>stop\</td><td>restart   查看状态\</td><td>开启\</td><td>关闭\</td><td>重启                                                                                                                                   永久关闭\</td><td>打开  ：（重启后生效） chkconfig iptables on\</td><td>off<br>注意：学习期间直接把防火墙关掉就是，工作期间也是运维人员来负责防火墙的。</td></tr><tr><td>添加  服务</td><td>1)    编辑脚本：vim myservice.sh                                                                                                                  编辑内容：                                                                                                                                                              （在最前面加一下两句）                                                                                                                              #chkconfig:   2345 80 90                                                                                                      #description:auto_run                                                                                                                                           (自己的服务脚本：开机时同步时间）                                                                                                           result=’ntpdate cn.ntp.org.cn’                                                                                                                                 退出编辑并保存：按esc键 按 ：wq                                                                                                                                                在ntpdate.log文件中输出打印：echo $result &gt; /usr/ntpdate.log                                                                                   2)    修改权限，使其拥有可执行权限:   chmod 700 myservice.sh                                                                3)    将脚本拷贝到/etc/init.d目录：                                                                                                                        4)    加入服务：chkconfig –add myservice.sh                                                                                                      5)    重启服务器，验证服务是否添加成功：date                                                                                          6）/usr目录下产生ntpdate.log</td></tr><tr><td>删除  服务</td><td>chkconfig –del name</td></tr><tr><td>更改  服务初   执行  等级</td><td>chkconfig –level 2345 服务名 off\</td><td>on   <br><br>若不加级别，默认是2345级别                                                                                                   chkconfig 服务名 on\</td><td>of f</td></tr></tbody></table><blockquote><p>各数字代表的系统初始化级别含义：</p><p>​    0：停机状态</p><p>　　1：单用户模式，root账户进行操作</p><p>　　2：多用户，不能使用net file system，一般很少用</p><p>　　3：完全多用户，一部分启动，一部分不启动，命令行界面</p><p>　　4：未使用、未定义的保留模式</p><p>　　5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。</p><p>　　6：停止所有进程，卸载文件系统，重新启动(reboot)</p><p>　　这些级别中1、2、4很少用，相对而言0、3、5、6用的会较多。3级别和5级别除了桌面相关的进程外没有什么区别。为了减少资源占用，推荐都用3级别.</p><p>注意 ：linux默认级别为3，不要把/etc/inittab 中initdefault 设置为0 和 6 </p></blockquote><h2 id="二、定时调度"><a href="#二、定时调度" class="headerlink" title="二、定时调度"></a>二、定时调度</h2><table><thead><tr><th>编辑定时任务</th><th>crontab –e                                                                                                                                                           格式：minute hour day month dayofweek command<br>分钟 小时 天 月 星期</th></tr></thead><tbody><tr><td>举例</td><td><em> </em> <em> </em> *  echo  “hello”                                                                                                                                       每分钟打印“hello”</td></tr><tr><td>时间  一到，                执行  操作  命令后</td><td>会出现：You have new mail in /var/spool/mail/root</td></tr><tr><td>查看任务执行情况</td><td>vim /var/spool/mail/root</td></tr><tr><td>查看所有用户的定时任务</td><td>ll /var/spool/cron</td></tr><tr><td>查看当前用户的定时任务</td><td>contab –l</td></tr><tr><td><em>注意</em></td><td>“*”代表任意的数字,<br> “/”代表”每隔多久”,                                                                                                         <br>  “-”代表从某个数字到某个数字, “,”分开几个离散的数字                                                                                                                    如：                                                                                                                                                                           30-40 12 <em> </em> * echo “hello”                                                                                                                                         ——–每天12点30分至40分期间，每分钟执行一次命令                                                                                      30,40                                                                                                                                                                                     ——–每天12点30分和12点40分                                                                                                                            0/5                                                                                                                                                                                 ——–每天的12点整至12点55分期间，每隔5分钟执行一次命令</td></tr></tbody></table><h2 id="三、进程操作"><a href="#三、进程操作" class="headerlink" title="三、进程操作"></a>三、进程操作</h2><table><thead><tr><th>查看  进程</th><th>ps  -aux                                                                                                                                                                           -a 列出所有          <br>-u 列出用户   <br>-x 详细列出，如cpu、内存等                                                                                     <br>-e 显示所有进程   <br> -f 全格式                                                                                                                                     ps  - ef  \</th><th>grep ssh   查看所有进程里CMD是ssh 的进程信息  ，进程号（PID）                                                                                  ps -aux –sort –pcpu   根据 CPU 使用来升序排序</th></tr></thead><tbody><tr><td>使程序   后台  运行</td><td>只需要在命令后添加  &amp; 符号                                                                                                                            echo “hello” &amp;   jobs –l      —–列出当前连接的所有后台进程（jobs仅适用于当前端）                                                   ps  -ef \</td><td>grep 进程名          —-（推荐）列出后台进程</td></tr><tr><td>杀死    进程</td><td>（强制）kill -9 pid   <br>ps 命令先查出对应程序的PID或PPID ，然后杀死掉进程</td></tr></tbody></table><h2 id="四、其他命令"><a href="#四、其他命令" class="headerlink" title="四、其他命令"></a>四、其他命令</h2><table><thead><tr><th>wget</th><th>1）   安装：yum install wget  –y                                                                                                        2）   用法：wget [option] 网址  -O  指定下载保存的路径                                                                                          3）   More Actions一个从网络上自动下载文件的自由工具；<br>支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议，可以使用 HTTP 代理；<br>也可用于做爬虫<br>4）举例：wget  <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a>  -O baidu.html</th></tr></thead><tbody><tr><td>yum</td><td>1）   备份原镜像：                                                                                                                                                   cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOSBase.repo.backup                                                                                 2）   下载新镜像：                                                                                                                                         <strong>wget</strong> -O /etc/yum.repos.d/CentOS-Base.repo      <a href="http://mirrors.aliyun.com/repo/Centos-6.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/repo/Centos-6.repo</a>                                                                                                                                                                             3）  查看文件内容：vim /etc/yum.repos.d/CentOS-Base.repo                                                                      4）  生成缓存：yum makecache<br>5）查看当前源:yum list \</td><td>head -50</td></tr><tr><td>rpm</td><td>yum list \</td><td>head -501）   安装 rpm –ivh rpm包                                                                                                                                       2）   查找已安装的rpm包：rpm –q ntp                                                                                                                  3）   卸载：rpm –e ntp-4.2.6p5-10.el6.centos.2.x86_64（全名）</td></tr><tr><td>tar</td><td>1）   解压：tar  -zvxf  xxxx.tar.gz                                                                                                                             2）   压缩：tar -zcf 压缩包命名 压缩目标                                                                                             3）   例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61                                                                                4）   -z   gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加                                                                                  <em>       -x  解压  -c  压缩   -f   目标文件，压缩文件新命名或解压文件名                                                                                       </em>       -v  解压缩过程信息打印</td></tr><tr><td>zip</td><td>1）  安装zip：yum install zip –y                                                                                                                          2）  压缩命令：zip   -r 包名 目标目录                                                                                                              3）  安装 ：unzip,yum   install unzip –y                                                                                                              4）  解压  ：unzip   filename</td></tr></tbody></table><h2 id="五、安装部署"><a href="#五、安装部署" class="headerlink" title="五、安装部署"></a>五、安装部署</h2><h3 id="JDK-部署"><a href="#JDK-部署" class="headerlink" title="JDK 部署"></a>JDK 部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1)     解压: tar -zxf jdk-7u80-linux-x64.tar.gz                                        2)     配置环境变量                                                                            编辑配置文件：vim /etc/profile                                                          编辑内容 ：                                                                            JAVA_HOME= /usr/soft/jdk1.7.0_75                                                      PATH=$PATH:$JAVA_HOME/bin                                                     3)     重新加载环境变量：source  /etc/profile                                           4)     验证: java  -version</span><br></pre></td></tr></table></figure><h3 id="mysql部署"><a href="#mysql部署" class="headerlink" title="mysql部署"></a>mysql部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum安装 mysql                                                                         1)   yum install mysql-server -y                                                     2)   yum install mysql-devel -y                                                       3)   service mysqld start                                                             4)   mysql -uroot -p                                                                 5)   mysqladmin -u root  password 123456</span><br></pre></td></tr></table></figure><h3 id="Tomcat部署"><a href="#Tomcat部署" class="headerlink" title="Tomcat部署"></a>Tomcat部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1)下载tomcat:</span><br><span class="line">http://tomcat.apache.org/</span><br><span class="line">2)启动tomcat</span><br><span class="line">在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务</span><br><span class="line">3)关闭tomcat服务，可以用shutdown.sh命令</span><br><span class="line">或者ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令。</span><br><span class="line">4)jps查看系统当前运行在jvm上的进程情况</span><br><span class="line">jps是JDK1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。</span><br><span class="line">Bootstrap是tomcat的进程名字，后面跟的是它的PID</span><br><span class="line">5）验证</span><br><span class="line">先把防火墙关了（service iptables stop），然后浏览器端访问虚拟机IP的8080端口</span><br></pre></td></tr></table></figure><h3 id="克隆虚拟机"><a href="#克隆虚拟机" class="headerlink" title="克隆虚拟机"></a>克隆虚拟机</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">修改配置：</span><br><span class="line">1)网卡ip     /etc/sysconfig/network-sr…/ifcfg-eth0 </span><br><span class="line">2)hostname   /etc/sysconfig/network</span><br><span class="line">3)删除网络规则 rm -rf /etc/udev/rules.d/70--….-net.rules</span><br><span class="line">4)重启生效 reboot</span><br></pre></td></tr></table></figure><h2 id="六、免密登录"><a href="#六、免密登录" class="headerlink" title="六、免密登录"></a>六、免密登录</h2><h3 id="1、工作原理："><a href="#1、工作原理：" class="headerlink" title="1、工作原理："></a>1、工作原理：</h3><blockquote><p>1.Server A向Server B发送一个连接请求。<br> 2.Server B得到Server A的信息后，在本地的authorized_keys文件中查找A存放在B上的公钥，如果有相应的公钥，则随机生成一个字符串，并用Server A的公钥加密，接着发送给Server A。<br> 3.Server A得到Server B发来的消息后，使用私钥进行解密，然后将解密后的字符串发送给Server B。Server B用原来随机生成的字符串和A发过来的字符串进行对比，如果一致，则允许免登录。<br> 总结：A要免密码登录到B，B首先要拥有A的公钥，然后B要做一次加密验证。对于非对称加密，公钥加密的密文不能公钥解开，只能私钥解开。</p></blockquote><h3 id="2、方法一："><a href="#2、方法一：" class="headerlink" title="2、方法一："></a>2、方法一：</h3><p> 1）   生成公钥和密钥：</p><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>并且回车3次                                                                                                  </p><p>（在用户的root目录生成一个 “.ssh”的文件夹）                                                                                                                 2）   查看公钥和私钥：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ll ~/.ssh</span><br></pre></td></tr></table></figure><p>（目录中会有以下几个文件）                                                                                                                         <code>authorized_keys</code>:存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥                                                 <code>id_rsa</code> : 生成的私钥文件                                                                                                                              <code>id_rsa.pub</code> ： 生成的公钥文件                                                                                                                                        <code>know_hosts</code> : 已知的主机公钥清单                                                                                                                                                       <em>                        如果希望ssh公钥生效需满足至少下面两个条件：                                                                                                  </em>                        1&gt; .ssh目录的权限必须是700                                                                                                                                *                        2&gt; .ssh/authorized_keys文件权限必须是600     </p><p>3）   将A的.ssh目录下的公钥追加拷贝到B的authorized_keys文件里     </p><ul><li>法一：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -p ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:/root/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>举例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@test .ssh]# scp -p ~/.ssh/id_rsa.pub root@192.168.91.135:/root/.ssh/authorized_keys</span><br><span class="line">root@192.168.91.135's password: </span><br><span class="line">id_rsa.pub 100% 408 0.4KB/s 00:00 </span><br><span class="line">[root@test .ssh]# </span><br><span class="line">[root@test .ssh]# </span><br><span class="line">[root@test .ssh]# </span><br><span class="line">[root@test .ssh]# ssh root@192.168.91.135</span><br><span class="line">Last login: Mon Oct 10 01:27:02 2016 from 192.168.91.133</span><br><span class="line">[root@localhost ~]#</span><br></pre></td></tr></table></figure><ul><li>法二：　分为两步操作：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scp ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:pub_key //将文件拷贝至远程服务器</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat ~/pub_key &gt;&gt;~/.ssh/authorized_keys //将内容追加到authorized_keys文件中， 不过要登录远程服务器来执行这条命令</span></span><br></pre></td></tr></table></figure><ul><li><p>法三：手动复制：如果有多台节点时：A  →  B</p><p> &lt; 1、拷贝A的.ssh目录下的公钥：</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim id_rsa.pub</span><br></pre></td></tr></table></figure><p>   &lt;2、将A的公钥复制好后，在主机B上的~/.ssh目录下，创建并编辑authorized_keys文件，接着黏贴进去。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim authorized_keys</span><br></pre></td></tr></table></figure><p>4)     验证：</p><p>用Scp远程拷贝命令，在A主机上随便拷贝一份文件到B主机上，如果不需要密码，则说明免密码登录配置成功。</p><h3 id="3、方法二：-通过ssh-copy-id的方式"><a href="#3、方法二：-通过ssh-copy-id的方式" class="headerlink" title="3、方法二： 通过ssh-copy-id的方式"></a>3、方法二： <strong>通过ssh-copy-id的方式</strong></h3><p>前提：公钥和私钥已经生成</p><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub &lt;romte_ip&gt;</span><br></pre></td></tr></table></figure><p>举例：</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@<span class="built_in">test</span> .ssh]<span class="comment"># ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.91.135 </span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> root@192.168.91.135<span class="string">'s password: </span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Now try logging into the machine, with <span class="string">"ssh '192.168.91.135'"</span>, and check <span class="keyword">in</span>:</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> .ssh/authorized_keys</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> to make sure we haven<span class="string">'t added extra keys that you weren'</span>t expecting.</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@<span class="built_in">test</span> .ssh]<span class="comment"># ssh root@192.168.91.135</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> Last login: Mon Oct 10 01:25:49 2016 from 192.168.91.133</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> [root@localhost ~]<span class="comment">#</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><p>常见错误：</p><blockquote><p>　[root@test ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.91.135</p><p>　-bash: ssh-copy-id: command not found //提示命令不存在</p><p>　解决办法：yum -y install openssh-clients</p></blockquote><h3 id="4、（此方法有待考究）法三："><a href="#4、（此方法有待考究）法三：" class="headerlink" title="4、（此方法有待考究）法三："></a>4、（<strong>此方法有待考究）</strong>法三：</h3><p>通过Ansible实现   批量   免密                                                                                                                   </p><p>   1）、 <strong>将需要做免密操作的机器hosts添加到/etc/ansible/hosts下：</strong>                                                                                             </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Avoid close]</span><br><span class="line"> 192.168.91.132</span><br><span class="line">　　192.168.91.133</span><br><span class="line">　　192.168.91.134</span><br></pre></td></tr></table></figure><p>​      2）、 <strong>执行命令进行免密操作</strong>  ：​                 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible &lt;groupname&gt; -m authorized_key -a &quot;user=root key=&apos;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;) &#125;&#125;&apos;&quot; -k</span><br></pre></td></tr></table></figure><p>​                                                                                                                                                                                                                                                                                                                                             示例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@test sshpass-1.05]# ansible test -m authorized_key -a "user=root key='&#123;&#123; lookup('file','/root/.ssh/id_rsa.pub') &#125;&#125;'" -k</span><br><span class="line">　　SSH password: -----&gt;输入密码</span><br><span class="line">　　192.168.91.135 | success &gt;&gt; &#123;</span><br><span class="line">　　"changed": true, </span><br><span class="line">　　"key": "ssh-rsa 　　 AAAAB3NzaC1yc2EAAAABIwAAAQEArZI4kxlYuw7j1nt5ueIpTPWfGBJoZ8Mb02OJHR8yGW7A3izwT3/uhkK7RkaGavBbAlprp5bxp3i0TyNxa/apBQG5NiqhYO8YCuiGYGsQAGwZCBlNLF3gq1/18B6FV5moE/8yTbFA4dBQahdtVP PejLlSAbb5ZoGK8AtLlcRq49IENoXB99tnFVn3gMM0aX24ido1ZF9RfRWzfYF7bVsLsrIiMPmVNe5KaGL9kZ0svzoZ708yjWQQCEYWp0m+sODbtGPC34HMGAHjFlsC/SJffLuT/ug/hhCJUYeExHIkJF8OyvfC6DeF7ArI6zdKER7D8M0SM　　WQmpKUltj2nltuv3w== root@localhost.localdomain", </span><br><span class="line">　　"key_options": null, </span><br><span class="line">　　"keyfile": "/root/.ssh/authorized_keys", </span><br><span class="line">　　"manage_dir": true, </span><br><span class="line">　　"path": null, </span><br><span class="line">　　"state": "present", </span><br><span class="line">　　"unique": false, </span><br><span class="line">　　"user": "root"</span><br><span class="line">　　&#125;</span><br><span class="line">　　[root@test sshpass-1.05]#</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用Linux命令的学习（二）</title>
      <link href="/2018/12/28/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A02/"/>
      <url>/2018/12/28/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A02/</url>
      
        <content type="html"><![CDATA[<p>​<br>​ </p><p>[TOC]</p><h2 id="一、磁盘指令"><a href="#一、磁盘指令" class="headerlink" title="一、磁盘指令"></a>一、磁盘指令</h2><h3 id="1、查看硬盘信息"><a href="#1、查看硬盘信息" class="headerlink" title="1、查看硬盘信息"></a>1、查看硬盘信息</h3><blockquote><p>命令：df</p></blockquote><p><code>（默认大小以kb显示） df -k（以kb为单位） df -m（ 以mb为单位） df –h （易于阅读）</code></p><h3 id="2、查看文件-目录的大小"><a href="#2、查看文件-目录的大小" class="headerlink" title="2、查看文件/目录的大小"></a>2、查看文件/目录的大小</h3><blockquote><p>命令：du filename|foldername</p></blockquote><p><code>（默认单位为kb）-k    kb单位 -m    mb单位 -a 所有文件和目录  -h 更易于阅读​    --max-depth=0    目录深度</code></p><h2 id="二、网络指令"><a href="#二、网络指令" class="headerlink" title="二、网络指令"></a>二、网络指令</h2><h3 id="1、查看网络配置信息"><a href="#1、查看网络配置信息" class="headerlink" title="1、查看网络配置信息"></a>1、查看网络配置信息</h3><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> 命令:ifconfig</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15t5lxs9oj30fe07fmzk.jpg" alt=""></p><blockquote><p>箭头1指向的是本机IP，箭头2为广播地址，箭头3位子网掩码。</p></blockquote><h3 id="2、测试与目标主机的连通性"><a href="#2、测试与目标主机的连通性" class="headerlink" title="2、测试与目标主机的连通性"></a>2、测试与目标主机的连通性</h3><blockquote><p>命令：ping remote_ip     </p></blockquote><p><code>ctrl + c :结束ping进程</code></p><p>可以ping通Windows系统的IP</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15t87brh1j30fe04675r.jpg" alt=""></p><p>输入ping 192.168.78.192代表测试本机和192主机的网络情况，</p><p>箭头1表示一共接收到了3个包，箭头2表示丢包率为0，表示两者之间的网络顺畅。</p><p>注意：linux系统的ping命令会一直发送数据包，进行测试，除非认为的按ctrl + c停止掉，</p><p>​           windows系统默认只会发4个包进行测试，以下为windows的dos命令。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15t9ie02zj30fe07vq5g.jpg" alt=""></p><h3 id="3、显示各种网络相关信息"><a href="#3、显示各种网络相关信息" class="headerlink" title="3、显示各种网络相关信息"></a>3、显示各种网络相关信息</h3><blockquote><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> netstat -anpt</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">-a (all)显示所有选项，默认不显示LISTEN相关</span><br><span class="line">-t (tcp)仅显示tcp相关选项</span><br><span class="line">-u (udp)仅显示udp相关选项</span><br><span class="line">-n 拒绝显示别名，能显示数字的全部转化成数字。</span><br><span class="line">-l 仅列出有在 Listen (监听) 的服務状态</span><br><span class="line"></span><br><span class="line">-p 显示建立相关链接的程序名</span><br><span class="line">-r 显示路由信息，路由表</span><br><span class="line">-e 显示扩展信息，例如uid等</span><br><span class="line">-s 按各个协议进行统计</span><br><span class="line">-c 每隔一个固定时间，执行该netstat命令。</span><br><span class="line"></span><br><span class="line">提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到</span><br></pre></td></tr></table></figure><p><code>查看端口号（是否被占用）(1)、lsof -i:端口号  （需要先安装lsof）(2)、netstat -tunlp|grep 端口号</code></p><h3 id="4、测试远程主机的网络端口"><a href="#4、测试远程主机的网络端口" class="headerlink" title="4、测试远程主机的网络端口"></a>4、测试远程主机的网络端口</h3><p>安装telnet:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install telnet -y</span><br></pre></td></tr></table></figure><p>查看本机能否连上远程主机的端口号:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet ip  port</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15tfzb6xtj30dv02y74u.jpg" alt=""></p><p><code>测试成功后，按ctrl + ] 键，然后弹出telnet&gt;时，再按q退出</code></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15tgyzcthj30fa04m3zb.jpg" alt=""></p><h3 id="5、http请求模拟"><a href="#5、http请求模拟" class="headerlink" title="5、http请求模拟"></a>5、http请求模拟</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">命令: curl  [option]  [url]</span><br></pre></td></tr></table></figure><p>举例：</p><p> 模拟请求百度</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X get www.baidu.com</span><br></pre></td></tr></table></figure><blockquote><p>用法解释：</p><p>-X/–request [GET|POST|PUT|DELETE|…]  使用指定的http method发出 http request</p><p>-H/–header               设定request里的header</p><p>-i/–include              显示response的header</p><p>-d/–data                  设定 http parameters </p><p>-v/–verbose               输出比较多的信息</p><p>-u/–user                  使用者账号，密码</p><p>-b/–cookie                cookie</p></blockquote><p>参数-X跟–request兩个功能是一样的 </p><p>举例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -X GET http://www.baidu.com/  </span><br><span class="line">curl --request GET http://www.baidu.com/ </span><br><span class="line">curl -X GET "http://www.rest.com/api/users"</span><br><span class="line">curl -X POST "http://www.rest.com/api/users"</span><br><span class="line">curl -X PUT "http://www.rest.com/api/users"</span><br><span class="line">curl -X DELETE "http://www.rest.com/api/users"</span><br></pre></td></tr></table></figure><h2 id="三、系统管理指令"><a href="#三、系统管理指令" class="headerlink" title="三、系统管理指令"></a>三、系统管理指令</h2><h3 id="1、用户操作"><a href="#1、用户操作" class="headerlink" title="1、用户操作"></a>1、用户操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    操作            命令</span><br><span class="line">创建用户       useradd|adduser username</span><br><span class="line">修改密码       passwd username</span><br><span class="line">删除用户       userdel –r username</span><br><span class="line">修改用户（已下线）：</span><br><span class="line">                 修改用户名: usermod –l new_name oldname</span><br><span class="line">                    锁定账户: usermod –L username</span><br><span class="line">                    解除账户： usermod –U username</span><br><span class="line">查看当前登录用户仅root 用户：whoami  或 cat /etc/shadow</span><br><span class="line">                 普通用户：cat /etc/passwd</span><br><span class="line">                 切换用户：su username</span><br><span class="line">                 推出当前用户：exit</span><br></pre></td></tr></table></figure><h3 id="2、用户组操作"><a href="#2、用户组操作" class="headerlink" title="2、用户组操作"></a>2、用户组操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   操作            命令</span><br><span class="line">   创建用户组         groupadd groupname</span><br><span class="line">删除用户组         groupdel groupname</span><br><span class="line">修改用户组         groupmod –n new_name old_name</span><br><span class="line">查看用户组         groups  （查看的是当前用户所在的用户组）</span><br></pre></td></tr></table></figure><h3 id="3、用户-用户组"><a href="#3、用户-用户组" class="headerlink" title="3、用户+用户组"></a>3、用户+用户组</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">     操作                命令</span><br><span class="line">   修改用户的主组         usermod –g groupname username</span><br><span class="line">给用户追加附加组    usermod –G groupname username</span><br><span class="line">查看用户组中用户数   cat /etc/group</span><br><span class="line">注意：创建用户时，系统默认会创建一个和用户名字一样的主组</span><br></pre></td></tr></table></figure><h3 id="4、系统权限"><a href="#4、系统权限" class="headerlink" title="4、系统权限"></a>4、系统权限</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    操作                     命令</span><br><span class="line"> 查看/usr下所有权限   ll /usr</span><br><span class="line">                       权限类别r（读取：4） w（写入：2） x（执行：1） </span><br><span class="line">                       三个为一组，无权限用 —代替</span><br><span class="line">                    UGO模型U（User） G(Group)  O(其他)</span><br><span class="line">权限修改    </span><br><span class="line">                     修改所有者：chown username file|folder</span><br><span class="line">              (递归)修改所有者和所属组： chown -r username：groupname file|folder </span><br><span class="line">              修改所属组：chgrp groupname file|folder</span><br><span class="line">              修改权限：  chmod ugo+rwx file|folder                     在Linux系统中，可以存在很多用户，（除了root）他们的权限在/home路径下对应用户名的目录下</span><br></pre></td></tr></table></figure><h2 id="四、系统配置指令"><a href="#四、系统配置指令" class="headerlink" title="四、系统配置指令"></a>四、系统配置指令</h2><h3 id="0-1环境变量"><a href="#0-1环境变量" class="headerlink" title="0.1环境变量"></a>0.1环境变量</h3><p>全局变量、局部变量</p><p>首先考虑一个问题，为什么我们先前敲的yum, service,date,useradd等等，可以直接使用，系统怎么知道这些命令对应的程序是放在哪里的呢？</p><p>这是由于无论是windows系统还是linux系统，都有一个叫做path的系统环境变量，当我们在敲命令时，系统会到path对应的目录下寻找，找到的话就会执行，找不到就会报没有这个命令。如下图：</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15ufak65qj309u031wf0.jpg" alt=""></p><p>配置系统环境变量，使得某些命令在执行时，系统可以找到命令对应的执行程序，命令才能正常执行。</p><p>我们可以查看一下，系统一共在哪些目录里寻找命令对应的程序。</p><p>命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $PATH</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15uidckb9j30fe01amxd.jpg" alt=""></p><p> 可以看到path里有很多路径，路径之间有冒号隔开。当用户敲命令时，系统会从左往右依次寻找对应的程序，有的话则运行该程序，没有的就报错，command not found.</p><p>配置全局环境变量：</p><blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> vim  /etc/profile  <span class="comment">#全局环境变量所在的文件</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>在文件中：</p><p>PATH=$PATH:(命令所在目录)</p><p>退出文件编辑后：</p><p>source  /etc/profile  </p><p> (重新加载资源，有的可能需要重启机器，这不适用于实际状况)</p></blockquote><p>配置局部环境变量：（推荐，限当前登录用户使用）</p><blockquote><p>查看所有文件(root目录下)</p><p>ls  -a    (发现隐藏文件    .bash.profile)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> vim  ~/.bash_profile   <span class="comment">#局部变量所在的文件</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"></span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>在控制台中：</p><p>export  PATH =$PATH:(命令所在目录)</p></blockquote><p>source ~/.bash_profile</p><h3 id="0-2脚本运行"><a href="#0-2脚本运行" class="headerlink" title="0.2脚本运行"></a>0.2脚本运行</h3><p>编辑脚本：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim test.sh</span><br></pre></td></tr></table></figure><p>脚本内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">！/bin/sh</span></span><br><span class="line">echo " hello test sh"</span><br></pre></td></tr></table></figure><p>给脚本添加权限：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod 700 test.sh</span><br></pre></td></tr></table></figure><p>运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">一种是到脚本的目录下执行：</span><br><span class="line">运行命令 ： ./test.sh  ,代表执行当前目录里的脚本test.sh</span><br><span class="line"></span><br><span class="line">一种是敲脚本的绝对路径：</span><br><span class="line">运行命令 ：/usr/test/test.sh</span><br><span class="line"></span><br><span class="line">第三种方式添加到环境变量中</span><br></pre></td></tr></table></figure><p>以上两种运行方式都不是很简便，因为先前我们执行yum service命令等，都是直接敲对应的命令的。所以我们也可以参照这样子做，只要我们配一个环境变量就好。</p><p>编辑环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><p>将test.sh 所在路径放到path后面即可</p><p>编辑完之后，执行命令，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><p>重新加载环境变量，此时会发现PATH路径多了一个/usr/test。</p><p>最后验证一下，直接执行test.sh</p><h3 id="1-修改主机名"><a href="#1-修改主机名" class="headerlink" title="1.修改主机名"></a>1.修改主机名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  编辑文件：      命令： vim /etc/sysconfig/network</span><br><span class="line">  文件内容：      HOSTNAME=node00</span><br><span class="line">（重启生效)reboot</span><br></pre></td></tr></table></figure><h3 id="2-DNS配置"><a href="#2-DNS配置" class="headerlink" title="2.DNS配置:"></a>2.DNS配置:</h3><p> /etc/resolv.conf 为DNS服务器的地址文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">查看DNS服务器的地址：  cat /etc/resolv.conf</span><br><span class="line">修改DNS服务器地址：  </span><br><span class="line">法一：</span><br><span class="line">编辑文件：   </span><br><span class="line">vim  /etc/sysconfig/network.scripts/ifconfig-eth0</span><br><span class="line">在配置网关时，配置DNS1=114.114.114.114（不推荐，江苏南京的IP）</span><br><span class="line"></span><br><span class="line">法二：</span><br><span class="line">编辑文件：     </span><br><span class="line">vim /etc/resolv.conf</span><br><span class="line">文件内容：（用本地网关解析）     </span><br><span class="line">nameserver 192.168.198.0( 此为虚拟机中的网关地址)</span><br></pre></td></tr></table></figure><h3 id="3-sudo权限配置"><a href="#3-sudo权限配置" class="headerlink" title="3.sudo权限配置"></a>3.sudo权限配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">   操作             命令</span><br><span class="line">编辑权限配置文件：    vim /etc/sudoers</span><br><span class="line">格式：</span><br><span class="line">        授权用户 主机=[(切换到哪些用户或用户组)] [是否需要密码验证] 路径/命令</span><br><span class="line">举例：</span><br><span class="line">        test  ALL=(root)  /usr/bin/yum,/sbin/service</span><br><span class="line">解释：</span><br><span class="line">        test用户就可以用yum和servie命令，</span><br><span class="line">   但是，使用时需要在前面加上sudo再敲命令。</span><br><span class="line">   第一次使用需要输入用户密码,且每个十五分钟需要一次密码验证</span><br><span class="line">修改：</span><br><span class="line">       test ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service</span><br><span class="line">这样就不需要密码了</span><br><span class="line">将权限赋予某个组，%+组名</span><br><span class="line">%group ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service</span><br><span class="line"></span><br><span class="line">列出用户所有的sudo权限       sudo –l</span><br><span class="line">切换用户   sudo username</span><br></pre></td></tr></table></figure><h3 id="4-系统时间"><a href="#4-系统时间" class="headerlink" title="4.系统时间"></a>4.系统时间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">操作                 命令</span><br><span class="line">查看系统时间          date           ---查看当前时间详情                                   </span><br><span class="line">                    cal            ---查看当前月日历</span><br><span class="line">                    cal 2018       ---查看2018年完整日历</span><br><span class="line">                    cal 12 2018    ---查看指定年月的日历       </span><br><span class="line"></span><br><span class="line">更新系统时间（推荐）   yum install ntpdate –y    ---安装ntp服务</span><br><span class="line">                    ntpdate cn.ntp.org.cn   ---到域名为cn.ntp.org.cn的时间服务器上同步时间</span><br></pre></td></tr></table></figure><p> 5.关于hosts配置</p><p>相当于给IP地址其别名，可以通过别名访问</p><table><thead><tr><th></th><th>路径：</th></tr></thead><tbody><tr><td>Windows系统</td><td><strong>C:/Windows/System32/drivers/etc/hosts</strong> <strong>文件</strong></td></tr><tr><td>Linux系统</td><td><strong>/etc/hosts</strong>文件：<strong>vim</strong>  +路径</td></tr><tr><td>统一 编辑格式</td><td><strong>IP</strong>地址  别名：192.168.198.128    node00</td></tr></tbody></table><h3 id="6-关于hostname配置"><a href="#6-关于hostname配置" class="headerlink" title="6.关于hostname配置"></a>6.关于hostname配置</h3><p>相当于给对应的虚拟机器起别名</p><table><thead><tr><th>Linux系统：</th><th><strong>vi /etc/sysconfig/network</strong></th></tr></thead><tbody><tr><td>编辑内容：</td><td><strong>HOSTNAME=node01</strong></td></tr></tbody></table><h2 id="五、重定向与管道符"><a href="#五、重定向与管道符" class="headerlink" title="五、重定向与管道符"></a>五、重定向与管道符</h2><table><thead><tr><th>输出  重定向</th><th>输出重定向到一个文件或设备：  <br>&gt;   覆盖原来的文件         &gt;&gt;   追加原来的文件                                                                                                              举例：<br> ls &gt; log                           — 在log文件中列出所有项，并覆盖原文件                                                         echo   “hello”&gt;&gt;log     —将hello追加到log文件中</th></tr></thead><tbody><tr><td>输入  重定向</td><td><strong>&lt;</strong>         输入重定向到一个程序 <br> 举例：cat <strong>&lt;</strong> log             —将log文件作为cat命令的输入，查看log文件的内容</td></tr><tr><td>标准   输出  重定向</td><td><strong>1 &gt;</strong> 或   <strong>&gt;</strong>                                                                                                                                                           含义：                                                                                                                                                                                 输出重定向时，只用正确的输出才会重定向到指的文件中                                                                                      错误的则会直接打印到屏幕上</td></tr><tr><td>错误   输出  重定向</td><td><strong>2 &gt;</strong>                                                                                                                                                                                     含义：                                                                                                                                                                                    错误的输出会重定向到指定文件里，正确的日志则直接打印到屏幕上。</td></tr><tr><td>结合  使用</td><td><strong>2&gt;&amp;1</strong>                                                                                                                                                                                 含义：                                                                                                                                                                                   将无论是正确的输出还是错误的输出都重定向到指定文件</td></tr><tr><td>管道</td><td>**\</td><td>**                                                                                                                                                                                        含义：                                                                                                                                                                                             把前一个输出当做后一个输入                                                                                                                                       grep 通过正则搜索文本，并将匹配的行打印出来                                                                                                   netstat -anp \</td><td>grep 22   把netstat –anp 命令的输出 当做是grep 命令的输入</td></tr><tr><td>命令  执行  控制</td><td><strong>&amp;&amp;</strong>   前一个命令执行成功才会执行后一个命令                                                                                                                   **\</td><td>\</td><td>**      前一个命令执行失败才会执行后一个命令</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用Linux命令的学习（一）</title>
      <link href="/2018/12/27/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A01/"/>
      <url>/2018/12/27/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A01/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="一、命令指南（manual）：man"><a href="#一、命令指南（manual）：man" class="headerlink" title="一、命令指南（manual）：man"></a>一、命令指南（manual）：man</h2><p>安装 man：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install man –y</span><br></pre></td></tr></table></figure><blockquote><p>（-y 表示获得允许，无需确认）</p></blockquote><p>查看ls命令指南：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">man ls</span><br></pre></td></tr></table></figure><h2 id="二、目录操作命令"><a href="#二、目录操作命令" class="headerlink" title="二、目录操作命令"></a>二、目录操作命令</h2><p><strong>切换</strong>目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd + 目录的路径</span><br></pre></td></tr></table></figure><p><strong>查看</strong>当前目录所在的完整路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pwd</span><br></pre></td></tr></table></figure><p><strong>新建</strong>目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir +目录名字</span><br></pre></td></tr></table></figure><p><strong>查看</strong> —当前目录所用有的子目录和文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls   ，ll等价于  ls –l</span><br></pre></td></tr></table></figure><p>​        查看—目录下的所有东西（包括隐藏文件）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls –al   等价于 ll -a</span><br></pre></td></tr></table></figure><p><strong>拷贝</strong>目录或文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp –r install.log  install2.log</span><br></pre></td></tr></table></figure><blockquote><p>复制文件，-r 表示递归复制，此时，可用于复制整个目录</p></blockquote><p><strong>删除</strong>目录或文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm  -r install.log</span><br></pre></td></tr></table></figure><blockquote><p>此时需要手动输入y ，代表确认删除。可加 –f参数，直接删除，无需确认。</p><p>当需要一个目录下所有东西时，加-r参数，代表遍历删除。</p><p>(rmdir只能删除空目录)</p></blockquote><p><strong>移动</strong>目录或文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv + 目录/文件名字 + 其他路径</span><br></pre></td></tr></table></figure><p>​         将test目录移动到  根目录/ 下 : </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv test /</span><br></pre></td></tr></table></figure><blockquote><p>（如果移动到当前目录，用另外一个名称，则可以实现重命名的效果）</p></blockquote><p><strong>更改</strong>文件或目录的名字：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv + 旧目录名字 + 新目录名</span><br></pre></td></tr></table></figure><blockquote><p>  <strong>-r</strong> 用于递归的拷贝，删除，移动目录</p></blockquote><h2 id="三、文件操作命令"><a href="#三、文件操作命令" class="headerlink" title="三、文件操作命令"></a>三、文件操作命令</h2><h3 id="1、一般文件操作"><a href="#1、一般文件操作" class="headerlink" title="1、一般文件操作"></a>1、一般文件操作</h3><p><code>在Linux中：一切皆文件</code></p><p><strong>新建</strong>文件：touch  install.log<br>​        (vim install.log   编辑文件，如果文件不存在，就会新建一个对应的文件，并进入文件的编辑模式，如果按 :wq 会保存文件并退出，如果按 :q 则不保存退出)<br>​<br><strong>查看</strong>文件内容：cat +（文件名）<br>（一次性显示整个文件的内容，文件内容过多时文本在屏幕上迅速闪过（滚屏），用,户体验不好）</p><p>一次命令显示一屏文本：</p><p>满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  more +（文件名）</span><br><span class="line">  </span><br><span class="line">  按键         效果  </span><br><span class="line">Space         显示下一屏文本内容</span><br><span class="line">B             显示上一屏文本内容</span><br><span class="line">Enter         显示下一行文本内容</span><br><span class="line">Q             退出查看</span><br></pre></td></tr></table></figure><pre><code>  less+（文件名）按键                  效果 Q                 退出less 命令  h                 显示帮助界面 u                 向后滚动半页  d                 向前翻半页  e | Enter（回车）  向后翻一行文本 space(空格键)      滚动一页  b                 向后翻一页  [pagedown]：      向下翻动一页  [pageup]：        向上翻动一页  上下键，          向上一行，向下一行</code></pre><p>从<strong>头部</strong>打印文件内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head  -10 +（文件名）  打印文件1到10行</span><br></pre></td></tr></table></figure><p>从<strong>尾部</strong>打印文件内容​  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tail -10 +（文件名）打印文件最后10行</span><br></pre></td></tr></table></figure><blockquote><p>tail还可用来查看文件内容的更新变化</p><p>tail -f (文件名)  </p></blockquote><p><strong>查找</strong>文件或目录​    </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find +（路径名） –name +（文件名）</span><br></pre></td></tr></table></figure><p>​        举例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -name profile</span><br></pre></td></tr></table></figure><p>​    在/(根目录)目录下查找 名字为profile的文件或目录</p><p> 也可利用正则：<br>​             举例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find /etc -name pro*</span><br></pre></td></tr></table></figure><p>​         在/etc目录下查找以pro开头的文件或目录</p><blockquote><p>注意：</p><ul><li><p>路径越精确，查找的范围越小，速度越快 </p></li><li><p>查找的目录必须是文件所在目录的父级目录</p></li></ul></blockquote><h3 id="2、文件编辑"><a href="#2、文件编辑" class="headerlink" title="2、文件编辑"></a>2、文件编辑</h3><h4 id="vi"><a href="#vi" class="headerlink" title="vi"></a>vi</h4><p>（1） vi    进入编辑模式 —–&gt;按i   进入插入模式 ——-&gt;  按Esc 退出编辑模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi  filename   :打开或新建文件，并将光标置于第一行首 </span><br><span class="line">vi +n filename ：打开文件，并将光标置于第n行首 </span><br><span class="line">vi + filename  ：打开文件，并将光标置于最后一行首 </span><br><span class="line">vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的字符串所在的行首</span><br></pre></td></tr></table></figure><blockquote><p> filename  为文件名</p></blockquote><p>（2）在文件vi（文件编辑）模式下</p><ul><li><strong>命令行模式</strong> </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">:w      保存</span><br><span class="line">:q      退出</span><br><span class="line">:wq     保存并退出</span><br><span class="line">:q!     强制退出，不保存</span><br><span class="line">:set nu |ctrl+g    显示文本行数</span><br><span class="line">:set nonu          去除显示的行数</span><br><span class="line">:s/p1/p2/g         将当前行中所有p1均用p2替代 </span><br><span class="line">:n1,n2s/p1/p2/g    将第n1至n2行中所有p1均用p2替代 </span><br><span class="line">:g/p1/s//p2/g      将文件中所有p1均用p2替换</span><br></pre></td></tr></table></figure><ul><li><strong>一般模式</strong> </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">按键：</span><br><span class="line">yy    复制光标所在行(常用) </span><br><span class="line">nyy   复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) </span><br><span class="line">p|P   p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)</span><br><span class="line">G     光标移至第最后一行</span><br><span class="line">nG    光标移动至第N行行首</span><br><span class="line">n+    光标下移n行 </span><br><span class="line">n-    光标上移n行 </span><br><span class="line">H     光标移至屏幕顶行 </span><br><span class="line">M     光标移至屏幕中间行 </span><br><span class="line">L     光标移至屏幕最后行 </span><br><span class="line"></span><br><span class="line">dd    删除所在行 </span><br><span class="line">x或X  删除一个字符，x删除光标后的，而X删除光标前的 </span><br><span class="line">u     撤销(常用)</span><br><span class="line"></span><br><span class="line">：N,Md    删除第N行到第M行</span><br><span class="line">：,$-1d   删除当前光标到倒数第一行数据</span><br><span class="line"></span><br><span class="line">按键：</span><br><span class="line">    i: 在当前光标所在字符的前面，转为输入模式；</span><br><span class="line">    a: 在当前光标所在字符的后面，转为输入模式；</span><br><span class="line">    o: 在当前光标所在行的下方，新建一行，并转为输入模式；</span><br><span class="line">    I：在当前光标所在行的行首，转换为输入模式</span><br><span class="line">    A：在当前光标所在行的行尾，转换为输入模式</span><br><span class="line">    O：在当前光标所在行的上方，新建一行，并转为输入模式；</span><br><span class="line"></span><br><span class="line">---逐字符移动：</span><br><span class="line">h: 左    l: 右</span><br><span class="line"></span><br><span class="line">j: 下k: 上</span><br></pre></td></tr></table></figure><blockquote><p>（不同：在我的xshell中是 yyn实现复制所在行的向下n行）</p><p>（nB|nb:光标向上移动n行）</p></blockquote><h4 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h4><p>Vim是从 vi 发展出来的一个文本编辑器</p><p>安装vim 软件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install vim -y</span><br></pre></td></tr></table></figure><ul><li><em>用vim 打开/etc/profile 文件，</em></li><li><em>特点：编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强</em> ，其他均与vi相同</li></ul><h3 id="3、文件上传下载"><a href="#3、文件上传下载" class="headerlink" title="3、文件上传下载"></a>3、文件上传下载</h3><p>安装上传下载命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install lrzsz -y</span><br></pre></td></tr></table></figure><h4 id="上传文件：（windows—-gt-linux）"><a href="#上传文件：（windows—-gt-linux）" class="headerlink" title="上传文件：（windows—&gt;linux）"></a>上传文件：（windows—&gt;linux）</h4><blockquote><p>命令 ：rz  </p><p>弹出windows上传文件窗口</p></blockquote><h4 id="下载文件：-linux—-gt-windows"><a href="#下载文件：-linux—-gt-windows" class="headerlink" title="下载文件：(linux—&gt;windows)"></a>下载文件：(linux—&gt;windows)</h4><p><code>注意</code>：sz命令只能下载文件，不能下载目录，推荐将目录压缩成tar包或使用工具软件：Winscp【Xftp】</p><blockquote><p>命令：sz  （文件名）</p><p>弹出windows下载窗口,下载文件到指定文件目录,下载完之后，按ctrl+c结束。</p></blockquote><h3 id="4、文件传输"><a href="#4、文件传输" class="headerlink" title="4、文件传输"></a>4、文件传输</h3><h4 id="1）本地→远程"><a href="#1）本地→远程" class="headerlink" title="(1）本地→远程"></a>(1）本地→远程</h4><blockquote><p>文件  ：  scp local_file remote_username@remote_ip:remote_folder   </p><p>目录 ：  scp -r local_folder remote_username@remote_ip:remote_folder</p></blockquote><p>举例：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">方式一：</span><br><span class="line">  scp -rf /etc/profile root@192.168.198.128:/etc/</span><br><span class="line"></span><br><span class="line">方式二:</span><br><span class="line">  scp  -r /etc/profile root@node01 /etc/ </span><br><span class="line">  </span><br><span class="line">方式三：</span><br><span class="line">  scp -r /etc/profile node01:/etc/</span><br><span class="line">  </span><br><span class="line">方式四：</span><br><span class="line">  scp -r /ec/profile node01:'pwd'</span><br></pre></td></tr></table></figure><blockquote><p><strong>scp</strong> ：远程传输文件命令</p><p><strong>-r</strong>  ：- 指的是后面跟的是参数    r  指的是遍历指定文件    <strong>f</strong>  指的是不用询问</p><p><strong>/etc/profile</strong>  : 是指定传输的文件</p><p><strong>root</strong>： 远程机器的账户名      <strong>@</strong>    远程机器的IP地址   <strong>：</strong>   <strong>/etc/</strong>    远程机器上指定的目录</p><p><strong>node01</strong>：远程机器的别名</p><p><strong>‘pwd’</strong>： 本地要远程传输文件所在的目录</p></blockquote><blockquote><p>第一次远程拷贝时，需要在箭头1初输入yes确认一下，验证一下远程主机。然后在箭头2处输入一下远程主机的密码。</p></blockquote><h4 id="2）远程→本地"><a href="#2）远程→本地" class="headerlink" title="(2）远程→本地"></a>(2）远程→本地</h4><blockquote><p>文件 ： scp remote_username@remote_ip:remote_file local_folder</p><p>目录 ： scp remote_username@remote_ip:remote_folder local_folder   </p></blockquote><h2 id="四、Linux系统目录结构"><a href="#四、Linux系统目录结构" class="headerlink" title="四、Linux系统目录结构"></a>四、Linux系统目录结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Linux目录结构：</span><br><span class="line"> </span><br><span class="line">bin  存放二进制可执行文件(ls,cat,mkdir等)                                                          </span><br><span class="line">boot  存放用于系统引导时使用的各种文件</span><br><span class="line"></span><br><span class="line">dev 用于存放设备文件</span><br><span class="line"></span><br><span class="line">etc  存放系统配置文件</span><br><span class="line"></span><br><span class="line">home 存放所有用户文件的根目录</span><br><span class="line"></span><br><span class="line">lib  存放跟文件系统中的程序运行所需要的共享库及内核模块</span><br><span class="line"></span><br><span class="line">mnt  系统管理员安装临时文件系统的安装点</span><br><span class="line"></span><br><span class="line">opt  额外安装的可选应用程序包所放置的位置</span><br><span class="line"></span><br><span class="line">proc  虚拟文件系统，存放当前内存的映射</span><br><span class="line"></span><br><span class="line">root  超级用户目录</span><br><span class="line"></span><br><span class="line">sbin  存放二进制可执行文件，只有root才能访问</span><br><span class="line"></span><br><span class="line">tmp  用于存放各种临时文件</span><br><span class="line"></span><br><span class="line">usr  用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录</span><br><span class="line"></span><br><span class="line">var  用于存放运行时需要改变数据的文件</span><br></pre></td></tr></table></figure><h2 id="五、shell脚本命令"><a href="#五、shell脚本命令" class="headerlink" title="五、shell脚本命令"></a>五、shell脚本命令</h2><p>shell编程：代码编辑器 、 脚本解释器</p><p>Java ： 强类型语言（变量的使用需要先声明），静态语言（变量类型声明后不可变），（半编译半解释）</p><p>shell ：弱类型语言（类型推断）， 动态语言（变量的类型可变） ，只需要一个解释器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Java语言执行shell脚本</span></span><br><span class="line">String cmd = <span class="string">"sh /root/test/test.sh "</span>;</span><br><span class="line">Process proc = Runtime.getRuntime().exec(cmd);</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 可执行程序的输出可能会比较多，而运行窗口的输出缓冲区有限，会造成waitFor一直阻塞。</span></span><br><span class="line"><span class="comment">* 解决的办法是，利用Java提供的Process类提供的getInputStream,getErrorStream方法</span></span><br><span class="line"><span class="comment">* 让Java虚拟机截获被调用程序的标准输出、错误输出，在waitfor()命令之前读掉输出缓冲区中的内容。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">String flag ;</span><br><span class="line">BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(proc.getInputStream()));</span><br><span class="line"><span class="keyword">while</span> ( (flag=bufferedReader.readLine()) != <span class="keyword">null</span>)&#123;</span><br><span class="line">System.out.println(<span class="string">"result ---- "</span>+flag);</span><br><span class="line">&#125;</span><br><span class="line">bufferedReader.close();</span><br></pre></td></tr></table></figure><p>编辑脚本文件</p><p>vim test.sh</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/bin/sh  指明解释器的位置</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world"</span></span><br><span class="line">name=<span class="string">"zhangsan"</span></span><br><span class="line"><span class="comment">#用$引用变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$name</span></span><br><span class="line"><span class="comment">#定义数组</span></span><br><span class="line">array=&#123;a b <span class="string">"c"</span> d&#125;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;array[0]&#125;</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#运算符</span></span><br><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line">+      <span class="string">'expr $a + $b'</span></span><br><span class="line">-      <span class="string">'expr $a - $b'</span></span><br><span class="line">*      <span class="string">'expr $a \* $b'</span></span><br><span class="line">/      <span class="string">'expr $a / $b'</span></span><br><span class="line">%      <span class="string">'expr $a % $b'</span></span><br><span class="line">=       a=<span class="variable">$b</span></span><br><span class="line">==     [<span class="variable">$a</span> == <span class="variable">$b</span>]   <span class="comment">#返回true或者false</span></span><br><span class="line">!=     [<span class="variable">$a</span> != <span class="variable">$b</span>]   <span class="comment">#返回true或者false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#关系预算符</span></span><br><span class="line">-eq  </span><br><span class="line"><span class="comment">#判断</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> = <span class="variable">$b</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$a</span> = <span class="variable">$b</span> : 等于"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">     <span class="built_in">echo</span> <span class="string">"<span class="variable">$a</span> = <span class="variable">$b</span> : 不等于"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$a</span> == 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">     <span class="built_in">echo</span> 1</span><br><span class="line"><span class="keyword">elif</span> [ <span class="variable">$a</span> == 2 ]</span><br><span class="line"><span class="keyword">then</span> </span><br><span class="line">     <span class="built_in">echo</span> 2</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">     <span class="built_in">echo</span> 3</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#循环</span></span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> 1 2 3 4 </span><br><span class="line"><span class="keyword">do</span> </span><br><span class="line">   <span class="built_in">echo</span> var</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> （( <span class="variable">$a</span>&lt;5 )）</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">   <span class="built_in">echo</span> <span class="variable">$a</span></span><br><span class="line">   <span class="built_in">let</span> <span class="string">"a++"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment">#函数：先定义后调用，执行顺序：自上而下</span></span><br><span class="line"><span class="function"><span class="title">demoFun</span></span>()&#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"shell函数"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"shell 函数开始执行了"</span></span><br><span class="line"></span><br><span class="line">demoFun</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"函数执行结束啦"</span></span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g276s2p0f6j30m10ct76a.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g276s2crhfj30lp03sgm7.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g276ssh6v7j30lt0adta6.jpg" alt=""></p><p>给文件添加权限</p><p>chmod 755 test.sh  </p><p>运行脚本</p><p>./test.sh</p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux系统数据库MySQL安装</title>
      <link href="/2018/12/26/Linux%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E5%AE%89%E8%A3%85/"/>
      <url>/2018/12/26/Linux%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="Linux系统数据库MySQL安装"><a href="#Linux系统数据库MySQL安装" class="headerlink" title="Linux系统数据库MySQL安装"></a>Linux系统数据库MySQL安装</h1><h2 id="一、第一次安装MySQL"><a href="#一、第一次安装MySQL" class="headerlink" title="一、第一次安装MySQL"></a>一、第一次安装MySQL</h2><h3 id="1、yum安装"><a href="#1、yum安装" class="headerlink" title="1、yum安装"></a>1、yum安装</h3><blockquote><p>命令 ： yum -y install mysql-server mysql-devel</p></blockquote><h3 id="2、登录"><a href="#2、登录" class="headerlink" title="2、登录"></a>2、登录</h3><blockquote><p>命令 ： mysql -u -p</p></blockquote><p><code>显示：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><h3 id="3、查看数据库-注意用‘-’结束"><a href="#3、查看数据库-注意用‘-’结束" class="headerlink" title="3、查看数据库(注意用‘ ; ’结束 )"></a>3、查看数据库(注意用‘ ; ’结束 )</h3><blockquote><p>命令 ： show databases;</p></blockquote><h3 id="4、退出："><a href="#4、退出：" class="headerlink" title="4、退出："></a>4、退出：</h3><blockquote><p>命令： quit；</p></blockquote><h3 id="5、创建用户："><a href="#5、创建用户：" class="headerlink" title="5、创建用户："></a>5、创建用户：</h3><blockquote><p>命令 ： mysqladmin -uroot password 123456</p></blockquote><h3 id="6、再登录："><a href="#6、再登录：" class="headerlink" title="6、再登录："></a>6、再登录：</h3><blockquote><p>命令 ：mysql -u root -p</p></blockquote><p><code>显示：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><p>说明成功了！</p><h3 id="7、数据库操作："><a href="#7、数据库操作：" class="headerlink" title="7、数据库操作："></a>7、数据库操作：</h3><blockquote><p>命令 ： use mysql;</p></blockquote><p><code>显示：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure><h3 id="8、查看用户数据表："><a href="#8、查看用户数据表：" class="headerlink" title="8、查看用户数据表："></a>8、查看用户数据表：</h3><blockquote><p>命令 ： show tables;</p></blockquote><h3 id="9、查询user表部分字段："><a href="#9、查询user表部分字段：" class="headerlink" title="9、查询user表部分字段："></a>9、查询user表部分字段：</h3><blockquote><p>命令 ： select host,user,password from user;</p></blockquote><h3 id="10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确"><a href="#10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确" class="headerlink" title="10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确"></a>10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确</h3><h4 id="（1）推荐"><a href="#（1）推荐" class="headerlink" title="（1）推荐"></a>（1）推荐</h4><p>现将user表中其他无密码的记录删除</p><blockquote><p>命令 ： delete from user where password = ‘ ‘;</p></blockquote><h4 id="2-更新有密码的记录的host字段值"><a href="#2-更新有密码的记录的host字段值" class="headerlink" title="(2)更新有密码的记录的host字段值"></a>(2)更新有密码的记录的host字段值</h4><blockquote><p>命令 ： update user set host = “%”;</p></blockquote><h4 id="3-刷新权限"><a href="#3-刷新权限" class="headerlink" title="(3)刷新权限"></a>(3)刷新权限</h4><blockquote><p> 命令 ：flush privileges;</p></blockquote><h4 id="4-退出"><a href="#4-退出" class="headerlink" title="(4)退出"></a>(4)退出</h4><blockquote><p>命令 ：quit;</p></blockquote><h2 id="二、Linux系统登录数据库MySQL报错"><a href="#二、Linux系统登录数据库MySQL报错" class="headerlink" title="二、Linux系统登录数据库MySQL报错"></a>二、Linux系统登录数据库MySQL报错</h2><h3 id="报错一："><a href="#报错一：" class="headerlink" title="报错一："></a>报错一：</h3><p>1、登录 </p><blockquote><p>mysqld -uroot -p123456</p></blockquote><p><code>报错：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (111)</span><br></pre></td></tr></table></figure><p><code>解决：</code></p><p>1）、先删除mysql.sock</p><blockquote><p>cd /var/lib/mysql</p></blockquote><blockquote><p>mv  mysql.sock  mysql.sock.bak</p></blockquote><p>2）、再次登陆</p><blockquote><p>mysql -uroot -p123456</p></blockquote><p><code>报错：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (2)</span><br></pre></td></tr></table></figure><p> 3）、看看mysql的状态，</p><blockquote><p> /etc/rc.d/init.d/mysqld status</p></blockquote><p> <code>显示：</code></p><blockquote><p> mysqld is stopped</p></blockquote><p>4）、看看是不是mysql的权限问题<br>在/var/lib目录下：</p><blockquote><p>ls -lt|grep mysql</p></blockquote><p><code>显示：</code></p><blockquote><p>drwxr-xr-x. 4 mysql   mysql 4096 Jan  6 11:09 mysql</p></blockquote><p>5）、说明mysql服务没有启动</p><p>2、启动mysql服务</p><blockquote><p>/etc/init.d/mysqld start</p><p>或</p><p>service mysqld start</p></blockquote><p><code>显示：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Starting mysqld:                                           [  OK  ]</span><br></pre></td></tr></table></figure><p>3、再次登录：</p><blockquote><p>mysql -uroot -p123456</p></blockquote><p><code>显示：</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.1.73 Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><p>4、退出</p><blockquote><p>quit</p></blockquote><p>5、解决出现mysql.sock的问题</p><blockquote><p>  （1）、vim /etc/mycnf</p></blockquote><p>  编辑内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">skip_name_resolve=on innodb_file_per_table=on</span><br></pre></td></tr></table></figure><p>  按esc :wq 保存并退出<br>  （2）使用命令：</p><blockquote><p>mysql_secure_installation</p></blockquote><p>  （3）直接[ enter ] 键，输入密码，</p><p>(另推荐：Jakie_ZHF老师的博客)</p><h3 id="报错二："><a href="#报错二：" class="headerlink" title="报错二："></a>报错二：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux系统环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux学习之CentOS 6系统安装</title>
      <link href="/2018/12/25/Linux%E7%B3%BB%E7%BB%9FCentOS%206%E5%AE%89%E8%A3%85/"/>
      <url>/2018/12/25/Linux%E7%B3%BB%E7%BB%9FCentOS%206%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h1 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h1><p> 资源准备：</p><p>CentOS-6.6-x86_64-minimal.iso（简易迷你版） ：</p><p>CentOS-6.7-x86_64-bin-DVD1.iso（完整版）：</p><h2 id="1、点击新建虚拟机"><a href="#1、点击新建虚拟机" class="headerlink" title="1、点击新建虚拟机"></a>1、点击新建虚拟机</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rdoec2rj30fe050t9e.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="2、选择典型。（专业人士使用的话建议选择高级）"><a href="#2、选择典型。（专业人士使用的话建议选择高级）" class="headerlink" title="2、选择典型。（专业人士使用的话建议选择高级）"></a>2、选择典型。（专业人士使用的话建议选择高级）</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rf3k4e2j30g20ar40h.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="3-选择稍后安装操作系统"><a href="#3-选择稍后安装操作系统" class="headerlink" title="3. 选择稍后安装操作系统"></a>3. 选择稍后安装操作系统</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rfsol81j30fe0e4403.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="4-选择操作系统类型，选择linux-centos-64位"><a href="#4-选择操作系统类型，选择linux-centos-64位" class="headerlink" title="4. 选择操作系统类型，选择linux,centos 64位"></a>4. 选择操作系统类型，选择linux,centos 64位</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rgu0v2lj30fe0ehdh3.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="5-选择虚拟机安装位置和名称。"><a href="#5-选择虚拟机安装位置和名称。" class="headerlink" title="5. 选择虚拟机安装位置和名称。"></a>5. 选择虚拟机安装位置和名称。</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rhkj8yfj30fe0ef75d.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="6-指定磁盘容量，默认20GB。"><a href="#6-指定磁盘容量，默认20GB。" class="headerlink" title="6. 指定磁盘容量，默认20GB。"></a>6. 指定磁盘容量，默认20GB。</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15ri7ptbaj30fe0evjtf.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="7-选择自定义硬件"><a href="#7-选择自定义硬件" class="headerlink" title="7. 选择自定义硬件"></a>7. 选择自定义硬件</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rk4qtotj30fe0e8gna.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="8-点击CD-DVD-然后选择操作系统的ISO映像文件，选择完后，点击关闭。"><a href="#8-点击CD-DVD-然后选择操作系统的ISO映像文件，选择完后，点击关闭。" class="headerlink" title="8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。"></a>8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rmsa9sjj30i40g5myh.jpg" alt="img"><span class="img-alt">img</span>  </p><h2 id="9-点击完成。"><a href="#9-点击完成。" class="headerlink" title="9.点击完成。"></a>9.点击完成。</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rojtrexj30g30f6aab.jpg" alt="img"><span class="img-alt">img</span> </p><h1 id="二、配置虚拟机"><a href="#二、配置虚拟机" class="headerlink" title="二、配置虚拟机"></a>二、<strong>配置虚拟机</strong></h1><h2 id="1-启动虚拟机。"><a href="#1-启动虚拟机。" class="headerlink" title="1. 启动虚拟机。"></a>1. 启动虚拟机。</h2><p>指定虚拟机：点击 “开启此虚拟机”</p><p>注意：如果启动虚拟机时，发生以下问题，说明是你的电脑默认未开启虚拟化技术。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rqc0xk7j30je0ekapy.jpg" alt="img"><span class="img-alt">img</span> </p><p>此时你应该把机器重启并进入bios界面（不同的机器进入bios界面的快捷键不同，一般为F1~F10键中的某个键，如果都不行，就得自己百度一下你的机器型号进入bios界面的快捷方式）。</p><p>​    当进入bios界面后，把虚拟机化选项（virtualization technology）打开,通过回车键，把disabled改成enabled,然后保存并重启机器。我这边是按F10，不同机器可能不一样，看右下角的提示信息。</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rqzwza8j30fe0ayak5.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="2-Test-Media-如果不需要的话，点Skip"><a href="#2-Test-Media-如果不需要的话，点Skip" class="headerlink" title="2.Test Media, 如果不需要的话，点Skip"></a>2.Test Media, 如果不需要的话，点Skip</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rrr9unej30fe06u0tt.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="3、单击Next按钮继续"><a href="#3、单击Next按钮继续" class="headerlink" title="3、单击Next按钮继续"></a>3、单击Next按钮继续</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rsck2rkj30ce08cmy0.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="4-选择安装期间显示的语言"><a href="#4-选择安装期间显示的语言" class="headerlink" title="4. 选择安装期间显示的语言"></a>4. 选择安装期间显示的语言</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rt1k6j2j30fe0afmyp.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="5、选择键盘语言"><a href="#5、选择键盘语言" class="headerlink" title="5、选择键盘语言"></a>5、选择键盘语言</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rtsr0fdj30fe09igm6.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="6、选择存储介质的类别。"><a href="#6、选择存储介质的类别。" class="headerlink" title="6、选择存储介质的类别。"></a>6、选择存储介质的类别。</h2><p>如果是将CentOS 6安装到本地硬盘上，选择 Basic Storage Devices，如果安装到网络存储介质如SANs上，选择 Specialized Storage Devices</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15run6jt7j30fe0an3zh.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="7-选择-yes-discard-any-data"><a href="#7-选择-yes-discard-any-data" class="headerlink" title="7.选择 yes,discard any data"></a>7.选择 yes,discard any data</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rvca0zlj30fe0admyi.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="8-设定主机名称（hostname）"><a href="#8-设定主机名称（hostname）" class="headerlink" title="8. 设定主机名称（hostname）"></a>8. 设定主机名称（hostname）</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rw3bi44j30fe0agq36.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="9-设定时区，"><a href="#9-设定时区，" class="headerlink" title="9. 设定时区，"></a>9. 设定时区，</h2><p>选择 Asia/Shanghai</p><h2 id="10-设定root帐户的密码"><a href="#10-设定root帐户的密码" class="headerlink" title="10. 设定root帐户的密码"></a>10. 设定root帐户的密码</h2><p>尽量使用较复杂的密码安装（根据实际情况，密码简单时，会有提示，点击user anyway 就行）</p><h2 id="11-选择安装类型，这里我选择-“Use-All-Space”"><a href="#11-选择安装类型，这里我选择-“Use-All-Space”" class="headerlink" title="11. 选择安装类型，这里我选择 “Use All Space”"></a>11. 选择安装类型，这里我选择 “Use All Space”</h2><p> <img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rxcagghj30f509gabh.jpg" alt="img"><span class="img-alt">img</span></p><h2 id="12-选择-“Write-changes-to-disk”，将分区数据写入硬盘"><a href="#12-选择-“Write-changes-to-disk”，将分区数据写入硬盘" class="headerlink" title="12. 选择 “Write changes to disk”，将分区数据写入硬盘"></a>12. 选择 “Write changes to disk”，将分区数据写入硬盘</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15ry9hsb2j30gt09x3z9.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="13-开始安装，此时只需等待即可"><a href="#13-开始安装，此时只需等待即可" class="headerlink" title="13. 开始安装，此时只需等待即可"></a>13. 开始安装，此时只需等待即可</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15rywwpq3j30fe0ae3zx.jpg" alt="img"><span class="img-alt">img</span> </p><h2 id="14-安装完结后，点击Reboot按钮"><a href="#14-安装完结后，点击Reboot按钮" class="headerlink" title="14. 安装完结后，点击Reboot按钮"></a>14. 安装完结后，点击Reboot按钮</h2><h1 id="三、网络配置"><a href="#三、网络配置" class="headerlink" title="三、网络配置"></a>三、网络配置</h1><h2 id="1、查看网关"><a href="#1、查看网关" class="headerlink" title="1、查看网关"></a>1、查看网关</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15wt10f42j307903smxt.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15wu8o6t5j30fe09475w.jpg" alt=""></p><h2 id="2、配置静态IP-NAT模式"><a href="#2、配置静态IP-NAT模式" class="headerlink" title="2、配置静态IP(NAT模式)"></a>2、配置静态IP(NAT模式)</h2><p>1、编辑配置文件,添加修改以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim  /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure><p>2、按i 进入文本编辑模式，出现游标，左下角会出现INSERT,即可以编辑</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=eth0       <span class="comment">#网卡设备名,请勿修改名字</span></span><br><span class="line">TYPE=Ethernet   <span class="comment">#网络类型，以太网</span></span><br><span class="line">BOOTPROTO=static   <span class="comment">#启用静态IP地址</span></span><br><span class="line">ONBOOT=yes        <span class="comment">#开启自动启用网络连接</span></span><br><span class="line">IPADDR=192.168.198.128  <span class="comment">#设置IP地址</span></span><br><span class="line">NETMASK=255.255.255.0  <span class="comment">#设置子网掩码</span></span><br><span class="line">GATEWAY=192.168.198.2   <span class="comment">#设置网关</span></span><br><span class="line">DNS1=114.114.114.114  <span class="comment">#设置备DNS</span></span><br></pre></td></tr></table></figure><p>按ESC退出编辑模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:wq  #保存退出</span><br></pre></td></tr></table></figure><p>3、重启网络连接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service network restart</span><br><span class="line">ifconfig  #查看IP地址</span><br></pre></td></tr></table></figure><p>4、单独配置DNS解析器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/resolv.conf</span><br></pre></td></tr></table></figure><p>编辑内容：i</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nameserver 网关</span><br></pre></td></tr></table></figure><p>:wq  #保存并退出</p><p>5、验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ping 虚拟网关 | 物理机（笔记本）IP | 外网</span><br><span class="line">ping www.baidu.com</span><br></pre></td></tr></table></figure><p>6、注意</p><p>a.保证VMware的虚拟网卡没有被禁用</p><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1g15zvoprnfj30fe03fwet.jpg" alt=""></p><p>b.网关IP不能被占用</p><h2 id="3、桥接和NAT模式的区别"><a href="#3、桥接和NAT模式的区别" class="headerlink" title="3、桥接和NAT模式的区别"></a>3、桥接和NAT模式的区别</h2><h3 id="桥接："><a href="#桥接：" class="headerlink" title="桥接："></a>桥接：</h3><p>​      结构：网络与物理机同一个网段（会占用外部IP）</p><p>​      特点：</p><p>​           1.外网能够访问</p><p>​           2.能够访问外网   </p><p>注意：桥接模式下的虚拟机网关必须改为与物理机网关一致</p><h3 id="NAT模式："><a href="#NAT模式：" class="headerlink" title="NAT模式："></a>NAT模式：</h3><p>​      结构：构成一个以物理机为网关的子网</p><p>​      特点：</p><p>​           1.子网的所有的服务器对外不可见     </p><p>​           2.子网能够正常访问外网</p><p>​      安全！！！ </p><p>​      节省IP资源</p><h1 id="四、拍快照"><a href="#四、拍快照" class="headerlink" title="四、拍快照"></a>四、拍快照</h1><p>（保存当时计算机所出状态的各种配置和资源，适度使用）</p><blockquote><p>选中指定虚拟计算机——鼠标右击—–选中“快照” ——“拍摄快照‘—-在页面中找到”拍摄快照“，并添加名称和描述</p><p>也可以删除，找到页面中的删除按钮</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CentOS 6 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手动安装maven坐标依赖</title>
      <link href="/2018/12/24/%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85maven%E5%9D%90%E6%A0%87%E4%BE%9D%E8%B5%96/"/>
      <url>/2018/12/24/%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85maven%E5%9D%90%E6%A0%87%E4%BE%9D%E8%B5%96/</url>
      
        <content type="html"><![CDATA[<h2 id="一、事件原因："><a href="#一、事件原因：" class="headerlink" title="一、事件原因："></a>一、事件原因：</h2><p>学习quartz框架时，在maven项目的pom.xml文件中添加quartz所需要的坐标依赖时，显示jar包不存在。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">提示："Dependency 'xxxx‘ not found"，</span><br><span class="line">并且添加的如下两个坐标依赖均报红。</span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="comment">&lt;!-- 工具 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz-jobs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>分析：</p><table><thead><tr><th>1、maven项目所需要的jar包均存放在maven的F:\m2\repository(项目所需的jar包仓库)文件夹中</th></tr></thead><tbody><tr><td>2、在F:\apache-maven-3.5.4\conf的settings.xml文件中有如下设置：（由于使用远程仓库太慢，阿里云给我们提供了一个镜像仓库，便于我们使用，且只包含central仓库中的jar）</td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--文件中原有的配置：远程仓库---&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>mirrorId<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>repositoryId<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>Human Readable Name for this Mirror.<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://my.repository.com/repo/path<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--文件中自己手动配置：阿里镜像仓库---&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="C:\Users\Administrator\Desktop\1.jpg" alt=""></p><table><thead><tr><th>3.可是我们在<a href="https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧" target="_blank" rel="noopener">https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧</a></th></tr></thead><tbody><tr><td>（如果有小伙伴有别的解决方案，还请指点一二。）</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure><h2 id="二、解决方案"><a href="#二、解决方案" class="headerlink" title="二、解决方案"></a>二、解决方案</h2><p>1、首先，我们需要从maven  Repository中下载我们需要的jar包（需要的两个jar包，下载原理相同）</p><p><img src="C:\Users\Administrator\Desktop\2.jpg" alt=""></p><p>2、注意我们的maven安装，需要配置环境变量，才能在dos窗口，指令安装jar包</p><p><img src="C:\Users\Administrator\Desktop\4.jpg" alt=""></p><p>因为我之前查资料时，有小伙伴说，java的环境变量配置也会影响，所以，我在这里也把java的环境变量配置也贴出来</p><p><img src="C:\Users\Administrator\Desktop\5.jpg" alt="1"><span class="img-alt">1</span></p><p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544699916763.png" alt="1544699916763"><span class="img-alt">1544699916763</span></p><p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544699989775.png" alt="1544699989775"><span class="img-alt">1544699989775</span></p><table><thead><tr><th>JAVA_HOME</th></tr></thead><tbody><tr><td>F:\Java\jdk1.8.0_131（  根据自己的jdk安装目录）</td></tr><tr><td><strong>CLASSPATH</strong></td></tr><tr><td>.;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar</td></tr><tr><td><strong>MAVEN_HOME</strong></td></tr><tr><td>F:\apache-maven-3.5.4（ 根据自己maven安装目录）</td></tr><tr><td><strong>Path</strong>（注意配置的时候，一定要和配置home时的变量名一致，如MAVEN_HOME,我配置成了%MVN_HOME%\bin;）</td></tr><tr><td>%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%MYSQL_HOME%\bin;%MAVEN_HOME%\bin;</td></tr></tbody></table><p>配置这些环境变量，在dos窗口才能使java  ，mvn  之类的指令可以用；</p><p>否则会出现如下显示。</p><table><thead><tr><th>‘mvn’ 不是内部或外部命令，也不是可运行的程序</th></tr></thead><tbody><tr><td>(这就是环境变量没有配成功的结果)</td></tr></tbody></table><p>3.安装</p><table><thead><tr><th>C:\Users\Administrator&gt;mvn -v</th></tr></thead><tbody><tr><td><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544701045091.png" alt="1544701045091"><span class="img-alt">1544701045091</span></td></tr><tr><td>C:\Users\Administrator&gt;mvn install:install-file -Dfile=F:/apache-maven-3.5.4/m2/quartz-2.3.0.jar（jar包所在路径） -DgroupId=org.quartz-scheduler -DartifactId=quartz -Dversion=2.3.0 -Dpackaging=jar</td></tr><tr><td>（根据下面所示的配置groupId、artifactId、version）</td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544702128551.png" alt="1544702128551"><span class="img-alt">1544702128551</span></p><p>如图所示，安装成功。</p><p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544702179172.png" alt="1544702179172"><span class="img-alt">1544702179172</span></p>]]></content>
      
      
      <categories>
          
          <category> maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/12/19/hello-world/"/>
      <url>/2018/12/19/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>

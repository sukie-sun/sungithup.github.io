<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Flume学习]]></title>
    <url>%2F2019%2F01%2F18%2FFlume%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、分类exec： Unix等操作系统执行命令行，如tail ，cat 。可监听文件 netcat 监听一个指定端口，并将接收到的数据的每一行转换为一个event事件 avro 序列化的一种，实现RPC（一种远程过程调用协议）。 监听AVRO端口来接收外部AVRO客户端事件流 capacity：默认该通道中最大的可以存储的event数量是1000 Trasaction Capacity：每次最大可以source中拿到或者送到sink中的event数量也是100 二、操作1、 netcat（监听端口，在本地控制台打印）（1） vim netcat_logger1234567891011121314151617181920212223# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 （2）命令操作 （在会话1端） 在node00节点的控制台输入启动命令： (方式一：指定配置文件的路径+文件名) flume-ng agent –conf-file /root/flume/netcat_logger –name a1 -Dflume.root.logger=INFO,console （方式二：配置文件在当前目录） flume-ng agent –conf ./ –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console 特别注意： #####官网方式######### flume-ng agent –conf conf –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console 解释：此命令适用于将配置文件放在flume解压安装目录的conf中（不常用） 控制台显示: 1​````` 19/01/18 12:27:31 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 12:27:31 INFO node.Application: Starting Sink k119/01/18 12:27:31 INFO node.Application: Starting Source r119/01/18 12:27:31 INFO source.NetcatSource: Source starting19/01/18 12:27:31 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]123456789101112131415161718* (在会话2端)&gt; 在node00节点的控制台输入命令：&gt;&gt; 1、在节点上安装telnet：&gt;&gt; yum install -y telnet&gt;&gt; yum -y install telnet-server&gt;&gt; 2、启动：&gt;&gt; telnet localhost 44444 &gt;&gt; `注意：`：&gt;&gt; a1.sources.r1.bind = localhost12345678910&gt;&gt; 前提是/etc/hosts中已经配置&gt;&gt; 如果此处配置localhost 那么启动时，localhost 或127.0.0.1都可以，node00就不行&gt;&gt; 如果此处配置node00那么启动时，node00或ip都可以，localhost就不行&gt;&gt; 3、在控制台输入任何内容;&gt;&gt; 都会在会话1端显示，且会话1端（ctrl+c）退出服务，会话2端也自动结束 yum list telnet* 查看telnet相关的安装包 直接yum –y install telnet 就OK yum -y install telnet-server 安装telnet服务 yum -y install telnet-client 安装telnet客户端(大部分系统默认安装) 1234### 2、avro（监听远程发送文件，在本地控制台打印）#### （1）vim avro_logger #test avro sources ##使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示 ##当前flume节点执行： #flume-ng agent –conf ./ –conf-file avro_loggers –name a1 -Dflume.root.logger=INFO,console ##其他flume节点执行： #flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./logs a1.sources=r1a1.channels=c1a1.sinks=k1 a1.sources.r1.type = avroa1.sources.r1.bind=192.168.198.128a1.sources.r1.port=55555 a1.sinks.k1.type=logger a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100 a1.sources.r1.channels=c1a1.sinks.k1.channel=c1123456789101112131415161718实现功能：&gt; 使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示#### （2）命令操作##### `启动` （在会话1端）在node00上* ##当前flume节点执行（配置文件在当前目录）：* &gt; flume-ng agent --conf ./ --conf-file avro_logger --name a1 -Dflume.root.logger=INFO,console`显示：` 19/01/18 13:53:16 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 13:53:16 INFO node.Application: Starting Sink k119/01/18 13:53:16 INFO node.Application: Starting Source r119/01/18 13:53:16 INFO source.AvroSource: Starting Avro source r1: { bindAddress: 192.168.198.128, port: 55555 }…19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 13:53:17 INFO source.AvroSource: Avro source r1 started. 12345678910##### 发送(在会话2端)在node00上发送文件到node00`启动`##可在本地和其他flume节点执行（配置文件在当前目录）： flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./flume.log12(在会话1端) 19/01/18 14:12:57 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata }12345678910时刻监听传输文件的内容`注意`&gt; 该过程也可应用于不同节点之间### 3、exec（监听某一命令，在本地控制台打印）#### （1）vim exec_logger #单节点flume配置 example.conf: A single-node Flume configuration#给agent三大结构命名 Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 #描述source的配置：类型、命令（监听/root/flume.log文件） Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /root/flume.log #描述sink的配置：类型 Describe the sinka1.sinks.k1.type = logger #在内存中使用一个channel缓存事件 Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 #将source和sink绑定到channel上 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1234567891011121314151617181920212223242526（xshell会话1：）&gt; 在node00上：`启动`&gt;&gt; 在exec_logger文件所在的目录下&gt;&gt; 命令：flume-ng agent --conf-file exec_logger --name a1 -Dflume.root.logger=INFO,console&gt;&gt; r1 启动&gt; （复制会话：会话2）&gt;&gt; 在node00上：&gt;&gt; 在root目录下&gt;&gt; 命令：echo hello bigdata &gt;&gt;flume.log&gt; 之后在会话2上&gt;&gt; `logger本地控制台打印：`&gt;&gt; 19/01/18 12:03:23 INFO sink.LoggerSink:Event:{ headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata }123456### 4、netcat–hdfs(监听数据，传到hdfs上)#### （1）vim netcat_hdfs a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1 a1.sources.r1.type = netcata1.sources.r1.bind = node00a1.sources.r1.port = 41414 a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://Sunrise/myflume/%y-%m-%da1.sinks.k1.hdfs.useLocalTimeStamp=true Define a memory channel called c1 on a1a1.channels.c1.type = memory #默认值，可省 #a1.channels.c1.capacity = 1000 #a1.channels.c1.transactionCapacity = 100 Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c112345678910111213141516#### (2)操作在node00的会话1上启动&gt; 在node00上：&gt;&gt; 在netcat_hdfs文件所在的目录下&gt;&gt; 命令：flume-ng agent --conf-file netcat_hdfs --name a1 -Dflume.root.logger=INFO,console显示： 19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 14:34:44 INFO node.Application: Starting Sink k119/01/18 14:34:44 INFO node.Application: Starting Source r119/01/18 14:34:44 INFO source.NetcatSource: Source starting19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started19/01/18 14:34:44 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/192.168.198.128:41414] 12345678910在node00的会话2上启动&gt; telnet node00 41414显示 Trying 192.168.198.128…Connected to node00.Escape character is ‘^]’. 1234输入任意内容在node00会话1端会显示 19/01/18 14:36:50 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false19/01/18 14:36:51 INFO hdfs.BucketWriter: Creating hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Closing hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Renaming hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp to hdfs://Sunrise/myflume/19-01-18/FlumeData.154782221025919/01/18 14:37:29 INFO hdfs.HDFSEventSink: Writer callback called. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758在HDF分布式系统上会显示，生成的文件![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4acaz5fj30u10blmzc.jpg)![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4bsv8d2j30wd08k74i.jpg)`注意：`这种情况会在hdfs上生成很多小文件，[在官网](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html)HDFS Sink：[**](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html#hdfs-sink)有很多关于文件生成过程中的配置| Name | Default | Description || ---------------------- | ------------ | ------------------------------------------------------------ || **channel** | – | || **type** | – | The component type name, needs to be `hdfs` || **hdfs.path** | – | HDFS directory path (eg hdfs://namenode/flume/webdata/) || hdfs.filePrefix | FlumeData | Name prefixed to files created by Flume in hdfs directory || hdfs.fileSuffix | – | Suffix to append to file (eg `.avro` - *NOTE: period is not automatically added*) || hdfs.inUsePrefix | – | Prefix that is used for temporal files that flume actively writes into || hdfs.inUseSuffix | `.tmp` | Suffix that is used for temporal files that flume actively writes into || hdfs.rollInterval | 30 | Number of seconds to wait before rolling current file (0 = never roll based on time interval) || hdfs.rollSize | 1024 | File size to trigger roll, in bytes (0: never roll based on file size) || hdfs.rollCount | 10 | Number of events written to file before it rolled (0 = never roll based on number of events) || hdfs.idleTimeout | 0 | Timeout after which inactive files get closed (0 = disable automatic closing of idle files) || hdfs.batchSize | 100 | number of events written to file before it is flushed to HDFS || hdfs.codeC | – | Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy || hdfs.fileType | SequenceFile | File format: currently `SequenceFile`, `DataStream` or `CompressedStream` (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC || hdfs.maxOpenFiles | 5000 | Allow only this number of open files. If this number is exceeded, the oldest file is closed. || hdfs.minBlockReplicas | – | Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath. || hdfs.writeFormat | Writable | Format for sequence file records. One of `Text` or `Writable`. Set to `Text` before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive. || hdfs.callTimeout | 10000 | Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring. || hdfs.threadsPoolSize | 10 | Number of threads per HDFS sink for HDFS IO ops (open, write, etc.) || hdfs.rollTimerPoolSize | 1 | Number of threads per HDFS sink for scheduling timed file rolling || hdfs.kerberosPrincipal | – | Kerberos user principal for accessing secure HDFS || hdfs.kerberosKeytab | – | Kerberos keytab for accessing secure HDFS || hdfs.proxyUser | | || hdfs.round | false | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) || hdfs.roundValue | 1 | Rounded down to the highest multiple of this (in the unit configured using `hdfs.roundUnit`), less than current time. || hdfs.roundUnit | second | The unit of the round down value - `second`, `minute` or `hour`. || hdfs.timeZone | Local Time | Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles. || hdfs.useLocalTimeStamp | false | Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. || hdfs.closeTries | 0 | Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart. || hdfs.retryInterval | 180 | Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension. || serializer | `TEXT` | Other possible options include `avro_event` or the fully-qualified class name of an implementation of the `EventSerializer.Builder` interface. |netcat-hdfs （配置方式二） a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1 a1.sources.r1.type = avroa1.sources.r1.bind=node01a1.sources.r1.port=55555 a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://shsxt/hdfsflume Define a memory channel called c1 on a1a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100 Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c1123456### 5、结合版（netcat-avro）#### （1）vim netcat2_logger（node00） example.conf: A single-node Flume configuration#flume-ng agent –conf ./ –conf-file netcat2_logger –name a1 -Dflume.root.logger=INFO,console #flume-ng –conf conf –conf-file /root/flume_test/netcat_hdfs -n a1 -Dflume.root.logger=INFO,console #telnet 192.168.235.15 44444 Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = node00 a1.sources.r1.port = 44444 Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = node01 a1.sinks.k1.port = 60000 Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 #————————— #flume-ng agent –conf-file etect2_logger –name a1 -#Dflume.root.logger=INFO,console #flume-ng agent –conf conf –conf-file netcat_logger –name a1 -#Dflume.root.logger=INFO,console12（node01） #flume-ng agent –conf ./ –conf-file avro2 -n a1a1.sources = r1a1.sinks = k1a1.channels = c1 a1.sources.r1.type = avroa1.sources.r1.bind = node01a1.sources.r1.port = 60000 a1.sinks.k1.type = logger Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1234567891011121314(2)操作&gt; 先启动后面的flume节点node01 ，在启动node00，最后启动node02在node01上`启动`&gt; flume-ng &gt;&gt; --conf conf --conf-file avro2 -n a1 -Dflume.root.logger=INFO,console显示 19/01/18 23:22:27 INFO node.Application: Starting Channel c119/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 23:22:28 INFO node.Application: Starting Sink k119/01/18 23:22:28 INFO node.Application: Starting Source r119/01/18 23:22:28 INFO source.AvroSource: Starting Avro source r1: { bindAddress: node01, port: 60000 }…19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 23:22:30 INFO source.AvroSource: Avro source r1 started. 1234567891011121314151617181920212223242526在node00上`启动`&gt; flume-ng agent&gt;&gt; --conf ./ --conf-file netcat2_logger --name a1 -Dflume.root.logger=INFO,console在node02上`启动`&gt; telnet node00 44444然后输入数据文件最后在node01节点上显示文件信息 19/01/18 23:33:01 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 0D hello world. } ` flume-ng agent –conf-file flumeproject –name a1 -Dflume.root.logger=INFO,console]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase性能优化]]></title>
    <url>%2F2019%2F01%2F17%2FHBase%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[HBase性能优化方案（一）、表的设计一、Pre-Creating Regions 预分区 解决海量导入数据时的热点问题 背景： 在创建HBase表的时候默认一张表只有一个region， 所有的put操作都会往这一个region中填充数据， 当这一个region过大时就会进行split。 如果在创建HBase的时候就进行预分区 则会减少当数据量猛增时由于region split带来的资源消耗。 注意： 每个region都有一个startKey和一个endKey来表示该region存储的rowKey范围。 1&gt; create &apos;t1&apos;, &apos;cf&apos;, SPLITS =&gt; [&apos;20150501000000000&apos;, &apos;20150515000000000&apos;, &apos;20150601000000000&apos;] 或者 123456&gt; create &apos;t2&apos;, &apos;cf&apos;, SPLITS_FILE =&gt; &apos;/home/hadoop/splitfile.txt&apos; /home/hadoop/splitfile.txt中存储内容如下： 201505010000000002015051500000000020150601000000000 二、row key HBase中row key用来检索表中的记录，支持以下三种方式： · 通过单个row key访问：即按照某个row key键值进行get操作； · 通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描；过滤器 · 全表扫描：即直接扫描整张表中所有行记录。 （二）、写表操作一、多HTable客户端并发写 二、HTable参数设置 三、批量写 四、多线程并发写 （三）、读表操作一、多HTable客户端并发读 二、HTable参数设置 三、批量读 四、多线程并发读 五、缓存查询结果 六、 Blockcache]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase学习]]></title>
    <url>%2F2019%2F01%2F15%2FHBase%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[非关系型数据库 官网 一、对数据库的 基本了解 1、简介 基于Hadoop 的分布式数据库 特点： 1、高可靠性 2、高性能 （以上两点：基于分布式的特点） 3、面向列 （以（K,V）存储，有唯一标记的rowkey，value包含是数据库中的列值） 4、可伸缩 （搭建在集群上） 5、实时读写 （用时间戳唯一标记每一版本的数据记录） 2、工作结构 1、利用Hadoop的HDFS作为其文件存储系统 2,利用Hadoop的MapReduce来处理HBase中的海量数据 3,利用Zookeeper作为其分布式协同服务 4,主要用来存储非结构化和半结构化的松散数据（NoSQL非关系型数据库有redis、MongoDB等 3、关系型数据库 1、定义 关系模型指的就是二维表格模型； 而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织 2、三大优点 容易理解 使用方便 易于维护 3、三大瓶颈 高并发读写需求 硬盘I/O是一个很大的瓶颈，并且很难能做到数据的强一致性。 海量数据的读写性能低 在一张包含海量数据的表中查询，效率是非常低的。 ​ 扩展性和可用性差 丰富的完整性使得横向扩展把难度加大了 4、非关系型数据库 1、存储格式：key value键值对，文档，图片等等 结构不固定 2、可以减少一些时间和空间的开销，仅需要根据id取出相应的value就可以完成查询。 3、一般不支持ACID特性，无需经过SQL解析，读写性能高 4、不提供where字段条件过滤 5、难以体现设计的完整性，只适合存储一些较为简单的数据 二、对HBase的基本里了解1、数据结构组成（ 1）Row key : 唯一标记决定一行数据按照字典排序最大只能存储64KB的字节数据设计非常关键 （2）Column Family列族 &amp; qualifier列 列族必须作为表模式(schema)定义的一部分预先给出， 表中的每个列都归属于某个列族； 权限控制、存储以及调优都是在列族层面进行的； 列名以列族作为前缀，每个“列族”都可以有多个列成员(column)； 新的列可以随后按需、动态加入； （3）Cell单元格 由行和列的坐标交叉决定； 单元格是有版本的（有时间戳决定）； 单元格的内容是未解析的字节数组；cell中的数据是没有类型的，全部是字节码形式存贮。 由{rowkey， column( = +)， version} 唯一确定的单元。 （4）Timestamp时间戳 在HBase每个cell存储单元对同一份数据有多个版本， 根据唯一的时间戳来区分每个版本之间的差异， 不同版本的数据按照时间倒序排序，最新的数据版本排在最前面 时间戳的类型是64位整型。时间戳可以由HBase(在数据写入时自动)赋值，精确到毫秒 时间戳。也可以由客户显式赋值，但必须唯一性 （5）HLog(WAL log) HLog文件就是一个普通的Hadoop SequenceFile HLog Sequence File的Key是HLogKey对象 ​ **HLogKey中记录了写入数据的归属信息，包括table和region名字，sequence number（起始值为0或是最近一次存入文件系统中sequence number）和timestamp（写入时间） HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue ​ **存储hbase表的操作记录，KV数据信息 2、体系架构 （1）Client 包含访问HBase的接口并维护cache来加快对HBase的访问 （2）Zookeeper 保证任何时候，集群中只有一个master； 存贮所有Region的寻址入口。 实时监控Region server的上线和下线信息。并实时通知Master 存储HBase的schema和table元数据 （3）Master 为Region server分配region； 负责Region server的负载均衡； 发现失效的Region server并将其上的region重新分配； 管理用户对table的增删改操作； （4）RegionServer 维护region，处理对这些region的IO请求 负责切分在运行过程中变得过大的region （5）Region 保存一个表里面某段连续的数据，每个表一开始只有一个region； 随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变） （HBase自动把表水平划分成多个区域(region)） 当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上 （6）Memstore与storefile 一个region由2-3store组成，一个store对应一个CF（列族） store包括位于内存中的memstore和位于磁盘的storefile。 写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile； 当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、majorcompaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile 当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡 客户端检索数据，先在memstore找，找不到再找storefile HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。 每个Strore又由一个memStore和0至多个StoreFile组成,StoreFile以HFile格式保存在HDFS上(HFile)。 三、Hbase 安装部署完全分布式搭建 1、安装包准备 Hbase（本文使用0.98版本） 将tar上传至Linux系统，进行解压安装 2、修改配置文件hbase-env.sh（在Hbase的解压目录的conf目录中） 修添加JAVA_HOME环境变量 123# The java implementation to use. Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/export JAVA_HOME=/usr/soft/jdk1.8.0_191 不使用HBase的默认zookeeper配置，（使用自己的）： 12# Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not. export HBASE_MANAGES_ZK=false 3、修改配置hbase-site.xml（在Hbase的解压目录的conf目录中） 123456789101112131415161718192021&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;!--Hdfs配置时的集群名--&gt;&lt;value&gt;hdfs://Sunrise:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--zookeeper的三台节点--&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;node00,node01,node02&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--配置http访问的port---&gt;&lt;name&gt;hbase.master.info.port&lt;/name&gt;&lt;value&gt;60010&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 注意：（会出bug的地方）： 1、问题描述： HBase启动时，警告：Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 解决方案： 原因：由于使用了JDK8 ，需要在HBase的配置文件中hbase-env.sh，注释掉两行。 123# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+#export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m&quot;#export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m&quot; 重新启动HBase。 2、问题描述： 配置好HBase后，各项服务正常，但想从浏览器通过端口60010看下节点情况，但是提示拒绝访问 检测：在服务器上netstat -natl|grep 60010 发现并没有60010端口 原因：HBase 1.0 之后的版本都需要在hbase-site.xml中配置端口，如下 1234&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt;&lt;/property&gt; 重新启动HBase,在浏览器再次访问，就ok了 4、添加配置regionservers 文件（在Hbase的解压目录的conf目录中） 添加配置的regionservers 的主机名 regionservers 123node00node01node02 5、添加配置backup-masters 添加配置的master备份的主机名（在Hbase的解压目录的conf目录中） backup-masters 1node02 6、将Hadoop安装解压目录/etc/hadoop目录下的hdfs-site.xml文件 拷贝到Hbase的解压目录的conf目录中 7、配置环境变量 ~/.bash_profile 123456789101112JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/binHIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/binSQOOP_HOME=/usr/soft/sqoop-1.4.6export PATH=$PATH:$SQOOP_HOME/binHBASE_HOME=/usr/soft/hbase-1.2.9export PATH=$PATH:$HBASE_HOME/bin source ~/.bash_profile 8、将如上配置远程发送至其他节点（Hbase 、 ./bash_profile） 9、各个节点注意要做时间同步 1ntpdate cn.ntp.org.cn 10、启动HDFS集群： 12zkServer.sh startstart-hdfs.sh 11、启动：start-hbase.sh 12345starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node00.outnode02: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node02.outnode01: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node01.outnode00: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node00.outnode02: starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node02.out 12、查看进程：jps 13、浏览器访问：node00:60010 14、关闭：stop-hbase.sh 四、通过hbase shell命令进入HBase 命令行接口通过help可查看所有命令的支持以及帮助手册 帮助创建 hbase(main):007:0&gt; help create 名称 Shell命令 举例 创建表 create ‘表名’, ‘列族名1’[,…] create ‘t1’，‘cf1’ 列出所有表 list list 添加记录 put ‘表名’, ‘RowKey’, ‘列族名称:列名’, ‘值’ put ‘t1’,‘rk_00101’,‘cf1:name’,‘zs’ 查看记录 get ‘表名’, ‘RowKey’, ‘列族名称:列名’ get ‘t1’,‘rk_00101’ get ‘t1’,‘rk_00101’,‘cf1:name’ 查看表中 记录总数 count ‘表名’ count ‘t1’ 删除记录 delete ‘表名’ , ‘RowKey’, ‘列族名称:列名’ delete ‘t1’,‘rk_00101’,‘cf1:name’ 删除一张表 先要屏蔽该表，才能对该表进行删除。 第一步 disable ‘表名称’ 第二步 drop ‘表名称’ disable ‘t1’ drop ‘t1’ 查看所有 记录 scan ‘表名 ‘ scan ‘t1’ create ‘t2’, {NAME =&gt; ‘cf1’, VERSIONS =&gt; 2}, METADATA =&gt; { ‘mykey’ =&gt; ‘myvalue’ } 查看未加工的数据中指定版本记录 scan ‘t1’, {RAW =&gt; true, VERSIONS =&gt; 3} raw 未加工的 3 查看保存 版本记录 scan ‘t1’, {VERSIONS =&gt; 2} 2 五、HBase优化详见HBase性能优化文档]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive优化]]></title>
    <url>%2F2019%2F01%2F14%2FHive%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、核心思想： 把Hive SQL 当做MapReduce程序进行优化 注意：以下不能HQL转化为Mapreduce任务运行 —select 仅查询本表字段 —where 仅对本表字段做条件过滤 二、explain 用以显示任务执行计划 格式： EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query 语法解释 从语法组成可以看出来是一个“explain ”+三个可选参数+查询语句。大家可以积极尝试一下，后面两个显示内容很简单的，我介绍一下第一个 extended 这个可以显示hql语句的语法树 其次，执行计划一共有三个部分： 这个语句的抽象语法树 这个计划不同阶段之间的依赖关系 对于每个阶段的详细描述 例子： 12&gt; hive&gt; explain select * from log;&gt; 拓展课下查询MySQl的执行计划。 三、Hive运行方式集群模式：12执行hql：hive&gt; select count(*) from log; 结论： 函数（如count）是在reduce阶段进行默认提交到yarn所在的节点上运行， 优化一:设置 本地模式（运行速度加快。但对加载文件有限制） 1234hive&gt;set hive.exec.mode.local.auto=true;查看：hive&gt;set hive.exec.mode.local.auto 但是如果加载文件的最大值大于配置（默认配置为100M），仍会使用集群模式运行（在yarn所在的节点） 12345查看最大加载文件hive&gt; set hive.exec.mode.local.auto.inputbytes.max;显示：hive.exec.mode.local.auto.inputbytes.max=134217728 优化二：设置 严格模式 123通过设置以下参数开启严格模式[防止误操作]：hive&gt; set hive.mapred.mode=strict;（默认为：nonstrict非严格模式） 但是存在查询限制 1、对分区表查询时，必须添加where对于分区字段的条件过滤； 12&gt; hive&gt; select * from day_table where dt='2019-01-13';&gt; 2、order by语句必须包含limit输出限制； 12345&gt; hive&gt; select * from log order by id limit 1;&gt; 这里的1， 表示显示前多少条记录，只能设一个数字&gt; 和Mysql（可以从0 开始）不同的是，它只能从1开始&gt; mysql可以有两个数字，表示从第几条开始，显示几条&gt; 3、限制执行笛卡尔积的查询 imit 四、Hive排序1、Order By— 对于查询结果做全局排序，只允许有一个reduce处理（当数据量较大时，reduce数量有限，应慎用。 ​ 严格模式下，必须结合limit来使用） 1select * from log order by id； 2、Sort By– 对于单个reduce的数据进行排序 –局部（单个reduce）有序，全局无序 1可以通过设置mapred.reduce.tasks的值来控制reduce的数，然后对reduce输出的结果做二次排序 案例 1select * from log sort by id; (结果无序) 显示 Time taken: 147.077 seconds, Fetched: 7 row(s) 3、Distribute By– 分区排序，经常和 Sort By 结合使用 全局有序，局部无序 1select * from log distribute by id; （结果无序） Time taken: 144.708 seconds, Fetched: 7 row(s) 注意：hive要求DISTRIBUTE BY语句出现在SORT BY语句之前 1Distribute By可以将Map阶段输出的数据按指定的字段划分到不同的reduce文件中，然后，sort by 对reduce阶段的输出数据做排序。 案例: 1select * from log distribute by id sort by id asc; （结果无序） 1select * from (select * from log distribute by id) a sort by a.id ;s 改良 123select a.* from (select * from log cluster by id ) a order by a.id limit 9 ; (结果有序)9 在这里是表中数据记录的总条数 显示： Time taken: 234.593 seconds, Fetched: 7 row(s) 1select * from log order by id limit 9; （结果有序） 显示： Time taken: 102.065 seconds, Fetched: 7 row(s) 4、Cluster By– 相当于 Sort By + Distribute By（Cluster By不能通过asc、desc的方式指定排序规则；可通过 distribute by column sort by column asc|desc 的方式） 1select * from log cluster by id； （结果无序） 五、Hive Join （重难点）1、Join 连接时，将小表（驱动表）放在join的左边2、Map Join ： 因为Map Join 是在Map端且在内存中进行的，所以不需要启动Reduce任务，也没有shuffle阶段，从而在一定程度上节省资源，提高Join效率。 方式：（两种）1、SQL方式：​ 在HQl语句中添加MapJoin标记（mapjoin）(将小表加入到内存，注意小表的大小) ​ 语法： 12SELECT /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value FROM smallTable JOIN bigTable ON smallTable.key = bigTable.key; 案例： 12SELECT /*+ MAPJOIN(log1) */ log.id,log1.name FROM log JOIN log1 ON log.id = log1.id; 2、自动的MapJoin​ 通过修改以下配置启用自动的mapjoin： 1hive&gt; set hive.auto.convert.join = true; ​ （ 该参数为true时，Hive自动对左边的表统计数据量，如果是小表就加入内存，即对小表使用Map join）其他相关配置参数： 1hive&gt; set hive.mapjoin.smalltable.filesize; （默认：大表小表判断的阈值25MB左右，如果表的大小小于该值则会被加载到内存中运行，可自定义） 1hive&gt; set hive.ignore.mapjoin.hint; （默认值：true；是否忽略mapjoin hint 即mapjoin标记；如果为false，这则需要添加-MapJoin标记，mapjoin（smalltable）） 1hive&gt; set hive.auto.convert.join.noconditionaltask; （默认值：true；将普通的join转化为普通的mapjoin时，是否将多个mapjoin转化为一个mapjoin） 1hive&gt; set hive.auto.convert.join.noconditionaltask.size; （默认：10M；将多个mapjoin转化为一个mapjoin时，其表的最大值为10M，可自定义） 六、Map-Side聚合 相当于聚合函数：count（） 设置参数，开启在Map端的聚合 1set hive.map.aggr=true; 相关配置参数： 1set hive.groupby.mapaggr.checkinterval； （默认为：100000，表示 map端group by执行聚合时处理的多少行数据） 1set hive.map.aggr.hash.min.reduction； （默认为：0.5，进行聚合的最小比例，预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合） 1set hive.map.aggr.hash.percentmemory; （默认： 0.5 ，map端聚合使用的内存的最大值） 1set hive.map.aggr.hash.force.flush.memory.threshold; （默认为：0.9，map端做聚合操作是hash表的最大可用内容，大于该值则会触发flush 1set hive.groupby.skewindata； （默认为：false，是否对GroupBy产生的数据倾斜做优化） 附加： 数据倾斜问题解决：多种方式（使用MapJoin、使用MapSide） 参考 http://www.sohu.com/a/224276626_543508 七、控制Hive中Map和Reduce的数量1、Map数量相关的参数1set mapred.max.split.size; （默认为：256M，一个split的最大值，即每个map处理文件的最大值） 1set mapred.min.split.size.per.node; (一个节点上最小split数：1个) 1set mapred.min.split.size.per.rack; (一个机架上最小split数：1个) 2、Reduce数量相关的参数1set mapred.reduce.tasks; (默认为：-1，强制指定reduce任务的数量。-1，是未定义，不发挥作用。如果指定了，就会按指定的数量执行) 1set hive.exec.reducers.bytes.per.reducer; （默认为：256M ，每个reduce任务处理的数据量） 1set hive.exec.reducers.max; （默认为：1009个，每个任务最大的reduce数 [Map数量 &gt;= Reduce数量 ]） 八、Hive - JVM重用适用场景：1、小文件个数过多2、task个数过多 原理： hadoop默认配置是使用派生JVM来执行map和reduce任务的，JVM重用可以使得JVM实例在同一个JOB中重新使用N次 1set mapred.job.reuse.jvm.num.tasks; (默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM) 缺点： 设置开启之后，task插槽会一直占用资源，不论是否有task运行，直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx入门学习（一）]]></title>
    <url>%2F2019%2F01%2F12%2FNginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E5%9B%9E%E5%90%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Nginx[^开发者]: 由俄罗斯的程序设计师Igor Sysoev所开发 [TOC] 一、产生背景1.巨大流量—海量的并发访问 2.单台服务器资源和能力有限 引发服务器宕机而无法提供服务 二、负载均衡(Load Balance)1、高并发（1）、高（大量的） （2）、并发就是可以使用多个线程或者多个进程，同时处理（就是并发）不同的操作 （3）、简而言之就是每秒内有多少个请求同时访问。 2、负载均衡（1）、将请求/数据【均匀】分摊到多个操作单元上执行 （2）、关键在于【均匀】,也是分布式系统架构设计中必须考虑的因素之一。 3、互联网分布式架构常见，分为客户端层、反向代理nginx层、站点层、服务层、数据层。只需要实现“将请求/数据 均匀分摊到多个操作单元上执行”，就能实现负载均衡。 三、对Nginx的基本了解1、什么是Nginx？1一款轻量级的Web 服务器/反向代理服务器【后面有介绍】及电子邮件（IMAP/POP3）代理服务器。 ​ 特点 12*是占有内存少，CPU、内存等资源消耗却非常低，*运行非常稳定并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。 2、Nginx VS Apache（1）、nginx相对于apache的优点：1234*轻量级，同样起web 服务，比apache 占用更少的内存及资源高并发，*nginx 处理请求是异步非阻塞（如前端ajax）的，而apache 则是阻塞型的，*在高并发下nginx能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单*Nginx 配置简洁, Apache 复杂 （2）、apache 相对于nginx 的优点：12* Rewrite重写 ，比nginx 的rewrite 强大模块超多，*基本想到的都可以找到少bug ，nginx 的bug 相对较多。（出身好起步高） 四、安装Nginx这里以安装tengine为例 1、安装之前准备配置依赖 gcc openssl-devel pcre-devel zlib-devel 安装： yum install gcc openssl-devel pcre-devel zlib-devel -y 2、下载（目前最新版）：tengine-2.2.3.tar 3、 解压缩 tar -zvxf tengine-2.2.3.tar 4、安装Nginx 在Nginx解压的目录下运行： ./configure make &amp;&amp; make install 默认安装目录：/usr/local/nginx 5、配置Nginx为系统服务，以方便管理（1）、在/etc/rc.d/init.d/目录中建立文本文件nginx（2）、在文件中粘贴下面的内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid # Source function library.. /etc/rc.d/init.d/functions # Source networking configuration.. /etc/sysconfig/network # Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0 nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot; [ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125; start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125; stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125; restart() &#123; configtest || return $? stop sleep 1 start&#125; reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125; force_reload() &#123; restart&#125; configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125; rh_status() &#123; status $prog&#125; rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125; case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac （3）、修改nginx文件的执行权限 chmod +x nginx （4）、添加该文件到系统服务中去 chkconfig –add nginx 查看是否添加成功 chkconfig –list nginx 启动，停止，重新装载 service nginx start|stop 五、Nginx配置1、查看配置 cd /usr/local/nginx/conf vim nginx.conf 2、配置解析12345678910111213141516171819202122232425262728293031323334#进程数，建议设置和CPU个数一样或2倍worker_processes 2;#日志级别error_log logs/error.log warning;(默认error级别)# nginx 启动后的pid 存放位置#pid logs/nginx.pid;events &#123; #配置每个进程的连接数，总的连接数= worker_processes * worker_connections #默认1024 worker_connections 10240;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on;#连接超时时间，单位秒keepalive_timeout 65; server &#123; listen 80; server_name localhost #默认请求 location / &#123; root html; #定义服务器的默认网站根目录位置 index index.php index.html index.htm; #定义首页索引文件的名称 &#125; #定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; 3、负载均衡配置1）安装Tomcat，参考 Tomcat配置 2）多负载均执行一下操作： 多负载的情况下，打开指定虚拟机器 open node01 node01 为指定虚拟机器的别名，在hosts文件中配置的 启动Tomcat 在Tomcat解压的目录下 ./startup.sh 注意： 记得关闭虚拟机器的防火墙 service iptables stop 浏览器访问 虚拟机器IP地址：8080 默认负载均衡配置 123456789101112131415161718192021&gt; http &#123; &gt; upstream shsxt&#123; &gt; # 以下均为实际执行服务的服务器&gt; #只有当hosts文件中给ip地址配置了别名，这里server后面才能用别名，&gt; #否则跟IP地址&gt; server node01; &gt; server node02; &gt; server node03; &gt; &#125; &gt; &gt; server &#123; &gt; #指定访问端口为80 ，那么Tomcat服务器端的port也要改为80&gt; listen 80; &gt; server_name localhost;&gt; location / &#123;&gt; proxy_pass http://shsxt; &gt; # shsxt 是指定的代理服务器&gt; &#125;&gt; &#125; &gt; &#125;&gt; 查看使用 80端口的程序 netstat -anp |grep 80 配置文件编辑结束后，启动nginx服务 service nginx start （1）、轮询负载均衡（默认）1- 对应用程序服务器的请求以循环方式分发 （2）、加权负载均衡 通过使用服务器权重，还可以进一步影响nginx负载均衡算法， 谁的权重越大，分发到的请求就越多。 权重总数：10 在nginx.conf文件中修改： 12345upstream shsxt &#123; server srv1.example.com weight=3;//域名为在/etc/hosts文件中取的别名 server srv2.example.com; server srv3.example.com; &#125; 配置修改之后，重启 service nginx restart （3）、最少连接负载平衡 在连接负载最少的情况下，nginx会尽量避免将过多的请求分发给繁忙的应用程序服务器， 而是将新请求分发给不太繁忙的服务器，避免服务器过载。 在nginx.conf文件中修改： 123456upstream shsxt &#123; least_conn; server srv1.example.com; server srv2.example.com; server srv3.example.com; &#125; （4）、会话持久性——ip-hash负载平衡机制特点：保证相同的客户端总是定向到相同的服务; (此方法可确保来自同一客户端的请求将始终定向到同一台服务器，除非此服务器不可用。) 在nginx.conf文件中修改： 123456upstream shsxt&#123; ip_hash; server （IP地址|别名）; server （IP地址|别名）; server （IP地址|别名）;&#125; (5)、Nginx的访问控制 Nginx还可以对IP的访问进行控制，allow代表允许，deny代表禁止. 12345678location / &#123;deny 192.168.2.180;allow 192.168.78.0/24;allow 10.1.1.0/16;allow 192.168.1.0/32;deny all;proxy_pass http://shsxt;&#125; 12345从上到下的顺序，匹配到了便跳出。如上的例子先禁止了1个，接下来允许了3个网段，其中包含了一个ipv6，最后未匹配的IP全部禁止访问]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习]]></title>
    <url>%2F2019%2F01%2F11%2FHive%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Hive是什么？1、基于 Hadoop 的一个数据仓库工具 可以将结构化的数据文件映射为一张hive数据库表； 这张Hive数据库表保存不了metadata元数据信息，而是将metadata存储在本地磁盘上的MySQL（关系型数据库）中 并提供简单的 sql 查询功能； 可以将 sql 语句转换为 MapReduce 任务进行运行。 2、快速实现简单的MapReduce 统计的工具 方便非Java编程者对HDFS的数据做mapreduce操作； 学习成本低，十分适合数据仓库的统计分析。 3、什么是数据仓库？ Data Warehouse(DW 或DWH）是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。 单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制. 数据仓库是用来做查询分析的数据库，基本不用来做插入，修改，删除操作。 4、数据处理的两大分类oltp+olap 联机事务处理OLTP（on-line transaction processing） OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作； 联机分析处理OLAP（On-Line Analytical Processing） OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。 数据文件按结构的分类 结构化数据：关系型 半结构化数据：K-V 松散型： 二、Hive架构原理用户接口主要有三个：CLI，Client 和 WUI。 三、Hive搭建及三种模式1、Hive的安装配置：（1）基本环境：Hadoop集群环境（至少3个节点） Hive是依赖于hadoop系统的，因此在运行Hive之前需要保证已经搭建好hadoop集群环境。 （2）安装一个关系型数据mysql 因为Hive数据仓库的元数据信息是存放在本地磁盘的关系数据库上的 安装步骤：详见 “Linux系统数据库MySQL安装.md” （3）解压安装（按需在指定节点上） 解压apache-hive-1.2.1-bin.tar.gz （4）追加配置环境变量 vim ~/.bash_profile 1234HIVE_HOME=Hive的解压路径HIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/bin （5）替换和添加相关jar包 修改HADOOP_HOME/share/hadoop/yarn/lib目录下的jline-*.jar 将其替换成HIVE_HOME/lib下的jline-2.12.jar。 –将如下(hive连接mysql)的jar包拷贝到hive解压目录的lib目录下 mysql-connector-java-5.1.32-bin.jar （6）修改配置文件（选择3种模式里哪一种）见三种安装模式 （7）启动 hive 2、三种模式 三种模式 A、内嵌模式（元数据保存在内嵌的derby中，允许一个会话链接，尝试多个会话链接时会报错）【了解】 B、本地模式（本地安装mysql 替代derby存储元数据）【重要】 C、远程模式（远程安装mysql 替代derby存储元数据）【重要】 （1）内嵌Derby单用户模式（了解） 元数据是内嵌在Derby数据库中的，只能允许一个会话连接，数据会存放到HDFS上。 存储方式简单，只需要hive-site.xml 注：使用 derby存储方式时，运行 hive 会在当前目录生成一个 derby 文件和一个metastore_db hive-site.xml ： 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.local&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; （2）本地用户模式（重要，多用于本地开发测试） 与嵌入式的区别 不再使用内嵌的Derby作为元数据的存储介质，而是使用其他数据库比如MySQL来存储元数据且是一个多用户的模式，运行多个用户client连接到一个数据库中。这种方式一般作为公司内部同时使用Hive。 这里有一个前提，每一个用户必须要有对MySQL的访问权利，即每一个客户端使用者需要知道MySQL的用户名和密码才行。 需要在本地运行一个 mysql 服务器 在node00上（与MySQL在同一个节点上）解压安装Hive MySQL Hive : node00 需要将 mysql 的 jar 包（mysql-connector-java-5.1.32-bin.jar）拷贝到$HIVE_HOME/lib 目录下 hive-site.xml 12345678910111213141516171819202122232425262728&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_local/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node00/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （3）远程模式（重要） remote 一体 将Hive解压安装与MySQL不同的节点上 MySQL ：node00 Hive ： node02 需要在 Hive服务器启动 meta服务 hive-site.xml (hadoop 2.6.5) 12345678910111213141516171819202122232425262728&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive1/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 如果在hadoop5.X环境下还需要添加 1234&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node01:9083&lt;/value&gt;&lt;/property&gt; 注：这里把hive的服务端和客户端都放在同一台服务器上了。服务端和客户端可以拆开 Remote 分开(公司企业经常用) 将 hive-site.xml 配置文件拆为如下两部分（此时不与MySQL在同一台节点上） MySql ： node00 服务端 ： node02 客户端 ： node01 1）、服务端配置文件（node02） hive-site.xml 123456789101112131415161718192021222324&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive2/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node00:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2）、客户端配置文件（node01） hive-site.xml 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive2/warehouse&lt;/value&gt; &lt;!--注意这里的路径要和服务端一致---&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node02:9083&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 服务端启动 hive 程序 hive –service metastore 客户端直接使用 hive 命令即可 Hive常见问题总汇：http://blog.csdn.net/freedomboy319/article/details/44828337 四、HQL详解Hql 就是HiveQl语句 1、DDL语句（数据库定义语言）（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDLHive的数据定义语言 （LanguageManual DDL;)） 重点 hive 的建表语句和分区。 （2）创建/删除/修改/使用数据库 创建数据库 （Hive搭建完毕后，会创建一个默认的数据库） 查看 show databases； CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment]; 举例： create database attribute; create database attr; 注意：创建数据时，数据库名不要和系统关键字冲突，否则会报错； 如下： 123456789101112131415161718192021命令：hive&gt; create database out;报错：FAILED: ParseException line 1:16 Failed to recognize predicate 'out'. Failed rule: 'identifier' in create database statement原因：在Hive1.2.0版本开始增加了如下配置选项，默认值为true：hive.support.sql11.reserved.keywords该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。解决：法一：弃用这个关键字，换个名字法二：弃用对保留关键字的支持在conf下的hive-site.xml配置文件中修改配置选项：&lt;property&gt; &lt;name&gt;hive.support.sql11.reserved.keywords&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 删除数据库 DROP (DATABASE|SCHEMA) [IF EXISTS] database_name; 举例： drop database attribute; 修改数据库(了解) ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …); ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; 使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库） USE database_name; 举例： use attr； （3）创建/删除/表（重点） 创建表（重要！） 数据类型： data_type : primitive_type 原始数据类型 | array_type 数组 | map_type map | struct_type | union_type – (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION | STRING 基本可以搞定一切 | BINARY | TIMESTAMP | DECIMAL | DECIMAL(precision, scale) | DATE | VARCHAR | CHAR array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], …&gt; union_type : UNIONTYPE &lt; data_type, data_type, … &gt; 1、准备数据 1231,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing 2、创建表 (如果没有指定进入某一数据库，就会在默认数据库中创建) 1234567891011create table log( id int, name string, age int, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; lines terminated by &apos;\n&apos;; 导入数据（属于DML但是为了演示需要在此应用） 123456LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [LOCAL]:从本地 | 若无，则为从HDFS [OVERWRITE] ： 会覆盖Hive表中的数据 | 若无则会追加 [PARTITION....] ： 创建分区 将log1文件中的数据填加到log表中 （log1中数据的格式要和log表格式保持一致，否则会乱；若文件已存在，则会自动重命名） 本地加载（相当于复制）数据到Hive的制定表中 12&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log;&gt; HDFS加载（相当于剪切）数据到Hive的制定表中 12&gt; LOAD DATA INPATH &apos;/root/su/log1&apos; INTO TABLE log ;&gt; 查看表中数据 1234567&gt; 对本表查询不会产生MapReduce任务&gt; hive&gt; select * from log;&gt; 使用函数查询会产生MapReduce任务&gt; hive&gt; select count(*) from log;&gt; 查询表的字段信息&gt; hive&gt; desc log;&gt; 第一个查询结果： 12341 zshang 18 [&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;1 zhaoliu 18 [&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;2 lishi 16 [&quot;shop&quot;,&quot;boy&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;hunan&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;3 wang2mazi 20 [&quot;fangniu&quot;,&quot;eat&quot;] &#123;&quot;stu_addr&quot;:&quot;shanghai&quot;,&quot;work_addr&quot;:&quot;tianjing&quot;&#125; 第二个查询结果： 14 附加题 查询表中likes字段中有girl的人 1hive&gt; select name from log2 where likes[1]=&quot;girl&quot;; 查询表中address字段有stu_addr为beijing的人 1hive&gt; select name from log2 where address[&quot;stu_addr&quot;]=&quot;beijing&quot;; 3、删除表 12&gt; DROP TABLE [IF EXISTS] table_name [PURGE];&gt; 举例： （用drop命令删除表，会将表中数据一并删除，其对应在MySQl中的表的元数据信息也会随之删除； ​ 用hdfs命令删除表对应的文件目录，表中数据也一并删除，但其元数据信息依然保存在My SQL上， ​ 再load数据，可恢复该表） 12&gt; drop table log1；&gt; 123456&gt; hdfs dfs -rmr /user/hive_local/warehouse/attr.db/log1&gt; &gt; hive&gt; use attr;&gt; hive&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;&gt; &gt; 创建外部表（重要） 外部关键字EXTERNAL允许您创建一个表,并提供一个位置,以便hive不使用这个表的默认位置。这方便如果你已经生成了数据，当删除一个外部表,表中的数据不会从文件系统中删除。外部表指向任何HDFS的存储位置,而不是存储在配置属性指定的文件夹 hive.metastore.warehouse.dir;).中 创建表： 1234567891011create EXTERNAL table log1( id int, name string, age int, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; lines terminated by &apos;\n&apos;; 加载数据： 1LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1; 删除外部表（相当于删除的是表的元数据信息，而表中的数据还保存） 1drop table log1； 结果： hive&gt; show tables; 无log1 MySQl中也无此表元数据信息 但是， 在HDFS文件系统中，此表数据依然存在 也就是说，词表还可以恢复 恢复表： 1重新创建log1表，该表即可恢复 （4）修改表,更新，删除数据(这些很少用)重命名表 1234&gt; ALTER TABLE table_name RENAME TO new_table_name;&gt; &gt; Eg: alter table meninem rename to jacke;&gt; 更新数据 1UPDATE tablename SET column = value [, column = value ...][WHERE expression] 删除数据 1DELETE FROM tablename [WHERE expression] 2、DML语句（数据库管理语言）（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML 重点是数据加载和查询插入语法 Hive数据操作语言（LanguageManual DML;)） （2）四种插入/导入数据(重要) Hive不能很好的支持用insert语句一条一条的进行插入操作，不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。 1234567create table log3( id int, name string, age int ) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos;; 1.直接加载数据12LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]load data local inpath &apos;/root/su/log1&apos; into table log1; 2.将表1查询结果插入表2注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 1234567创建person2表，然后从表person1查询数据导入：覆盖：INSERT OVERWRITE TABLE person2 [PARTITION(dt=&apos;2008-06-08&apos;, country)] SELECT id,name, age From ppt;追加：INSERT INTO TABLE log3 SELECT id,name, age From log; 3.将表1、表2查询结果插入表3、表4注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 1234567891011121314FROM person t1INSERT OVERWRITE | INTO TABLE person1 [PARTITION(dt=&apos;2008-06-08&apos;, country)] SELECT t1.id, t1.name, t1.age ; FROM log t1,log1 t2 INSERT OVERWRITE TABLE log4 SELECT t1.id,t1.name,t2.age ; 是否存在笛卡尔积：？？？？存在。 为了防止笛卡尔积： FROM log t1,log1 t2 INSERT OVERWRITE TABLE log4 SELECT t1.id,t1.name,t2.age where t1.id =t2.id; 1234【from放前面好处就是后面可以插入多条语句 】FROM abc t1,sufei t2 INSERT OVERWRITE TABLE qidu SELECT t1.id,t1.name,t1.age,t2.likes,t2.address ; 12345FROM abc t1,sufei t2 INSERT OVERWRITE TABLE qidu SELECT t1.id,t1.name,t1.age,t1.likes,t1.address where…INSERT OVERWRITE TABLE wbb SELECT t2.id,t2.name,t2.age,t2.likes,t2.address where…; 4.直接列出数据插入表中（大量数据时不推荐）注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 12INSERT INTO TABLE students VALUES (1,&apos;zs&apos;,18,&apos;boy&apos;,&apos;beijng&apos;),(2,&apos;wh&apos;,&apos;girl&apos;,&apos;stu_addr&apos;:shanghai&apos;); 本地load数据和从HDFS上load加载数据的过程有什么区别？ 本地： local 会自动复制到HDFS上的hive的**目录下 Hdfs导入 后移动到hive的**目录下 （3）查询数据并保存 保存数据到本地： 123456789101112insert overwrite local directory &apos;/opt/datas/hive_exp_emp2&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; select * from db_1128.emp ;留意两种的区别：保存的数据格式insert overwrite local directory &apos;/sun/temp/hive_save1&apos; row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; select * from log2 ; 这里如果将 overwrite 改为into 会报错。 12//查看数据!cat /sun/temp/hive_save1/000000_0; 保存数据到HDFS上： 12345678910insert overwrite directory &apos;/user/beifeng/hive/hive_exp_emp&apos; select * from db_1128.emp ;insert overwrite directory &apos;/sun/hive/temp/hive_save1&apos; row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; select * from log2 ; 这里如果将 overwrite 改为into 会报错。 在外部shell中将数据重定向到文件中： 123(注意：需要指明是哪个数据库的表)# hive -e &quot;select * from attr.log;&quot; &gt; /sun/hive/temp/hive_save2# cat /sun/hive/temp/hive_save2 （4）备份数据或还原数据（在HDFS上） 备份数据（包括表的元数据和表中的数据）： 1EXPORT TABLE log to &apos;/sun/hive/datas/export/cp1&apos; 删除再还原数据： 12345先删除表。drop table log;show tables ;再还原数据：IMPORT FROM &apos;/sun/hive/datas/export/cp1&apos; ; （5）其他Hql操作Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别：http://www.2cto.com/kf/201609/545560.html 3、Hive SerDe（序列化、反序列化）(1)定义Hive SerDe - Serializer and Deserializer SerDe 用于做序列化和反序列化。 构建在数据存储和执行引擎之间，对两者实现解耦。 对数据实现序列化，清洗数据，使之成为有效数据并加载。 Hive通过ROW FORMAT DELIMITED以及SERDE进行内容的读写。 （2）实现1234567row_format: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] : SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 1234567891011121314151617181920212223242526272829303132Hive正则匹配（实现数据清洗）创建表 logtbl： CREATE TABLE logtbl ( host STRING, identity STRING, t_user STRING, time STRING, request STRING, referer STRING, agent STRING) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos; WITH SERDEPROPERTIES ( &quot;input.regex&quot;=&quot;([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \&quot;(.*)\&quot; (-|[0-9]*) (-|[0-9]*)&quot;) STORED AS TEXTFILE; 加载数据:load data local inpath &apos;/root/su/localhost_access_log.2016-02-29&apos; into table logtbl;查看数据：select * from logtbl;显示：192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /bg-upper.png HTTP/1.1 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /bg-nav.png HTTP/1.1 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /asf-logo.png HTTP/1.1 304 -...(省略。。。) 表数据见数据文件：localhost_access_log.2016-02-29.txt 12345678910111213141516171819202122192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 - 五、Beeline和Hiveserver2（Hive的升级）1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）1# ./hiveserver2 若已经配置环境变量则启动方式为： 1# hivesever2 2、启动 beeline（可在服务端|客户端启动，相当于客户端） 因为beeline是在Hive安装目录的/bin下，所以只要有hive包都可以启动 1234567891011121314151617181920212223242526272829303132333435363738# ./beelinebeeline&gt; !connect jdbc:hive2://node00:10000 root 123456显示：Connecting to jdbc:hive2://node00:10000Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://node00:10000&gt;使用：列出数据库0: jdbc:hive2://node00:10000&gt; show databases;+----------------+--+| database_name |+----------------+--+| attr || attribute || default |+----------------+--+3 rows selected (7.493 seconds)0: jdbc:hive2://node00:10000&gt;而在服务端：显示：[root@node00 ~]# hiveserver219/01/07 08:52:09 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not existOKOKOKOK退出：服务端：ctrl + c客户端：！quit； 或 ctrl + c作用：对操作结果添加了美化。不过不太常用，耗内存，数据大的时候，还影响页面。 六、Hive的JDBC 一般是平台使用展示或接口，服务端启动hiveserver2后，在java代码中通过调用hive的jdbc访问默认端口10000进行连接、访问 12345678910111213141516171819public class HivejdbcClient &#123; private static String driverName = "org.apache.hive.jdbc.HiveDriver"; public static void main(String[] args)&#123; try&#123; Class.forName(driverName); &#125;catch (ClassNotFoundException)&#123; e.printStackTrace(); System.exit(1); &#125;// repalace "hive" here with the name of user the queries should run as Connection con = DriverManager.getConnection("jdbc:hive2://node00:10000/default","root","123456"); Statement stmt = con.createStatement(); String sql = "select * from log limit 0"; ResultSet rs = stmt.executeQuery(sql); while(rs.next())&#123; System.out.println(rs.getInt(1)+"-"+rs.getString("name")); &#125; &#125;&#125; 七、Hive分区与自定义函数UDF UDAF UDTF1、Hive的分区partition（重要） 功能： 为了方便海量数据的管理和查询，可以对数据建立分区（可按日期、部门、类型等具体业务）。进行分门别类的管理。 注意： 必须在定义表的时候创建partition分区 存储数据时，添加分区字段的数据，直接将数据按分区进行存储。 添加分区时： ​ 时间的格式：/ ： 存储时会乱码，用 - 不会。 ​ 需要指定分区 ​ 多个分区时，存在父子目录关系，按顺序对应，对应父子 ​ 创建表时，已经指定分区个数，就只能填加指定个数的字段数据 删除分区时： ​ 若该分区是父分区的最后一个子区，则父分区也会被删除 ​ 若删除父分区，其所有子分区也都会备删除 ​ 若删除的分区，分别在多个不同父分区中存在，则都会被删除 重命名分区时： ​ 修改之后的名字不能是已经存在的 注意：在创建 删除多分区等操作时一定要注意分区的先后顺序，他们是父子节点的关系。分区字段不要和表字段相同 类别： 单分区和多分区 静态分区和动态分区 （1）创建分区 单分区建表 123456create table day_table(id int, content string) partitioned by (dt string) row format delimited fields terminated by &apos;,&apos;; 注意：【单分区表，按天分区，在表结构中存在id，content，dt三列；以dt为文件夹区分】 双分区建表 123456create table day_hour_table (id int,content string) partitioned by (dt string, hour string) row format delimited fields terminated by &apos;,&apos;; 注意： 【双分区表，按天和小时分区，在表结构中新增加了dt和hour两列；先以dt为文件夹，再以hour子文件夹区分】 （2）添加分区表的分区（表已创建，在此基础上添加分区：按什么分区）： 注意：报错：此时添加，要注意分区的个数相对应，否则会报错： 1FAILED: ValidationFailureSemanticException Partition spec &#123;dt=2008-08-08, hour=08&#125; contains non-partition columns 注意：报错此时添加，要注意分区的字段名要对应添加，否则会保如下错误： 1FAILED: ValidationFailureSemanticException Partition spec &#123;d=2008-08-08&#125; contains non-partition columns 注意：一定是存在分区，才可添加 添加分区： 12ALTER TABLE table_nameADD partition_spec [ LOCATION &apos;location1&apos; ] partition_spec [ LOCATION &apos;location2&apos; ] ... 123例： ALTER TABLE day_table ADD PARTITION (dt=&apos;2028-08-08&apos;, hour=&apos;08&apos;);ALTER TABLE day_table ADD PARTITION (dt=&apos;2028-08-08&apos;); （3）删除分区语法：（– 用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。） 删除如双分区中的子级分区时，如果仅剩一个子分区，那么父级分区也会被删除。（连坐） 1ALTER TABLE table_name DROP partition_spec, partition_spec,... 1234例：ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;); （4）数据加载进分区表中语法： 1LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1,partcol2=val2 ...)] 12345HDFS：LOAD DATA INPATH &apos;/user/pv.txt&apos; INTO TABLE day_hour_table PARTITION(dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);本地：LOAD DATA local INPATH &apos;/user/hua/*&apos; INTO TABLE day_hour partition(dt=&apos;2010-07-07&apos;); （5）查看表的所有分区123hive&gt; show partitions day_hour_table;show partitions day_table; （6）重命名分区语法： 1ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec; 12例：ALTER TABLE day_table PARTITION (tian=&apos;2018-05-01&apos;) RENAME TO PARTITION (tain=&apos;2018-06-01&apos;); Hive的函数课参考官网，用时查阅即可： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF （7）动态分区(重要)–注意外部表 在本地文件/home/grid/a.txt中写入以下4行数据 aaa,US,CA aaa,US,CB bbb,CA,BB bbb,CA,BC 建立非分区表并加载数据 创建表 123456CREATE TABLE info1 ( name STRING, cty STRING, st STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 加载数据 1LOAD DATA LOCAL INPATH &apos;/root/su/a&apos; INTO TABLE info1; 查看 1SELECT * FROM info1; 建立外部分区表并动态加载数据 （注意删除外部表的相关事项） 123456&gt; CREATE EXTERNAL TABLE info2 (&gt; name STRING&gt; ) &gt; PARTITIONED BY (country STRING, state STRING); &gt; &gt; 这时候就需要使用动态分区来实现，使用动态分区需要注意设定以下参数： hive.exec.dynamic.partition 默认值：false 是否开启动态分区功能，默认false关闭。 使用动态分区时候，该参数必须设置成true; hive.exec.dynamic.partition.mode 默认值：strict 动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。 一般需要设置为nonstrict hive.exec.max.dynamic.partitions.pernode 默认值：100 在每个执行MR的节点上，最大可以创建多少个动态分区。 该参数需要根据实际的数据来设定。 比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。 hive.exec.max.dynamic.partitions 默认值：1000 在所有执行MR的节点上，最大一共可以创建多少个动态分区。 同上参数解释。 hive.exec.max.created.files 默认值：100000 整个MR Job中，最大可以创建多少个HDFS文件。 一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。 hive.error.on.empty.partition 默认值：false 当有空分区生成时，是否抛出异常。 一般不需要设置。 1234567891011set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.max.dynamic.partitions.pernode=1000; INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; SELECT * FROM info2; 2、自定义函数UDF UDAF UDTF 自定义函数包括三种 UDF、UDAF、UDTF UDF：一进一出 UDAF：聚集函数，多进一出。如：Count/max/min UDTF：一进多出，如 lateralview explore()，（类似于mysql中的视图） 使用方式 ：在HIVE会话中add自定义函数的jar 文件，然后创建 function 继而使用函数 （1）UDF 开发（用的多一点）1、UDF函数可以直接应用于 select 语句，对查询结构做格式化处理后，再输出内容。 2、编写 UDF 函数的时候需要注意一下几点： a）自定义 UDF 需要继承 org.apache.hadoop.hive.ql.UDF。 b）需要实现 evaluate 函数，evaluate 函数支持重载。 3、步骤 a）把程序打包放到目标机器上去； （需要hive和hadoop，jdk 的相关jar包） 函数一：脱敏处理 123456789101112131415161718192021222324package com.bigdata.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public class TuoMing extends UDF &#123; private Text res = new Text(); public Text evaluate(String string) &#123; // 校验参数是否为空 if(string==null)&#123; return null; &#125; // 若为单个字符 if(string.length()==1)&#123; res.set("*"); &#125; String str1 = string.substring(0,1); String str2 = string.substring(string.length()-1,string.length()); res.set(str1+"***"+str2); return res; &#125; &#125; 函数二：add函数 12345678910111213141516171819202122package com.bigdata.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public class Add extends UDF &#123; private Text res = new Text(); public Text evaluate(String num1,String num2) &#123; // 校验参数是否为空 if(num1==null)&#123; return null; &#125;else if(num2==null)&#123; res.set(num1); return res; &#125; int n = Integer.parseInt(num1)+Integer.parseInt(num2); String str =n+""; res.set(str); return res; &#125; &#125; b）进入 hive 客户端，添加 jar 包 1234hive&gt;add jar /root/su/TuoMing.jar;(相当于添加到环境变量中)(清除缓存时记得删除jar包： delete jar /*)delete jar /jar/udf_test.jar; c）创建临时函数： 123hive&gt;CREATE TEMPORARY FUNCTION add_example AS &apos;hive.udf.add&apos;;CREATE TEMPORARY FUNCTION tm_example AS &apos;com.bigdata.hive.udf.TuoMing&apos;;（as 后面添加的是：包名+类名） d）查询 HQL 语句： 12345SELECT add_example(8, 9) FROM scores;SELECT add_example(scores.math, scores.art) FROM scores;SELECT tm_example(id) FROM log; e）销毁临时函数： 1hive&gt; DROP TEMPORARY FUNCTION tm_example; （2）UDAF自定义集函数(用的少) 多行进一行出，如 sum()、min()，用在 group by 时 1.必须继承org.apache.hadoop.hive.ql.exec.UDAF(函数类继承) org.apache.hadoop.hive.ql.exec.UDAFEvaluator(内部类 Eval uator 实现 UDAFEvaluator 接口) 2.Evaluator 需要实现 init、iterate、terminatePartial、merge、terminate 这几个函数 init():类似于构造函数，用于 UDAF 的初始化 iterate():接收传入的参数，并进行内部的轮转，返回 boolean terminatePartial():无参数，其为 iterate 函数轮转结束后，返回轮转数据， 类似于 hadoop 的Combinermerge():接收 terminatePartial 的返回结果，进行数据 merge 操作， ​ 其返回类型为 boolean terminate():返回最终的聚集函数结果 开发一个功能同： Oracle 的 wm_concat()函数 Mysql 的 group_concat() Hive UDF 的数据类型： Hive UDF 的数据类型： （3）UDTF（用的少一点）UDTF：一进多出，如 lateral view explode( ) 返回一个数组表 Hive Lateral View 视图 Lateral View用于和UDTF函数（explode、split）结合来使用。 首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。 主要解决 在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题 语法： LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias) 例： 统计人员表中共有多少种爱好、多少个城市? 1234&gt; select count(distinct(myCol1)), count(distinct(myCol2))，count(distinct(myCol3))from log2 &gt; LATERAL VIEW explode(likes) myTable1 AS myCol1 &gt; LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;&gt; 123select myCol1, myCol2 from log2 LATERAL VIEW explode(likes) myTable1 AS myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; distinct(myCol1) 表示去重 LATERAL VIEW explode(likes) myTable1 AS myCol1 将likes查询结果放到mytable1表中，作为字段myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; 将address查询结果放到myTable2 表中，作为字段myCol2，myCol3，因为address是包含K-V的（两个） 八、Hive索引(知道) 一个表上创建索引： 使用给定的列表的列作为键创建一个索引。 详见创建索引;)设计文档。 12345678910111213CREATE INDEX index_name ON TABLE base_table_name (col_name, ...) AS index_type [WITH DEFERRED REBUILD] [IDXPROPERTIES (property_name=property_value, ...)] [IN TABLE index_table_name] [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] [LOCATION hdfs_path] [TBLPROPERTIES (...)] [COMMENT &quot;index comment&quot;]; 九、案例实践案例一：(基站掉话率)基站掉话率 1、创建表cell_monitor表 1234567891011121314create table cell_monitor( record_time string, imei string, cell string, ph_num int, call_num int, drop_num int, duration int, drop_rate DOUBLE, net_type string, erl string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;STORED AS TEXTFILE; 结果表cell_drop_monitor 12345678create table cell_drop_monitor(imei string,total_call_num int,total_drop_num int,d_rate DOUBLE) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;STORED AS TEXTFILE; 2、load数据1LOAD DATA LOCAL INPATH &apos;/root/su/cdr_summ_imei_cell_info.csv&apos; OVERWRITE INTO TABLE cell_monitor; 3、找出掉线率最高的基站12345from cell_monitor cm insert overwrite table cell_drop_monitor select cm.imei ,sum(cm.drop_num),sum(cm.duration),sum(cm.drop_num)/sum(cm.duration) d_rate group by cm.imei sort by d_rate desc; 案例二：（单词统计）1、建表12create table docs(line string);create table wc(word string, totalword int); 2、加载数据1load data local inpath &apos;/tmp/wc&apos; into table docs; 3、统计12345from (select explode(split(line, &apos; &apos;)) as word from docs) w insert into table wc select word, count(1) as totalword group by word order by word; 4、查询结果1select * from wc; 十、分桶（重要）1、概念 主要应用于数据抽样。 通过对列值取哈希值的方式，将不同数据放到不同的文件中存储。 对Hive中每个表、分区都可以进行分桶。 列的哈希值 /桶的个数→决定每条数据划分到哪个桶中 2、开启支持分桶1hive&gt; set hive.enforce.bucketing=true; 默认：false； 设置为true之后，mr运行时会根据bucket的个数自动分配reduce task个数。 （用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用） 一次作业产生的桶数 = reducde task数 3、往分桶表中加载数据12insert into table bucket_table select columns from tbl;insert overwrite table bucket_table select columns from tbl; 4、桶表抽样查询1select * from bucket_table tablesample(bucket 1 out of 4 on columns); TABLESAMPLE语法： 12&gt; TABLESAMPLE(BUCKET x OUT OF y)&gt; x：表示从哪个bucket开始抽取数据，x&lt;=y y：必须为该表总bucket数的倍数或因子 理解： 分桶表已经按age分为4桶，然后，有y个人去抽，从第(x 取模 桶数)桶中抽 5、实战创建普通表 123456CREATE TABLE mm( id INT, name STRING, age INT)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 测试数据 123456781,tom,112,cat,223,dog,334,hive,445,hbase,556,mr,667,alice,778,scala,88 加载数据： 1load data local inpath &apos;/root/su/mm&apos; into table mm; 创建分桶表 1234567CREATE TABLE psnbucket( id INT, name STRING, age INT)CLUSTERED BY (age) INTO 4 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 加载数据： 1insert into table psnbucket select id, name, age from mm; 抽样 1select id, name, age from psnbucket tablesample(bucket 2 out of 4 on age); 注意： hive&gt; select id, name, age from psnbucket tablesample(bucket 4 out of 2 on age);FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table psnbucket denominator : 分母 十一、运行方式1、Hive运行模式 – 命令行方式cli：控制台模式 1234567--与hdfs交互 * 执行dfs命令 * 例 ：hive&gt; dfs -ls / --与Linux交互 * ！ 开头 * hive&gt; !pwd –脚本运行方式：（生产中常用） 123456789101112131415161718192021222324在外部shell中执行,指定数据库,分号可加可不加# hive -e &quot;select * from attr.log &quot;# hive -e &quot;select * from attr.log；select * from default.log2&quot;--------------------------------------------------------------将执行结果重定向到指定文件：# hive -e &quot;select * from attr.log &quot; &gt;&gt;log1--------------------------------------------------------------静默模式执行，不打印log日志# hive -S -e &quot;select * from attr.log &quot; &gt;&gt;log1--------------------------------------------------------------脚本执行先编辑脚本问价# vim file1编辑内容select * from attr.log where id = 1;select * from attr.log where id &lt; 3;执行脚本# hive -f file1--------------------------------------------------------------?? 使用命令文件执行hive-init.sql?? # hive -i /home/hive-init.sql--------------------------------------------------------------在hive cli中执行脚本文件hive&gt; source file1 ？未解决？ ?? 使用命令文件执行hive-init.sql?? # hive -i /home/hive-init.sql 十二、hive的GUI接口（web页面）Hive Web GUI接口 web界面安装：1、下载源码包apache-hive-1.2.1-src.tar.gz, 2、在本地Windows系统中解压 并将\apache-hive-1.2.1-src\hwi\web路径中所有的文件打成war包 制作方法： war包 1、到\apache-hive-1.2.1-src\hwi\web路径下 2、在路径栏输入命令：jar -cvf hive-hwi.war * 3、即可生成文件：hive-hwi.war 3、将hwi-war包放在$HIVE_HOME/lib/中（Linux系统） 4、复制tools.jar(在jdk的lib目录下)到$HIVE_HOME/lib下 5、修改hive-site.xml 路径：/usr/soft/apache-hive-1.2.1-bin/conf/hive-site.xml 123456789101112&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi.war&lt;/value&gt; &lt;/property&gt; 6、启动hwi服务(端口号9999) 1hive --service hwi 7、浏览器通过以下链接来访问 http://node00:9999/hwi/ 8、登录页面： USER: GROUPS: 自已定义 十三、权限管理Hive - SQL Standards Based Authorization in HiveServer2 （1）三种授权模型 （2）常用：基于SQL标准的完全兼容SQL的授权模型特点： 支持对于用户的授权认证 支持角色role的授权认证 role可理解为是一组权限的集合，通过role为用户授权 一个用户可以具有一个或多个角色 ​ 默认包含俩种角色：public、admin 限制 （3）操作在hive服务端修改配置文件hive-site.xml添加以下配置内容： 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.users.in.admin.role&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator&lt;/value&gt;&lt;/property&gt; 服务端启动hiveserver2；客户端通过beeline进行连接 角色的添加、删除、查看、设置： 第一次操作无权限： 需要：CREATE ROLE admin； 12345CREATE ROLE role_name; -- 创建角色DROP ROLE role_name; -- 删除角色SET ROLE (role_name|ALL|NONE); -- 设置角色SHOW CURRENT ROLES; -- 查看当前具有的角色SHOW ROLES; -- 查看所有存在的角色 【官网：权限】 Action Select Insert Update Delete Owership Admin URL Privilege(RWX Permission + Ownership) ALTER DATABASE Y ALTER INDEX PROPERTIES Y ALTER INDEX REBUILD Y ALTER PARTITION LOCATION Y Y (for new partition location) ALTER TABLE (all of them except the ones above) Y ALTER TABLE ADD PARTITION Y Y (for partition location) ALTER TABLE DROP PARTITION Y ALTER TABLE LOCATION Y Y (for new location) ALTER VIEW PROPERTIES Y ALTER VIEW RENAME Y ANALYZE TABLE Y Y CREATE DATABASE Y (if custom location specified) CREATE FUNCTION Y CREATE INDEX Y (of table) CREATE MACRO Y CREATE TABLE Y (of database) Y (for create external table – the location) CREATE TABLE AS SELECT Y (of input) Y (of database) CREATE VIEW Y + G DELETE Y DESCRIBE TABLE Y DROP DATABASE Y DROP FUNCTION Y DROP INDEX Y DROP MACRO Y DROP TABLE Y DROP VIEW Y DROP VIEW PROPERTIES Y EXPLAIN Y INSERT Y Y (for OVERWRITE) LOAD Y (output) Y (output) Y (input location) MSCK (metastore check) Y SELECT Y SHOW COLUMNS Y SHOW CREATE TABLE Y+G SHOW PARTITIONS Y SHOW TABLE PROPERTIES Y SHOW TABLE STATUS Y TRUNCATE TABLE Y UPDATE Y 十四、Hive优化（重点）详见Hive优化文档 HIve常用函数： https://www.cnblogs.com/kimbo/p/6288516.html https://www.iteblog.com/archives/2258.html#3_avg https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions MapReducde底层源码： http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1 http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3 http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 入门学习（一）]]></title>
    <url>%2F2019%2F01%2F11%2FLinux%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%2B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0(%E7%AC%AC%E4%B8%80%E5%9B%9E%E5%90%88)%2F</url>
    <content type="text"><![CDATA[Linux网络配置+常用命令学习[TOC] 一、 Linux概述1.1. 简介imgLinux是一个自由的，免费的，源码开放的操作系统。也是开源软件中最著名的例子。其最主要的目的就是为了建立不受任何商品化软件版权制约的，全世界都能使用的类Unix兼容产品.而我们将服务器部署在Linux将会更加的稳定、安全、高效以及出色的性能这时windows无法比的。 1.2.Linux作者img 林纳斯·本纳第克特·托瓦兹（Linus Benedict Torvalds, 1969年~ ），著名的电脑程序员、黑客。Linux内核的发明人及该计划的合作者。托瓦兹利用个人时间及器材创造出了这套当今全球最流行的操作系统（作业系统）内核之一。现受聘于开放源代码开发实验室（OSDL：Open Source Development Labs, Inc），全力开发Linux内核。 1.3.Linux 发行版发行版是基于 Linux 内核的一个操作系统。它带有用户可以使用的软件集合。更多的，它还包含系统管理包。目前有许多 Linux 发行版。因为我们不能数清目前所有的 Linux 发行版，所以我们来看一下一些有名的版本： Ubuntu、Fedora、Opensuse、Red hat Linux 和 Debian 等是几个非常受欢迎的 Linux 发行版。 img C**entos** img img Ubuntu imgimg Rehat img img 1.4.Linux的特点开放性，多用户，多任务，丰富的网络功能，可靠的系统安全，良好的可移植性，具有标准兼容性 二、环境准备2.1. Vmware2.1.1 Vmware简介大多数服务器的容量（CPU,内存，磁盘等）利用率不足 30%，这不仅导致了资源浪费，也加大了服务器的数量。实现服务器虚拟化后，多个操作系统可以作为虚拟机在单台物理服务器上运行，并且每个操作系统都可以访问底层服务器的计算资源，从而解决效率低下问题。 Vmware虚拟机化技术由此诞生，它可以将一台服务器虚拟化出多台虚拟机，供多人同时使用，提高资源利用率。 2.1.2 Vmware workstation安装详细见vmware安装文档 2.2. linux安装详细见Linux安装文档 2.3.网络配置2.3.1 查看网关img img 2.3.2 配置静态IP(NAT模式)1.编辑配置文件,添加修改以下内容 vi /etc/sysconfig/network-scripts/ifcfg-eth0 按i 进入文本编辑模式，出现游标，左下角会出现INSERT,即可以编辑 img 应包含以下配置，除此之外的可以删除掉。 123456789101112131415DEVICE=eth0 #网卡设备名,请勿修改名字TYPE=Ethernet #网络类型，以太网BOOTPROTO=static #启用静态IP地址ONBOOT=yes #开启自动启用网络连接 IPADDR=192.168.78.100 #设置IP地址NETMASK=255.255.255.0 #设置子网掩码GATEWAY=192.168.78.2 #设置网关DNS1=114.114.114.114 #设置备DNS 按ESC退出编辑模式 :wq #保存退出 2.修改完后执行以下命令 service network restart #重启网络连接 ifconfig #查看IP地址 3.验证是否配置成功: 虚拟机能ping通虚拟网关 img 虚拟机与物理机（笔记本）相互可ping通 img 虚拟机与公网上的百度网址相互可ping通（此步ping通，才说明网络配置成功，Ctrl键+C停止） 命令：ping www.baidu.com 注意： a.保证VMware的虚拟网卡没有被禁用img b.网关IP不能被占用 2.4.XShell安装与使用2.4.1安装步骤除了安装路径需要修改，其他一直下一步。 2.4.2 连接虚拟机 打开xshell软件新建一个会话 img 填写所要连接的虚拟机IP，会话名称可改可不改，点击确定。 img 3.连接虚拟机。 img 4．输入root用户名，可以勾选”记住用户名” img 5.填写密码，可以勾选“记住密码” img 6.登录成功。 img 三、文件系统Linux文件系统中的文件是数据的集合，文件系统不仅包含着文件中的数据而且还有文件系统的结构，所有Linux 用户和程序看到的文件、目录、软连接及文件保护信息等都存储在其中。 Linux目录结构： img bin 存放二进制可执行文件(ls,cat,mkdir等) boot 存放用于系统引导时使用的各种文件 dev 用于存放设备文件 etc 存放系统配置文件 home 存放所有用户文件的根目录 lib 存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt 系统管理员安装临时文件系统的安装点 opt 额外安装的可选应用程序包所放置的位置 proc 虚拟文件系统，存放当前内存的映射 root 超级用户目录 sbin 存放二进制可执行文件，只有root才能访问 tmp 用于存放各种临时文件 usr 用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录 var 用于存放运行时需要改变数据的文件 3.1目录操作3.1.1**切换目录命令：cd + 目录的路径 查看当前目录的完整路径 ：pwd img 命令 cd .. 返回到父目录 img 3.1.2**新建目录命令：mkdir+ 目录名字 查看当前目录下拥有的子目录和文件: ls img 3.1.3 拷贝目录cp source dest -r img 3.1.4删除目录rmdir directory img 注意：rmdir只能删除空目录,若要删除非空目录则用rm命令 rm -rf dir 3.1.5移动/更改 目录​ 移动文件或目录：mv + 目录/文件名字 + 其他路径 ​ mv test / 将test目录移动到 根目录/ 下 ​ img ​ 更改文件或目录的名字：mv + 旧目录名字 + 新目录名字。 ​ img 3.2.文件操作3.2.1新建文件：（一切皆文件）touch web.log 创建一个空文件。 img 3.2.2 复制文​ cp web.log web_cp.log img 复制文件，加个-r 参数，代表遍历复制，此时可用于复制一个目录。 3.2.3删除文件rm web_cp.log img 此时需要手动输入y ，代表确认删除。可加 –f参数，直接删除，无需确认。当需要一个目录下所有东西时，加-r参数，代表遍历删除。 rm -f web.log img 3.2.4**查看3.2.4.1**查看目录下的东西​ ls / ll 命令 ls -l 等价于 ll img 查看目录下的所有东西（包括隐藏文件） 命令：ls –al 等价于 ll –a img 3.2.4.2查看文件内容cat filename: 一次性显示整个文件的内容 img 注意：当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。 因此，一般用more等命令分屏显示. more filename 该命令一次显示一屏文本，满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比。 按Space键，显示文本的下一屏内容。按Enter键，只显示文本的下一行内容。 按B键，显示上一屏内容。 按Q键，退出。 命令：more /etc/profile img 显示的内容： img ​ less命令 与 more命令 非常类似 less filename: ​ h 显示帮助界面 Q 退出less 命令 u 向后滚动半页 d 向前翻半页 空格键 滚动一页 b 向后翻一页 回车键 滚动一行 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 以及上下键，向上一行，向下一行 3.2.4.3从头打印**文件内容​ head -10 filename 打印文件1到10行 img 3.2.4.4从尾部打印文件内容 tail -10 filename 打印文件最后10行 img 注意：tail 还经常可以拿来查看文件的内容变化 加-f参数，tail –f filename 3.2.5查找文件或目录​ find pathname –name filename ​ 例子：find / -name profile ​ 该命令表示为，在/目录下查找 名字为profile的文件或目录，最后列出它的绝对路径 ​ img ​ 最后发现，linux系统根目录/ 下 一共有两个名字为profile， ​ 其中/etc/profile是一个文件，/etc/lvm/profile为目录 还可以按正则表达式来查找，且pathname越精确，查找的范围越小，速度越快。 ​ find /etc -name pro* 注意：（命令执行时，其查找的目录必须是所在目录的父级目录） ​ 该命令表示为：在/etc目录下查找以pro开头的文件或目录。 img 四、文本编辑4.1.vi编辑模式 vi filename :打开或新建文件，并将光标置于第一行首 vi +n filename ：打开文件，并将光标置于第n行首 vi + filename ：打开文件，并将光标置于最后一行首 vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的串处 • q!：不保存文件并退出vi – 在VI的命令模式下输入“:set nu”，就有行号了。 – 在VI的命令模式下输入“:set nonu”，取消行号。 一般模式 • yy 复制光标所在行(常用) • nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) （不同：在我的xshell中是 yyn实现复制所在行的向下n行） • p,P p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用) G:光标移至第最后一行 nG：光标移动至第N行行首 n（shift）+：光标下移n行 （nB|nb:光标向上移动n行） n-：光标上移n行 H ：光标移至屏幕顶行 M ：光标移至屏幕中间行 L ：光标移至屏幕最后行 • dd：删除 行 x或X：删除一个字符，x删除光标后的，而X删除光标前的 • u 恢复前一个动作(常用) 删除第N行到第M行： :N,Md 4.2.vimVim是从 vi 发展出来的一个文本编辑器。代码补完、语法高亮、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用. 安装vim 软件 yum install vim img 按y确认, 这中间一共要按两次确认 img 可以在书写命令时就加y,这样就不用逐一确认。 yum install vim -y 用vim 打开/etc/profile 文件，会发现编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强 命令：vim /etc/profile img 五、文件传输5.1.远程拷贝5.1.1将本地文件复制到远程机器 举例： 123456789101112&gt; 方式一：&gt; scp -rf /etc/profile root@192.168.198.128:/etc/&gt; &gt; 方式二:&gt; scp -r /etc/profile root@node01 /etc/ &gt; &gt; 方式三：&gt; scp -r /etc/profile node01:/etc/&gt; &gt; 方式四：&gt; scp -r /ec/profile node01:&apos;pwd&apos;&gt; scp ：远程传输文件命令 -r ：- 指的是后面跟的是参数 r 指的是遍历指定文件 f 指的是不用询问 /etc/profile : 是指定传输的文件 root： 远程机器的账户名 @ 远程机器的IP地址 ： /etc/ 远程机器上指定的目录 node01：远程机器的别名 ‘pwd’： 本地要远程传输文件所在的目录 scp local_file remote_username@remote_ip:remote_folder img 第一次远程拷贝时，需要在箭头1初输入yes确认一下，验证一下远程主机。然后在箭头2处输入一下远程主机的密码。 5.1.2将本地目录复制到远程机器scp -r local_folder remote_username@remote_ip:remote_folder img 在test目录下创建一个myfile文件，然后将test目录远程拷贝到192虚拟机的根目录下。 5.2.上传​ 需先安装好lrzsz : yum install lrzsz -y 安装好后，输入上传的命令rz,弹出一下界面： img 选择一个windows系统里的文件上传至虚拟机的当前目录下,然后ll命令，查看结果 img 5.3.下载 下载命令为sz，sz命令只能下载文件，不能是目录，可先将目录压缩成一个包，再下载至windows系统。下载完之后，按ctrl+c结束。 img 5.4 Xftp的安装与使用​ 除了可以用rz sz命令进行本地windows系统和虚拟机之间的文件传输，还可以使用XFTP软件。 六、网络指令6.1.查看网络配置信息命令:ifconfig img 箭头1指向的是本机IP，箭头2为广播地址，箭头3位子网掩码。 6.2.测试与目标主机的连通性命令：ping remote_ip（可以ping通Windows系统的IP） img 输入ping 192.168.78.192代表测试本机和192主机的网络情况， 箭头1表示一共接收到了3个包，箭头2表示丢包率为0，表示两者之间的网络顺畅。 注意：linux系统的ping命令会一直发送数据包，进行测试，除非认为的按ctrl + c停止掉， ​ windows系统默认只会发4个包进行测试，以下为windows的dos命令。 img 6.3.显示各种网络相关信息​ 命令：netstat –a n p t -a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-l 仅列出有在 Listen (监听) 的服務状态 -p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令。 提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到 img 七、系统配置7.1 主机名配置 若要修改主机名字，可在/etc/sysconfig/network文件里修改. vim /etc/sysconfig/network ​ img 机器重启才能生效 7.2 DNS配置 /etc/resolv.conf 为DNS服务器的地址文件 img 7.3 环境变量Linux系统的环境变量是在/etc/profile文件里配置。 首先考虑一个问题，为什么我们先前敲的yum, service,date,useradd等等，可以直接使用，系统怎么知道这些命令对应的程序是放在哪里的呢？ 这是由于无论是windows系统还是linux系统，都有一个叫做path的系统环境变量，当我们在敲命令时，系统会到path对应的目录下寻找，找到的话就会执行，找不到就会报没有这个命令。如下图： img 我们可以查看一下，系统一共在哪些目录里寻找命令对应的程序。 命令：echo $PATH img 可以看到path里有很多路径，路径之间有冒号隔开。当用户敲命令时，系统会从左往右依次寻找对应的程序，有的话则运行该程序，没有的就报错，command not found. 那如果我写了一个脚本（脚本后面会专门讲），我该怎样运行它呢？ img img 对test.sh添加可执行权限，chmod 700 test.sh img 运行方法有三种： 一种是到脚本的目录下执行： 运行命令 ： ./test.sh ,代表执行当前目录里的脚本test.sh img 一种是敲脚本的绝对路径：/usr/test/test.sh img 以上两种运行方式都不是很简便，因为先前我们执行yum service命令等，都是直接敲对应的命令的。所以我们也可以参照这样子做，只要我们配一个环境变量就好。 编辑： vim /etc/profile 将test.sh所在目录添加到PATH里就OK，我这里test.sh是在/usr/test目录（通过pwd查看）下。 img 编辑完之后，执行source /etc/profile命令，重新加载环境变量，此时会发现PATH路径多了一个/usr/test。 img ​ 最后验证一下，直接执行test.sh ​ img 八、服务操作8.1 列出所有服务命令：chkconfig 查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。 img 各数字代表的系统初始化级别含义： ​ 0：停机状态 1：单用户模式，root账户进行操作 2：多用户，不能使用net file system，一般很少用 3：完全多用户，一部分启动，一部分不启动，命令行界面 4：未使用、未定义的保留模式 5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。 6：停止所有进程，卸载文件系统，重新启动(reboot) 这些级别中1、2、4很少用，相对而言0、3、5、6用的会较多。3级别和5级别除了桌面相关的进程外没有什么区别。为了减少资源占用，推荐都用3级别. 注意 ：linux默认级别为3，不要把/etc/inittab 中initdefault 设置为0 和 6 8.2 服务操作service 服务名 start/stop/status/restart 例子：对防火墙服务进行操作，防火墙的服务名为：iptables. ​ 查看防火墙服务运行状态。 img 关闭防火墙. img 开启防火墙 img 8.3 关闭防火墙service iptables start/stop/status 注：学习期间直接把防火墙关掉就是，工作期间也是运维人员来负责防火墙的。 永久开启/关闭防火墙 chkconfig iptables on/off 8.4 服务初执行等级更改chkconfig –level 2345 name off|on ​ （服务名） img 若不加级别，默认是2345级别 命令：chkconfig name on|off ​ （服务名） 九、linux进程操作9.1 查看所有进程命令： ps -aux ​ -a 列出所有 ​ -u 列出用户 ​ -x 详细列出，如cpu、内存等 e ​ -f img 命令： ps - ef | grep ssh 查看所有进程里CMD是ssh 的进程信息。 img 其中箭头所指的是sshd服务进程的进程号（PID） 9.2 杀死进程Kill 用法 kill pid -9：强制杀死 ps 命令先查出对应程序的PID或PPID ，然后杀死掉进程。 img 十、 其他常用命令10.1 yumyum是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 由于centos系统的yum默认是到国外网站下载，有时下载速度会很慢，故我们可以换一个yum的下载源，这里我们换一个国内的下载源 阿里云镜像。 第一步：备份你的原镜像文件，以免出错后可以恢复。 cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup img 第二步：下载新的CentOS-Base.rep到/etc/yum.repos.d/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo img 下载完之后，vim /etc/yum.repos.d/CentOS-Base.repo 查看一下文件内容。 img 第三步：运行yum makecache生成缓存 img 查看当前源 yum list | head -50 10.2 wgetwget 是一个从网络上自动下载文件的自由工具，支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议 下载，并可以使用 HTTP 代理 需先安装 yum install wget –y wget用法:wget [option] 网址 -O 指定下载保存的路径 img img wget 工具还可以用来做一些简单的爬虫，这里不是我们的学习重点，如果想做爬虫，可以用java或python语言来做。 img 10.3 tartar ​ -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加 ​ -x 解压 ​ -c 压缩 ​ -f 目标文件，压缩文件新命名或解压文件名 ​ -v 解压缩过程信息打印 解压命令：tar -zvxf xxxx.tar.gz 例子：先用rz命令或wscp上传一个tar包，然后解压。 img img 解压后： img 压缩命令：tar -zcf 压缩包命名 压缩目标 例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61 将 apache-tomcat-7.0.61 目录压缩成tomcat.tar.gz包。 img 十一、JDK部署11.1 官网下载http://www.oracle.com/technetwork/java/javase/downloads/index.html img img 11.2 上传并解压用xftp将jdk包上传到linux系统里，我这里上传到/usr/soft目录下。 然后解压: tar -zxf jdk-7u80-linux-x64.tar.gz img 11.3配置环境变量配置全局JAVA_HOME，并在PATH路径里加入java_home/bin. 注意：新的path路径必须要包含旧的PATH路径，且每个路径之间以冒号隔开，而不是分号 vim /etc/profile JAVA_HOME= /usr/soft/jdk1.7.0_75 PATH=$PATH:$JAVA_HOME/bin img 重新加载环境变量：source /etc/profile img 11.4 验证java -version img 如出现上图，则表示java环境变量配置成功。 十二、部署Tomcat12.1 官网下载下载tomcat http://tomcat.apache.org/ 12**.2 上传并解压**我这里上传至/usr/soft目录下，然后解压。 img 12.3 启动tomcat在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务 img 关闭tomcat服务，可以用shutdown.sh命令。 或者ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令。 12.4 jpsjps是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 img 如上图所示，jps命令显示出了，系统当前运行在jvm上的进程情况。其中Bootstrap是tomcat的进程名字，1996是tomcat的PID 13.5验证先把防火墙关了（service iptables stop），然后访问虚拟机IP的8080端口 img]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 入门学习（二）]]></title>
    <url>%2F2019%2F01%2F11%2FLinux%20%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%9B%9E%E5%90%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2018年12月18日 周二 晴 今日学习要点： [TOC] Linux后半程一、Linux系统配置1.主机名配置： vim /etc/sysconfig/network 1545214876278 配置完成之后需要重启机器才能生效 reboot 2.DNS配置 查看DNS服务器的地址cat /etc/resolv.conf 修改DNS服务器地址方式一：vim /etc/sysconfig/network.scripts/ifconfig-eth0 ​ 在配置网关时，配置DNS1=114.114.114.114（不推荐，江苏南京的IP） 方式二：vim /etc/resolv.conf （用本地网关解析） ​ nameserver 192.168.198.0 ( 此为虚拟机中的网关地址) 3.环境变量 配置系统环境变量，使得某些命令在执行时，系统可以找到命令对应的执行程序，命令才能正常执行。 查看系统一共在哪些目录里寻找命令对应的程序 命令：echo $PATH 1545215894631 注意：路径之间有冒号隔开，系统会从左往右依次寻找对应的程序 ​ 一般命令会存放在 bin目录，或sbin目录 配置全局环境变量： vim /etc/profile 在文件中： PATH=$PATH:(命令所在目录) 退出文件编辑后： source /etc/profile (重新加载资源，有的可能需要重启机器，这不适用于实际状况) 配置局部环境变量：（推荐，限当前登录用户使用） 查看所有文件(root目录下) ls -a (发现隐藏文件 .bash.profile) vim ~/ bash_profile 在文件中： export PATH =$PATH:(命令所在目录) 4.拍快照（保存当时计算机所出状态的各种配置和资源，适度使用） 选中指定虚拟计算机——鼠标右击—–选中“快照” ——“拍摄快照‘—-在页面中找到”拍摄快照“，并添加名称和描述 也可以删除，找到页面中的删除按钮 二、服务操作1、查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。 命令：chkconfig 1545219619458 12345678各数字代表的系统初始化级别： 0：停机状态 1：单用户模式，root账户进行操作 2：多用户，不能使用net file system，一般很少用 3：完全多用户，一部分启动，一部分不启动，命令行界面 4：未使用、未定义的保留模式 5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。 6：停止所有进程，卸载文件系统，重新启动(reboot) 1、2、4很少用，0、3、5、6常用，3级别和5级别除了桌面相关的进程外没有什么区别，推荐都用3级别； linux默认级别为3； 不要把 /etc/inittab 中 initdefault 设置为0 和 6； 2、服务操作 service 服务名 start/stop/status/restart 举例：对防火墙服务进行操作 防火墙的服务名为：iptables 查看防火墙服务运行状态 service iptables status 关闭防火墙 service iptables stop 开启防火墙 service iptables start 永久开启/关闭防火墙 chkconfig iptables on/off 3、服务初执行等级更改 chkconfig –level 2345 name off|on​ （ 服务名） 举例：防火墙 chkconfig –level 2345 iptables off 若不加级别，默认是2345级别 命令：chkconfig name on|off​ （服务名） 三、linux进程操作1、查看所有进程 命令： ps -aux 12345 -a 列出所有-u 列出用户-x 详细列出，如cpu、内存等 -e select all processes 相当于-a -f does full-format listing 将所有格式详细列出来 查看所有进程里CMD是ssh 的进程信息（包括pid 进程号） 命令： ps - ef | grep ssh （| 管道符 ：前一个输出，变为后一个的输入） 举例： ps -ef | grep redis 2、杀死进程kill 命令：kill pid -9 强制杀死 用法：用ps 命令先查出对应程序的PID或PPID ，然后用kill杀死掉进程。 四、其他常用命令1、yum 基于RPM包管理 能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装 跟换yum下载源（默认是到国外网站下载） 第一步：备份你的原镜像文件，以免出错后可以恢复 cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 第二步：下载新的CentOS-Base.rep到/etc/yum.repos.d/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 下载完之后，查看一下文件内容 vim /etc/yum.repos.d/CentOS-Base.repo 第三步：生成缓存 运行yum makecache 查看当前源 yum list | head -50 2、 wget 一个从网络上自动下载文件的自由工具 支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议，可以使用 HTTP 代理 安装： yum install wget –y 用法： wget [option] 网址 -O 指定下载保存的路径 举例： wget www.baidu.com -O baidu.html 3、tar12345 -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加-x 解压-c 压缩-f 目标文件，压缩文件新命名或解压文件名-v 解压缩过程信息打印 解压命令：tar -zvxf xxxx.tar.gz 压缩命令：tar -zcf 压缩包命名 压缩目标举例： tar -zcf tomcat.tar.gz apache-tomcat-7.0.61将 apache-tomcat-7.0.61 目录压缩成tomcat.tar.gz包 4、man作用：用于查看指定命令的具体解释 安装 yum install man -y (下载并安装man 并确认) 使用 man ps 五、JDK部署1、准备JDK安装包：（这是使用 .rpm 格式的安装包） 官网下载：http://www.oracle.com/technetwork/java/javase/downloads/index.html 云盘资源： jdk-8u191-linux-x64.rpm ： ​ 根据用户喜好放到虚拟机器的文件目录中 2、解压并安装，展示编译过程 rpm -ivh jdk-8u191-linux-x64.rpm 安装放到了 /usr 目录下，有/java目录 3、配置环境变量 vim ~/.bash_profile 在文件中： JAVA_HOME=(jdk文件所在的路径+jdk文件名) export PATH=$PATH:$JAVA_HOME/bin 注意： 新的path路径必须要包含旧的PATH路径，且每个路径之间以冒号隔开，而不是分号 配置完成，退出编辑框后 source ~/.hash_profile 4、测试： java -version 或 echo $JAVA_HOME echo 标准输出，打印 六、Tomcat部署1、官网下载http://tomcat.apache.org/ 云盘资源：apache-tomcat-7.0.61.tar 2、上传并解压 tar -zvxf apache-tomcat-7.0.61.tar 3、启动tomcat在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务 ./startup.sh 4、关闭tomcat服务方式一：可以用shutdown.sh命令 方式二：ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令 5、验证先把防火墙关了（service iptables stop），然后访问虚拟机IP的8080端口]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统CentOS 6]]></title>
    <url>%2F2019%2F01%2F11%2FLinux%E7%B3%BB%E7%BB%9FCentOS%206%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、安装 资源准备： CentOS-6.6-x86_64-minimal.iso（简易迷你版） ： CentOS-6.7-x86_64-bin-DVD1.iso（完整版）： 1、点击新建虚拟机img 2、选择典型。（专业人士使用的话建议选择高级）img 3. 选择稍后安装操作系统img 4. 选择操作系统类型，选择linux,centos 64位img 5. 选择虚拟机安装位置和名称。img 6. 指定磁盘容量，默认20GB。img 7. 选择自定义硬件img 8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。img 9.点击完成。img 二、配置虚拟机1. 启动虚拟机。img注意：如果启动虚拟机时，发生以下问题，说明是你的电脑默认未开启虚拟化技术。 img 此时你应该把机器重启并进入bios界面（不同的机器进入bios界面的快捷键不同，一般为F1~F10键中的某个键，如果都不行，就得自己百度一下你的机器型号进入bios界面的快捷方式）。 ​ 当进入bios界面后，把虚拟机化选项（virtualization technology）打开,通过回车键，把disabled改成enabled,然后保存并重启机器。我这边是按F10，不同机器可能不一样，看右下角的提示信息。 img 2.Test Media, 如果不需要的话，点Skipimg 3、单击Next按钮继续img 4. 选择安装期间显示的语言img 5、选择键盘语言img 6、选择存储介质的类别。如果是将CentOS 6安装到本地硬盘上，选择 Basic Storage Devices，如果安装到网络存储介质如SANs上，选择 Specialized Storage Devices img 7.选择 yes,discard any dataimg 8. 设定主机名称（hostname）img 9. 设定时区，选择 Asia/Shanghaiimg 10. 设定root帐户的密码尽量使用较复杂的密码安装（根据实际情况，密码简单时，会有提示，点击user anyway 就行） img 11. 选择安装类型，这里我选择 “Use All Space” img 12. 选择 “Write changes to disk”，将分区数据写入硬盘img 13. 开始安装，此时只需等待即可img 14. 安装完结后，点击Reboot按钮img]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS 6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统数据库MySQL安装]]></title>
    <url>%2F2019%2F01%2F10%2FLinux%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[[TOC] Linux系统数据库MySQL安装一、第一次安装MySQL1、yum安装 命令 ： yum -y install mysql-server mysql-devel 2、登录 命令 ： mysql -u -p 显示： 1mysql&gt; 3、查看数据库(注意用‘ ; ’结束 ) 命令 ： show databases; 4、退出： 命令： quit； 5、创建用户： 命令 ： mysqladmin -uroot password 123456 6、再登录： 命令 ：mysql -u root -p 显示： 1mysql&gt; 说明成功了！ 7、数据库操作： 命令 ： use mysql; 显示： 1234Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 8、查看用户数据表： 命令 ： show tables; 9、查询user表部分字段： 命令 ： select host,user,password from user; 10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确（1）推荐现将user表中其他无密码的记录删除 命令 ： delete from user where password = ‘ ‘; (2)更新有密码的记录的host字段值 命令 ： update user set host = “%”; (3)刷新权限 命令 ：flush privileges; (4)退出 命令 ：quit; 二、Linux系统登录数据库MySQL报错报错一：1、登录 mysqld -uroot -p123456 报错： 1Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (111) 解决： 1）、先删除mysql.sock cd /var/lib/mysql mv mysql.sock mysql.sock.bak 2）、再次登陆 mysql -uroot -p123456 报错： 1Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (2) 3）、看看mysql的状态， /etc/rc.d/init.d/mysqld status 显示： mysqld is stopped 4）、看看是不是mysql的权限问题在/var/lib目录下： ls -lt|grep mysql 显示： drwxr-xr-x. 4 mysql mysql 4096 Jan 6 11:09 mysql 5）、说明mysql服务没有启动 2、启动mysql服务 /etc/init.d/mysqld start 显示： 1Starting mysqld: [ OK ] 3、再次登录： mysql -uroot -p123456 显示： 12345678910111213Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; 4、退出 quit 5、解决出现mysql.sock的问题 （1）、vim /etc/mycnf 编辑内容： 12[mysqld]skip_name_resolve=on innodb_file_per_table=on 按esc :wq 保存并退出 （2）使用命令： mysql_secure_installation （3）直接[ enter ] 键，输入密码， (另推荐：Jakie_ZHF老师的博客) 报错二：1ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Linux系统环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据思想]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[[TOC] 1、大数据核心问题：==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）== 2、大数据思维分而治之 把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q） enter description here 3、业务场景仓储、数牌 业务一：找{重复行}(chongfuhang)++现有1TB的TXT文件 ;格式：数字+字符 ；网速：500M/s ；服务器内存大小：128M ；条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++ enter description here ==方法== 答：共需要2次IO：2*30min=1h ==第一次IO==： 给每一行内容加上唯一标记（hashcode（内容），value（行号））。对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。 `对每一行的hash值进行取模运算，并放置于归类分区的小文件中`。由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。 ==第二次IO==： 在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。 业务二：快{排序}(paixu)++现有1TB的TXT文件 ;格式：数字；网速：500M/s ；服务器内存大小：128M ；条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++ 两次IO，2 * 30分钟 = 1小时 enter description here ==方法一：先全局有序后局部有序== 1.对全局按分区排序（由大到小）。​ 用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················） 2.对局部进行排序（由大到小）。​ 对每个分区进行排序。 enter description here ==方法二：先局部有序后全局有序== 先实现局部有序(小–&gt;大)。将文件划分为N个分区，在每个分区内部进行排序 使用归并实现全局有序。每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。 知否]]></content>
      <categories>
        <category>头脑风暴</category>
      </categories>
      <tags>
        <tag>分而治之</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce学习]]></title>
    <url>%2F2019%2F01%2F05%2FMapReduce%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、MapReduce是什么1、概念 MapReduce是一种分布式离线计算框架，是一种编程模型，用于在分布式系统上大规模数据集(大于1TB)的并行运算。 分布式编程： 借助一个集群，通过多台机器去并行处理大规模数据集，从而获得海量计算能力。 2、理解 Map(映射) Reduce(归约) 指定一个Map(映射)函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce(归约)函数，用来保证所有映射的键值对中的每一个共享相同的键组。 二、MapReduce设计理念1、分布式计算 分布式计算将该应用分解成许多小的部分，分配给多台计算机节点进行处理。这样可以节约整体计算时间，大大提高计算效率。 分而治之 策略： 一个存储在分布式文件系统中的大规模数据集， 会被切分成许多独立的分片（split）， 这些分片可以被 多个Map任务并行处理 2、移动计算，而分移动数据 将计算程序应用移动到具有数据的集群计算机节点之上进行计算操作； 将有用、准确、及时的信息提供给任何时间、任何地点的任何客户。 3、Master/Slave架构 包括一个Master和若干个Slave。Master上运行JobTracker，Slave上运行TaskTracker 三、MapReduce计算框架的组成 MR 1、 Mapper负责“分”，即把得到的复杂的任务分解为若干个“简单的任务”执行。 ​ “简单的任务”： 数据或计算规模相对于原任务要大大缩小； 就近计算，即会被分配到存放了所需数据的节点进行计算； 每个map任务之间可以并行计算，不产生任何通信。 split 2、Split规则：（取三者的中间值） – max.split(100M) – min.split(10M) – block(64M) max(min.split,min(max.split,block)) split实际大小=block大小（2.X：128M） Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数 3、Reduce详解（总·重要） – Reduce的任务是对map阶段的结果进行“汇总”并输出。 Reducer的数目由mapred-site.xml配置文件里的项目mapred.reduce.tasks决定。缺省值为1，用户可自定义。 4、Shuffle详解（总·核心） – 在mapper和reducer中间的一个步骤 可以把mapper的输出按照某种key值重新切分和组合成n份，把key值符合某种范围的输出送到特定的reducer那里去处理。 – 可以简化reducer过程 Partitoner ： hash(key) mod R 四、MapReduce架构1、非共享式架构每个节点都有自己的内存，容错性比较好。 2、一主多从架构可扩展性好，硬件要求易达到。 – 主 JobTracker:（ResourceManager资源管理） 负责调度分配每一个子任务task运行于TaskTracker上， 如果发现有失败的task就重新分配其任务到其他节点。 每一个hadoop集群中只一个 JobTracker, 一般它运行在Master节点上。 – 从TaskTracker:（NodeManager） TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务， 为了减少网络带宽TaskTracker最好运行在HDFS的DataNode上。 MapReduce的体系结构MapReduce主要有以下4个部分组成 1234567891011121314151617181 ）Client•用户编写的MapReduce程序通过Client提交到JobTracker端•用户可通过Client提供的一些接口查看作业运行状态2 ）JobTracker•JobTracker负责资源监控和作业调度•JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点•JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源3 ）TaskTracker•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）•TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask和Reduce Task使用（所以最好放在DataNode上）4 ）TaskTask 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 五、MapReduce搭建1、节点分布情况 NN DN JN ZK ZKFC RM node00 √ √ node01 √ √ node02 √ 2、配置文件 修改配置文件 (1)mapred-site.xml:（配置mapreudce需要的框架环境） 路径：F:\hadoop-2.6.5\etc\hadoop\mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （2）yarn-site.xml:（配置yarn的任务调度的计算框架） 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 因为ResourceManager 和NodeManager主从结构，RM存在单点故障，要对它做HA（通过ZK） 修改yarn-site.xml配置文件,完整的内容如下： 12345678910111213141516171819202122232425262728 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;Sunrise&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node04&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; 六、个人理解 基于源码，对mapreduce的工作流程的描述： 12345678910111213141516171819一个应用程序要进行大规模数据处理分析数据文件保存在HDFS中，分块存储在分布式节点上首先是将数据文件切分成许多split切片每一个split切片单独启动一个map任务，所以会启动多个map任务map阶段的输入是诸多(key,value),输出是新的（key,value）,然后被拉去到不同的reduce上并行处理操作所以每个map的输出阶段都执行分区操作，并决定reduce任务的个数然后对map输出结果进行排序、归并、合并，这个过程叫map阶段的shuffleshuffle结束后，将相应的结果分发给reduce，让reduce完成后续的工作 结束后，将结果输出给HDFS。不同的map之间不会通信，不同的reduce也不会通信，整个过程对用户透明。 shuffle MapReduce执行的各个阶段： 123456789101112131、从HDFS中加载文件，加载读取由INputFormat模块来完成，对输入负责格式验证，同时，对数据进行逻辑上切分成split2、由record read具体根据分片的位置长度信息去找各个block，以（key，value）输出，作为map的输入，3、map中有用户自定义的map函数就可以进行相应的数据处理，并输出一堆（key，value），作为中间结果4、之后，是shuffle（洗牌）过程对这中间结果进行分区、排序、合并，并溢写到磁盘，5、相应的reduce任务就会来fetch对应的分区（key，value（list））6、reduce中有用户自定义的reduce函数就可以完成对数据的分析，结果以新的（key，value）输出7、输出结果借助OutputFormat模块对输出格式进行检查，以及相关目录是否存在等，最后写入到HDFS中。 split 关于split的切分的理解： 1234561、InputFormat将大的数据文件分成很多split2、文件在HDFS中是以很多个物理块block分布式存储不同的节点上3、切片是用户自定义的逻辑分片4、split的数量决定map任务的数量5、切片过多会导致map任务启动过多，map任务之间切换的时候就会耗费相关的管理资源，所以切片过多会影响执行效率6、 切片过少又会影响任务执行的并行度，所以理想情况用block块的大小作为切片的大小。 关于shuffle的理解 123456789map端shuffle1、从HDFS输入数据和执行map任务，在map任务执行之前，RecordReader阅读器还将数据变成满足Map函数所需的（K，V）形式，然后InputFormat会将其切分成若干切片（一堆（K，V））。2、每个切片会分配一个map任务，每个map任务会分配一个默认的缓存，一般默认缓存为100M.map的输出键值对作为中间结果先写入到缓存（直接写入磁盘会增加寻址开销，所以集中写入磁盘一次寻址就可以完成批量写入，就可以将寻址开销分摊到大量数据中，这就是缓存的作用）。3、当写入的内容达到缓存空间的一定比例后（溢写比，一般为0.8，就是80M的时候，为了不影响map任务的继续执行），会启动溢写进程，把缓存中相关数据写入磁盘。4、在溢写过程中，会执行分区（partition）、排序（sort，按照key值）和可能的合并（combine，为了减少溢写到磁盘的数据量，慎用）操作，写入磁盘，生成磁盘的溢写文件。5、在map任务运行结束前，系统会对溢写文件进行归并（merge），形成大文件（里面的键值对是分区，排序的）,文件格式为（key,value&lt;list&gt;），归并时如果溢写文件大于预定值（默认为3），会再次合并reduce端shuffle1、reduce任务会询问JobTracker，去拉取map机器上的属于自己的分区，对来自不同机器的数据进行归并、合并，然后输入到reduce函数中进行数据的处理分析，再写入磁盘 我 MapReduce应用程序执行过程]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2.X]]></title>
    <url>%2F2019%2F01%2F04%2FHadoop2.X%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Hadoop 2.x产生背景1、Hadoop 1.0存在的问题（1）HDFS存在的问题 NameNode单点故障，难以应用于在线场景 NameNode（一个）压力过大，内存受限，影响系统扩展性 （2）MapReduce存在的问题 JobTracker访问压力大，影响系统扩展性 难以支持MapReduce以外的计算框架，比如Spark、Storm 2、Hadoop 2.0分支HDFS：分布式文件存储系统MapReduce：计算框架YARN：资源管理系统 3、特点 1）. 解决单点故障：HDFS HA（高可用） 通过主备NameNode解决，如果主NameNode发生故障，就切换到备NameNode上 | 2).解决内存受限问题：HDFS Federation（联邦制）、HA HA：两个NameNode (3.0就实现了一组多从：水平扩展，支持多个NameNode；每个NameNode分管一部分目录；所有NameNode共享所有DataNode资源) 3).仅架构上发生变化使用方式不变 二、HDFS HA结构及功能**HADN：DataNode（数据节点） 存放数据block块；遵循心跳机制向NN Active和NN Standby汇报block块信息，但只执行active的命令 主备NN：NameNode Active 和 NameNode Standby （主备名称节点） 主NN对外提供读写服务，备NN同步主NN元数据，以待切换，所有的DN同时向两个NN汇报数据块信息 元数据信息加载到主NN，并写入JN（至少写两台：过半原则）； 备NN可以从JN中同步元数据信息； 解决单点故障； –两种切换方式： 手动：通过命令实现主备切换 自动：基于Zookeeper实现（详情见搭建步骤） JN：JournalNode（至少3台） 存储主NN元数据信息，实现主备NN间数据共享； （遵循过半原则：至少有过半的数量参与投票） ZKFC：FailoverController（竞争锁） 谁拿到了这个所，谁就是active NN 心跳机制监控主备NN状态，一旦出现一台挂机，就会释放锁，另一个NN就会立即启动竞争锁，成为active NN ZK：Zookeeper（至少3台） （实现主备NN切换） **联邦 通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使到namenode/namespace可以通过增加机器来进行水平扩展 通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中。 三、YARN(资源管理)???????详见Yarn学习.md 1、核心思想：SourceManager（资源管理）+ReplicationMaster（任务调度） 2.yarn的引入使得多个计算框架可以应用到一个集群中 四、Zookeeper工作原理详见Zookeeper学习.md 五、Hadoop2.X 集群搭建1、linux环境下搭建 NN DN JN ZKFC ZK SM RM node00 √ √ √ √ √ √ √ node01 √ √ √ √ √ √ node02 √ √ √ √ 0.在搭建环境之前的准备 三台虚拟机： 1234567关闭防火墙安装jdk编辑/etc/hosts/给各个节点服务器起别名时间服务器：ntpdate 安装：yum install ntpdate -y 生成：ntpdate cn.ntp.org.cn免密登录环境准备 在hadoop安装目录下hadoop-2.6.5/etc/hadoop/ 编辑hadoop-env.sh 1export JAVA_HOME=/usr/soft/jdk1.8.0_191 2.编辑core-site.xml 123456789101112131415&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Sunrise&lt;/value&gt;&lt;!--配置集群的名字--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node00:2181,node01:2181,node02:2181&lt;/value&gt; &lt;!--配置zookeeper：三个节点--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop&lt;/value&gt;&lt;!--配置hadoop基础配置存放的路径--&gt;&lt;/property&gt;&lt;/configuration&gt; 3.编辑hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;sxt&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.Sunrise&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node01:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node00:8485;node01:8485;node02:8485/sxt&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.Sunrise&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 配置隔离机制为ssh 防止脑裂：保证activeNN仅有一台--&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 指定秘钥的位置 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt; &lt;!--免密登录是生成的文件，有的是id_rsa--&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定journalnode日志文件存储的路径 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 开启自动故障转移 --&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 配置hadoop中的slaves（主从架构：datanode） 123node00node01node02 5.准备zookeeper 三台zookeeper：node00，node01，node02 编辑zookeeper-3.4.13/conf/zoo.cfg 123456789tickTime=2000initLimit=10syncLimit=5dataDir=/usr/soft/zookeeper-3.4.13/datadataLogDir=/usr/soft/zookeeper-3.4.13/logsclientPort=2181server.1=node00:2888:3888server.2=node01:2888:3888server.3=node02:2888:3888 在dataDir目录中创建文件myid，三台节点的文件内容分别为1，2，3 6.配置环境变量 vim ~/.bash_profile 123456JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/bin source ~/.bash_profile 使其成为资源文件，发送到其他节点后，也需要此操作 7.将以上配置文件远程发送至其他节点服务器 scp -r filename nodename:pwd 8.命令操作： 123456789101112131415161. 启动三个zookeeper：./zkServer.sh start2. 启动三个JournalNode：./hadoop-daemon.sh start journalnode3. （生成fsimage文件）在其中一个namenode上格式化： hdfs namenode -format4. 把刚刚格式化之后的元数据拷贝到另外一个namenode上 a) 启动刚刚格式化的namenode : hadoop-daemon.sh start namenode b) （同步fsimage文件）在另一个（没有格式化的）namenode上执行： hdfs namenode -bootstrapStandby c) 启动没格式化的namenode： hadoop-daemon.sh start namenode5. （初始化竞争锁zookeeper）在其中一个namenode上初始化zkfc： hdfs zkfc -formatZK6. 停止上面节点：stop-dfs.sh7. 全面启动（三个节点）：start-dfs.sh8. 启动yarn资源管理器 yarn-daemon.sh start resourcemanager (yarn resourcemanager ) 2、使用（启动步骤） 1234(1)关闭防火墙：service iptables stop （3台）(2)启动zookeeper:zkServer.sh start （3台）(3)启动集群：start-dfs.sh |（start-all.sh : 同时启动hdfs和yarn)(4)启动yarn：yarn-daemon.sh start resourcemanager （可3台） （关闭步骤） 123(1)关闭yarn：yarn-daemon.sh stop resourcemanager （开几台关几台）(2)关闭集群：stop-dfs.sh |（stop-all.sh :同时关闭hdfs和yarn） （3台）(3)关闭zookeeper：zkServer.sh stop （3台） 12345678有可能会出错的地方1， 确认每台机器防火墙均关掉2， 确认每台机器的时间是一致的3， 确认配置文件无误，并且确认每台机器上面的配置文件一样4， 如果还有问题想重新格式化，那么先把所有节点的进程关掉5， 删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录，所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录6， 接上面的第6步又可以重新格式化已经启动了7， 最终Active Namenode停掉的时候，StandBy可以自动接管！]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN的入门学习]]></title>
    <url>%2F2019%2F01%2F03%2FYarn%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 一、简介yarn（资源管理器）（1）存在背景：MR1.0存在缺陷： 单点故障： 仅有一个JobTracker负责整个作业的调度、管理、监控、资源调度 （一个作业拿到后会分解多个任务去执行mapduce，JobTracker把任务分配给TaskTracker来具体负责执行相关map或reduce任务） JobTracker‘大包大揽’，管理事项过多 （上限4000个节点） 容易出现内存溢出 资源划分不合理 （强行划分slot，map资源和reduce资源不能互用，导致忙的忙死，闲的闲死） 1既是一个计算框架，也是一个资源管理框架 （2）yarn产生 对JobTracker进行功能分解，将资源管理功能分给ResourceManager，将任务调度和任务监控分给ApplicationMaster，将TaskTracker的任务交给NodeManager 12纯粹的资源管理框架被剥离资源管理调度功能的MapReduce就变成了MR2.0，他就是一个运行在YARN上的一个纯粹的计算框架，由YARN为其提供资源管理调度服务 什么叫纯粹的计算框架？？ 它提供一些计算基类，使用时，编写map类和reduce类的子类，去继承它。然后计算框架去做后台自动分片，shuffle过程。 资源管理框架？？ 它专门管理CPU内存资源的分配 二、YARN设计思路 三、YARN体系结构三大核心： 1、RecourceManager（RM） ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager） 调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行就近选择，从而实现“计算向数据靠拢” 容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量 调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也允许用户根据自己的需求重新设计调度器 应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等 2、ApplicationMaster ResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMasterApplicationMaster的主要功能是：（1）当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，ResourceManager会以容器的形式为ApplicationMaster分配资源； （2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源的“二次分配”； （3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行失败恢复（即重新申请资源重启任务）； （4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信息； （5）当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。 3、NodeManager NodeManager是驻留在一个YARN集群中的每个节点上的代理，有所需数据的节点，主要负责： 容器生命周期管理 监控每个容器的资源（CPU、内存等）使用情况 跟踪节点健康状况 以“心跳”的方式与ResourceManager保持通信 向ResourceManager汇报作业的资源使用情况和每个容器的运行状态 接收来自ApplicationMaster的启动/停止容器的各种请求 需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与NodeManager通信来掌握各个任务的执行状态 四、YARN 工作流程 五、YARN框架与MapReduce1.0框架的对比分析 从MapReduce1.0框架发展到YARN框架，客户端并没有发生变化，其大部分调用API及接口都保持兼容，因此，原来针对Hadoop1.0开发的代码不用做大的改动，就可以直接放到Hadoop2.0平台上运行 总体而言，YARN相对于MapReduce1.0来说具有以下优势： 大大减少了承担中心服务功能的ResourceManager的资源消耗 ApplicationMaster来完成需要大量资源消耗的任务调度和监控 多个作业对应多个ApplicationMaster，实现了监控分布化 MapReduce1.0既是一个计算框架，又是一个资源管理调度框架，但是，只能支持MapReduce编程模型。而YARN则是一个纯粹的资源调度管理框架，在它上面可以运行包括MapReduce在内的不同类型的计算框架，只要编程实现相应的ApplicationMaster YARN中的资源管理比MapReduce1.0更加高效 以容器为单位，而不是以slot为单位 六、YARN 的发展目标YARN 的目标就是实现“一个集群多个框架”？ ，为什么？ 一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架 MapReduce实现离线批处理 使用Impala实现实时交互式查询分析 使用Storm实现流式数据实时分析 使用Spark实现迭代计算 这些产品通常来自不同的开发团队，具有各自的资源调度管理机制 为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分别安装运行不同的计算框架，即“一个框架一个集群” 导致问题 集群资源利用率低 数据无法共享 维护代价高 YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源调度管理框架YARN，在YARN之上可以部署其他各种计算框架 由YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩 可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率 不同计算框架可以共享底层存储，避免了数据集跨集群移动]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS学习]]></title>
    <url>%2F2019%2F01%2F03%2FHDFS%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] Hadoop学习一、分布式文件存储系统HDFS1、什么是分布式？ 定义：将海量的数据，复杂的业务分发到不同的计算机节点和服务器上分开处理和计算。 特点： 多副本，提高服务的容错率、安全性、高可靠性 适合批处理，提高服务的效率和速度， 减轻单台服务的压力 具有很好的可扩展性 计算向数据靠拢，安全，高效 大数据三驾马车：GFS、MapReduce、Bigtable 2、什么是HDFS？（1）HDFS为什么会出现？ 主要解决大量【pb级以上】的大数据的分布式存储问题 （2）HDFS的特点 $$ 分布式特性： 适合大数据处理：GB、TB、PB以上的数据 百万规模以上的文件数量:10K+ 节点 适合批处理：移动计算而非数据(MR),数据位置暴露给计算框架 $$ 自身特性： 可构建在廉价机器上 高可靠性：通过多副本提提高 高容错性：数据自动保存多个副本；副本丢失后，自动恢复,提供了恢复机制 $$ 缺点： —–低延迟高数据吞吐访问问题（不适合低延迟数据访问，Hbase适合） 不支持毫秒级 吞吐量大但有限制于其延迟（瓶颈：低延迟无法突破） —–小文件存取占用NameNode大量内存(寻道时间超过读取时间,约占99%) ——-不支持多用户写入及任意修改文件 不支持文件修改：一个文件只能有一个写者 文件仅支持append不支持修改 （其实本身是支持的，主要为了用空间换时间，节约成本） $$ 实现目标： 兼容廉价的硬件设施 实现流数据读写 支持大数据集 支持简单的文件模型 强大的跨平台兼容性 （3）HDFS架构图HDFS架构图 HDFS架构图 关系型数据库：安全，存储在磁盘中；如MySql、Oracle、SQlServer 非关系型数据库：不安全，存储在内存中；如Redis、MemcacheDB、mongDB、Hbase 3、HDFS的功能模块及原理详解 HDFS数据存储模型（block）block （1）文件被线性切分固定大小的数据块：block 通过偏移量offset（单位：byte）标记 默认数据块大小为64MB (hadoop1.x，hadoop2.x默认为128M）)，可自定义配置 若文件大小不到64MB ，则单独存成一个block （2）一个文件存储方式 按大小被切分成若干个block ，存储到不同节点上 默认情况下每个block都有2个副本 共3个副本 副本数不大于节点数 （3）Block大小和副本数通过Client端上传文件时设置， 文件上传成功后副本数可以变更，Block Size大小不可变更 块的大小远远大于普通文件系统，可以最小化寻址开销 NameNode（简称NN） 存储元数据； 元数据保存在内存中； 保存文件、block块、datanode之间的映射关系 1&gt; NN主要功能： 接收客户端的读写服务；接收DN汇报block位置关系 2&gt; NN保存metadate元信息 基于内存存储，不会和磁盘发生交换 ​ metadata元数据信息包括以下 文件的归属（ownership）和权限（permission） 文件大小和写入时间 block列表【偏移量】：即一个完整文件有哪些block（b0+b1+b2+..=file） 位置信息（动态的）：Block每个副本保存在哪个DataNode中 *注意*：位置信息是由DN启动时上报给NN ，因为它会随时变化，所以不会保存在内存和磁盘中 3&gt; NameNode的metadate信息在启动后会加载到内存 同时： metadata信息也会保存fsimage文件中（fsimage文件是位于磁盘上的镜像文件） 对metadata的操作日志也会记录在edits 文件中（edits文件是位于磁盘上的日志文件） SecondaryNameNode（简称SNN）1&gt;SNN主要功能 帮助NameNode合并edits和fsimage文件，减少NN启动时间； SecondaryNameNode一般是单独运行在一台机器上； 它不是NN的备份（但可以做备份)。 2&gt;合并流程SNN合并 123456789101112SecondaryNameNode的工作情况：（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件， 暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成， 上层写日志的函数完全感觉不到差别；（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文 件，并下载到本地的相应目录下；（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件 中的各项更新操作，使得内存中的FsImage保持最新； 这个过程就是EditLog和FsImage文件合并；（4）SecondaryNameNode执行完（3）操作之后， 会通过post方式将新的FsImage文件发送到NameNode节点上（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件， 同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了 3&gt;合并机制 ——-SNN执行合并时间和机制 A、根据配置文件设置的时间间隔fs.checkpoint.period 默认3600秒 B、根据配置文件设置edits log大小 fs.checkpoint.size ​ 规定edits文件的最大值默认是64MB DataNode（简称DN）1&gt; DN主要功能 存储文件内容（block）； 文件内容保存在磁盘； 维护了block id 到datanode本地文件的映射关系 启动DN线程的时候会向NameNode汇报block位置信息 2&gt; DN工作机制12345• 数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，• 会根据客户端或者是名称节点的调度来进行数据的存储和检索，• 并且通过心跳机制向名称节点定期发送自己所存储的块的列表，保持与其联系（3秒一次） （如果NN 10分钟没有收到DN的心跳，则认为其已经lost，并copy其上的block到其它DN）• 每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中 3&gt; block的副本放置策略 – 第一个副本：放置在上传文件的DN（集群内提交）； ​ 如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。 – 第二个副本：放置在于第一个副本不同的机架的节点上。 – 第三个副本：与第二个副本相同机架的不同节点。 – 更多副本：随机节点 block块存放位置 4、HDFS读写流程 读文件过程read 1、首先client端调用FileSystem对象（FS）的open方法，（FS：一个DistributedFileSystem的实例）。2、DistributedFileSystem通过rpc协议从NameNode（NN）获得文件的第一批block的locations，（同一个block按副本数会返回多个locations，因为同一文件的block分布式存储在不同节点上），这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面（就近选择）。 3、前两步会返回一个FSDataInputStream对象，该对象会被封装DFSInputStream对象，DFSInputStream可以方便的管理DN和NN的数据流。客户端调用read方法，DFSInputStream会连接离客户端最近的DN，数据从DN源源不断的流向客户端（对客户端是透明的，只能看到一个读入的Input流）。 4、如果第一批block都读完了， DFSInputStream就会去NN拿下一批block的locations，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流 读 注意： 123456如果在读数据的时候， DFSInputStream和DN的通讯发生异常，就会尝试连接正在读的block的排序第二近的DN,并且会记录哪个DN发生错误，剩余的blocks读的时候就会直接跳过该DN。 DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到NN，然后DFSInputStream在其他的DN上读该block的镜像。该设计就是客户端直接连接DN来检索数据，并且NN来负责为每一个block提供最优的DN，NN仅仅处理block location的请求，这些信息都加载在NN的内存中，hdfs通过DN集群可以承受大量客户端的并发访问。* RPC *（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。 写文件过程write 1.客户端通过调用DistributedFileSystem的create方法创建新文件。 2.DistributedFileSystem通过RPC调用NN去创建一个没有blocks关联的新文件，创建前，NN会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NN就会记录下新文件，否则就会抛出IO异常。 3.前两步结束后，会返回FSDataOutputStream的对象，封装在DFSOutputStream，客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的packet，然后排成队列dataQuene。 4.NN会给这个新的block分配最适合存储的几个datanode，DFSOutputStream把packet包排成一个管道pipeline输出。先按队列输出到管道的第一个datanode中，并将该Packet从dataQueue队列中移到ackQueue队列中，第一个datanode又把packet输出到第二个datanode中，以此类推。 5.DFSOutputStream中的ackQuene，也是由packet组成，等待DN的收到响应，当pipeline中的DN都表示已经收到数据的时候，这时ackQuene才会把对应的packet包移除掉。 如果在写的过程中某个DN发生错误，会采取以下几步： ​ 1) pipeline被关闭掉； ​ 2)为了防止丢包，ackQuene里的packet会同步到dataQuene里;新建pipeline管道接到其他正常DN上 ​ 4)剩下的部分被写到剩下的正常的datanode中； ​ 5)NN找到另外的DN去创建这个块的复制。（对客户端透明） 6.客户端完成写数据后调用close方法关闭写入流 注意：客户端执行write操作后，写完的block才是可见的，正在写的block对客户端是不可见的 写 5.HDFS文件权限和安全模式？？HDFS文件权限？？– 与Linux文件权限类似 • r: read; w:write; x:execute，权限x对于文件忽略，对于文件夹表示是否允许访问其内容 – 如果Linux系统用户zs使用hadoop命令创建一个文件，那么这个 文件在HDFS中owner就是zs。 – HDFS的权限目的：阻止好人做错事，而不是阻止坏人做坏事。 ？？安全模式？？ NN启动的时候，首先将映像文件(fsimage)载入内存，并执行编辑日志(edits)中的各项操作。 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的fsimage文件(这个操作不需要SecondaryNameNode)和一个空的编辑日志。 此刻namenode运行在安全模式。即namenode的文件系统对于客服端来说是只读的。(显示目录，显示文件内容等。写、删除、重命名都会失败)。 在此阶段Namenode收集各个datanode的报告，当数据块达到最小副本数以上时，会被认为是“安全”的， 在一定比例（可设置）的数据块被确定为“安全”后，再过若干时间，安全模式结束 当检测到副本数不足的数据块时，该块会被复制直到达到最小副本数，系统中数据块的位置并不是由namenode维护的，而是以块列表形式存储在datanode中。 异常 二、完全分布式搭建及eclipse插件1、完全分布式搭建（必备）(1)环境的准备 Linux (前面已经安装好了) JDK（前面已经安装好了） 准备至少3台机器（通过克隆虚拟机；) (网络配置、JDK搭建、hosts配置，保证节点间能互ping通） 时间同步 (ntpdate time.nist.gov) ssh免秘钥登录 (两两互通免秘钥) （2）完全分布式搭建步骤详情见Hadoop2.X.md文件 2、HDFS命令(0) 命令 ：hdfs dfs(1)上传文件到HDFS： hdfs dfs -put fileName[本地文件名] PATH【hdfs的文件路劲】 上传本地文件install.log到/myhdfs目录下 hdfs dfs -put install.log /myhdfs ​ （文件路径) (上传目录） (2)创建文件夹 hdfs dfs -mkdir[-p] (3)删除文件或文件夹 hdfs dfs -rm -r /myhadoop1.0 123456789hdfs dfs -du -s URI[URI ...] 显示文件(夹)大小. hdfs dfs -cp -f]URI[URI...]&lt;dest&gt; 复制文件(夹)，可以覆盖，可以保留原有权限信息hdfs dfs -count -q&lt;paths&gt;列出文件夹数量、文件数量、内容大小.hdfs dfs -chown -R[:[GROUP]]URI[URI] 修改所有者.hdfs dfs -chmod [-R]&lt;MODE[,MODE]...|OCTALMODE&gt;URI[URI ...] 修改权限. （4）指定block大小 123456789产生100000条数据：for i in `seq 100000`;do echo &quot;hello sxt $i&quot; &gt;&gt; test.txt;done上传文件test.txt到指定的Java22目录下，并指定block块的大小1M：hdfs dfs -D dfs.blocksize=1048576-put test.txt /java22-D ----设置属性 3、eclipse插件安装配置（1）、导入插件 将以下jar包放入eclipse的plugins文件夹中 ​ hadoop-eclipse-plugin-2.6.0.jar 启动eclipse：出现界面如下： 插件应用 （2）配置环境变量Eclipse插件安装完后修改windows下的用户名，然后重启Eclipse： 环境变量 （3）新建Java项目 三、网盘1、代码编写 新建Java项目，导入所需要的jar包 1234567hadoop中的share\hadoop\hdfshadoop中的share\hadoop\hdfs\libhadoop中的share\hadoop\commonhadoop中的share\hadoop\common\lib下的jar包。 block底层—offset偏移量来读取字节数组 123456789101112131415161718private static void blk() throws Exception &#123; Path ifile = new Path(""); FileStatus file = fs.getFileStatus(ifile );// 获取block的location信息HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容 BlockLocation[] blk = fs.getFileBlockLocations(file,0, file.getLen()); for (BlockLocation bb : blk) &#123; System.out.println(bb); &#125; FSDataInputStream input = fs.open(ifile); System.out.println((char)input.readByte()); System.out.println((char)input.readByte()); // 指定从哪个offset的位置偏移量来读 input.seek(1048576); System.out.println((char)input.readByte()); input.seek(1048576); System.out.println((char)input.readByte()); &#125; private static void blk() throws Exception { Path ifile = new Path(“”); FileStatus file = fs.getFileStatus(ifile ); // 获取block的location信息 HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容 BlockLocation[] blk = fs.getFileBlockLocations(file , 0, file.getLen()); for (BlockLocation bb : blk) { System.out.println(bb); } FSDataInputStream input = fs.open(ifile); System.out.println((char)input.readByte()); System.out.println((char)input.readByte()); // 指定从哪个offset的位置偏移量来读 input.seek(1048576); System.out.println((char)input.readByte()); input.seek(1048576); System.out.println((char)input.readByte()); }]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper学习]]></title>
    <url>%2F2019%2F01%2F03%2FZookeeper%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 动物园管理员 推荐图书：《从Paxo到Zookeeper》 Zookeeper1、简介 开源的、分布式应用程序，提供一致性服务，是Haoop （实现HA）和Hbase（和zookeeper是强依赖关系）的重要组件 提供的功能： 配置维护 域名维护 分布式的同步 组服务 Zookeeper→提供通用分布式锁服务，用以协调分布式应用 Keepalived→实现节点健康检查，采用优先级监控，没有协同工作，功能单一，可扩展性差。 2、Zookeep而角色 （一般很少配置Observer，因为用的少，而且配置的节点一般为奇数） Zookeeper需保证高可用和强一致性； ​ 为了支持更多的客户端，需要增加更多Server； ​ Server增多，投票阶段延迟增大，影响性能； ​ 权衡伸缩性和高吞吐率，引入Observer ​ Observer不参与投票； ​ Observers接受客户端的连接，并将写请求转发给leader节点； ​ 加入更多Observer节点，提高伸缩性，同时不影响吞吐率。 3、Zookeeper特点 特点 说明 最终一致性 为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能（与强一致性相对） 可靠性 如果消息被到一台服务器接受，那么它将被所有的服务器接受. 实时性 Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 独立性 各个Client之间互不干预 原子性 更新只能成功或者失败，没有中间状态。 顺序性 所有Server，同一消息发布顺序一致。 4、安装部署：官网： 下载： （1）修改配置文件： 在Zokeeper的安装目录中的conf目录下，将zoo_sample.cfg文件改名为zoo.cfg mv zoo_sample.cfg zoo.cfg 编辑： vim /usr/soft/zookeeper-3.4.13/conf/zoo.cfg 12345678910111213#发送心跳的间隔时间，单位：毫秒tickTime=2000 dataDir=/usr/soft/zookeeper-3.4.13/datadataLogDir=/usr/soft/zookeeper-3.4.13/logsdataLogDir=/Users/zdandljb/zookeeper/dataLog#客户端连接 Zookeeper 服务器的端口，clientPort=2181 #Zookeeper 会监听这个端口，接受客户端的访问请求。initLimit=5syncLimit=2server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888 配置解释: initLimit： 这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5 个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒 syncLimit：这个配置项标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的心跳时间长度，总的时间长度就是 2*2000=4 秒 server.A=B：C：D：其 中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号 (2)创建myid文件（在上面配置文件中配置dataDir 的目录下） 123server1机器的内容为：1，server2机器的内容为：2，server3机器的内容为：3 （3）将zookeeper包发到各个节点上 Paxo算法官网： 1、简介一个基于消息传递的一致性算法，广泛应用于分布式计算中，是到目前为止唯一的分布式一致性算法。 前提： Paxos 有一个前提：没有拜占庭将军问题。就是说 Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。 2、结合故事的对应理解 小岛(Island)——ZK Server Cluster议员(Senator)——ZK Server提议(Proposal)——ZNode Change(Create/Delete/SetData…)提议编号(PID)——Zxid(ZooKeeper Transaction Id)正式法令——所有 ZNode 及其数据 总统——ZK Server Leader zookeeper的节点及工作原理1、工作原理 1.每个Server在内存中存储了一份数据； 2.Zookeeper启动时，将从实例中选举一个leader（Paxos协议） 3.Leader负责处理数据更新等操作 4.一个更新操作成功，当且仅当大多数Server在内存中成功修改数据。 Zookeeper的核心是原子广播，这个机制保证了各个server之间的同步。实现这个机制的协议叫做Zab协议。 Zab协议有两种模式，它们分别是恢复模式和广播模式。 当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数server的完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和server具有相同的系统状态。一旦leader已经和多数的follower进行了状态同步后，他就可以开始广播消息了，即进入广播状态。这时候当一个server加入zookeeper服务中，它会在恢复模式下启动，发现leader，并和leader进行状态同步。待到同步结束，它也参与消息广播。Zookeeper服务一直维持在Broadcast状态，直到leader崩溃了或者leader失去了大部分的followers支持. 广播模式需要保证proposal被按顺序处理，因此zk采用了递增的事务id号(zxid)来保证。所有的提议(proposal)都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch。低32位是个递增计数。 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的server都恢复到一个正确的状态。 2、Znode节点（1）Znode有两种类型，短暂的（ephemeral）和持久的（persistent） Znode的类型在创建时确定并且之后不能再修改。 短暂znode的客户端会话结束时，zookeeper会将该短暂znode删除，短暂znode不可以有子节点 持久znode不依赖于客户端会话，只有当客户端明确要删除该持久znode时才会被删除 （2）Znode有四种形式的目录节点 PERSISTENT、持久的 EPHEMERAL、短暂的 PERSISTENT_SEQUENTIAL、持久且有序的 EPHEMERAL_SEQUENTIAL 短暂且有序的 3、shell操作启动服务端：./zkServer.sh start 停止服务：./zkServer.sh stop 启动客户端：./zkCli.sh -server 127.0.0.1 : 2081 ​ (localhost、node01) ​ （也可连接其他节点） ​ (port默认2081,可省；ip也可省) 退出客户端：quit 操作指南：help 查看根目录：ll / ​ （ll +路径） 获取具体服务内容：get / ​ (get +路径+服务)可查看注册zookeeper服务的节点信息 （如果作为leader的namenode挂了，最新文件会相应的更换数据信息，如果没有nn，那么就没有相应的最新文件，只会有记录上一个阶段数据的文件） 创建服务：create /sun aabbcc ​ (create +路径 + 数据内容) 在其他节点也可启动客户端，创建服务 删除服务：rmr /sun 4、API操作 见代码testzookeeper 总结 Zookeeper 作为 Hadoop 项目中的一个子项目，是Hadoop 集群管理的一个必不可少的模块，它主要用来控制集群中的数据，如它管理 Hadoop 集群中的NameNode，还有 Hbase 中 Master、 Server 之间状态同步等。 ​ Zoopkeeper 提供了一套很好的分布式集群管理的机制，就是它这种基于层次型的目录树的数据结构，并对树中的节点进行有效管理，从而可以设计出多种多样的分布式的数据管理模型。]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习(总)]]></title>
    <url>%2F2019%2F01%2F02%2FNginx%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] Nginx学习：大型网站高并发运行处理一、Nginx使用背景1、背景 1）高并发（海量数据，复杂业务，大量线程）集中访问服务器 2)服务器资源和能力有限 使得服务器宕机，无法提供服务 2、概念理解 1)高并发 海量数据访问，多个进程同时处理不同操作 2）负载均衡 均匀分配请求|数据到不同操作单元上 3）常见互联网架构 客户端层→反向代理层→站点层→服务层→数据层 二、Nginx入门1、了解nginx是什么 nginx是一款轻量级（开发方便，配置简捷）的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器 2、特点 占有内存少，并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。 （使用C语言编写） 3、配置搭建Nginx（Linux系统环境下）资源： Tengine（推荐）：Tengine-2.2.3.tar.gz ​ 其他版本 nginx：nginx/Windows-1.8.1 1）安装依赖 命令：yum -y install gcc openssl-devel pcre-devel zlib-devel 2）解压tar包 命令：tar -zxvf Tengine-2.2.3.tar.gz 3）configure配置：在解压后的源码目录中 两种方案： 命令： ./configure 默认配置/usr/soft/nginx 命令 : ./configure –profix==/usr/soft/nginx 配置在指定路径 4）编译并安装(默认会在/usr/local下生成nginx目录) make &amp;&amp; make install 5）配置nginx服务 在/etc/rc.d/init.d/目录中建立文本文件nginx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid # Source function library.. /etc/rc.d/init.d/functions # Source networking configuration.. /etc/sysconfig/network # Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0 nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot; [ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125; start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125; stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125; restart() &#123; configtest || return $? stop sleep 1 start&#125; reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125; force_reload() &#123; restart&#125; configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125; rh_status() &#123; status $prog&#125; rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125; case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac 6）修改nginx文件的权限 命令 ： chmod +x nginx 7）将文件添加到系统服务中 chkconfig –add nginx 8）验证 chkconfig –list nginx 9）启动|停止服务 service nginx start|stop 4、负载均衡配置1）编辑配置文件： 命令 ： vim /usr/local/nginx/conf/nginx.conf 具体负载配置： 1234567891011121314151617181920212223242526272829303132333435363738（1）（默认）轮询负载http &#123; upstream shsxt&#123; server node01; server node02; &#125; server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125;（2）加权负载upstream shsxt&#123; server sxt1.com weight=3; server sxt2.com;&#125;(3)最少连接负载upstream shsxt&#123; least_conn; servcer node01; server node02;&#125;(4)ip_hash负载（保持回话持久性）upstream shsxt&#123; ip_hash; server node01; server node02;&#125;3. 访问控制（allow 代表允许其访问，deny 禁止其访问）location / &#123; deny 192.168.4.29; allow 192.168.198.0/24; deny all; proxy_pass http://shsxt;&#125; 5、session一致性问题 实现session共享即可解决这个问题 实现工具：memcached 作用：专门管理session的工具 1）在tomcat的lib目录下导入连接memcached所需的jar包 asm-3.2.jar kryo-1.04.jar kryo-serializers-0.11.jar memcached-session-manager-1.7.0.jar memcached-session-manager-tc7-1.8.1.jar minlog-1.2.jar msm-kryo-serializer-1.7.0.jar reflectasm-1.01.jar spymemcached-2.7.3.jar 2）在tomcat的conf目录下编辑context.xml文件 1234567&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.17.9:11211" sticky="true" lockingMode="auto" sessionBackupAsync="false" requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; （注意：配置memcachedNodes属性时，配置其ip和端口，默认为11211，存在多个memecacahed数据库时，用都逗号隔开） 3）验证session：修改index.jsp（在/usr/soft/apache-tomcat-8.5.24/webapps/ROOT/index.jsp），取sessionid看一看 12345SessionID:&lt;%=session.getId()%&gt;&lt;/br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;/br&gt;&lt;h1&gt;tomcat1&lt;/h1&gt; 4)安装： memcached 命令 ：yum install memcached –y 5）启动memcached (IP地址为memcached安装的节点的IP地址) memcached -d -m 128m -p 11211 -l 192.168.198.128 -u root -P /tmp/ 6）在浏览器段访问服务器，默认端口 ： 80 ，对此测验，就会发现sessionID不会改变 三、虚拟主机1、什么是虚拟主机？（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。 （2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。 2、虚拟主有啥特点？（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用 （2）也大大简化了服务器管理的复杂性； 3、虚拟主机有哪些类别？（1）基于域名 1234567891011121314151617181920212223242526http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03; &#125; server &#123; listen 80; //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里 server_name sxt2.com; location / &#123; proxy_pass http://bjsxt; &#125; &#125; server &#123; listen 80; //访问sxt1.com的时候，会把请求导到shsxt的服务器组里 server_name sxt1.com; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; 注意： （1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。 （C:\Windows\System32\drivers\etc\hosts 给IP取别名） 如：192.168.198.130 sxt1.com （2）每台服务器的Tomcat的端口不与配置中的listen一致，那么windows系统浏览器访问时，需要加上Tomcat的端口，（192.168.198.128：8080） ​ 如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80 （2）基于端口 12345678910111213141516171819202122232425http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03 &#125; server &#123; //当访问nginx的80端口时，将请求导给bjsxt组 listen 8080; server_name 192.168.198.128; location / &#123; proxy_pass http://bjsxt; &#125;&#125; server &#123; //当访问nginx的81端口时，将请求导给shsxt组 listen 81; server_name 192.168.198.128; //nginx服务器的IP location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; （3）基于IP ：（不常用） 四、正向代理和反向代理1、正向代理理解： 代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见） 举例： 国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙） 但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口 2、反向代理理解： 代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器 举例： 如我们访问www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。 Nginx就是性能很好的反向代理服务器，用来作负载均衡。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动安装maven坐标依赖]]></title>
    <url>%2F2018%2F12%2F28%2F%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85maven%E5%9D%90%E6%A0%87%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[一、事件原因：学习quartz框架时，在maven项目的pom.xml文件中添加quartz所需要的坐标依赖时，显示jar包不存在。 12345678910111213提示："Dependency 'xxxx‘ not found"， 并且添加的如下两个坐标依赖均报红。 &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 分析： 1、maven项目所需要的jar包均存放在maven的F:\m2\repository(项目所需的jar包仓库)文件夹中 2、在F:\apache-maven-3.5.4\conf的settings.xml文件中有如下设置：（由于使用远程仓库太慢，阿里云给我们提供了一个镜像仓库，便于我们使用，且只包含central仓库中的jar） 1234567&lt;!--文件中原有的配置：远程仓库---&gt;&lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; 1234567&lt;!--文件中自己手动配置：阿里镜像仓库---&gt;&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 3.可是我们在https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧 （如果有小伙伴有别的解决方案，还请指点一二。） 1&lt;!--more--&gt; 二、解决方案1、首先，我们需要从maven Repository中下载我们需要的jar包（需要的两个jar包，下载原理相同） 2、注意我们的maven安装，需要配置环境变量，才能在dos窗口，指令安装jar包 因为我之前查资料时，有小伙伴说，java的环境变量配置也会影响，所以，我在这里也把java的环境变量配置也贴出来 1 1544699916763 1544699989775 JAVA_HOME F:\Java\jdk1.8.0_131（ 根据自己的jdk安装目录） CLASSPATH .;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar MAVEN_HOME F:\apache-maven-3.5.4（ 根据自己maven安装目录） Path（注意配置的时候，一定要和配置home时的变量名一致，如MAVEN_HOME,我配置成了%MVN_HOME%\bin;） %JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%MYSQL_HOME%\bin;%MAVEN_HOME%\bin; 配置这些环境变量，在dos窗口才能使java ，mvn 之类的指令可以用； 否则会出现如下显示。 ‘mvn’ 不是内部或外部命令，也不是可运行的程序 (这就是环境变量没有配成功的结果) 3.安装 C:\Users\Administrator&gt;mvn -v 1544701045091 C:\Users\Administrator&gt;mvn install:install-file -Dfile=F:/apache-maven-3.5.4/m2/quartz-2.3.0.jar（jar包所在路径） -DgroupId=org.quartz-scheduler -DartifactId=quartz -Dversion=2.3.0 -Dpackaging=jar （根据下面所示的配置groupId、artifactId、version） 12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 1544702128551 如图所示，安装成功。 1544702179172]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（二）]]></title>
    <url>%2F2018%2F12%2F28%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​​ [TOC] 一、磁盘指令 查看硬盘信息 命令：df （默认大小以kb显示） df -k（以kb为单位） df -m（ 以mb为单位） df –h （易于阅读） 查看文件/目录的大小 命令：du filename|foldername （默认单位为kb）-k kb单位 -m mb单位 -a 所有文件和目录 -h 更易于阅读 ​ --max-depth=0 目录深度 二、网络指令 查看网络配置信息 命令:ifconfig 测试与目标主机的连通性 命令：ping remote_ip ctrl + c :结束ping进程 显示各种网络相关信息 命令：netstat 查看端口号（是否被占用） (1)、lsof -i:端口号 （需要先安装lsof） (2)、netstat -tunlp|grep 端口号 测试远程主机的网络端口 命令： telnet ip port （需要先安装telnet） 测试成功后，按ctrl + ] 键，然后弹出telnet&gt;时，再按q退出 http请求模拟 curl -X get www.baidu.com 模拟请求百度 三、系统管理指令 用户操作 12345678910 操作 命令创建用户 useradd|adduser username修改密码 passwd username删除用户 userdel –r username修改用户（已下线）： 修改用户名: usermod –l new_name oldname 锁定账户: usermod –L username 解除账户： usermod –U username查看当前登录用户 仅root 用户：whoami | cat /etc/shadow 普通用户：cat /etc/pqsswd 用户组操作 12345 操作 命令 创建用户组 groupadd groupname删除用户组 groupdel groupname修改用户组 groupmod –n new_name old_name查看用户组 groups （查看的是当前用户所在的用户组） 用户+用户组 12345 操作 命令 修改用户的主组 usermod –g groupname username给用户追加附加组 usermod –G groupname username查看用户组中用户数 cat /etc/group注意：创建用户时，系统默认会创建一个和用户名字一样的主组 系统权限 12345678910 操作 命令 查看/usr下所有权限 ll /usr 权限类别 r（读取：4） w（写入：2） x（执行：1） 三个为一组，无权限用 —代替 UGO模型 U（User） G(Group) O(其他)权限修改 修改所有者：chown username file|folder (递归)修改所有者和所属组： chown -r username：groupname file|folder 修改所属组：chgrp groupname file|folder 修改权限：chmod ugo+rwx file|folder 四、系统配置指令 1.修改主机名 123 编辑文件： 命令： vim /etc/sysconfig/network 文件内容： HOSTNAME=node00（重启生效)reboot 2.DNS配置 12编辑文件： 命令：vim /etc/resolv.conf文件内容： nameserver 192.168.198.0 3.sudo权限配置 1234567891011121314151617 操作 命令编辑权限配置文件： vim /etc/sudoers格式： 授权用户 主机=[(切换到哪些用户或用户组)] [是否需要密码验证] 路径/命令举例： test ALL=(root) /usr/bin/yum,/sbin/service解释： test用户就可以用yum和servie命令， 但是，使用时需要在前面加上sudo再敲命令。 第一次使用需要输入用户密码,且每个十五分钟需要一次密码验证修改： test ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service这样就不需要密码了将权限赋予某个组，%+组名%group ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service列出用户所有的sudo权限 sudo –l 4.系统时间 12345678操作 命令查看系统时间 date ---查看当前时间详情 cal ---查看当前月日历 cal 2018 ---查看2018年完整日历 cal 12 2018 ---查看指定年月的日历 更新系统时间（推荐） yum install ntpdate –y ---安装ntp服务 ntpdate cn.ntp.org.cn ---到域名为cn.ntp.org.cn的时间服务器上同步时间 5.关于hosts配置 相当于给IP地址其别名，可以通过别名访问 路径： Windows系统 C:/Windows/System32/drivers/etc/hosts 文件 Linux系统 /etc/hosts文件：vim +路径 统一 编辑格式 IP地址 别名：192.168.198.128 node00 6.关于hostname配置 相当于给对应的虚拟机器起别名 Linux系统： vi /etc/sysconfig/network 编辑内容： HOSTNAME=node01 五、重定向与管道符 输出重定向 输出重定向到一个文件或设备： &gt; 覆盖原来的文件 &gt;&gt; 追加原来的文件 举例： ls &gt; log — 在log文件中列出所有项，并覆盖原文件 echo “hello”&gt;&gt;log —将hello追加到log文件中 输入重定向 &lt; 输入重定向到一个程序 举例：cat &lt; log —将log文件作为cat命令的输入，查看log文件的内容 标准 输出 重定向 1 &gt; 或 &gt; 含义： 输出重定向时，只用正确的输出才会重定向到指的文件中 错误的则会直接打印到屏幕上 错误 输出 重定向 2 &gt; 含义： 错误的输出会重定向到指定文件里，正确的日志则直接打印到屏幕上。 结合 使用 2&gt;&amp;1 含义： 将无论是正确的输出还是错误的输出都重定向到指定文件 管道 **\ ** 含义： 把前一个输出当做后一个输入 grep 通过正则搜索文本，并将匹配的行打印出来 netstat -anp \ grep 22 把netstat –anp 命令的输出 当做是grep 命令的输入 命令 执行 控制 &amp;&amp; 前一个命令执行成功才会执行后一个命令 **\ \ ** 前一个命令执行失败才会执行后一个命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（一）]]></title>
    <url>%2F2018%2F12%2F27%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、命令指南（manual）：man 安装：yum install man –y （-y 表示获得允许，无需确认） 查看ls命令指南： man ls 二、目录命令切换目录：cd + 目录的路径 查看当前目录所在的完整路径：pwd 新建目录：mkdir +目录名字 查看当前目录所用有的子目录和文件：ls ，ll等价于 ls –l ​ 查看目录下的所有东西（包括隐藏文件）： ls –al 等价于 ll -a​拷贝目录或文件：cp –r install.log install2.log 删除目录或文件：rm -r install.log (rmdir只能删除空目录) 移动目录或文件：mv + 目录/文件名字 + 其他路径 ​ 将test目录移动到 根目录/ 下 : mv test / （如果移动到当前目录，用另外一个名称，则可以实现重命名的效果） 更改文件或目录的名字：mv + 旧目录名字 + 新目录名 ( -r 用于递归的拷贝，删除，移动目录) 三、文件命令1、一般文件操作新建文件：touch install.log​ (vim install.log 编辑文件，如果文件不存在，就会新建一个对应的文件，并进入文件的编辑模式，如果按 :wq 会保存文件并退出，如果按 :q 则不保存退出)​查看文件内容：cat +（文件名）（一次性显示整个文件的内容，文件内容过多时用户体验不好） 一次命令显示一屏文本： 1234567 more +（文件名） 按键 效果 Space 显示下一屏文本内容B 显示上一屏文本内容Enter 显示下一行文本内容Q 退出查看 less+（文件名） 按键 效果 h 显示帮助界面 u 向后滚动半页 d 向前翻半页 e | Enter 向后翻一行文本 space 滚动一页 b 向后翻一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 上下键，向上一行，向下一行 从头打印文件内容：​ head -10 +（文件名） 打印文件1到10行 从尾部打印文件内容​ tail -10 +（文件名）打印文件最后10行 tail -f (文件名) 常用于查看文件内容的更新变化 查找文件或目录​ find +（路径名） –name +（文件名）​ 举例：find / -name profile​ 在/(根目录)目录下查找 名字为profile的文件或目录 ​ 也可利用正则：​ 举例： find /etc -name pro*​ 在/etc目录下查找以pro开头的文件或目录 路径越精确，查找的范围越小，速度越快 i 2、文件编辑vi（1） vi 进入编辑模式 —–&gt;按i 进入插入模式 ——-&gt; 按Esc 退出编辑模式 1234vi filename :打开或新建文件，并将光标置于第一行首 vi +n filename ：打开文件，并将光标置于第n行首 vi + filename ：打开文件，并将光标置于最后一行首 vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的字符串所在的行首 filename 为文件名 （2）在文件vi（文件编辑）模式下 命令行模式 123456789:w 保存:q 退出:wq 保存并退出:q! 强制退出:set nu |ctrl+g 显示文本行数:set nonu 去除显示的行数:s/p1/p2/g 将当前行中所有p1均用p2替代 :n1,n2s/p1/p2/g 将第n1至n2行中所有p1均用p2替代 :g/p1/s//p2/g 将文件中所有p1均用p2替换 一般模式 12345678910111213141516171819202122232425262728293031按键：yy 复制光标所在行(常用) nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) p|P p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)G 光标移至第最后一行nG 光标移动至第N行行首n+ 光标下移n行 n- 光标上移n行 H 光标移至屏幕顶行 M 光标移至屏幕中间行 L 光标移至屏幕最后行 dd 删除所在行 x或X 删除一个字符，x删除光标后的，而X删除光标前的 u 撤销(常用)删除第N行到第M行：N,Md：,$-1d 删除当前光标到到数第一行数据按键： i: 在当前光标所在字符的前面，转为输入模式； a: 在当前光标所在字符的后面，转为输入模式； o: 在当前光标所在行的下方，新建一行，并转为输入模式； I：在当前光标所在行的行首，转换为输入模式 A：在当前光标所在行的行尾，转换为输入模式 O：在当前光标所在行的上方，新建一行，并转为输入模式；---逐字符移动：h: 左 l: 右j: 下 k: 上 vim 安装：yum install vim -y 用vim 打开/etc/profile 文件， 特点：编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强 ，其他均与vi相同 3、文件上传下载 安装上传下载命令：yum install lrzsz -y 上传文件：（windows—&gt;linux） 命令 ：rz 弹出windows上传文件窗口 下载文件：(linux—&gt;windows) 注意：sz命令只能下载文件，不能下载目录，推荐将目录压缩成tar包或使用工具软件：Winscp【Xftp】 命令：sz （文件名） 弹出windows下载窗口,下载文件到指定文件目录 4、文件传输(1)、本地→远程 文件 ： scp local_file remote_username@remote_ip:remote_folder 目录 ： scp -r local_folder remote_username@remote_ip:remote_folder (2）、远程→本地 文件 ： scp remote_username@remote_ip:remote_file local_folder 目录 ： scp remote_username@remote_ip:remote_folder local_folder]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（三）]]></title>
    <url>%2F2018%2F12%2F27%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、服务操作 列出 所有 服务 chkconfig 服务 操作 service 服务名 start\ stop\ status\ restart 永久关闭\ 打开 （启动后生效） chkconfig iptables on\ off 添加 服务 1) 编辑脚本：vim myservice.sh 编辑内容： （在最前面加一下两句） #chkconfig: 2345 80 90 #description:auto_run (自己的服务脚本：开机时同步时间） result=’ntpdate cn.ntp.org.cn’ 退出编辑并保存：按esc键 按 ：wq 在ntpdate.log文件中输出打印：echo $result &gt; /usr/ntpdate.log 2) 修改权限，使其拥有可执行权限: chmod 700 myservice.sh 3) 将脚本拷贝到/etc/init.d目录： 4) 加入服务：chkconfig –add myservice.sh 5) 重启服务器，验证服务是否添加成功：date 6）/usr目录下产生ntpdate.log 删除 服务 chkconfig –del name 更改 服务初 执行 等级 chkconfig –level 2345 服务名 off\ on chkconfig 服务名 on\ of f 二、定时调度 编辑定时任务 crontab –e 格式：minute hour day month dayofweek command 举例 * echo “hello” 每分钟打印“hello” 时间一到，执行操作命令后 会出现：You have new mail in /var/spool/mail/root 查看任务执行情况 vim /var/spool/mail/root 查看所有用户的定时任务 ll /var/spool/cron 查看当前用户的定时任务 contab –l 注意 “”代表任意的数字, “/”代表”每隔多久”, “-”代表从某个数字到某个数字, “,”分开几个离散的数字 如： 30-40 12 echo “hello” ——–每天12点30分至40分期间，每分钟执行一次命令 30,40 ——–每天12点30分和12点40分 0/5 ——–每天的12点整至12点55分期间，每隔5分钟执行一次命令 三、进程操作 查看 进程 ps -aux -a 列出所有 -u 列出用户 -x 详细列出，如cpu、内存等 -e 显示所有进程 -f 全格式 ps - ef \ grep ssh 查看所有进程里CMD是ssh 的进程信息 ps -aux –sort –pcpu 根据 CPU 使用来升序排序 使程序 后台运行 只需要在命令后添加 &amp; 符号 echo “hello” &amp; jobs –l –列出当前连接的所有后台进程（jobs仅适用于当前端） ps -ef \ grep 进程名 —-（推荐）列出后台进程 杀死进程 （强制）kill -9 pid —-pid为进程号 四、其他命令 wget 1） 安装：yum install wget –y 2） 用法：wget [option] 网址 -O 指定下载保存的路径 3） 也可用于做爬虫 yum 1） 备份原镜像： cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOSBase.repo.backup 2） 下载新镜像： wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 3） 查看文件内容：vim /etc/yum.repos.d/CentOS-Base.repo 4） 生成缓存：yum makecache rpm 1） 安装 rpm –ivh rpm包 2） 查找已安装的rpm包：rpm –q ntp 3） 卸载：rpm –e ntp-4.2.6p5-10.el6.centos.2.x86_64（全名） tar 1） 解压：tar -zvxf xxxx.tar.gz 2） 压缩：tar -zcf 压缩包命名 压缩目标 3） 例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61 4） -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加 -x 解压 -c 压缩 -f 目标文件，压缩文件新命名或解压文件名 -v 解压缩过程信息打印 zip 1）安装zip：yum install zip –y 2）压缩命令：zip -r 包名 目标目录 3）安装 ：unzip,yum install unzip –y 4）解压 ：unzip filename 五、安装部署 JDK 部署 1) 解压: tar -zxf jdk-7u80-linux-x64.tar.gz 2) 配置环境变量 编辑配置文件：vim /etc/profile 编辑内容 ： JAVA_HOME= /usr/soft/jdk1.7.0_75 PATH=$PATH:$JAVA_HOME/bin 3) 重新加载环境变量：source /etc/profile 4) 验证: java -version mysql部署 yum安装 mysql 1) yum install mysql-server -y 2) yum install mysql-devel -y 3) service mysqld start 4) mysql -uroot -p 5) mysqladmin -u root password 123456 六、免密登录| 法一 | 1） 生成公钥和密钥：ssh-keygen -t rsa ，并且回车3次 （在用户的根目录生成一个 “.ssh”的文件夹） 2） 查看公钥和私钥：ll ~/.ssh （目录中会有以下几个文件） authorized_keys:存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥 id_rsa : 生成的私钥文件 id_rsa.pub ： 生成的公钥文件 know_hosts : 已知的主机公钥清单 如果希望ssh公钥生效需满足至少下面两个条件： 1&gt; .ssh目录的权限必须是700 * 2&gt; .ssh/authorized_keys文件权限必须是600 3） 将A的.ssh目录下的公钥追加拷贝到B的authorized_keys文件里 scp -p ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:/root/.ssh/authorized_keys 4) 验证：将文件远程拷贝到远程主机上，看是否需要密码 | ——————————————————— :———————————————————– （此方法有待考究）法二：通过Ansible实现 批量 免密 1）、 将需要做免密操作的机器hosts添加到/etc/ansible/hosts下： [&nbsp;Avoid close] 192.168.91.132 192.168.91.133 192.168.91.134 2）、 执行命令进行免密操作 ：​ 1ansible &lt;groupname&gt; -m authorized_key -a &quot;user=root key=&apos;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;) &#125;&#125;&apos;&quot; -k ​ |]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx入门学习（二）]]></title>
    <url>%2F2018%2F12%2F20%2FNginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%9B%9E%E5%90%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2018年12月20日 周四 阴 一、虚拟主机1、什么是虚拟主机？（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。 （2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。 2、虚拟主有啥特点？（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用 （2）也大大简化了服务器管理的复杂性； 3、虚拟主机有哪些类别？（1）基于域名 1234567891011121314151617181920212223242526http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03; &#125; server &#123; listen 80; //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里 server_name sxt2.com; location / &#123; proxy_pass http://bjsxt; &#125; &#125; server &#123; listen 80; //访问sxt1.com的时候，会把请求导到shsxt的服务器组里 server_name sxt1.com; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; 注意： （1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。 （C:\Windows\System32\drivers\etc\hosts 给IP取别名） 如：192.168.198.130 sxt1.com （2）每台服务器的Tomcat的端口不与配置的listen一致，那么windows系统浏览器访问时，需要加上TOmcat的端口，（192.168.198.128：8080） ​ 如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80 （2）基于端口 12345678910111213141516171819202122232425http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03 &#125; server &#123; //当访问nginx的80端口时，将请求导给bjsxt组 listen 8080; server_name 192.168.198.128; location / &#123; proxy_pass http://bjsxt; &#125;&#125; server &#123; //当访问nginx的81端口时，将请求导给shsxt组 listen 81; server_name 192.168.198.128; //nginx服务器的IP location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; （3）基于IP ：（不常用） 二、正向代理和反向代理1、正向代理理解： 代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见） 举例： 国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙） 但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口 2、反向代理理解： 代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器 举例： 如我们访问www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。 Nginx就是性能很好的反向代理服务器，用来作负载均衡。 三、Nginx的session一致性问题1、背景：http协议是无状态的，多次访问如果是不同服务器响应请求，就会出现上次访问留下的session或cookie失效。这就引发了session共享的问题。 2、Session一致性解决方案（1）–session复制 tomcat 本身带有复制session的功能。 （2）-共享session 需要专门管理session的软件， memcached 缓存服务，可以和tomcat整合，帮助tomcat共享管理session。 3、安装memcachedmemcached （同redis一样）是基于内存的数据库 1、安装 yum –y install memcached 可以用telnet localhost 11211 启动： memcached -d -m 128m -p 11211 -l 192.168.235.113 -u root -P /tmp/ 2.web服务器连接memcached的jar包拷贝到tomcat的lib目录下 访问Tomcat服务器期间产生的session通过相关jar包，才能写入到memcached数据库中 memcached-session-manager-1.7.0.jar memcached-session-manager-tc7-1.8.1.jar 3.配置tomcat的conf目录下的context.xml 1234567&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.198.128:11211" sticky="true" lockingMode="auto" sessionBackupAsync="false" requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; 配置memcachedNodes属性， 配置memcached数据库的ip和端口，默认11211，多个的话用逗号隔开. 目的是为了让tomcat服务器从memcached缓存里面拿session或者是放session 将配置完成的context.xml发送到其他虚拟机器上 scp -r context.xml root@node01:pwd 或 scp -r context.xml node01:pwd 或 scp -r context.xml root@192.168.198.130:pwd 4.修改tomcat目录中webapps/ROOT下的 index.jsp，取sessionid看一看 12345678&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;html lang="en"&gt;SessionID:&lt;%=session.getId()%&gt;&lt;/br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;/br&gt;&lt;h1&gt;tomcat1&lt;/h1&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>

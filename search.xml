<?xml version="1.0" encoding="utf-8"?>
<search>
    
    
    <entry>
        <title></title>
        <url>http://sungithup.github.io/2019/01/23/Nginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%9B%9E%E5%90%88%EF%BC%89/</url>
        <content type="html"><![CDATA[<h1 id="Nginx入门学习（第二回合）"><a href="#Nginx入门学习（第二回合）" class="headerlink" title="Nginx入门学习（第二回合）"></a>Nginx入门学习（第二回合）</h1><p><code>2018年12月20日  周四  阴</code></p>
<h2 id="一、虚拟主机"><a href="#一、虚拟主机" class="headerlink" title="一、虚拟主机"></a>一、虚拟主机</h2><h3 id="1、什么是虚拟主机？"><a href="#1、什么是虚拟主机？" class="headerlink" title="1、什么是虚拟主机？"></a>1、什么是虚拟主机？</h3><p>（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。</p>
<p>（2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。</p>
<h3 id="2、虚拟主有啥特点？"><a href="#2、虚拟主有啥特点？" class="headerlink" title="2、虚拟主有啥特点？"></a>2、虚拟主有啥特点？</h3><p>（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用</p>
<p>（2）也大大简化了服务器管理的复杂性；</p>
<h3 id="3、虚拟主机有哪些类别？"><a href="#3、虚拟主机有哪些类别？" class="headerlink" title="3、虚拟主机有哪些类别？"></a>3、虚拟主机有哪些类别？</h3><p>（1）基于域名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">     &#125; </span><br><span class="line">	upstream bjsxt&#123; </span><br><span class="line">        server node03; </span><br><span class="line">     &#125; </span><br><span class="line">     </span><br><span class="line">     server &#123;    </span><br><span class="line">            listen 80; </span><br><span class="line">            //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里</span><br><span class="line">            server_name  sxt2.com;</span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://bjsxt;</span><br><span class="line">            &#125;</span><br><span class="line">      &#125; </span><br><span class="line">      server &#123; </span><br><span class="line">            listen 80; </span><br><span class="line">           //访问sxt1.com的时候，会把请求导到shsxt的服务器组里</span><br><span class="line">            server_name  sxt1.com; </span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://shsxt;</span><br><span class="line">            &#125;</span><br><span class="line">      &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
</blockquote>
<blockquote>
<p>（1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。</p>
<p>（C:\Windows\System32\drivers\etc\hosts     给IP取别名）</p>
<p>如：192.168.198.130   sxt1.com</p>
</blockquote>
<blockquote>
<p>（2）每台服务器的Tomcat的端口不与配置的listen一致，那么windows系统浏览器访问时，需要加上TOmcat的端口，（192.168.198.128：8080）</p>
<p>​         如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80</p>
</blockquote>
<p>（2）基于端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">     &#125; </span><br><span class="line">	upstream bjsxt&#123; </span><br><span class="line">        server node03</span><br><span class="line">    &#125; </span><br><span class="line"> server &#123; </span><br><span class="line">       //当访问nginx的80端口时，将请求导给bjsxt组</span><br><span class="line">        listen 8080; </span><br><span class="line">        server_name 192.168.198.128;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://bjsxt;</span><br><span class="line">        &#125;</span><br><span class="line">&#125; </span><br><span class="line">  server &#123; </span><br><span class="line">           //当访问nginx的81端口时，将请求导给shsxt组</span><br><span class="line">            listen 81; </span><br><span class="line">            server_name 192.168.198.128;  //nginx服务器的IP</span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://shsxt;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（3）基于IP  ：（不常用）</p>
<h2 id="二、正向代理和反向代理"><a href="#二、正向代理和反向代理" class="headerlink" title="二、正向代理和反向代理"></a>二、正向代理和反向代理</h2><h3 id="1、正向代理"><a href="#1、正向代理" class="headerlink" title="1、正向代理"></a>1、正向代理</h3><p>理解：</p>
<blockquote>
<p>代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见）</p>
</blockquote>
<p>举例：</p>
<blockquote>
<p>国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙）</p>
<p>但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口</p>
</blockquote>
<h3 id="2、反向代理"><a href="#2、反向代理" class="headerlink" title="2、反向代理"></a>2、反向代理</h3><p>理解：</p>
<blockquote>
<p>代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器</p>
</blockquote>
<p>举例：</p>
<blockquote>
<p>如我们访问<a href="http://www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。" target="_blank" rel="noopener">www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。</a></p>
</blockquote>
<p>Nginx就是性能很好的反向代理服务器，用来作负载均衡。</p>
<h2 id="三、Nginx的session一致性问题"><a href="#三、Nginx的session一致性问题" class="headerlink" title="三、Nginx的session一致性问题"></a>三、Nginx的session一致性问题</h2><h4 id="1、背景："><a href="#1、背景：" class="headerlink" title="1、背景："></a>1、背景：</h4><p>http协议是无状态的，多次访问如果是不同服务器响应请求，就会出现上次访问留下的session或cookie失效。这就引发了session共享的问题。</p>
<h4 id="2、Session一致性解决方案"><a href="#2、Session一致性解决方案" class="headerlink" title="2、Session一致性解决方案"></a>2、Session一致性解决方案</h4><p>（1）–session复制<br>   tomcat 本身带有复制session的功能。</p>
<p>（2）-共享session</p>
<p>  需要专门管理session的软件，<br>   memcached 缓存服务，可以和tomcat整合，帮助tomcat共享管理session。</p>
<h4 id="3、安装memcached"><a href="#3、安装memcached" class="headerlink" title="3、安装memcached"></a>3、安装memcached</h4><p>memcached （同redis一样）是基于内存的数据库</p>
<p>1、安装</p>
<blockquote>
<p>  yum –y install memcached</p>
</blockquote>
<blockquote>
<p>   可以用telnet localhost 11211</p>
</blockquote>
<blockquote>
<p>   启动： memcached -d -m 128m -p 11211 -l 192.168.235.113 -u root -P /tmp/</p>
</blockquote>
<p>2.web服务器连接memcached的jar包拷贝到tomcat的lib目录下</p>
<p><code>访问Tomcat服务器期间产生的session通过相关jar包，才能写入到memcached数据库中</code></p>
<blockquote>
<p>memcached-session-manager-1.7.0.jar</p>
<p>memcached-session-manager-tc7-1.8.1.jar</p>
</blockquote>
<p>3.配置tomcat的conf目录下的context.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Manager</span> <span class="attr">className</span>=<span class="string">"de.javakaffee.web.msm.MemcachedBackupSessionManager"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">memcachedNodes</span>=<span class="string">"n1:192.168.198.128:11211"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">sticky</span>=<span class="string">"true"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">lockingMode</span>=<span class="string">"auto"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">sessionBackupAsync</span>=<span class="string">"false"</span></span></span><br><span class="line"><span class="tag">   <span class="attr">requestUriIgnorePattern</span>=<span class="string">".*\.(ico|png|gif|jpg|css|js)$"</span></span></span><br><span class="line"><span class="tag"><span class="attr">sessionBackupTimeout</span>=<span class="string">"1000"</span> <span class="attr">transcoderFactoryClass</span>=<span class="string">"de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory"</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<p>配置memcachedNodes属性，</p>
<blockquote>
<p>配置memcached数据库的ip和端口，默认11211，多个的话用逗号隔开.</p>
<p>目的是为了让tomcat服务器从memcached缓存里面拿session或者是放session</p>
</blockquote>
<p>将配置完成的context.xml发送到其他虚拟机器上</p>
<blockquote>
<p>scp -r context.xml root@node01:<code>pwd</code></p>
<p>或</p>
<p>scp -r context.xml node01:<code>pwd</code></p>
<p>或</p>
<p>scp -r context.xml <a href="mailto:root@192.168.198.130" target="_blank" rel="noopener">root@192.168.198.130</a>:<code>pwd</code></p>
</blockquote>
<p>4.修改tomcat目录中webapps/ROOT下的 index.jsp，取sessionid看一看</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;%@ page language=<span class="string">"java"</span> contentType=<span class="string">"text/html; charset=UTF-8"</span>  pageEncoding=<span class="string">"UTF-8"</span>%&gt;</span><br><span class="line">&lt;html lang=<span class="string">"en"</span>&gt;</span><br><span class="line">SessionID:&lt;%=session.getId()%&gt;</span><br><span class="line">&lt;/br&gt;</span><br><span class="line">SessionIP:&lt;%=request.getServerName()%&gt;</span><br><span class="line">&lt;/br&gt;</span><br><span class="line">&lt;h1&gt;tomcat1&lt;/h1&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
]]></content>
        
        
    </entry>
    
    <entry>
        <title></title>
        <url>http://sungithup.github.io/2019/01/23/Linux%E7%B3%BB%E7%BB%9FCentOS%206%E5%AE%89%E8%A3%85/</url>
        <content type="html"><![CDATA[<h1 id="Linux系统CentOS-6"><a href="#Linux系统CentOS-6" class="headerlink" title="Linux系统CentOS 6"></a>Linux系统<strong>CentOS 6</strong></h1><h1 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h1><p> 资源准备：</p>
<p>CentOS-6.6-x86_64-minimal.iso（简易迷你版） ：</p>
<p>CentOS-6.7-x86_64-bin-DVD1.iso（完整版）：</p>
<h2 id="1、点击新建虚拟机"><a href="#1、点击新建虚拟机" class="headerlink" title="1、点击新建虚拟机"></a>1、点击新建虚拟机</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps46F1.tmp.jpg" alt="img"> </p>
<h2 id="2、选择典型。（专业人士使用的话建议选择高级）"><a href="#2、选择典型。（专业人士使用的话建议选择高级）" class="headerlink" title="2、选择典型。（专业人士使用的话建议选择高级）"></a>2、选择典型。（专业人士使用的话建议选择高级）</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps46F2.tmp.jpg" alt="img"> </p>
<h2 id="3-选择稍后安装操作系统"><a href="#3-选择稍后安装操作系统" class="headerlink" title="3. 选择稍后安装操作系统"></a>3. 选择稍后安装操作系统</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps46F3.tmp.jpg" alt="img"> </p>
<h2 id="4-选择操作系统类型，选择linux-centos-64位"><a href="#4-选择操作系统类型，选择linux-centos-64位" class="headerlink" title="4. 选择操作系统类型，选择linux,centos 64位"></a>4. 选择操作系统类型，选择linux,centos 64位</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps46F4.tmp.jpg" alt="img"> </p>
<h2 id="5-选择虚拟机安装位置和名称。"><a href="#5-选择虚拟机安装位置和名称。" class="headerlink" title="5. 选择虚拟机安装位置和名称。"></a>5. 选择虚拟机安装位置和名称。</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps46F5.tmp.jpg" alt="img"> </p>
<h2 id="6-指定磁盘容量，默认20GB。"><a href="#6-指定磁盘容量，默认20GB。" class="headerlink" title="6. 指定磁盘容量，默认20GB。"></a>6. 指定磁盘容量，默认20GB。</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4706.tmp.jpg" alt="img"> </p>
<h2 id="7-选择自定义硬件"><a href="#7-选择自定义硬件" class="headerlink" title="7. 选择自定义硬件"></a>7. 选择自定义硬件</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4707.tmp.jpg" alt="img"> </p>
<h2 id="8-点击CD-DVD-然后选择操作系统的ISO映像文件，选择完后，点击关闭。"><a href="#8-点击CD-DVD-然后选择操作系统的ISO映像文件，选择完后，点击关闭。" class="headerlink" title="8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。"></a>8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4708.tmp.jpg" alt="img">  </p>
<h2 id="9-点击完成。"><a href="#9-点击完成。" class="headerlink" title="9.点击完成。"></a>9.点击完成。</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4709.tmp.jpg" alt="img"> </p>
<h1 id="二、配置虚拟机"><a href="#二、配置虚拟机" class="headerlink" title="二、配置虚拟机"></a>二、<strong>配置虚拟机</strong></h1><h2 id="1-启动虚拟机。"><a href="#1-启动虚拟机。" class="headerlink" title="1. 启动虚拟机。"></a>1. 启动虚拟机。<img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps470A.tmp.jpg" alt="img"></h2><p>注意：如果启动虚拟机时，发生以下问题，说明是你的电脑默认未开启虚拟化技术。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps470B.tmp.png" alt="img"> </p>
<p>此时你应该把机器重启并进入bios界面（不同的机器进入bios界面的快捷键不同，一般为F1~F10键中的某个键，如果都不行，就得自己百度一下你的机器型号进入bios界面的快捷方式）。</p>
<p>​    当进入bios界面后，把虚拟机化选项（virtualization technology）打开,通过回车键，把disabled改成enabled,然后保存并重启机器。我这边是按F10，不同机器可能不一样，看右下角的提示信息。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps470C.tmp.jpg" alt="img"> </p>
<h2 id="2-Test-Media-如果不需要的话，点Skip"><a href="#2-Test-Media-如果不需要的话，点Skip" class="headerlink" title="2.Test Media, 如果不需要的话，点Skip"></a>2.Test Media, 如果不需要的话，点Skip</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps471C.tmp.jpg" alt="img"> </p>
<h2 id="3、单击Next按钮继续"><a href="#3、单击Next按钮继续" class="headerlink" title="3、单击Next按钮继续"></a>3、单击Next按钮继续</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps471D.tmp.jpg" alt="img"> </p>
<h2 id="4-选择安装期间显示的语言"><a href="#4-选择安装期间显示的语言" class="headerlink" title="4. 选择安装期间显示的语言"></a>4. 选择安装期间显示的语言</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps471E.tmp.jpg" alt="img"> </p>
<h2 id="5、选择键盘语言"><a href="#5、选择键盘语言" class="headerlink" title="5、选择键盘语言"></a>5、选择键盘语言</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps471F.tmp.jpg" alt="img"> </p>
<h2 id="6、选择存储介质的类别。"><a href="#6、选择存储介质的类别。" class="headerlink" title="6、选择存储介质的类别。"></a>6、选择存储介质的类别。</h2><p>如果是将CentOS 6安装到本地硬盘上，选择 Basic Storage Devices，如果安装到网络存储介质如SANs上，选择 Specialized Storage Devices</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4720.tmp.jpg" alt="img"> </p>
<h2 id="7-选择-yes-discard-any-data"><a href="#7-选择-yes-discard-any-data" class="headerlink" title="7.选择 yes,discard any data"></a>7.选择 yes,discard any data</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4721.tmp.jpg" alt="img"> </p>
<h2 id="8-设定主机名称（hostname）"><a href="#8-设定主机名称（hostname）" class="headerlink" title="8. 设定主机名称（hostname）"></a>8. 设定主机名称（hostname）</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4722.tmp.jpg" alt="img"> </p>
<h2 id="9-设定时区，选择-Asia-Shanghai"><a href="#9-设定时区，选择-Asia-Shanghai" class="headerlink" title="9. 设定时区，选择 Asia/Shanghai"></a>9. 设定时区，选择 Asia/Shanghai</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4723.tmp.jpg" alt="img"> </p>
<h2 id="10-设定root帐户的密码"><a href="#10-设定root帐户的密码" class="headerlink" title="10. 设定root帐户的密码"></a>10. 设定root帐户的密码</h2><p>尽量使用较复杂的密码安装（根据实际情况，密码简单时，会有提示，点击user anyway 就行）</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4734.tmp.jpg" alt="img"> </p>
<h2 id="11-选择安装类型，这里我选择-“Use-All-Space”"><a href="#11-选择安装类型，这里我选择-“Use-All-Space”" class="headerlink" title="11. 选择安装类型，这里我选择 “Use All Space”"></a>11. 选择安装类型，这里我选择 “Use All Space”</h2><p> <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4735.tmp.jpg" alt="img"></p>
<h2 id="12-选择-“Write-changes-to-disk”，将分区数据写入硬盘"><a href="#12-选择-“Write-changes-to-disk”，将分区数据写入硬盘" class="headerlink" title="12. 选择 “Write changes to disk”，将分区数据写入硬盘"></a>12. 选择 “Write changes to disk”，将分区数据写入硬盘</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4736.tmp.jpg" alt="img"> </p>
<h2 id="13-开始安装，此时只需等待即可"><a href="#13-开始安装，此时只需等待即可" class="headerlink" title="13. 开始安装，此时只需等待即可"></a>13. 开始安装，此时只需等待即可</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4737.tmp.jpg" alt="img"> </p>
<h2 id="14-安装完结后，点击Reboot按钮"><a href="#14-安装完结后，点击Reboot按钮" class="headerlink" title="14. 安装完结后，点击Reboot按钮"></a>14. 安装完结后，点击Reboot按钮</h2><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wps4738.tmp.jpg" alt="img"> </p>
]]></content>
        
        
    </entry>
    
    <entry>
        <title></title>
        <url>http://sungithup.github.io/2019/01/23/Linux%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE+%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0(%E7%AC%AC%E4%B8%80%E5%9B%9E%E5%90%88)/</url>
        <content type="html"><![CDATA[<h1 id="Linux网络配置-常用命令学习"><a href="#Linux网络配置-常用命令学习" class="headerlink" title="Linux网络配置+常用命令学习"></a>Linux网络配置+常用命令学习</h1><p>[TOC]</p>
<h2 id="一、-Linux概述"><a href="#一、-Linux概述" class="headerlink" title="一、 Linux概述"></a>一、 <strong>Linux</strong>概述</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1. 简介"></a><strong>1.</strong>1. 简介</h3><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB68.tmp.jpg" alt="img">Linux是一个自由的，免费的，源码开放的操作系统。也是开源软件中最著名的例子。其最主要的目的就是为了建立不受任何商品化软件版权制约的，全世界都能使用的类Unix兼容产品.而我们将服务器部署在Linux将会更加的稳定、安全、高效以及出色的性能这时windows无法比的。</p>
<h3 id="1-2-Linux作者"><a href="#1-2-Linux作者" class="headerlink" title="1.2.Linux作者"></a><strong>1.2.L</strong>inux作者</h3><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB69.tmp.png" alt="img"> </p>
<p>林纳斯·本纳第克特·托瓦兹（Linus Benedict Torvalds, 1969年~ ），著名的电脑程序员、黑客。Linux内核的发明人及该计划的合作者。托瓦兹利用个人时间及器材创造出了这套当今全球最流行的操作系统（作业系统）内核之一。现受聘于开放源代码开发实验室（OSDL：Open Source Development Labs, Inc），全力开发Linux内核。</p>
<h3 id="1-3-Linux-发行版"><a href="#1-3-Linux-发行版" class="headerlink" title="1.3.Linux 发行版"></a><strong>1.3.</strong>Linux 发行版</h3><p>发行版是基于 Linux 内核的一个操作系统。它带有用户可以使用的软件集合。更多的，它还包含系统管理包。目前有许多 Linux 发行版。因为我们不能数清目前所有的 Linux 发行版，所以我们来看一下一些有名的版本： Ubuntu、Fedora、Opensuse、Red hat Linux 和 Debian 等是几个非常受欢迎的 Linux 发行版。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB6A.tmp.jpg" alt="img"> </p>
<p><strong>C**</strong>entos**</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB6B.tmp.jpg" alt="img"> <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB6C.tmp.jpg" alt="img"></p>
<p><strong>U</strong>buntu</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB6D.tmp.jpg" alt="img"><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB6E.tmp.jpg" alt="img"> </p>
<p><strong>R</strong>ehat</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB6F.tmp.jpg" alt="img"> <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB80.tmp.jpg" alt="img"></p>
<h3 id="1-4-Linux的特点"><a href="#1-4-Linux的特点" class="headerlink" title="1.4.Linux的特点"></a><strong>1.4.</strong>Linux的特点</h3><p>开放性，多用户，多任务，丰富的网络功能，可靠的系统安全，良好的可移植性，具有标准兼容性</p>
<h2 id="二、环境准备"><a href="#二、环境准备" class="headerlink" title="二、环境准备"></a><strong>二、环境</strong>准备</h2><h3 id="2-1-Vmware"><a href="#2-1-Vmware" class="headerlink" title="2.1. Vmware"></a><strong>2.1. Vmware</strong></h3><h4 id="2-1-1-Vmware简介"><a href="#2-1-1-Vmware简介" class="headerlink" title="2.1.1 Vmware简介"></a><strong>2.1.1</strong> <strong>Vmware</strong>简介</h4><p>大多数服务器的容量（CPU,内存，磁盘等）利用率不足 30%，这不仅导致了资源浪费，也加大了服务器的数量。实现服务器虚拟化后，多个操作系统可以作为虚拟机在单台物理服务器上运行，并且每个操作系统都可以访问底层服务器的计算资源，从而解决效率低下问题。</p>
<p>Vmware虚拟机化技术由此诞生，它可以将一台服务器虚拟化出多台虚拟机，供多人同时使用，提高资源利用率。</p>
<h4 id="2-1-2-Vmware-workstation安装"><a href="#2-1-2-Vmware-workstation安装" class="headerlink" title="2.1.2 Vmware workstation安装"></a><strong>2.</strong>1<strong>.</strong>2 Vmware workstation<strong>安装</strong></h4><p>详细见vmware安装文档</p>
<h3 id="2-2-linux安装"><a href="#2-2-linux安装" class="headerlink" title="2.2. linux安装"></a><strong>2.</strong>2. <strong>l</strong>inux安装</h3><p>详细见Linux安装文档</p>
<h3 id="2-3-网络配置"><a href="#2-3-网络配置" class="headerlink" title="2.3.网络配置"></a><strong>2.3</strong>.网络配置</h3><h4 id="2-3-1-查看网关"><a href="#2-3-1-查看网关" class="headerlink" title="2.3.1 查看网关"></a><strong>2.</strong>3.1 <strong>查看网关</strong></h4><p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB81.tmp.jpg" alt="img"> </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB82.tmp.jpg" alt="img"> </p>
<h4 id="2-3-2-配置静态IP-NAT模式"><a href="#2-3-2-配置静态IP-NAT模式" class="headerlink" title="2.3.2 配置静态IP(NAT模式)"></a><strong>2.</strong>3<strong>.2</strong> <strong>配置静态IP(NAT模式)</strong></h4><p>1.编辑配置文件,添加修改以下内容</p>
<p>vi /etc/sysconfig/network-scripts/ifcfg-eth0</p>
<p>按i 进入文本编辑模式，出现游标，左下角会出现INSERT,即可以编辑</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB83.tmp.jpg" alt="img"> </p>
<p>应包含以下配置，除此之外的可以删除掉。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=eth0     #网卡设备名,请勿修改名字</span><br><span class="line"></span><br><span class="line">TYPE=Ethernet	#网络类型，以太网</span><br><span class="line"></span><br><span class="line">BOOTPROTO=static   #启用静态IP地址</span><br><span class="line"></span><br><span class="line">ONBOOT=yes  #开启自动启用网络连接	</span><br><span class="line"></span><br><span class="line">IPADDR=192.168.78.100  #设置IP地址</span><br><span class="line"></span><br><span class="line">NETMASK=255.255.255.0  #设置子网掩码</span><br><span class="line"></span><br><span class="line">GATEWAY=192.168.78.2   #设置网关</span><br><span class="line"></span><br><span class="line">DNS1=114.114.114.114  #设置备DNS</span><br></pre></td></tr></table></figure>
<p>按ESC退出编辑模式</p>
<p>:wq  #保存退出</p>
<p>2.修改完后执行以下命令</p>
<p>service network restart  #重启网络连接</p>
<p>ifconfig  #查看IP地址</p>
<p>3.验证是否配置成功:</p>
<p>虚拟机能ping通虚拟网关</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB84.tmp.jpg" alt="img"> </p>
<p>虚拟机与物理机（笔记本）相互可ping通</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEB95.tmp.jpg" alt="img"> </p>
<p>虚拟机与公网上的百度网址相互可ping通（此步ping通，才说明网络配置成功，Ctrl键+C停止）</p>
<p>命令：ping  <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a> </p>
<p><strong>注意：</strong></p>
<p>a.保证VMware的虚拟网卡没有被禁用<img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEBF3.tmp.jpg" alt="img"> </p>
<p>b.网关IP不能被占用</p>
<h3 id="2-4-XShell安装与使用"><a href="#2-4-XShell安装与使用" class="headerlink" title="2.4.XShell安装与使用"></a><strong>2.</strong>4.XShell安装与使用</h3><h4 id="2-4-1安装步骤"><a href="#2-4-1安装步骤" class="headerlink" title="2.4.1安装步骤"></a><strong>2.4.1安装步骤</strong></h4><p>除了安装路径需要修改，其他一直下一步。</p>
<h4 id="2-4-2-连接虚拟机"><a href="#2-4-2-连接虚拟机" class="headerlink" title="2.4.2 连接虚拟机"></a><strong>2.4.2 连接虚拟机</strong></h4><ol>
<li>打开xshell软件新建一个会话</li>
</ol>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEBF4.tmp.jpg" alt="img"> </p>
<ol start="2">
<li>填写所要连接的虚拟机IP，会话名称可改可不改，点击确定。</li>
</ol>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEBF5.tmp.jpg" alt="img"> </p>
<p>3.连接虚拟机。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEBF6.tmp.jpg" alt="img"> </p>
<p>4．输入root用户名，可以勾选”记住用户名”</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEBF7.tmp.jpg" alt="img"> </p>
<p>5.填写密码，可以勾选“记住密码”</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC08.tmp.jpg" alt="img"> </p>
<p>6.登录成功。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC09.tmp.jpg" alt="img"> </p>
<h2 id="三、文件系统"><a href="#三、文件系统" class="headerlink" title="三、文件系统"></a><strong>三、</strong>文件系统</h2><p>Linux文件系统中的文件是数据的集合，文件系统不仅包含着文件中的数据而且还有文件系统的结构，所有Linux 用户和程序看到的文件、目录、软连接及文件保护信息等都存储在其中。</p>
<p>Linux目录结构：</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC0A.tmp.png" alt="img"> </p>
<p>   bin  存放二进制可执行文件(ls,cat,mkdir等)                                                           </p>
<p>  boot  存放用于系统引导时使用的各种文件</p>
<p>  dev 用于存放设备文件</p>
<p>  etc  存放系统配置文件</p>
<p>  home 存放所有用户文件的根目录</p>
<p>  lib  存放跟文件系统中的程序运行所需要的共享库及内核模块</p>
<p>  mnt  系统管理员安装临时文件系统的安装点</p>
<p>  opt  额外安装的可选应用程序包所放置的位置</p>
<p>  proc  虚拟文件系统，存放当前内存的映射</p>
<p>  root  超级用户目录</p>
<p>  sbin  存放二进制可执行文件，只有root才能访问</p>
<p>  tmp  用于存放各种临时文件</p>
<p>  usr  用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录</p>
<p>var  用于存放运行时需要改变数据的文件</p>
<h3 id="3-1目录操作"><a href="#3-1目录操作" class="headerlink" title="3.1目录操作"></a><strong>3</strong>.1目录操作</h3><h4 id="3-1-1-切换目录"><a href="#3-1-1-切换目录" class="headerlink" title="3.1.1**切换目录"></a><strong>3.</strong>1<strong><strong>.</strong></strong>1<em>**</em>切换目录</h4><p>命令：<strong>cd</strong> + 目录的路径  </p>
<p>查看当前目录的完整路径 ：pwd</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC0B.tmp.jpg" alt="img"> </p>
<p>命令 cd  ..   返回到父目录</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC0C.tmp.jpg" alt="img"> </p>
<h4 id="3-1-2-新建目录"><a href="#3-1-2-新建目录" class="headerlink" title="3.1.2**新建目录"></a><strong>3.</strong>1<strong><strong>.</strong></strong>2<em>**</em>新建目录</h4><p>命令：<strong>m</strong>kdir+ 目录名字 </p>
<p>查看当前目录下拥有的子目录和文件: ls</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC0D.tmp.jpg" alt="img"> </p>
<h4 id="3-1-3-拷贝目录"><a href="#3-1-3-拷贝目录" class="headerlink" title="3.1.3  拷贝目录"></a><strong>3.1.</strong>3  拷贝目录</h4><p><strong>cp</strong> source dest  -r</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC1E.tmp.jpg" alt="img"> </p>
<h4 id="3-1-4删除目录"><a href="#3-1-4删除目录" class="headerlink" title="3.1.4删除目录"></a><strong>3.1.4删除目录</strong></h4><p><strong>rmdir</strong> directory</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC1F.tmp.jpg" alt="img"> </p>
<p>注意：rmdir只能删除空目录,若要删除非空目录则用rm命令</p>
<p><strong>rm</strong> -rf dir </p>
<h4 id="3-1-5移动-更改-目录"><a href="#3-1-5移动-更改-目录" class="headerlink" title="3.1.5移动/更改 目录"></a><strong>3.1.5移动/更改 目录</strong></h4><p>​            移动文件或目录：<strong>mv</strong> + 目录/文件名字 + 其他路径</p>
<p>​          <strong>m</strong>v  test  /     将test目录移动到 根目录/ 下</p>
<p>​    <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC20.tmp.jpg" alt="img"></p>
<p>​    更改文件或目录的名字：<strong>m</strong>v + 旧目录名字 + 新目录名字。</p>
<p>​    <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC21.tmp.jpg" alt="img"></p>
<h3 id="3-2-文件操作"><a href="#3-2-文件操作" class="headerlink" title="3.2.文件操作"></a><strong>3</strong>.2.文件操作</h3><h4 id="3-2-1新建文件：（一切皆文件）"><a href="#3-2-1新建文件：（一切皆文件）" class="headerlink" title="3.2.1新建文件：（一切皆文件）"></a><strong>3</strong>.2.1新建文件：（一切皆文件）</h4><p><strong>touch</strong> web.log   创建一个空文件。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC22.tmp.jpg" alt="img"> </p>
<h4 id="3-2-2-复制文"><a href="#3-2-2-复制文" class="headerlink" title="3.2.2 复制文"></a><strong>3.</strong>2.2 <strong>复制文</strong></h4><p>​     <strong>cp</strong> web.log web_cp.log</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC23.tmp.jpg" alt="img"> </p>
<p>  复制文件，加个-r 参数，代表遍历复制，此时可用于复制一个目录。</p>
<h4 id="3-2-3删除文件"><a href="#3-2-3删除文件" class="headerlink" title="3.2.3删除文件"></a><strong>3.</strong>2<strong><strong>.</strong></strong>3删除文件</h4><p><strong>r</strong>m web_cp.log</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC24.tmp.jpg" alt="img"> </p>
<p>此时需要手动输入y ，代表确认删除。可加 –f参数，直接删除，无需确认。当需要一个目录下所有东西时，加-r参数，代表遍历删除。</p>
<p><strong>rm</strong> -f web.log</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC25.tmp.jpg" alt="img"> </p>
<h4 id="3-2-4-查看"><a href="#3-2-4-查看" class="headerlink" title="3.2.4**查看"></a><strong>3.2</strong>.4<em>**</em>查看</h4><h5 id="3-2-4-1-查看目录下的东西"><a href="#3-2-4-1-查看目录下的东西" class="headerlink" title="3.2.4.1**查看目录下的东西"></a><strong>3.</strong>2<strong><strong>.</strong></strong>4<strong><strong>.</strong></strong>1<em>**</em>查看目录下的东西</h5><p>​    ls / ll</p>
<p>   命令 ls -l 等价于 ll </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC26.tmp.jpg" alt="img"> </p>
<p>   查看目录下的所有东西（包括隐藏文件） </p>
<p>命令：ls –al 等价于  ll –a </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC27.tmp.jpg" alt="img"> </p>
<h5 id="3-2-4-2查看文件内容"><a href="#3-2-4-2查看文件内容" class="headerlink" title="3.2.4.2查看文件内容"></a><strong>3.</strong>2<strong><strong>.</strong></strong>4<strong><strong>.</strong></strong>2<strong><strong>查看文件</strong></strong>内容</h5><p><strong>cat</strong> filename: 一次性显示整个文件的内容</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC37.tmp.jpg" alt="img"> </p>
<p>注意：当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。</p>
<p>因此，一般用<a href="http://www.linuxde.net/tag/more" target="_blank" rel="noopener">more</a>等命令分屏显示.</p>
<p><strong>more</strong> filename 该命令一次显示一屏文本，满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比。</p>
<p>按Space键，显示文本的下一屏内容。<br>按Enter键，只显示文本的下一行内容。</p>
<p>按B键，显示上一屏内容。</p>
<p>按Q键，退出。</p>
<p><strong>命令：</strong>more  /etc/profile</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC38.tmp.jpg" alt="img"> </p>
<p>显示的内容：</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC39.tmp.jpg" alt="img"> </p>
<p>​        </p>
<p>less命令 与 more命令 非常类似</p>
<p>less filename:</p>
<p>​    h 显示帮助界面 </p>
<p>Q 退出less 命令 </p>
<p>u 向后滚动半页 </p>
<p>d 向前翻半页 </p>
<p>空格键 滚动一页 </p>
<p>b 向后翻一页 </p>
<p>回车键 滚动一行</p>
<p>[pagedown]： 向下翻动一页 </p>
<p>[pageup]： 向上翻动一页</p>
<p>以及上下键，向上一行，向下一行</p>
<h5 id="3-2-4-3从头打印-文件内容"><a href="#3-2-4-3从头打印-文件内容" class="headerlink" title="3.2.4.3从头打印**文件内容"></a><strong>3.</strong>2<strong><strong>.</strong></strong>4<strong><strong>.</strong></strong>3<strong><strong>从</strong></strong>头打印<em>**</em>文件内容</h5><p>​        <strong>head</strong>  -10 filename  打印文件1到10行</p>
<p>  <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC3A.tmp.jpg" alt="img"></p>
<h5 id="3-2-4-4从尾部打印文件内容"><a href="#3-2-4-4从尾部打印文件内容" class="headerlink" title="3.2.4.4从尾部打印文件内容"></a><strong>3</strong>.<strong><strong>2</strong></strong>.<strong><strong>4</strong></strong>.<strong><strong>4</strong></strong>从<strong><strong>尾部打印</strong></strong>文件内容</h5><p>  <strong>tail</strong> -10 filename 打印文件最后10行</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC3B.tmp.jpg" alt="img"> </p>
<p>   注意：tail 还经常可以拿来查看文件的内容变化</p>
<p>加-f参数，tail –f filename</p>
<h4 id="3-2-5查找文件或目录"><a href="#3-2-5查找文件或目录" class="headerlink" title="3.2.5查找文件或目录"></a><strong>3.2.5</strong>查找<strong><strong>文件</strong></strong>或目录</h4><p>​        <strong>f</strong>ind  pathname –name filename</p>
<p>​      例子：find / -name profile</p>
<p>​      该命令表示为，在/目录下查找 名字为profile的文件或目录，最后列出它的绝对路径</p>
<p>​      <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC3C.tmp.jpg" alt="img"></p>
<p>​      最后发现，linux系统根目录/ 下 一共有两个名字为profile，</p>
<p>​      其中/etc/profile是一个文件，/etc/lvm/profile为目录</p>
<p>还可以按正则表达式来查找，且pathname越精确，查找的范围越小，速度越快。</p>
<p>​      find /etc -name pro*</p>
<p>  注意：（命令执行时，其查找的目录必须是所在目录的父级目录）</p>
<p>​      该命令表示为：在/etc目录下查找以pro开头的文件或目录。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC3D.tmp.jpg" alt="img"> </p>
<h2 id="四、文本编辑"><a href="#四、文本编辑" class="headerlink" title="四、文本编辑"></a><strong>四、</strong>文本编辑</h2><h3 id="4-1-vi"><a href="#4-1-vi" class="headerlink" title="4.1.vi"></a><strong>4</strong>.1.vi</h3><p><strong>编辑</strong>模式</p>
<p>vi filename :打开或新建文件，并将光标置于第一行首 </p>
<p>vi +n filename ：打开文件，并将光标置于第n行首 </p>
<p>vi + filename ：打开文件，并将光标置于最后一行首 </p>
<p>vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的串处 </p>
<p>• q!：不保存文件并退出vi </p>
<p>– 在VI的命令模式下输入“:set nu”，就有行号了。 </p>
<p>– 在VI的命令模式下输入“:set nonu”，取消行号。 </p>
<p><strong>一般模式</strong></p>
<p>• yy 复制光标所在行(常用) </p>
<p>• nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) </p>
<blockquote>
<p>（不同：在我的xshell中是 yyn实现复制所在行的向下n行）</p>
</blockquote>
<p>• p,P   p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)</p>
<p>G:光标移至第最后一行</p>
<p>nG：光标移动至第N行行首</p>
<p>n（shift）+：光标下移n行 </p>
<blockquote>
<p>（nB|nb:光标向上移动n行）</p>
</blockquote>
<p>n-：光标上移n行 </p>
<p>H ：光标移至屏幕顶行 </p>
<p>M ：光标移至屏幕中间行 </p>
<p>L ：光标移至屏幕最后行 </p>
<p>• dd：删除 行 </p>
<p>x或X：删除一个字符，x删除光标后的，而X删除光标前的 </p>
<p>• u 恢复前一个动作(常用)</p>
<p>删除第N行到第M行：</p>
<p>  :N,Md</p>
<h3 id="4-2-vim"><a href="#4-2-vim" class="headerlink" title="4.2.vim"></a><strong>4.</strong>2.vim</h3><p>Vim是从 vi 发展出来的一个文本编辑器。代码补完、语法高亮、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用.</p>
<p><strong>安装vim 软件</strong></p>
<p><strong>yum</strong>  install  vim </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC4E.tmp.jpg" alt="img"> </p>
<p><strong>按y确认, 这中间一共要按两次确认</strong></p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC4F.tmp.jpg" alt="img"> </p>
<p>可以在书写命令时就加y,这样就不用逐一确认。</p>
<p><strong>yum</strong> install vim  -y</p>
<p>   用vim 打开/etc/profile 文件，会发现编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强</p>
<p>命令：vim /etc/profile</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC50.tmp.jpg" alt="img"> </p>
<h2 id="五、文件传输"><a href="#五、文件传输" class="headerlink" title="五、文件传输"></a><strong>五、</strong>文<strong><strong>件</strong></strong>传输</h2><h3 id="5-1-远程拷贝"><a href="#5-1-远程拷贝" class="headerlink" title="5.1.远程拷贝"></a><strong>5.</strong>1.远程拷贝</h3><h4 id="5-1-1将本地文件复制到远程机器"><a href="#5-1-1将本地文件复制到远程机器" class="headerlink" title="5.1.1将本地文件复制到远程机器"></a><strong>5.1.1将本地文件复制到远程机器</strong></h4><p> <code>举例：</code></p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt; 方式一：</span><br><span class="line">&gt;   scp -rf /etc/profile root@192.168.198.128:/etc/</span><br><span class="line">&gt; </span><br><span class="line">&gt; 方式二:</span><br><span class="line">&gt;   scp  -r /etc/profile root@node01 /etc/ </span><br><span class="line">&gt;   </span><br><span class="line">&gt; 方式三：</span><br><span class="line">&gt;   scp -r /etc/profile node01:/etc/</span><br><span class="line">&gt;   </span><br><span class="line">&gt; 方式四：</span><br><span class="line">&gt;   scp -r /ec/profile node01:&apos;pwd&apos;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>scp</strong> ：远程传输文件命令</p>
<p><strong>-r</strong>  ：- 指的是后面跟的是参数    r  指的是遍历指定文件    <strong>f</strong>  指的是不用询问</p>
<p><strong>/etc/profile</strong>  : 是指定传输的文件</p>
<p><strong>root</strong>： 远程机器的账户名      <strong>@</strong>    远程机器的IP地址   <strong>：</strong>   <strong>/etc/</strong>    远程机器上指定的目录</p>
<p><strong>node01</strong>：远程机器的别名</p>
<p><strong>‘pwd’</strong>： 本地要远程传输文件所在的目录</p>
<p><strong>scp</strong> local_file remote_username@remote_ip:remote_folder   </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC51.tmp.jpg" alt="img"> </p>
<p>第一次远程拷贝时，需要在箭头1初输入yes确认一下，验证一下远程主机。然后在箭头2处输入一下远程主机的密码。</p>
<h4 id="5-1-2将本地目录复制到远程机器"><a href="#5-1-2将本地目录复制到远程机器" class="headerlink" title="5.1.2将本地目录复制到远程机器"></a><strong>5.1.2将本地目录复制到远程机器</strong></h4><p><strong>scp</strong> -r local_folder remote_username@remote_ip:remote_folder </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC52.tmp.jpg" alt="img"> </p>
<p>在test目录下创建一个myfile文件，然后将test目录远程拷贝到192虚拟机的根目录下。</p>
<h3 id="5-2-上传"><a href="#5-2-上传" class="headerlink" title="5.2.上传"></a><strong>5</strong>.2.上传</h3><p>​    需先安装好lrzsz :  <strong>yum</strong>  install  lrzsz  -y</p>
<p>安装好后，输入上传的命令rz,弹出一下界面：</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC53.tmp.jpg" alt="img"> </p>
<p>选择一个windows系统里的文件上传至虚拟机的当前目录下,然后ll命令，查看结果</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC54.tmp.jpg" alt="img"> </p>
<h3 id="5-3-下载"><a href="#5-3-下载" class="headerlink" title="5.3.下载"></a><strong>5.</strong>3.下载</h3><p>  下载命令为sz，sz命令只能下载文件，不能是目录，可先将目录压缩成一个包，再下载至windows系统。下载完之后，按ctrl+c结束。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC65.tmp.jpg" alt="img"> </p>
<h3 id="5-4-Xftp的安装与使用"><a href="#5-4-Xftp的安装与使用" class="headerlink" title="5.4    Xftp的安装与使用"></a><strong>5.4</strong>    Xftp的安装与使用</h3><p>​    除了可以用rz sz命令进行本地windows系统和虚拟机之间的文件传输，还可以使用XFTP软件。</p>
<h2 id="六、网络指令"><a href="#六、网络指令" class="headerlink" title="六、网络指令"></a><strong>六、网络指令</strong></h2><h3 id="6-1-查看网络配置信息"><a href="#6-1-查看网络配置信息" class="headerlink" title="6.1.查看网络配置信息"></a><strong>6.1.查看网络配置信息</strong></h3><p>命令:ifconfig</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC66.tmp.jpg" alt="img"> </p>
<p>箭头1指向的是本机IP，箭头2为广播地址，箭头3位子网掩码。</p>
<h3 id="6-2-测试与目标主机的连通性"><a href="#6-2-测试与目标主机的连通性" class="headerlink" title="6.2.测试与目标主机的连通性"></a><strong>6.</strong>2.测试与目标主机的连通性</h3><p>命令：ping remote_ip（可以ping通Windows系统的IP）</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC67.tmp.jpg" alt="img"> </p>
<p>输入ping 192.168.78.192代表测试本机和192主机的网络情况，</p>
<p>箭头1表示一共接收到了3个包，箭头2表示丢包率为0，表示两者之间的网络顺畅。</p>
<p>注意：linux系统的ping命令会一直发送数据包，进行测试，除非认为的按ctrl + c停止掉，</p>
<p>​           windows系统默认只会发4个包进行测试，以下为windows的dos命令。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC68.tmp.jpg" alt="img"> </p>
<h3 id="6-3-显示各种网络相关信息"><a href="#6-3-显示各种网络相关信息" class="headerlink" title="6.3.显示各种网络相关信息"></a><strong>6.</strong>3.显示各种网络相关信息</h3><p>​    命令：<strong>n</strong>etstat <strong>–a n p t</strong></p>
<p>-a (all)显示所有选项，默认不显示LISTEN相关<br>-t (tcp)仅显示tcp相关选项<br>-u (udp)仅显示udp相关选项<br>-n 拒绝显示别名，能显示数字的全部转化成数字。<br>-l 仅列出有在 Listen (监听) 的服務状态</p>
<p>-p 显示建立相关链接的程序名<br>-r 显示路由信息，路由表<br>-e 显示扩展信息，例如uid等<br>-s 按各个协议进行统计<br>-c 每隔一个固定时间，执行该netstat命令。</p>
<p>提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC69.tmp.jpg" alt="img"> </p>
<h2 id="七、系统配置"><a href="#七、系统配置" class="headerlink" title="七、系统配置"></a><strong>七、系统配置</strong></h2><h3 id="7-1-主机名配置"><a href="#7-1-主机名配置" class="headerlink" title="7.1 主机名配置"></a><strong>7.1</strong> <strong>主机名配置</strong></h3><p>  若要修改主机名字，可在/etc/sysconfig/network文件里修改.</p>
<p>vim /etc/sysconfig/network</p>
<p>​       <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC6A.tmp.jpg" alt="img"></p>
<p>   机器重启才能生效</p>
<h3 id="7-2-DNS配置"><a href="#7-2-DNS配置" class="headerlink" title="7.2 DNS配置"></a><strong>7.2</strong> <strong>DNS配置</strong></h3><p> /etc/resolv.conf 为DNS服务器的地址文件</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC7A.tmp.jpg" alt="img"> </p>
<h3 id="7-3-环境变量"><a href="#7-3-环境变量" class="headerlink" title="7.3 环境变量"></a><strong>7.3</strong> <strong>环境变量</strong></h3><p>Linux系统的环境变量是在/etc/profile文件里配置。</p>
<p>首先考虑一个问题，为什么我们先前敲的yum, service,date,useradd等等，可以直接使用，系统怎么知道这些命令对应的程序是放在哪里的呢？</p>
<p>这是由于无论是windows系统还是linux系统，都有一个叫做path的系统环境变量，当我们在敲命令时，系统会到path对应的目录下寻找，找到的话就会执行，找不到就会报没有这个命令。如下图：</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC7B.tmp.jpg" alt="img"> </p>
<p>我们可以查看一下，系统一共在哪些目录里寻找命令对应的程序。</p>
<p>命令：echo $PATH</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC7C.tmp.jpg" alt="img"> </p>
<p>   可以看到path里有很多路径，路径之间有冒号隔开。当用户敲命令时，系统会从左往右依次寻找对应的程序，有的话则运行该程序，没有的就报错，command not found.</p>
<p>那如果我写了一个脚本（脚本后面会专门讲），我该怎样运行它呢？</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC7D.tmp.jpg" alt="img"> </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC7E.tmp.jpg" alt="img"> </p>
<p>对test.sh添加可执行权限，chmod 700 test.sh</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC7F.tmp.jpg" alt="img"> </p>
<p>运行方法有三种：</p>
<p>一种是到脚本的目录下执行：</p>
<p>运行命令 ： ./test.sh  ,代表执行当前目录里的脚本test.sh</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC80.tmp.jpg" alt="img"> </p>
<p>一种是敲脚本的绝对路径：/usr/test/test.sh</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC81.tmp.jpg" alt="img"> </p>
<p>以上两种运行方式都不是很简便，因为先前我们执行yum service命令等，都是直接敲对应的命令的。所以我们也可以参照这样子做，只要我们配一个环境变量就好。</p>
<p>编辑： vim /etc/profile 将test.sh所在目录添加到PATH里就OK，我这里test.sh是在/usr/test目录（通过pwd查看）下。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC92.tmp.jpg" alt="img"> </p>
<p>编辑完之后，执行source /etc/profile命令，重新加载环境变量，此时会发现PATH路径多了一个/usr/test。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC93.tmp.jpg" alt="img"> </p>
<p>​       最后验证一下，直接执行test.sh</p>
<p>​       <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC94.tmp.jpg" alt="img"></p>
<h2 id="八、服务操作"><a href="#八、服务操作" class="headerlink" title="八、服务操作"></a><strong>八、服务操作</strong></h2><h3 id="8-1-列出所有服务"><a href="#8-1-列出所有服务" class="headerlink" title="8.1 列出所有服务"></a><strong>8.1 列出所有服务</strong></h3><p>命令：chkconfig</p>
<p>查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC95.tmp.jpg" alt="img"> </p>
<p>各数字代表的系统初始化级别含义：</p>
<p>​     0：停机状态</p>
<p>　　1：单用户模式，root账户进行操作</p>
<p>　　2：多用户，不能使用net file system，一般很少用</p>
<p>　　3：完全多用户，一部分启动，一部分不启动，命令行界面</p>
<p>　　4：未使用、未定义的保留模式</p>
<p>　　5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。</p>
<p>　　6：停止所有进程，卸载文件系统，重新启动(reboot)</p>
<p>　　这些级别中1、2、4很少用，相对而言0、3、5、6用的会较多。3级别和5级别除了桌面相关的进程外没有什么区别。为了减少资源占用，推荐都用3级别.</p>
<p>注意 ：linux默认级别为3，不要把/etc/inittab 中initdefault 设置为0 和 6 </p>
<h3 id="8-2-服务操作"><a href="#8-2-服务操作" class="headerlink" title="8.2 服务操作"></a><strong>8.2</strong> <strong>服务操作</strong></h3><p>service 服务名 start/stop/status/restart</p>
<p>例子：对防火墙服务进行操作，防火墙的服务名为：iptables.</p>
<p>​    查看防火墙服务运行状态。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEC96.tmp.jpg" alt="img"> </p>
<p>   关闭防火墙.  </p>
<p> <img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsECA6.tmp.jpg" alt="img"></p>
<p>开启防火墙 </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsECA7.tmp.jpg" alt="img"> </p>
<h3 id="8-3-关闭防火墙"><a href="#8-3-关闭防火墙" class="headerlink" title="8.3 关闭防火墙"></a><strong>8.3 关闭防火墙</strong></h3><p>service iptables start/stop/status</p>
<p>注：学习期间直接把防火墙关掉就是，工作期间也是运维人员来负责防火墙的。</p>
<p>永久开启/关闭防火墙</p>
<p>chkconfig iptables on/off</p>
<h3 id="8-4-服务初执行等级更改"><a href="#8-4-服务初执行等级更改" class="headerlink" title="8.4 服务初执行等级更改"></a><strong>8.4</strong> <strong>服务初执行等级更改</strong></h3><p>chkconfig –level   2345    name      off|on</p>
<p>​                                        <code>（服务名）</code></p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDC2.tmp.jpg" alt="img"> </p>
<p>若不加级别，默认是2345级别</p>
<p>命令：chkconfig name on|off</p>
<p>​                          <code>（服务名）</code></p>
<h2 id="九、linux进程操作"><a href="#九、linux进程操作" class="headerlink" title="九、linux进程操作"></a><strong>九、linux进程操作</strong></h2><h3 id="9-1-查看所有进程"><a href="#9-1-查看所有进程" class="headerlink" title="9.1 查看所有进程"></a><strong>9.1 查看所有进程</strong></h3><p>命令： ps  -aux</p>
<p>​    -a 列出所有</p>
<p>​    -u 列出用户</p>
<p>​    -x 详细列出，如cpu、内存等</p>
<ul>
<li>e </li>
</ul>
<p>​    -f </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDC3.tmp.jpg" alt="img"> </p>
<p>命令： ps  - ef  |  grep ssh</p>
<p> 查看所有进程里CMD是ssh 的进程信息。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD3.tmp.jpg" alt="img"> </p>
<p>其中箭头所指的是sshd服务进程的进程号（PID）</p>
<h3 id="9-2-杀死进程"><a href="#9-2-杀死进程" class="headerlink" title="9.2 杀死进程"></a><strong>9.2</strong> <strong>杀死进程</strong></h3><p>Kill</p>
<p>用法 kill pid </p>
<p>-9：强制杀死</p>
<p>ps 命令先查出对应程序的PID或PPID ，然后杀死掉进程。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD4.tmp.jpg" alt="img"> </p>
<h2 id="十、-其他常用命令"><a href="#十、-其他常用命令" class="headerlink" title="十、 其他常用命令"></a>十、 <strong>其他常用命令</strong></h2><h3 id="10-1-yum"><a href="#10-1-yum" class="headerlink" title="10.1 yum"></a><strong>10.1</strong> <strong>yum</strong></h3><p>yum是一个在Fedora和RedHat以及CentOS中的<a href="https://baike.baidu.com/item/Shell" target="_blank" rel="noopener">Shell</a>前端软件包管理器。基于<a href="https://baike.baidu.com/item/RPM" target="_blank" rel="noopener">RPM</a>包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。</p>
<p>由于centos系统的yum默认是到国外网站下载，有时下载速度会很慢，故我们可以换一个yum的下载源，这里我们换一个国内的下载源 阿里云镜像。</p>
<p>第一步：备份你的原镜像文件，以免出错后可以恢复。</p>
<p>cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD5.tmp.jpg" alt="img"> </p>
<p>第二步：下载新的CentOS-Base.rep到/etc/yum.repos.d/</p>
<p>wget -O /etc/yum.repos.d/CentOS-Base.repo <a href="http://mirrors.aliyun.com/repo/Centos-6.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/repo/Centos-6.repo</a></p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD6.tmp.jpg" alt="img"> </p>
<p>下载完之后，vim /etc/yum.repos.d/CentOS-Base.repo 查看一下文件内容。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD7.tmp.jpg" alt="img"> </p>
<p>第三步：运行yum makecache生成缓存</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD8.tmp.jpg" alt="img"> </p>
<p>查看当前源</p>
<p>yum list | head -50</p>
<h3 id="10-2-wget"><a href="#10-2-wget" class="headerlink" title="10.2 wget"></a>10.2 <strong>wget</strong></h3><p>wget 是一个从网络上自动下载文件的自由工具，支持通过 HTTP、HTTPS、FTP 三个最常见的 <a href="https://baike.baidu.com/item/TCP%2FIP%E5%8D%8F%E8%AE%AE" target="_blank" rel="noopener">TCP/IP协议</a> 下载，并可以使用 HTTP 代理</p>
<p>需先安装 yum install wget  –y </p>
<p>wget用法:wget [option] 网址  -O    指定下载保存的路径    </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDD9.tmp.jpg" alt="img"> </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDEA.tmp.jpg" alt="img"> </p>
<p>wget 工具还可以用来做一些简单的爬虫，这里不是我们的学习重点，如果想做爬虫，可以用java或python语言来做。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDEB.tmp.jpg" alt="img"> </p>
<h3 id="10-3-tar"><a href="#10-3-tar" class="headerlink" title="10.3 tar"></a><strong>10.3 tar</strong></h3><p>tar </p>
<p>​    -z    gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加</p>
<p>​    -x    解压</p>
<p>​    -c    压缩</p>
<p>​    -f    目标文件，压缩文件新命名或解压文件名</p>
<p>​    -v    解压缩过程信息打印</p>
<p>解压命令：tar  -zvxf  xxxx.tar.gz</p>
<p>例子：先用rz命令或wscp上传一个tar包，然后解压。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDEC.tmp.jpg" alt="img"> </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDED.tmp.jpg" alt="img"> </p>
<p>解压后：</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDEE.tmp.jpg" alt="img"> </p>
<p>压缩命令：tar -zcf 压缩包命名 压缩目标</p>
<p>例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61 </p>
<p>将 apache-tomcat-7.0.61 目录压缩成tomcat.tar.gz包。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEDEF.tmp.jpg" alt="img"> </p>
<h2 id="十一、JDK部署"><a href="#十一、JDK部署" class="headerlink" title="十一、JDK部署"></a><strong>十一、JDK部署</strong></h2><h3 id="11-1-官网下载"><a href="#11-1-官网下载" class="headerlink" title="11.1 官网下载"></a><strong>11.1 官网下载</strong></h3><p><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE00.tmp.jpg" alt="img"> </p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE01.tmp.jpg" alt="img"> </p>
<h3 id="11-2-上传并解压"><a href="#11-2-上传并解压" class="headerlink" title="11.2 上传并解压"></a><strong>11.2 上传并解压</strong></h3><p>用xftp将jdk包上传到linux系统里，我这里上传到/usr/soft目录下。</p>
<p>然后解压: tar -zxf jdk-7u80-linux-x64.tar.gz</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE02.tmp.jpg" alt="img"> </p>
<h3 id="11-3配置环境变量"><a href="#11-3配置环境变量" class="headerlink" title="11.3配置环境变量"></a><strong>11.</strong>3<strong>配置环境变量</strong></h3><p>配置全局JAVA_HOME，并在PATH路径里加入java_home/bin.</p>
<p>注意：新的path路径必须要包含旧的PATH路径，且每个路径之间以冒号隔开，而不是分号</p>
<p>vim /etc/profile</p>
<p>JAVA_HOME= /usr/soft/jdk1.7.0_75</p>
<p>PATH=$PATH:$JAVA_HOME/bin</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE03.tmp.jpg" alt="img"> </p>
<p>重新加载环境变量：source  /etc/profile</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE04.tmp.jpg" alt="img"> </p>
<h3 id="11-4-验证"><a href="#11-4-验证" class="headerlink" title="11.4 验证"></a><strong>11.4 </strong>验证</h3><p>java  -version</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE05.tmp.jpg" alt="img"> </p>
<p>如出现上图，则表示java环境变量配置成功。</p>
<h2 id="十二、部署Tomcat"><a href="#十二、部署Tomcat" class="headerlink" title="十二、部署Tomcat"></a><strong>十二、部署Tomcat</strong></h2><h3 id="12-1-官网下载"><a href="#12-1-官网下载" class="headerlink" title="12.1 官网下载"></a><strong>1</strong>2<strong><strong>.</strong></strong>1 官网下载</h3><p>下载tomcat</p>
<p><a href="http://tomcat.apache.org/" target="_blank" rel="noopener">http://tomcat.apache.org/</a></p>
<h3 id="12-2-上传并解压"><a href="#12-2-上传并解压" class="headerlink" title="12**.2 上传并解压**"></a><strong>1</strong>2<strong>**.2 </strong>上传并解压**</h3><p>我这里上传至/usr/soft目录下，然后解压。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE06.tmp.jpg" alt="img"> </p>
<h3 id="12-3-启动tomcat"><a href="#12-3-启动tomcat" class="headerlink" title="12.3 启动tomcat"></a><strong>12.3 启动tomcat</strong></h3><p>在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE16.tmp.jpg" alt="img"> </p>
<p>关闭tomcat服务，可以用shutdown.sh命令。</p>
<p>或者ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令。</p>
<h3 id="12-4-jps"><a href="#12-4-jps" class="headerlink" title="12.4 jps"></a><strong>12.4</strong> <strong>jps</strong></h3><p>jps是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE17.tmp.jpg" alt="img"> </p>
<p>如上图所示，jps命令显示出了，系统当前运行在jvm上的进程情况。其中Bootstrap是tomcat的进程名字，1996是tomcat的PID</p>
<h3 id="13-5验证"><a href="#13-5验证" class="headerlink" title="13.5验证"></a><strong>13.</strong>5验证</h3><p>先把防火墙关了（service iptables stop），然后访问虚拟机IP的8080端口</p>
<p><img src="file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml\wpsEE18.tmp.jpg" alt="img"> </p>
]]></content>
        
        
    </entry>
    
    <entry>
        <title></title>
        <url>http://sungithup.github.io/2019/01/23/Linux%20%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%9B%9E%E5%90%88%EF%BC%89/</url>
        <content type="html"><![CDATA[<h1 id="Linux-入门学习（第二回合）"><a href="#Linux-入门学习（第二回合）" class="headerlink" title="Linux 入门学习（第二回合）"></a>Linux 入门学习（第二回合）</h1><p><code>2018年12月18日  周二  晴</code></p>
<p><strong>今日学习要点：</strong></p>
<p>[TOC]</p>
<h2 id="Linux后半程"><a href="#Linux后半程" class="headerlink" title="Linux后半程"></a>Linux后半程</h2><h3 id="一、Linux系统配置"><a href="#一、Linux系统配置" class="headerlink" title="一、Linux系统配置"></a>一、Linux系统配置</h3><h4 id="1-主机名配置："><a href="#1-主机名配置：" class="headerlink" title="1.主机名配置："></a>1.主机名配置：</h4><blockquote>
<p>vim  /etc/sysconfig/network</p>
</blockquote>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1545214876278.png" alt="1545214876278"></p>
<p>配置完成之后需要重启机器才能生效</p>
<blockquote>
<p>reboot</p>
</blockquote>
<h4 id="2-DNS配置"><a href="#2-DNS配置" class="headerlink" title="2.DNS配置"></a>2.DNS配置</h4><blockquote>
<h1 id="查看DNS服务器的地址"><a href="#查看DNS服务器的地址" class="headerlink" title="查看DNS服务器的地址"></a>查看DNS服务器的地址</h1><p>cat  /etc/resolv.conf</p>
<h1 id="修改DNS服务器地址"><a href="#修改DNS服务器地址" class="headerlink" title="修改DNS服务器地址"></a>修改DNS服务器地址</h1><p>方式一：vim  /etc/sysconfig/network.scripts/ifconfig-eth0</p>
<p>​             在配置网关时，配置DNS1=114.114.114.114（不推荐，江苏南京的IP）</p>
<p>方式二：vim   /etc/resolv.conf    （用本地网关解析）</p>
<p>​              nameserver   192.168.198.0   ( 此为虚拟机中的网关地址)</p>
</blockquote>
<h4 id="3-环境变量"><a href="#3-环境变量" class="headerlink" title="3.环境变量"></a>3.环境变量</h4><blockquote>
<p>配置系统环境变量，使得某些命令在执行时，系统可以找到命令对应的执行程序，命令才能正常执行。</p>
</blockquote>
<p>查看系统一共在哪些目录里寻找命令对应的程序</p>
<blockquote>
<p>命令：echo $PATH</p>
</blockquote>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1545215894631.png" alt="1545215894631"></p>
<p><code>注意</code>：路径之间有冒号隔开，系统会从左往右依次寻找对应的程序</p>
<p>​             一般命令会存放在  bin目录，或sbin目录</p>
<p>配置全局环境变量：</p>
<blockquote>
<p>vim  /etc/profile</p>
<p>在文件中：</p>
<p>PATH=$PATH:(命令所在目录)</p>
<p>退出文件编辑后：</p>
<p>source  /etc/profile  </p>
<p> (重新加载资源，有的可能需要重启机器，这不适用于实际状况)</p>
</blockquote>
<p>配置局部环境变量：（推荐，限当前登录用户使用）</p>
<blockquote>
<p>查看所有文件(root目录下)</p>
<p>ls  -a    (发现隐藏文件    .bash.profile)</p>
<p>vim  ~/ bash_profile</p>
<p>在文件中：</p>
<p>export  PATH =$PATH:(命令所在目录)</p>
</blockquote>
<h4 id="4-拍快照"><a href="#4-拍快照" class="headerlink" title="4.拍快照"></a>4.拍快照</h4><p>（保存当时计算机所出状态的各种配置和资源，适度使用）</p>
<blockquote>
<p>选中指定虚拟计算机——鼠标右击—–选中“快照” ——“拍摄快照‘—-在页面中找到”拍摄快照“，并添加名称和描述</p>
<p>也可以删除，找到页面中的删除按钮</p>
</blockquote>
<h3 id="二、服务操作"><a href="#二、服务操作" class="headerlink" title="二、服务操作"></a>二、服务操作</h3><h4 id="1、查询操作系统"><a href="#1、查询操作系统" class="headerlink" title="1、查询操作系统"></a>1、查询操作系统</h4><p>在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。</p>
<blockquote>
<p>命令：chkconfig</p>
</blockquote>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1545219619458.png" alt="1545219619458"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">各数字代表的系统初始化级别：</span><br><span class="line">    0：停机状态</span><br><span class="line">　　1：单用户模式，root账户进行操作</span><br><span class="line">　　2：多用户，不能使用net file system，一般很少用</span><br><span class="line">　　3：完全多用户，一部分启动，一部分不启动，命令行界面</span><br><span class="line">　　4：未使用、未定义的保留模式</span><br><span class="line">　　5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。</span><br><span class="line">　　6：停止所有进程，卸载文件系统，重新启动(reboot)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>1、2、4很少用，0、3、5、6常用，3级别和5级别除了桌面相关的进程外没有什么区别，推荐都用3级别；</p>
<p>linux默认级别为3；</p>
<p>不要把 /etc/inittab  中 initdefault 设置为0 和 6； </p>
</blockquote>
<h4 id="2、服务操作"><a href="#2、服务操作" class="headerlink" title="2、服务操作"></a>2、服务操作</h4><blockquote>
<p>service 服务名 start/stop/status/restart</p>
</blockquote>
<p>举例：对防火墙服务进行操作</p>
<blockquote>
<p>防火墙的服务名为：iptables</p>
</blockquote>
<p>查看防火墙服务运行状态</p>
<blockquote>
<p>service  iptables status</p>
</blockquote>
<p>关闭防火墙</p>
<blockquote>
<p>service  iptables stop</p>
</blockquote>
<p>开启防火墙</p>
<blockquote>
<p>service  iptables start</p>
</blockquote>
<p>永久开启/关闭防火墙</p>
<blockquote>
<p>chkconfig iptables on/off</p>
</blockquote>
<h4 id="3、服务初执行等级更改"><a href="#3、服务初执行等级更改" class="headerlink" title="3、服务初执行等级更改"></a>3、服务初执行等级更改</h4><blockquote>
<p>chkconfig –level 2345 <code>name</code> off|on<br>​                                   （ <code>服务名</code>）</p>
<p>举例：防火墙</p>
<p>chkconfig –level 2345 iptables   off</p>
</blockquote>
<p>若不加级别，默认是2345级别</p>
<blockquote>
<p>命令：chkconfig <code>name</code> on|off<br>​                         （<code>服务名</code>）</p>
</blockquote>
<h3 id="三、linux进程操作"><a href="#三、linux进程操作" class="headerlink" title="三、linux进程操作"></a>三、linux进程操作</h3><h4 id="1、查看所有进程"><a href="#1、查看所有进程" class="headerlink" title="1、查看所有进程"></a>1、查看所有进程</h4><blockquote>
<p>命令： ps  -aux</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   -a 列出所有</span><br><span class="line">-u 列出用户</span><br><span class="line">-x 详细列出，如cpu、内存等</span><br><span class="line">    -e select all processes    相当于-a</span><br><span class="line">    -f does full-format listing   将所有格式详细列出来</span><br></pre></td></tr></table></figure>
<p>查看所有进程里CMD是ssh 的进程信息（包括pid 进程号）</p>
<blockquote>
<p>命令： ps  - ef | grep ssh      </p>
<p>（|  管道符  ：前一个输出，变为后一个的输入）</p>
<p>举例：</p>
<p>ps -ef | grep redis</p>
</blockquote>
<h4 id="2、杀死进程"><a href="#2、杀死进程" class="headerlink" title="2、杀死进程"></a>2、杀死进程</h4><p><code>kill</code></p>
<blockquote>
<p>命令：kill pid </p>
<p>-9    强制杀死</p>
<p>用法：用ps 命令先查出对应程序的PID或PPID ，然后用kill杀死掉进程。</p>
</blockquote>
<h3 id="四、其他常用命令"><a href="#四、其他常用命令" class="headerlink" title="四、其他常用命令"></a>四、其他常用命令</h3><h4 id="1、yum"><a href="#1、yum" class="headerlink" title="1、yum"></a>1、<strong>yum</strong></h4><table>
<thead>
<tr>
<th>基于<a href="https://baike.baidu.com/item/RPM" target="_blank" rel="noopener">RPM</a>包管理</th>
</tr>
</thead>
<tbody>
<tr>
<td>能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装</td>
</tr>
</tbody>
</table>
<p>跟换yum下载源（默认是到国外网站下载）</p>
<p>第一步：备份你的原镜像文件，以免出错后可以恢复</p>
<blockquote>
<p>cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</p>
</blockquote>
<p>第二步：下载新的CentOS-Base.rep到/etc/yum.repos.d/</p>
<blockquote>
<p>wget -O /etc/yum.repos.d/CentOS-Base.repo <a href="http://mirrors.aliyun.com/repo/Centos-6.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/repo/Centos-6.repo</a></p>
</blockquote>
<p>下载完之后，查看一下文件内容</p>
<blockquote>
<p>vim  /etc/yum.repos.d/CentOS-Base.repo</p>
</blockquote>
<p>第三步：生成缓存</p>
<blockquote>
<p>运行yum makecache</p>
</blockquote>
<p>查看当前源</p>
<blockquote>
<p>yum list | head -50</p>
</blockquote>
<h4 id="2、-wget"><a href="#2、-wget" class="headerlink" title="2、 wget"></a>2、 <strong>wget</strong></h4><table>
<thead>
<tr>
<th>一个从网络上自动下载文件的自由工具</th>
</tr>
</thead>
<tbody>
<tr>
<td>支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议，可以使用 HTTP 代理</td>
</tr>
</tbody>
</table>
<p>安装：</p>
<blockquote>
<p>yum install wget  –y</p>
</blockquote>
<p>用法：</p>
<blockquote>
<p>wget  [option] 网址  -O    指定下载保存的路径</p>
<p>举例：</p>
<p>wget  <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a>  -O baidu.html</p>
</blockquote>
<h4 id="3、tar"><a href="#3、tar" class="headerlink" title="3、tar"></a>3、<strong>tar</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   -z	gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加</span><br><span class="line">-x	解压</span><br><span class="line">-c	压缩</span><br><span class="line">-f	目标文件，压缩文件新命名或解压文件名</span><br><span class="line">-v	解压缩过程信息打印</span><br></pre></td></tr></table></figure>
<blockquote>
<p>解压命令：tar  -zvxf  xxxx.tar.gz</p>
</blockquote>
<blockquote>
<p>压缩命令：tar -zcf 压缩包命名 压缩目标<br>举例：</p>
<p>tar -zcf  tomcat.tar.gz  apache-tomcat-7.0.61<br>将 apache-tomcat-7.0.61 目录压缩成tomcat.tar.gz包</p>
</blockquote>
<h4 id="4、man"><a href="#4、man" class="headerlink" title="4、man"></a>4、<strong>man</strong></h4><p>作用：用于查看指定命令的具体解释</p>
<p>安装</p>
<blockquote>
<p>yum install  man  -y</p>
<p>(下载并安装man  并确认)</p>
</blockquote>
<p>使用</p>
<blockquote>
<p>man  ps</p>
</blockquote>
<h3 id="五、JDK部署"><a href="#五、JDK部署" class="headerlink" title="五、JDK部署"></a>五、JDK部署</h3><h4 id="1、准备JDK安装包："><a href="#1、准备JDK安装包：" class="headerlink" title="1、准备JDK安装包："></a>1、准备JDK安装包：</h4><p>（这是使用   .rpm  格式的安装包）</p>
<p>官网下载：<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<p>云盘资源：  <a href="https://pan.baidu.com/s/1LzeQbOnG9PROZrtlbYHlDg" target="_blank" rel="noopener">jdk-8u191-linux-x64.rpm</a>  ：</p>
<p>​     根据用户喜好放到虚拟机器的文件目录中</p>
<h4 id="2、解压并安装，展示编译过程"><a href="#2、解压并安装，展示编译过程" class="headerlink" title="2、解压并安装，展示编译过程"></a>2、解压并安装，展示编译过程</h4><blockquote>
<p>rpm   -ivh  jdk-8u191-linux-x64.rpm</p>
</blockquote>
<p>安装放到了 /usr 目录下，有/java目录</p>
<h4 id="3、配置环境变量"><a href="#3、配置环境变量" class="headerlink" title="3、配置环境变量"></a>3、配置环境变量</h4><blockquote>
<p>vim  ~/.bash_profile</p>
<p>在文件中：</p>
<p>JAVA_HOME=(jdk文件所在的路径+jdk文件名)</p>
<p>export  PATH=$PATH:$JAVA_HOME/bin</p>
</blockquote>
<p>注意：</p>
<p>新的path路径必须要包含旧的PATH路径，且每个路径之间以冒号隔开，而不是分号</p>
<blockquote>
<p>配置完成，退出编辑框后</p>
<p>source  ~/.hash_profile</p>
</blockquote>
<h4 id="4、测试："><a href="#4、测试：" class="headerlink" title="4、测试："></a>4、测试：</h4><blockquote>
<p>java  -version</p>
<p>或</p>
<p>echo  $JAVA_HOME</p>
</blockquote>
<p><code>echo   标准输出，打印</code></p>
<h3 id="六、Tomcat部署"><a href="#六、Tomcat部署" class="headerlink" title="六、Tomcat部署"></a>六、Tomcat部署</h3><h4 id="1、官网下载"><a href="#1、官网下载" class="headerlink" title="1、官网下载"></a>1、官网下载</h4><p><a href="http://tomcat.apache.org/" target="_blank" rel="noopener">http://tomcat.apache.org/</a></p>
<p>云盘资源：<a href="https://pan.baidu.com/s/1e7OSIf3K9YpFCzXncMj_9Q" target="_blank" rel="noopener">apache-tomcat-7.0.61.tar</a></p>
<h4 id="2、上传并解压"><a href="#2、上传并解压" class="headerlink" title="2、上传并解压"></a>2、上传并解压</h4><blockquote>
<p>tar -zvxf  apache-tomcat-7.0.61.tar</p>
</blockquote>
<h4 id="3、启动tomcat"><a href="#3、启动tomcat" class="headerlink" title="3、启动tomcat"></a>3、启动tomcat</h4><p>在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务</p>
<blockquote>
<p>./startup.sh</p>
</blockquote>
<h4 id="4、关闭tomcat服务"><a href="#4、关闭tomcat服务" class="headerlink" title="4、关闭tomcat服务"></a>4、关闭tomcat服务</h4><p>方式一：可以用shutdown.sh命令</p>
<p>方式二：ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令</p>
<h4 id="5、验证"><a href="#5、验证" class="headerlink" title="5、验证"></a>5、验证</h4><p>先把防火墙关了（service iptables stop），然后访问虚拟机IP的8080端口</p>
]]></content>
        
        
    </entry>
    
    <entry>
        <title><![CDATA[Flume学习]]></title>
        <url>http://sungithup.github.io/2019/01/18/Flume%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h2 id="一、分类"><a href="#一、分类" class="headerlink" title="一、分类"></a>一、分类</h2><p><code>exec</code>：</p>
<blockquote>
<p> Unix等操作系统执行命令行，如tail ，cat 。可监听文件</p>
</blockquote>
<p><code>netcat</code></p>
<blockquote>
<p>监听一个指定端口，并将接收到的数据的每一行转换为一个event事件</p>
</blockquote>
<p><code>avro</code></p>
<blockquote>
<p>序列化的一种，实现RPC（一种远程过程调用协议）。</p>
<p>监听AVRO端口来接收外部AVRO客户端事件流</p>
</blockquote>
<blockquote>
<p>capacity：默认该通道中最大的可以存储的event数量是1000</p>
<p>Trasaction Capacity：每次最大可以source中拿到或者送到sink中的event数量也是100</p>
</blockquote>
<h2 id="二、操作"><a href="#二、操作" class="headerlink" title="二、操作"></a>二、操作</h2><h3 id="1、-netcat（监听端口，在本地控制台打印）"><a href="#1、-netcat（监听端口，在本地控制台打印）" class="headerlink" title="1、 netcat（监听端口，在本地控制台打印）"></a>1、 netcat（监听端口，在本地控制台打印）</h3><h4 id="（1）-vim-netcat-logger"><a href="#（1）-vim-netcat-logger" class="headerlink" title="（1） vim netcat_logger"></a>（1） vim netcat_logger</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># example.conf: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<h4 id="（2）命令操作"><a href="#（2）命令操作" class="headerlink" title="（2）命令操作"></a>（2）命令操作</h4><ul>
<li>（在会话1端）</li>
</ul>
<blockquote>
<p>在node00节点的控制台输入<code>启动</code>命令：</p>
<p>(<code>方式一</code>：指定配置文件的路径+文件名)</p>
<p>flume-ng agent</p>
<p> –conf-file /root/flume/netcat_logger –name a1 -Dflume.root.logger=INFO,console</p>
<p>（<code>方式二</code>：配置文件在当前目录）</p>
<p>flume-ng agent –conf ./ –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console</p>
</blockquote>
<p><code>特别注意</code>：</p>
<p>#####<code>官网方式</code>#########</p>
<blockquote>
<p>flume-ng agent</p>
<p> –conf conf –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console</p>
<p>解释：此命令适用于将配置文件放在flume解压安装目录的conf中（不常用）</p>
</blockquote>
<p><code>控制台显示</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">​`````</span><br></pre></td></tr></table></figure>
<p>19/01/18 12:27:31 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started<br>19/01/18 12:27:31 INFO node.Application: Starting Sink k1<br>19/01/18 12:27:31 INFO node.Application: Starting Source r1<br>19/01/18 12:27:31 INFO source.NetcatSource: Source starting<br>19/01/18 12:27:31 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* (在会话2端)</span><br><span class="line"></span><br><span class="line">&gt; 在node00节点的控制台输入命令：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1、在节点上安装telnet：</span><br><span class="line">&gt;</span><br><span class="line">&gt; yum install -y telnet</span><br><span class="line">&gt;</span><br><span class="line">&gt; yum -y install telnet-server</span><br><span class="line">&gt;</span><br><span class="line">&gt; 2、启动：</span><br><span class="line">&gt;</span><br><span class="line">&gt; telnet localhost 44444  </span><br><span class="line">&gt;</span><br><span class="line">&gt; `注意：`：</span><br><span class="line">&gt;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>a1.sources.r1.bind = localhost<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;</span><br><span class="line">&gt; 前提是/etc/hosts中已经配置</span><br><span class="line">&gt;</span><br><span class="line">&gt; 如果此处配置localhost 那么启动时，localhost  或127.0.0.1都可以，node00就不行</span><br><span class="line">&gt;</span><br><span class="line">&gt; 如果此处配置node00那么启动时，node00或ip都可以，localhost就不行</span><br><span class="line">&gt;</span><br><span class="line">&gt; 3、在控制台输入任何内容;</span><br><span class="line">&gt;</span><br><span class="line">&gt; 都会在会话1端显示，且会话1端（ctrl+c）退出服务，会话2端也自动结束</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>  yum list telnet*   查看telnet相关的安装包<br>   直接yum –y install telnet 就OK<br>     yum -y install telnet-server 安装telnet服务<br>     yum -y install telnet-client  安装telnet客户端(大部分系统默认安装)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2、avro（监听远程发送文件，在本地控制台打印）</span><br><span class="line"></span><br><span class="line">#### （1）vim avro_logger</span><br></pre></td></tr></table></figure>
<p>#test avro sources</p>
<p>##使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示</p>
<p>##当前flume节点执行：</p>
<p>#flume-ng agent –conf ./ –conf-file avro_loggers –name a1 -Dflume.root.logger=INFO,console</p>
<p>##其他flume节点执行：</p>
<p>#flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./logs</p>
<p>a1.sources=r1<br>a1.channels=c1<br>a1.sinks=k1</p>
<p>a1.sources.r1.type = avro<br>a1.sources.r1.bind=192.168.198.128<br>a1.sources.r1.port=55555</p>
<p>a1.sinks.k1.type=logger</p>
<p>a1.channels.c1.type = memory<br>a1.channels.c1.capacity=1000<br>a1.channels.c1.transactionCapacity = 100</p>
<p>a1.sources.r1.channels=c1<br>a1.sinks.k1.channel=c1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">实现功能：</span><br><span class="line"></span><br><span class="line">&gt; 使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示</span><br><span class="line"></span><br><span class="line">#### （2）命令操作</span><br><span class="line"></span><br><span class="line">##### `启动` </span><br><span class="line"></span><br><span class="line">（在会话1端）</span><br><span class="line"></span><br><span class="line">在node00上</span><br><span class="line"></span><br><span class="line">*  ##当前flume节点执行（配置文件在当前目录）：</span><br><span class="line"></span><br><span class="line">* &gt;  flume-ng agent --conf ./ --conf-file avro_logger --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">`显示：`</span><br></pre></td></tr></table></figure></p>
<p>19/01/18 13:53:16 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started<br>19/01/18 13:53:16 INFO node.Application: Starting Sink k1<br>19/01/18 13:53:16 INFO node.Application: Starting Source r1<br>19/01/18 13:53:16 INFO source.AvroSource: Starting Avro source r1: { bindAddress: 192.168.198.128, port: 55555 }…<br>19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.<br>19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started<br>19/01/18 13:53:17 INFO source.AvroSource: Avro source r1 started.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#####  发送</span><br><span class="line"></span><br><span class="line">(在会话2端)</span><br><span class="line"></span><br><span class="line">在node00上发送文件到node00</span><br><span class="line"></span><br><span class="line">`启动`</span><br><span class="line"></span><br><span class="line">##可在本地和其他flume节点执行（配置文件在当前目录）：</span><br></pre></td></tr></table></figure>
<p>flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./flume.log<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(在会话1端)</span><br></pre></td></tr></table></figure></p>
<p>19/01/18 14:12:57 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61          hello bigdata }<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">时刻监听传输文件的内容</span><br><span class="line"></span><br><span class="line">`注意`</span><br><span class="line"></span><br><span class="line">&gt; 该过程也可应用于不同节点之间</span><br><span class="line"></span><br><span class="line">### 3、exec（监听某一命令，在本地控制台打印）</span><br><span class="line"></span><br><span class="line">#### （1）vim exec_logger</span><br></pre></td></tr></table></figure></p>
<p>#单节点flume配置</p>
<h1 id="example-conf-A-single-node-Flume-configuration"><a href="#example-conf-A-single-node-Flume-configuration" class="headerlink" title="example.conf: A single-node Flume configuration"></a>example.conf: A single-node Flume configuration</h1><p>#给agent三大结构命名</p>
<h1 id="Name-the-components-on-this-agent"><a href="#Name-the-components-on-this-agent" class="headerlink" title="Name the components on this agent"></a>Name the components on this agent</h1><p>a1.sources = r1<br>a1.sinks = k1<br>a1.channels = c1</p>
<p>#描述source的配置：类型、命令（监听/root/flume.log文件）</p>
<h1 id="Describe-configure-the-source"><a href="#Describe-configure-the-source" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>a1.sources.r1.type = exec<br>a1.sources.r1.command = tail -F /root/flume.log</p>
<p>#描述sink的配置：类型</p>
<h1 id="Describe-the-sink"><a href="#Describe-the-sink" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>a1.sinks.k1.type = logger</p>
<p>#在内存中使用一个channel缓存事件</p>
<h1 id="Use-a-channel-which-buffers-events-in-memory"><a href="#Use-a-channel-which-buffers-events-in-memory" class="headerlink" title="Use a channel which buffers events in memory"></a>Use a channel which buffers events in memory</h1><p>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = 1000<br>a1.channels.c1.transactionCapacity = 100</p>
<p>#将source和sink绑定到channel上</p>
<h1 id="Bind-the-source-and-sink-to-the-channel"><a href="#Bind-the-source-and-sink-to-the-channel" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a1.sources.r1.channels = c1<br>a1.sinks.k1.channel = c1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">（xshell会话1：）</span><br><span class="line"></span><br><span class="line">&gt; 在node00上：`启动`</span><br><span class="line">&gt;</span><br><span class="line">&gt; 在exec_logger文件所在的目录下</span><br><span class="line">&gt;</span><br><span class="line">&gt; 命令：flume-ng agent  --conf-file exec_logger --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line">&gt;</span><br><span class="line">&gt; r1 启动</span><br><span class="line"></span><br><span class="line">&gt; （复制会话：会话2）</span><br><span class="line">&gt;</span><br><span class="line">&gt; 在node00上：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 在root目录下</span><br><span class="line">&gt;</span><br><span class="line">&gt; 命令：echo hello bigdata &gt;&gt;flume.log</span><br><span class="line"></span><br><span class="line">&gt; 之后在会话2上</span><br><span class="line">&gt;</span><br><span class="line">&gt; `logger本地控制台打印：`</span><br><span class="line">&gt;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>19/01/18 12:03:23 INFO sink.LoggerSink:<br>Event:<br>{ headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61  hello bigdata }<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 4、netcat–hdfs(监听数据，传到hdfs上)</span><br><span class="line"></span><br><span class="line">#### （1）vim netcat_hdfs</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h1 id="a1-which-ones-we-want-to-activate"><a href="#a1-which-ones-we-want-to-activate" class="headerlink" title="a1 which ones we want to activate."></a>a1 which ones we want to activate.</h1><p>a1.channels = c1<br>a1.sources = r1<br>a1.sinks = k1</p>
<p>a1.sources.r1.type = netcat<br>a1.sources.r1.bind = node00<br>a1.sources.r1.port = 41414</p>
<p>a1.sinks.k1.type = hdfs<br>a1.sinks.k1.hdfs.path = hdfs://Sunrise/myflume/%y-%m-%d<br>a1.sinks.k1.hdfs.useLocalTimeStamp=true</p>
<h1 id="Define-a-memory-channel-called-c1-on-a1"><a href="#Define-a-memory-channel-called-c1-on-a1" class="headerlink" title="Define a memory channel called c1 on a1"></a>Define a memory channel called c1 on a1</h1><p>a1.channels.c1.type = memory</p>
<p>#默认值，可省</p>
<p>#a1.channels.c1.capacity = 1000</p>
<p>#a1.channels.c1.transactionCapacity = 100</p>
<h1 id="Define-an-Avro-source-called-r1-on-a1-and-tell-it"><a href="#Define-an-Avro-source-called-r1-on-a1-and-tell-it" class="headerlink" title="Define an Avro source called r1 on a1 and tell it"></a>Define an Avro source called r1 on a1 and tell it</h1><p>a1.sources.r1.channels = c1<br>a1.sinks.k1.channel = c1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### (2)操作</span><br><span class="line"></span><br><span class="line">在node00的会话1上</span><br><span class="line"></span><br><span class="line">启动</span><br><span class="line"></span><br><span class="line">&gt; 在node00上：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 在netcat_hdfs文件所在的目录下</span><br><span class="line">&gt;</span><br><span class="line">&gt; 命令：flume-ng agent  --conf-file netcat_hdfs --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">显示：</span><br></pre></td></tr></table></figure></p>
<p>19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started<br>19/01/18 14:34:44 INFO node.Application: Starting Sink k1<br>19/01/18 14:34:44 INFO node.Application: Starting Source r1<br>19/01/18 14:34:44 INFO source.NetcatSource: Source starting<br>19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.<br>19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started<br>19/01/18 14:34:44 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/192.168.198.128:41414]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在node00的会话2上</span><br><span class="line"></span><br><span class="line">启动</span><br><span class="line"></span><br><span class="line">&gt; telnet node00 41414</span><br><span class="line"></span><br><span class="line">显示</span><br></pre></td></tr></table></figure>
<p>Trying 192.168.198.128…<br>Connected to node00.<br>Escape character is ‘^]’.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">输入任意内容</span><br><span class="line"></span><br><span class="line">在node00会话1端会显示</span><br></pre></td></tr></table></figure>
<p>19/01/18 14:36:50 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false<br>19/01/18 14:36:51 INFO hdfs.BucketWriter: Creating hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp<br>19/01/18 14:37:29 INFO hdfs.BucketWriter: Closing hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp<br>19/01/18 14:37:29 INFO hdfs.BucketWriter: Renaming hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp to hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259<br>19/01/18 14:37:29 INFO hdfs.HDFSEventSink: Writer callback called.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在HDF分布式系统上会显示，生成的文件</span><br><span class="line"></span><br><span class="line">![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4acaz5fj30u10blmzc.jpg)</span><br><span class="line"></span><br><span class="line">![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4bsv8d2j30wd08k74i.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">`注意：`</span><br><span class="line"></span><br><span class="line">这种情况会在hdfs上生成很多小文件，</span><br><span class="line"></span><br><span class="line">[在官网](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html)</span><br><span class="line"></span><br><span class="line">#### HDFS Sink[¶](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html#hdfs-sink)</span><br><span class="line"></span><br><span class="line">有很多关于文件生成过程中的配置</span><br><span class="line"></span><br><span class="line">| Name                   | Default      | Description                                                  |</span><br><span class="line">| ---------------------- | ------------ | ------------------------------------------------------------ |</span><br><span class="line">| **channel**            | –            |                                                              |</span><br><span class="line">| **type**               | –            | The component type name, needs to be `hdfs`                  |</span><br><span class="line">| **hdfs.path**          | –            | HDFS directory path (eg hdfs://namenode/flume/webdata/)      |</span><br><span class="line">| hdfs.filePrefix        | FlumeData    | Name prefixed to files created by Flume in hdfs directory    |</span><br><span class="line">| hdfs.fileSuffix        | –            | Suffix to append to file (eg `.avro` - *NOTE: period is not automatically added*) |</span><br><span class="line">| hdfs.inUsePrefix       | –            | Prefix that is used for temporal files that flume actively writes into |</span><br><span class="line">| hdfs.inUseSuffix       | `.tmp`       | Suffix that is used for temporal files that flume actively writes into |</span><br><span class="line">| hdfs.rollInterval      | 30           | Number of seconds to wait before rolling current file (0 = never roll based on time interval) |</span><br><span class="line">| hdfs.rollSize          | 1024         | File size to trigger roll, in bytes (0: never roll based on file size) |</span><br><span class="line">| hdfs.rollCount         | 10           | Number of events written to file before it rolled (0 = never roll based on number of events) |</span><br><span class="line">| hdfs.idleTimeout       | 0            | Timeout after which inactive files get closed (0 = disable automatic closing of idle files) |</span><br><span class="line">| hdfs.batchSize         | 100          | number of events written to file before it is flushed to HDFS |</span><br><span class="line">| hdfs.codeC             | –            | Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy |</span><br><span class="line">| hdfs.fileType          | SequenceFile | File format: currently `SequenceFile`, `DataStream` or `CompressedStream` (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC |</span><br><span class="line">| hdfs.maxOpenFiles      | 5000         | Allow only this number of open files. If this number is exceeded, the oldest file is closed. |</span><br><span class="line">| hdfs.minBlockReplicas  | –            | Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath. |</span><br><span class="line">| hdfs.writeFormat       | Writable     | Format for sequence file records. One of `Text` or `Writable`. Set to `Text` before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive. |</span><br><span class="line">| hdfs.callTimeout       | 10000        | Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring. |</span><br><span class="line">| hdfs.threadsPoolSize   | 10           | Number of threads per HDFS sink for HDFS IO ops (open, write, etc.) |</span><br><span class="line">| hdfs.rollTimerPoolSize | 1            | Number of threads per HDFS sink for scheduling timed file rolling |</span><br><span class="line">| hdfs.kerberosPrincipal | –            | Kerberos user principal for accessing secure HDFS            |</span><br><span class="line">| hdfs.kerberosKeytab    | –            | Kerberos keytab for accessing secure HDFS                    |</span><br><span class="line">| hdfs.proxyUser         |              |                                                              |</span><br><span class="line">| hdfs.round             | false        | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) |</span><br><span class="line">| hdfs.roundValue        | 1            | Rounded down to the highest multiple of this (in the unit configured using `hdfs.roundUnit`), less than current time. |</span><br><span class="line">| hdfs.roundUnit         | second       | The unit of the round down value - `second`, `minute` or `hour`. |</span><br><span class="line">| hdfs.timeZone          | Local Time   | Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles. |</span><br><span class="line">| hdfs.useLocalTimeStamp | false        | Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. |</span><br><span class="line">| hdfs.closeTries        | 0            | Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart. |</span><br><span class="line">| hdfs.retryInterval     | 180          | Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension. |</span><br><span class="line">| serializer             | `TEXT`       | Other possible options include `avro_event` or the fully-qualified class name of an implementation of the `EventSerializer.Builder` interface. |</span><br><span class="line"></span><br><span class="line">netcat-hdfs </span><br><span class="line"></span><br><span class="line">（配置方式二）</span><br></pre></td></tr></table></figure>
<h1 id="a1-which-ones-we-want-to-activate-1"><a href="#a1-which-ones-we-want-to-activate-1" class="headerlink" title="a1 which ones we want to activate."></a>a1 which ones we want to activate.</h1><p>a1.channels = c1<br>a1.sources = r1<br>a1.sinks = k1</p>
<p>a1.sources.r1.type = avro<br>a1.sources.r1.bind=node01<br>a1.sources.r1.port=55555</p>
<p>a1.sinks.k1.type = hdfs<br>a1.sinks.k1.hdfs.path = hdfs://shsxt/hdfsflume</p>
<h1 id="Define-a-memory-channel-called-c1-on-a1-1"><a href="#Define-a-memory-channel-called-c1-on-a1-1" class="headerlink" title="Define a memory channel called c1 on a1"></a>Define a memory channel called c1 on a1</h1><p>a1.channels.c1.type = memory<br>a1.channels.c1.capacity=1000<br>a1.channels.c1.transactionCapacity = 100</p>
<h1 id="Define-an-Avro-source-called-r1-on-a1-and-tell-it-1"><a href="#Define-an-Avro-source-called-r1-on-a1-and-tell-it-1" class="headerlink" title="Define an Avro source called r1 on a1 and tell it"></a>Define an Avro source called r1 on a1 and tell it</h1><p>a1.sources.r1.channels = c1<br>a1.sinks.k1.channel = c1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 5、结合版（netcat-avro）</span><br><span class="line"></span><br><span class="line">#### （1）vim netcat2_logger</span><br><span class="line"></span><br><span class="line">（node00）</span><br></pre></td></tr></table></figure></p>
<h1 id="example-conf-A-single-node-Flume-configuration-1"><a href="#example-conf-A-single-node-Flume-configuration-1" class="headerlink" title="example.conf: A single-node Flume configuration"></a>example.conf: A single-node Flume configuration</h1><p>#flume-ng agent –conf ./ –conf-file netcat2_logger –name a1 -Dflume.root.logger=INFO,console</p>
<p>#flume-ng –conf conf –conf-file /root/flume_test/netcat_hdfs -n a1 -Dflume.root.logger=INFO,console</p>
<p>#telnet 192.168.235.15 44444</p>
<h1 id="Name-the-components-on-this-agent-1"><a href="#Name-the-components-on-this-agent-1" class="headerlink" title="Name the components on this agent"></a>Name the components on this agent</h1><p> a1.sources = r1<br> a1.sinks = k1<br> a1.channels = c1</p>
<h1 id="Describe-configure-the-source-1"><a href="#Describe-configure-the-source-1" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p> a1.sources.r1.type = netcat<br> a1.sources.r1.bind = node00<br> a1.sources.r1.port = 44444</p>
<h1 id="Describe-the-sink-1"><a href="#Describe-the-sink-1" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p> a1.sinks.k1.type = avro<br> a1.sinks.k1.hostname = node01<br> a1.sinks.k1.port = 60000</p>
<h1 id="Use-a-channel-which-buffers-events-in-memory-1"><a href="#Use-a-channel-which-buffers-events-in-memory-1" class="headerlink" title="Use a channel which buffers events in memory"></a>Use a channel which buffers events in memory</h1><p> a1.channels.c1.type = memory<br> a1.channels.c1.capacity = 1000<br> a1.channels.c1.transactionCapacity = 100</p>
<h1 id="Bind-the-source-and-sink-to-the-channel-1"><a href="#Bind-the-source-and-sink-to-the-channel-1" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p> a1.sources.r1.channels = c1<br> a1.sinks.k1.channel = c1</p>
<p>#—————————</p>
<p>#flume-ng agent –conf-file etect2_logger –name a1 -#Dflume.root.logger=INFO,console</p>
<p>#flume-ng agent –conf conf –conf-file netcat_logger –name a1 -#Dflume.root.logger=INFO,console<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">（node01）</span><br></pre></td></tr></table></figure></p>
<p>#flume-ng agent –conf ./ –conf-file avro2 -n a1<br>a1.sources = r1<br>a1.sinks = k1<br>a1.channels = c1</p>
<p>a1.sources.r1.type = avro<br>a1.sources.r1.bind = node01<br>a1.sources.r1.port = 60000</p>
<p>a1.sinks.k1.type = logger</p>
<h1 id="Use-a-channel-which-buffers-events-in-memory-2"><a href="#Use-a-channel-which-buffers-events-in-memory-2" class="headerlink" title="Use a channel which buffers events in memory"></a>Use a channel which buffers events in memory</h1><p>a1.channels.c1.type = memory<br>a1.channels.c1.capacity = 1000<br>a1.channels.c1.transactionCapacity = 100</p>
<h1 id="Bind-the-source-and-sink-to-the-channel-2"><a href="#Bind-the-source-and-sink-to-the-channel-2" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a1.sources.r1.channels = c1<br>a1.sinks.k1.channel = c1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(2)操作</span><br><span class="line"></span><br><span class="line">&gt; 先启动后面的flume节点node01  ，在启动node00，最后启动node02</span><br><span class="line"></span><br><span class="line">在node01上</span><br><span class="line"></span><br><span class="line">`启动`</span><br><span class="line"></span><br><span class="line">&gt; flume-ng </span><br><span class="line">&gt;</span><br><span class="line">&gt; --conf conf --conf-file avro2 -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">显示</span><br></pre></td></tr></table></figure>
<p>19/01/18 23:22:27 INFO node.Application: Starting Channel c1<br>19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.<br>19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started<br>19/01/18 23:22:28 INFO node.Application: Starting Sink k1<br>19/01/18 23:22:28 INFO node.Application: Starting Source r1<br>19/01/18 23:22:28 INFO source.AvroSource: Starting Avro source r1: { bindAddress: node01, port: 60000 }…<br>19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.<br>19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started<br>19/01/18 23:22:30 INFO source.AvroSource: Avro source r1 started.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在node00上</span><br><span class="line"></span><br><span class="line">`启动`</span><br><span class="line"></span><br><span class="line">&gt; flume-ng agent</span><br><span class="line">&gt;</span><br><span class="line">&gt; --conf ./ --conf-file netcat2_logger --name a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在node02上</span><br><span class="line"></span><br><span class="line">`启动`</span><br><span class="line"></span><br><span class="line">&gt; telnet node00 44444</span><br><span class="line"></span><br><span class="line">然后输入数据文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">最后在</span><br><span class="line"></span><br><span class="line">node01节点上</span><br><span class="line"></span><br><span class="line">显示文件信息</span><br></pre></td></tr></table></figure>
<p>19/01/18 23:33:01 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 0D             hello world. }</p>
<p><code>`</code></p>
<p>flume-ng agent  –conf-file flumeproject –name a1 -Dflume.root.logger=INFO,console</p>
]]></content>
        
        <categories>
            
            <category> flume </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> HDFS </tag>
            
            <tag> Base </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[HBase性能优化]]></title>
        <url>http://sungithup.github.io/2019/01/17/HBase%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
        <content type="html"><![CDATA[<h1 id="HBase性能优化"><a href="#HBase性能优化" class="headerlink" title="HBase性能优化"></a>HBase性能优化</h1><h1 id="（一）、表的设计"><a href="#（一）、表的设计" class="headerlink" title="（一）、表的设计"></a>（一）、表的设计</h1><p>一、Pre-Creating Regions 预分区</p>
<blockquote>
<p>解决海量导入数据时的热点问题</p>
</blockquote>
<p><code>背景：</code></p>
<blockquote>
<p>在创建HBase表的时候默认一张表只有一个region，</p>
<p>所有的put操作都会往这一个region中填充数据，</p>
<p>当这一个region过大时就会进行split。</p>
<p>如果在创建HBase的时候就进行预分区</p>
<p>则会减少当数据量猛增时由于region split带来的资源消耗。</p>
</blockquote>
<p><code>注意：</code></p>
<blockquote>
<p>每个region都有一个startKey和一个endKey来表示该region存储的rowKey范围。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; create &apos;t1&apos;, &apos;cf&apos;, SPLITS =&gt; [&apos;20150501000000000&apos;, &apos;20150515000000000&apos;, &apos;20150601000000000&apos;]</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; create &apos;t2&apos;, &apos;cf&apos;, SPLITS_FILE =&gt; &apos;/home/hadoop/splitfile.txt&apos; </span><br><span class="line"></span><br><span class="line">/home/hadoop/splitfile.txt中存储内容如下： </span><br><span class="line">20150501000000000</span><br><span class="line">20150515000000000</span><br><span class="line">20150601000000000</span><br></pre></td></tr></table></figure>
<p>二、row key</p>
<p>HBase中row key用来检索表中的记录，支持以下三种方式：</p>
<blockquote>
<p>· 通过单个row key访问：即按照某个row key键值进行get操作；</p>
<p>·  通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描；过滤器</p>
<p>·  全表扫描：即直接扫描整张表中所有行记录。</p>
</blockquote>
<h1 id="（二）、写表操作"><a href="#（二）、写表操作" class="headerlink" title="（二）、写表操作"></a>（二）、写表操作</h1><p>一、多HTable客户端并发写</p>
<p>二、HTable参数设置</p>
<p>三、批量写</p>
<p>四、多线程并发写</p>
<h1 id="（三）、读表操作"><a href="#（三）、读表操作" class="headerlink" title="（三）、读表操作"></a>（三）、读表操作</h1><p>一、多HTable客户端并发读</p>
<p>二、HTable参数设置</p>
<p>三、批量读</p>
<p>四、多线程并发读</p>
<p>五、缓存查询结果</p>
<p>六、 Blockcache</p>
]]></content>
        
        <categories>
            
            <category> HBase </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> HDFS </tag>
            
            <tag> HBase </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[HBase学习]]></title>
        <url>http://sungithup.github.io/2019/01/15/HBase%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="Hbase学习"><a href="#Hbase学习" class="headerlink" title="Hbase学习"></a>Hbase学习</h1><p>非关系型数据库</p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration" target="_blank" rel="noopener">官网</a></p>
<h2 id="一、对数据库的-基本了解"><a href="#一、对数据库的-基本了解" class="headerlink" title="一、对数据库的 基本了解"></a>一、对数据库的 基本了解</h2><h3 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h3><blockquote>
<p>基于Hadoop 的分布式数据库</p>
<p>特点：</p>
<p>1、高可靠性</p>
<p>2、高性能</p>
<p>（以上两点：基于分布式的特点）</p>
<p>3、面向列</p>
<p>（以（K,V）存储，有唯一标记的rowkey，value包含是数据库中的列值）</p>
<p>4、可伸缩</p>
<p>（搭建在集群上）</p>
<p>5、实时读写</p>
<p>（用时间戳唯一标记每一版本的数据记录）</p>
</blockquote>
<h3 id="2、工作结构"><a href="#2、工作结构" class="headerlink" title="2、工作结构"></a>2、工作结构</h3><blockquote>
<p>1、利用Hadoop的HDFS作为其文件存储系统</p>
<p>2,利用Hadoop的MapReduce来处理HBase中的海量数据</p>
<p>3,利用Zookeeper作为其分布式协同服务</p>
<p>4,主要用来存储非结构化和半结构化的松散数据（NoSQL非关系型数据库有redis、MongoDB等</p>
</blockquote>
<h3 id="3、关系型数据库"><a href="#3、关系型数据库" class="headerlink" title="3、关系型数据库"></a>3、关系型数据库</h3><blockquote>
<p>1、定义</p>
<p>关系模型指的就是二维表格模型；</p>
<p>而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织</p>
<p>2、三大优点</p>
<ul>
<li>容易理解</li>
<li>使用方便</li>
<li>易于维护</li>
</ul>
<p>3、三大瓶颈</p>
<ul>
<li>高并发读写需求</li>
</ul>
<p>硬盘I/O是一个很大的瓶颈，并且很难能做到数据的强一致性。</p>
<ul>
<li>海量数据的读写性能低</li>
</ul>
<p>在一张包含海量数据的表中查询，效率是非常低的。</p>
<ul>
<li>​    扩展性和可用性差</li>
</ul>
<p>丰富的完整性使得横向扩展把难度加大了</p>
</blockquote>
<h3 id="4、非关系型数据库"><a href="#4、非关系型数据库" class="headerlink" title="4、非关系型数据库"></a>4、非关系型数据库</h3><blockquote>
<p>1、存储格式：key value键值对，文档，图片等等 结构不固定<br>  2、可以减少一些时间和空间的开销，仅需要根据id取出相应的value就可以完成查询。</p>
<p>3、一般不支持ACID特性，无需经过SQL解析，读写性能高</p>
<p> 4、不提供where字段条件过滤</p>
<p>5、难以体现设计的完整性，只适合存储一些较为简单的数据</p>
</blockquote>
<h2 id="二、对HBase的基本里了解"><a href="#二、对HBase的基本里了解" class="headerlink" title="二、对HBase的基本里了解"></a>二、对HBase的基本里了解</h2><h3 id="1、数据结构组成"><a href="#1、数据结构组成" class="headerlink" title="1、数据结构组成"></a>1、数据结构组成</h3><p>（ 1）Row key  :</p>
<blockquote>
<p>唯一标记决定一行数据<br>按照字典排序<br>最大只能存储64KB的字节数据<br>设计非常关键</p>
</blockquote>
<p>（2）Column Family列族 &amp; qualifier列</p>
<blockquote>
<p><code>列族</code>必须作为表模式(schema)定义的一部分预先给出，</p>
<p>表中的每个列都归属于某个列族；</p>
<p>权限控制、存储以及调优都是在列族层面进行的；</p>
<p><code>列名</code>以列族作为前缀，每个“列族”都可以有多个列成员(column)； </p>
<p>新的列可以随后按需、动态加入；</p>
</blockquote>
<p>（3）Cell单元格</p>
<blockquote>
<p> 由行和列的坐标交叉决定； 单元格是有版本的（有时间戳决定）；</p>
<p> 单元格的内容是未解析的字节数组；cell中的数据是没有类型的，全部是字节码形式存贮。</p>
<p>由{rowkey， column( =<family> +<qualifier>)， version} 唯一确定的单元。</qualifier></family></p>
</blockquote>
<p>（4）Timestamp时间戳</p>
<blockquote>
<p>在HBase每个cell存储单元对同一份数据有多个版本，</p>
<p>根据唯一的时间戳来区分每个版本之间的差异，</p>
<p>不同版本的数据按照时间倒序排序，最新的数据版本排在最前面</p>
<p>时间戳的类型是64位整型。<br>时间戳可以由HBase(在数据写入时自动)赋值，精确到毫秒</p>
<p>时间戳。也可以由客户显式赋值，但必须唯一性</p>
</blockquote>
<p>（5）HLog(WAL log)</p>
<blockquote>
<ul>
<li><p>HLog文件就是一个普通的Hadoop SequenceFile</p>
</li>
<li><p>HLog Sequence File的Key是HLogKey对象<br> ​         **HLogKey中记录了写入数据的归属信息，包括table和region名字，sequence number（起始值为0或是最近一次存入文件系统中sequence  number）和timestamp（写入时间）</p>
</li>
<li><p>HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue</p>
</li>
</ul>
<p>​               **存储hbase表的操作记录，KV数据信息   </p>
</blockquote>
<p>2、体系架构</p>
<p>（1）Client</p>
<blockquote>
<p>包含访问HBase的接口并维护cache来加快对HBase的访问</p>
</blockquote>
<p>（2）Zookeeper</p>
<blockquote>
<ul>
<li><p>保证任何时候，集群中只有一个master；</p>
</li>
<li><p>存贮所有Region的寻址入口。</p>
</li>
<li><p>实时监控Region server的上线和下线信息。并实时通知Master</p>
</li>
<li><p>存储HBase的schema和table元数据</p>
</li>
</ul>
</blockquote>
<p>（3）Master</p>
<blockquote>
<p>为Region server分配region；</p>
<ul>
<li><p>负责Region server的负载均衡；</p>
</li>
<li><p>发现失效的Region server并将其上的region重新分配；</p>
</li>
<li><p>管理用户对table的增删改操作；</p>
</li>
</ul>
</blockquote>
<p>（4）RegionServer    </p>
<blockquote>
<ul>
<li><p>维护region，处理对这些region的IO请求</p>
</li>
<li><p>负责切分在运行过程中变得过大的region</p>
</li>
</ul>
</blockquote>
<p>（5）Region</p>
<blockquote>
<ul>
<li><p>保存一个表里面某段连续的数据，每个表一开始只有一个region；</p>
</li>
<li><p>随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）</p>
<p>（HBase自动把表水平划分成多个区域(region)）</p>
</li>
<li><p>当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上</p>
</li>
</ul>
</blockquote>
<p>（6）Memstore与storefile</p>
<blockquote>
<ul>
<li>一个region由2-3store组成，一个store对应一个CF（列族）</li>
<li>store包括位于内存中的memstore和位于磁盘的storefile。</li>
<li>写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile；</li>
<li>当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major<br>compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile</li>
<li>当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡</li>
<li>客户端检索数据，先在memstore找，找不到再找storefile</li>
<li>HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。</li>
<li>每个Strore又由一个memStore和0至多个StoreFile组成,StoreFile以HFile格式保存在HDFS上(HFile)。</li>
</ul>
</blockquote>
<h2 id="三、Hbase-安装部署"><a href="#三、Hbase-安装部署" class="headerlink" title="三、Hbase 安装部署"></a>三、Hbase 安装部署</h2><p>完全分布式搭建</p>
<p>1、安装包准备</p>
<p><a href="http://hbase.apache.org/downloads.html" target="_blank" rel="noopener">Hbase</a>（本文使用0.98版本）</p>
<p>将tar上传至Linux系统，进行解压安装</p>
<p>2、修改配置文件hbase-env.sh（在Hbase的解压目录的conf目录中）</p>
<p>修添加JAVA_HOME环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use.  Java 1.7+ required.</span><br><span class="line"># export JAVA_HOME=/usr/java/jdk1.6.0/</span><br><span class="line">export JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br></pre></td></tr></table></figure>
<p>不使用HBase的默认zookeeper配置，（使用自己的）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not.</span><br><span class="line"> export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>
<p>3、修改配置hbase-site.xml（在Hbase的解压目录的conf目录中）</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="comment">&lt;!--Hdfs配置时的集群名--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://Sunrise:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--zookeeper的三台节点--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>node00,node01,node02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置http访问的port---&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.info.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>60010<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>注意：（会出bug的地方）</code>：</p>
<p>1、问题描述：</p>
<blockquote>
<p>HBase启动时，警告：<br>Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 </p>
</blockquote>
<p>解决方案：</p>
<p><code>原因：</code>由于使用了JDK8 ，需要在HBase的配置文件中hbase-env.sh，注释掉两行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+</span><br><span class="line">#export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m&quot;</span><br><span class="line">#export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m&quot;</span><br></pre></td></tr></table></figure>
<p>重新启动HBase。</p>
<p>2、问题描述：</p>
<blockquote>
<p>配置好HBase后，各项服务正常，但想从浏览器通过端口60010看下节点情况，但是提示拒绝访问</p>
</blockquote>
<p><code>检测：</code>在服务器上netstat -natl|grep 60010 发现并没有60010端口</p>
<p><code>原因：</code>HBase 1.0 之后的版本都需要在hbase-site.xml中配置端口，如下</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.info.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60010<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>重新启动HBase,在浏览器再次访问，就ok了</p>
<p>4、添加配置regionservers 文件（在Hbase的解压目录的conf目录中）</p>
<p>添加配置的regionservers 的主机名</p>
<p>regionservers</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node00</span><br><span class="line">node01</span><br><span class="line">node02</span><br></pre></td></tr></table></figure>
<p>5、添加配置backup-masters</p>
<p>添加配置的master备份的主机名（在Hbase的解压目录的conf目录中）</p>
<p>backup-masters</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node02</span><br></pre></td></tr></table></figure>
<p>6、将Hadoop安装解压目录/etc/hadoop目录下的hdfs-site.xml文件 拷贝到Hbase的解压目录的conf目录中</p>
<p>7、配置环境变量 ~/.bash_profile</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">HADOOP_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">SQOOP_HOME=/usr/soft/sqoop-1.4.6</span><br><span class="line">export PATH=$PATH:$SQOOP_HOME/bin</span><br><span class="line">HBASE_HOME=/usr/soft/hbase-1.2.9</span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure>
<blockquote>
<p>source ~/.bash_profile</p>
</blockquote>
<p>8、将如上配置远程发送至其他节点（Hbase 、 ./bash_profile）</p>
<p>9、各个节点注意要做时间同步</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate cn.ntp.org.cn</span><br></pre></td></tr></table></figure>
<p>10、启动HDFS集群：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br><span class="line">start-hdfs.sh</span><br></pre></td></tr></table></figure>
<p>11、启动：start-hbase.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node00.out</span><br><span class="line">node02: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node02.out</span><br><span class="line">node01: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node01.out</span><br><span class="line">node00: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node00.out</span><br><span class="line">node02: starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node02.out</span><br></pre></td></tr></table></figure>
<p>12、查看进程：jps</p>
<p>13、浏览器访问：node00:60010</p>
<p>14、关闭：stop-hbase.sh</p>
<h2 id="四、通过hbase-shell命令进入HBase-命令行接口"><a href="#四、通过hbase-shell命令进入HBase-命令行接口" class="headerlink" title="四、通过hbase shell命令进入HBase 命令行接口"></a>四、通过hbase shell命令进入HBase 命令行接口</h2><p>通过<code>help</code>可查看所有命令的支持以及帮助手册</p>
<blockquote>
<p>帮助创建</p>
<p>hbase(main):007:0&gt;  help create</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th>Shell命令</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建表</td>
<td>create ‘表名’, ‘列族名1’[,…]</td>
<td>create ‘t1’，‘cf1’</td>
</tr>
<tr>
<td style="text-align:center">列出所有表</td>
<td>list</td>
<td>list</td>
</tr>
<tr>
<td style="text-align:center">添加记录</td>
<td>put ‘表名’, ‘RowKey’, ‘列族名称:列名’, ‘值’</td>
<td>put        ‘t1’,‘rk_00101’,‘cf1:name’,‘zs’</td>
</tr>
<tr>
<td style="text-align:center">查看记录</td>
<td>get ‘表名’, ‘RowKey’, ‘列族名称:列名’</td>
<td>get ‘t1’,‘rk_00101’                         get ‘t1’,‘rk_00101’,‘cf1:name’</td>
</tr>
<tr>
<td style="text-align:center">查看表中                           记录总数</td>
<td>count  ‘表名’</td>
<td>count ‘t1’</td>
</tr>
<tr>
<td style="text-align:center">删除记录</td>
<td>delete  ‘表名’   , ‘RowKey’,   ‘列族名称:列名’</td>
<td>delete ‘t1’,‘rk_00101’,‘cf1:name’</td>
</tr>
<tr>
<td style="text-align:center">删除一张表</td>
<td>先要屏蔽该表，才能对该表进行删除。                                           第一步 disable   ‘表名称’                                                                         第二步 drop   ‘表名称’</td>
<td>disable ‘t1’                                                      drop ‘t1’</td>
</tr>
<tr>
<td style="text-align:center">查看所有              记录</td>
<td>scan   ‘表名 ‘</td>
<td>scan ‘t1’</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td>create ‘t2’, {NAME =&gt; ‘cf1’, VERSIONS =&gt; 2}, METADATA =&gt; { ‘mykey’ =&gt; ‘myvalue’ }</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">查看未加工的数据中指定版本记录</td>
<td>scan ‘t1’, {RAW =&gt; true, VERSIONS =&gt; 3}                           raw  未加工的</td>
<td>3</td>
</tr>
<tr>
<td style="text-align:center">查看保存   版本记录</td>
<td>scan ‘t1’, {VERSIONS =&gt; 2}</td>
<td>2</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="五、HBase优化"><a href="#五、HBase优化" class="headerlink" title="五、HBase优化"></a>五、HBase优化</h2><p><code>详见HBase性能优化文档</code></p>
]]></content>
        
        <categories>
            
            <category> Hbase </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> BigData </tag>
            
            <tag> Hadoop </tag>
            
            <tag> NoSQL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Hive优化]]></title>
        <url>http://sungithup.github.io/2019/01/14/Hive%E4%BC%98%E5%8C%96/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="Hive优化"><a href="#Hive优化" class="headerlink" title="Hive优化"></a>Hive优化</h1><h2 id="一、核心思想："><a href="#一、核心思想：" class="headerlink" title="一、核心思想："></a>一、核心思想：</h2><blockquote>
<p>把Hive SQL 当做MapReduce程序进行优化</p>
</blockquote>
<p><code>注意：</code>以下不能HQL转化为Mapreduce任务运行</p>
<p>—select 仅查询本表字段</p>
<p>—where 仅对本表字段做条件过滤</p>
<h2 id="二、explain"><a href="#二、explain" class="headerlink" title="二、explain"></a>二、explain</h2><blockquote>
<p>用以显示任务执行计划</p>
<p>格式：</p>
<p>EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query</p>
</blockquote>
<p><code>语法解释</code></p>
<blockquote>
<p>从语法组成可以看出来是一个“explain ”+三个可选参数+查询语句。大家可以积极尝试一下，后面两个显示内容很简单的，我介绍一下第一个 extended 这个可以显示hql语句的语法树</p>
<p>其次，执行计划一共有三个部分：</p>
<ul>
<li>这个语句的抽象语法树</li>
<li><p>这个计划不同阶段之间的依赖关系</p>
</li>
<li><p>对于每个阶段的详细描述</p>
</li>
</ul>
</blockquote>
<p><code>例子：</code></p>
<blockquote>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; hive&gt; explain select * from log;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><code>拓展</code>课下查询MySQl的执行计划。</p>
<h2 id="三、Hive运行方式"><a href="#三、Hive运行方式" class="headerlink" title="三、Hive运行方式"></a>三、Hive运行方式</h2><h3 id="集群模式："><a href="#集群模式：" class="headerlink" title="集群模式："></a>集群模式：</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">执行hql：</span><br><span class="line">hive&gt; select count(*) from log;</span><br></pre></td></tr></table></figure>
<p><code>结论：</code></p>
<blockquote>
<p>函数（如count）是在reduce阶段进行<br>默认提交到yarn所在的节点上运行，</p>
</blockquote>
<hr>
<h3 id="优化一"><a href="#优化一" class="headerlink" title="优化一:"></a>优化一:</h3><p>设置  本地模式（运行速度加快。但对加载文件有限制）</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;set hive.exec.mode.local.auto=true;</span><br><span class="line"></span><br><span class="line">查看：</span><br><span class="line">hive&gt;set hive.exec.mode.local.auto</span><br></pre></td></tr></table></figure>
<p><code>但是</code>如果加载文件的最大值大于配置（默认配置为100M），仍会使用集群模式运行（在yarn所在的节点）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看最大加载文件</span><br><span class="line">hive&gt; set hive.exec.mode.local.auto.inputbytes.max;</span><br><span class="line"></span><br><span class="line">显示：</span><br><span class="line">hive.exec.mode.local.auto.inputbytes.max=134217728</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="优化二："><a href="#优化二：" class="headerlink" title="优化二："></a>优化二：</h3><p>设置 严格模式</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通过设置以下参数开启严格模式[防止误操作]：</span><br><span class="line">hive&gt; set hive.mapred.mode=strict;</span><br><span class="line">（默认为：nonstrict非严格模式）</span><br></pre></td></tr></table></figure>
<p><code>但是</code>存在查询限制</p>
<blockquote>
<p>1、对分区表查询时，必须添加where对于分区字段的条件过滤；</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; hive&gt; select * from day_table where dt='2019-01-13';</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>2、order by语句必须包含limit输出限制；</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; hive&gt; select * from log order by id limit 1;</span><br><span class="line">&gt; 这里的1， 表示显示前多少条记录，只能设一个数字</span><br><span class="line">&gt; 和Mysql（可以从0 开始）不同的是，它只能从1开始</span><br><span class="line">&gt; mysql可以有两个数字，表示从第几条开始，显示几条</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>3、限制执行笛卡尔积的查询</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzhry9ni0xj30m10cjdgp.jpg" alt="imit"></p>
<h2 id="四、Hive排序"><a href="#四、Hive排序" class="headerlink" title="四、Hive排序"></a>四、Hive排序</h2><h3 id="1、Order-By"><a href="#1、Order-By" class="headerlink" title="1、Order By"></a>1、Order By</h3><p>— 对于查询结果做<code>全局</code>排序，只允许有<code>一个</code>reduce处理<br>（当数据量较大时，reduce数量有限，应慎用。</p>
<p>​     严格模式下，必须结合limit来使用）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from log order by id；</span><br></pre></td></tr></table></figure>
<h3 id="2、Sort-By"><a href="#2、Sort-By" class="headerlink" title="2、Sort By"></a>2、Sort By</h3><p>– 对于<code>单个</code>reduce的数据进行排序</p>
<p>–局部（单个reduce）有序，全局无序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以通过设置mapred.reduce.tasks的值来控制reduce的数，然后对reduce输出的结果做二次排序</span><br></pre></td></tr></table></figure>
<p><code>案例</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">sort</span> <span class="keyword">by</span> <span class="keyword">id</span>;       (结果无序)</span><br></pre></td></tr></table></figure>
<p><code>显示</code></p>
<blockquote>
<p>Time taken: 147.077 seconds, Fetched: 7 row(s)</p>
</blockquote>
<h3 id="3、Distribute-By"><a href="#3、Distribute-By" class="headerlink" title="3、Distribute By"></a>3、Distribute By</h3><p>– 分区排序，经常和 Sort By 结合使用 全局有序，局部无序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">id</span>;     （结果无序）</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Time taken: 144.708 seconds, Fetched: 7 row(s)</p>
</blockquote>
<p><code>注意：</code>hive要求DISTRIBUTE BY语句出现在SORT BY语句之前</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Distribute By可以将Map阶段输出的数据按指定的字段划分到不同的reduce文件中，然后，sort by 对reduce阶段的输出数据做排序。</span><br></pre></td></tr></table></figure>
<p><code>案例:</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">distribute</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">sort</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">asc</span>;   （结果无序）</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from (select * from log distribute by id) a sort by a.id ;s</span><br></pre></td></tr></table></figure>
<p><code>改良</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.* <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> cluster <span class="keyword">by</span> <span class="keyword">id</span> ) a <span class="keyword">order</span> <span class="keyword">by</span> a.id <span class="keyword">limit</span> <span class="number">9</span> ; (结果有序)</span><br><span class="line"></span><br><span class="line">9 在这里是表中数据记录的总条数</span><br></pre></td></tr></table></figure>
<p>显示：</p>
<blockquote>
<p> Time taken: 234.593 seconds, Fetched: 7 row(s)</p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">id</span> <span class="keyword">limit</span> <span class="number">9</span>;    （结果有序）</span><br></pre></td></tr></table></figure>
<p>显示：</p>
<blockquote>
<p> Time taken: 102.065 seconds, Fetched: 7 row(s)</p>
</blockquote>
<h3 id="4、Cluster-By"><a href="#4、Cluster-By" class="headerlink" title="4、Cluster By"></a>4、Cluster By</h3><p>– 相当于 Sort By + Distribute By<br>（Cluster By不能通过asc、desc的方式指定排序规则；<br>可通过 distribute by column sort by column asc|desc 的方式）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> cluster <span class="keyword">by</span> <span class="keyword">id</span>；    （结果无序）</span><br></pre></td></tr></table></figure>
<h2 id="五、Hive-Join-（重难点）"><a href="#五、Hive-Join-（重难点）" class="headerlink" title="五、Hive Join  （重难点）"></a>五、Hive Join  （重难点）</h2><h3 id="1、Join-连接时，将小表（驱动表）放在join的左边"><a href="#1、Join-连接时，将小表（驱动表）放在join的左边" class="headerlink" title="1、Join 连接时，将小表（驱动表）放在join的左边"></a>1、Join 连接时，将小表（驱动表）放在join的左边</h3><h3 id="2、Map-Join-："><a href="#2、Map-Join-：" class="headerlink" title="2、Map Join ："></a>2、Map Join ：</h3><blockquote>
<p>因为Map Join 是在Map端且在内存中进行的，所以不需要启动Reduce任务，也没有shuffle阶段，从而在一定程度上节省资源，提高Join效率。</p>
</blockquote>
<h3 id="方式：（两种）"><a href="#方式：（两种）" class="headerlink" title="方式：（两种）"></a>方式：（两种）</h3><h4 id="1、SQL方式："><a href="#1、SQL方式：" class="headerlink" title="1、SQL方式："></a>1、SQL方式：</h4><p>​     在HQl语句中添加MapJoin标记（mapjoin）(将小表加入到内存，注意小表的大小)</p>
<p>​     语法：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(smallTable) */</span>  smallTable.key,  bigTable.value </span><br><span class="line"><span class="keyword">FROM</span>  smallTable  <span class="keyword">JOIN</span>  bigTable  <span class="keyword">ON</span>  smallTable.key  = bigTable.key;</span><br></pre></td></tr></table></figure>
<p><code>案例：</code></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(log1) */</span>  log.id,log1.name </span><br><span class="line"><span class="keyword">FROM</span>  <span class="keyword">log</span>  <span class="keyword">JOIN</span>  log1  <span class="keyword">ON</span>  log.id  = log1.id;</span><br></pre></td></tr></table></figure>
<h4 id="2、自动的MapJoin"><a href="#2、自动的MapJoin" class="headerlink" title="2、自动的MapJoin"></a>2、自动的MapJoin</h4><p>​           通过修改以下配置启用自动的mapjoin：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.auto.convert.join = true;</span><br></pre></td></tr></table></figure>
<p>​    （  该参数为true时，Hive自动对左边的表统计数据量，如果是小表就加入内存，即对小表使用Map join）<br>其他相关配置参数：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.mapjoin.smalltable.filesize;</span><br></pre></td></tr></table></figure>
<p>（默认：大表小表判断的阈值25MB左右，如果表的大小小于该值则会被加载到内存中运行，可自定义）</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.ignore.mapjoin.hint;</span><br></pre></td></tr></table></figure>
<p>（默认值：true；是否忽略mapjoin hint 即mapjoin标记；如果为false，这则需要添加-MapJoin标记，mapjoin（smalltable））</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.auto.convert.join.noconditionaltask;</span><br></pre></td></tr></table></figure>
<p>（默认值：true；将普通的join转化为普通的mapjoin时，是否将多个mapjoin转化为一个mapjoin）</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.auto.convert.join.noconditionaltask.size;</span><br></pre></td></tr></table></figure>
<p>（默认：10M；将多个mapjoin转化为一个mapjoin时，其表的最大值为10M，可自定义）</p>
<h2 id="六、Map-Side聚合"><a href="#六、Map-Side聚合" class="headerlink" title="六、Map-Side聚合"></a>六、Map-Side聚合</h2><blockquote>
<p>相当于聚合函数：count（）</p>
</blockquote>
<p>设置参数，开启在Map端的聚合</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>
<p>相关配置参数：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval；</span><br></pre></td></tr></table></figure>
<p>（默认为：100000，表示 map端group by执行聚合时处理的多少行数据）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.min.reduction；</span><br></pre></td></tr></table></figure>
<p>（默认为：0.5，进行聚合的最小比例，预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.percentmemory;</span><br></pre></td></tr></table></figure>
<p>（默认： 0.5 ，map端聚合使用的内存的最大值）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr.hash.force.flush.memory.threshold;</span><br></pre></td></tr></table></figure>
<p>（默认为：0.9，map端做聚合操作是hash表的最大可用内容，大于该值则会触发flush</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.groupby.skewindata；</span><br></pre></td></tr></table></figure>
<p>（默认为：false，是否对GroupBy产生的数据倾斜做优化）</p>
<p><code>附加：</code></p>
<blockquote>
<ul>
<li>数据倾斜问题解决：多种方式（使用MapJoin、使用MapSide）</li>
</ul>
</blockquote>
<p><code>参考</code></p>
<p><a href="http://www.sohu.com/a/224276626_543508" target="_blank" rel="noopener">http://www.sohu.com/a/224276626_543508</a></p>
<h2 id="七、控制Hive中Map和Reduce的数量"><a href="#七、控制Hive中Map和Reduce的数量" class="headerlink" title="七、控制Hive中Map和Reduce的数量"></a>七、控制Hive中Map和Reduce的数量</h2><h3 id="1、Map数量相关的参数"><a href="#1、Map数量相关的参数" class="headerlink" title="1、Map数量相关的参数"></a>1、Map数量相关的参数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size;</span><br></pre></td></tr></table></figure>
<p>（默认为：256M，一个split的最大值，即每个map处理文件的最大值）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node;</span><br></pre></td></tr></table></figure>
<p>(一个节点上最小split数：1个)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack;</span><br></pre></td></tr></table></figure>
<p>(一个机架上最小split数：1个)</p>
<h3 id="2、Reduce数量相关的参数"><a href="#2、Reduce数量相关的参数" class="headerlink" title="2、Reduce数量相关的参数"></a>2、Reduce数量相关的参数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>
<p>(默认为：-1，强制指定reduce任务的数量。-1，是未定义，不发挥作用。如果指定了，就会按指定的数量执行)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;</span><br></pre></td></tr></table></figure>
<p>（默认为：256M ，每个reduce任务处理的数据量）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;</span><br></pre></td></tr></table></figure>
<p>（默认为：1009个，每个任务最大的reduce数 [Map数量 &gt;= Reduce数量 ]）</p>
<h2 id="八、Hive-JVM重用"><a href="#八、Hive-JVM重用" class="headerlink" title="八、Hive - JVM重用"></a>八、Hive - JVM重用</h2><p><code>适用场景：</code><br>1、小文件个数过多<br>2、task个数过多</p>
<p><code>原理：</code></p>
<p>hadoop默认配置是使用派生JVM来执行map和reduce任务的，JVM重用可以使得JVM实例在同一个JOB中重新使用N次</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.job.reuse.jvm.num.tasks;</span><br></pre></td></tr></table></figure>
<p>(默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM)</p>
<p><code>缺点：</code></p>
<p>设置开启之后，task插槽会一直占用资源，不论是否有task运行，<br>直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！</p>
]]></content>
        
        <categories>
            
            <category> Hive </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> HDFS </tag>
            
            <tag> 数据仓库 </tag>
            
            <tag> MapReduce </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Nginx入门学习（第一回合）]]></title>
        <url>http://sungithup.github.io/2019/01/12/Nginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E5%9B%9E%E5%90%88%EF%BC%89/</url>
        <content type="html"><![CDATA[<h1 id="Nginx入门学习（第一回合）"><a href="#Nginx入门学习（第一回合）" class="headerlink" title="Nginx入门学习（第一回合）"></a>Nginx入门学习（第一回合）</h1><p>[^开发者]: 由俄罗斯的程序设计师Igor Sysoev所开发</p>
<p>[TOC]</p>
<h2 id="一、产生背景"><a href="#一、产生背景" class="headerlink" title="一、产生背景"></a>一、产生背景</h2><p>1.巨大流量—海量的并发访问</p>
<p>2.单台服务器资源和能力有限</p>
<p>引发服务器宕机而无法提供服务</p>
<h2 id="二、负载均衡-Load-Balance"><a href="#二、负载均衡-Load-Balance" class="headerlink" title="二、负载均衡(Load Balance)"></a>二、负载均衡(Load Balance)</h2><h3 id="1、高并发"><a href="#1、高并发" class="headerlink" title="1、高并发"></a>1、高并发</h3><p>（1）、高（大量的）</p>
<p>（2）、并发就是可以使用多个线程或者多个进程，同时处理（就是并发）不同的操作</p>
<p>（3）、简而言之就是每秒内有多少个请求同时访问。</p>
<h3 id="2、负载均衡"><a href="#2、负载均衡" class="headerlink" title="2、负载均衡"></a>2、负载均衡</h3><p>（1）、将请求/数据【均匀】分摊到多个操作单元上执行</p>
<p>（2）、关键在于【均匀】,也是分布式系统架构设计中必须考虑的因素之一。</p>
<h3 id="3、互联网分布式架构"><a href="#3、互联网分布式架构" class="headerlink" title="3、互联网分布式架构"></a>3、互联网分布式架构</h3><p>常见，分为客户端层、反向代理nginx层、站点层、服务层、数据层。只需要实现“将请求/数据 均匀分摊到多个操作单元上执行”，就能实现负载均衡。</p>
<h2 id="三、对Nginx的基本了解"><a href="#三、对Nginx的基本了解" class="headerlink" title="三、对Nginx的基本了解"></a>三、对Nginx的基本了解</h2><h3 id="1、什么是Nginx？"><a href="#1、什么是Nginx？" class="headerlink" title="1、什么是Nginx？"></a>1、什么是Nginx？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一款轻量级的Web 服务器/反向代理服务器【后面有介绍】及电子邮件（IMAP/POP3）代理服务器。</span><br></pre></td></tr></table></figure>
<p>​     <code>特点</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">*是占有内存少，CPU、内存等资源消耗却非常低，</span><br><span class="line">*运行非常稳定并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。</span><br></pre></td></tr></table></figure>
<h3 id="2、Nginx-VS-Apache"><a href="#2、Nginx-VS-Apache" class="headerlink" title="2、Nginx   VS   Apache"></a>2、Nginx   VS   Apache</h3><h1 id="（1）、nginx相对于apache的优点："><a href="#（1）、nginx相对于apache的优点：" class="headerlink" title="（1）、nginx相对于apache的优点："></a>（1）、nginx相对于apache的优点：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">*轻量级，同样起web 服务，比apache 占用更少的内存及资源高并发，</span><br><span class="line">*nginx 处理请求是异步非阻塞（如前端ajax）的，而apache 则是阻塞型的，</span><br><span class="line">*在高并发下nginx能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单</span><br><span class="line">*Nginx 配置简洁, Apache 复杂</span><br></pre></td></tr></table></figure>
<h1 id="（2）、apache-相对于nginx-的优点："><a href="#（2）、apache-相对于nginx-的优点：" class="headerlink" title="（2）、apache 相对于nginx 的优点："></a>（2）、apache 相对于nginx 的优点：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* Rewrite重写 ，比nginx 的rewrite 强大模块超多，</span><br><span class="line">*基本想到的都可以找到少bug ，nginx 的bug 相对较多。（出身好起步高）</span><br></pre></td></tr></table></figure>
<h2 id="四、安装Nginx"><a href="#四、安装Nginx" class="headerlink" title="四、安装Nginx"></a>四、安装Nginx</h2><p><code>这里以安装tengine为例</code></p>
<h3 id="1、安装之前准备"><a href="#1、安装之前准备" class="headerlink" title="1、安装之前准备"></a>1、安装之前准备</h3><p>配置依赖 gcc openssl-devel pcre-devel zlib-devel</p>
<p> 安装：</p>
<blockquote>
<p>yum install gcc openssl-devel pcre-devel zlib-devel -y</p>
</blockquote>
<h3 id="2、下载"><a href="#2、下载" class="headerlink" title="2、下载"></a>2、下载</h3><p>（目前最新版）：<a href="http://tengine.taobao.org/download.html" target="_blank" rel="noopener">tengine-2.2.3.tar</a></p>
<h3 id="3、-解压缩"><a href="#3、-解压缩" class="headerlink" title="3、 解压缩"></a>3、 解压缩</h3><blockquote>
<p>tar  -zvxf   tengine-2.2.3.tar </p>
</blockquote>
<h3 id="4、安装Nginx"><a href="#4、安装Nginx" class="headerlink" title="4、安装Nginx"></a>4、安装Nginx</h3><blockquote>
<p>在Nginx解压的目录下运行：</p>
<p>./configure</p>
<p>make &amp;&amp; make install</p>
</blockquote>
<p>默认安装目录：<br>/usr/local/nginx</p>
<h3 id="5、配置Nginx为系统服务，以方便管理"><a href="#5、配置Nginx为系统服务，以方便管理" class="headerlink" title="5、配置Nginx为系统服务，以方便管理"></a>5、配置Nginx为系统服务，以方便管理</h3><h4 id="（1）、在-etc-rc-d-init-d-目录中建立文本文件nginx"><a href="#（1）、在-etc-rc-d-init-d-目录中建立文本文件nginx" class="headerlink" title="（1）、在/etc/rc.d/init.d/目录中建立文本文件nginx"></a>（1）、在/etc/rc.d/init.d/目录中建立文本文件nginx</h4><h4 id="（2）、在文件中粘贴下面的内容："><a href="#（2）、在文件中粘贴下面的内容：" class="headerlink" title="（2）、在文件中粘贴下面的内容："></a>（2）、在文件中粘贴下面的内容：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">#</span><br><span class="line"># nginx - this script starts and stops the nginx daemon</span><br><span class="line">#</span><br><span class="line"># chkconfig:   - 85 15 </span><br><span class="line"># description:  Nginx is an HTTP(S) server, HTTP(S) reverse \</span><br><span class="line">#               proxy and IMAP/POP3 proxy server</span><br><span class="line"># processname: nginx</span><br><span class="line"># config:      /etc/nginx/nginx.conf</span><br><span class="line"># config:      /etc/sysconfig/nginx</span><br><span class="line"># pidfile:     /var/run/nginx.pid</span><br><span class="line"> </span><br><span class="line"># Source function library.</span><br><span class="line">. /etc/rc.d/init.d/functions</span><br><span class="line"> </span><br><span class="line"># Source networking configuration.</span><br><span class="line">. /etc/sysconfig/network</span><br><span class="line"> </span><br><span class="line"># Check that networking is up.</span><br><span class="line">[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0</span><br><span class="line"> </span><br><span class="line">nginx=&quot;/usr/local/nginx/sbin/nginx&quot;</span><br><span class="line">prog=$(basename $nginx)</span><br><span class="line"> </span><br><span class="line">NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot;</span><br><span class="line"> </span><br><span class="line">[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx</span><br><span class="line"> </span><br><span class="line">lockfile=/var/lock/subsys/nginx</span><br><span class="line"> </span><br><span class="line">make_dirs() &#123;</span><br><span class="line">   # make required directories</span><br><span class="line">   user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -`</span><br><span class="line">   options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;`</span><br><span class="line">   for opt in $options; do</span><br><span class="line">       if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then</span><br><span class="line">           value=`echo $opt | cut -d &quot;=&quot; -f 2`</span><br><span class="line">           if [ ! -d &quot;$value&quot; ]; then</span><br><span class="line">               # echo &quot;creating&quot; $value</span><br><span class="line">               mkdir -p $value &amp;&amp; chown -R $user $value</span><br><span class="line">           fi</span><br><span class="line">       fi</span><br><span class="line">   done</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">start() &#123;</span><br><span class="line">    [ -x $nginx ] || exit 5</span><br><span class="line">    [ -f $NGINX_CONF_FILE ] || exit 6</span><br><span class="line">    make_dirs</span><br><span class="line">    echo -n $&quot;Starting $prog: &quot;</span><br><span class="line">    daemon $nginx -c $NGINX_CONF_FILE</span><br><span class="line">    retval=$?</span><br><span class="line">    echo</span><br><span class="line">    [ $retval -eq 0 ] &amp;&amp; touch $lockfile</span><br><span class="line">    return $retval</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">stop() &#123;</span><br><span class="line">    echo -n $&quot;Stopping $prog: &quot;</span><br><span class="line">    killproc $prog -QUIT</span><br><span class="line">    retval=$?</span><br><span class="line">    echo</span><br><span class="line">    [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile</span><br><span class="line">    return $retval</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">restart() &#123;</span><br><span class="line">    configtest || return $?</span><br><span class="line">    stop</span><br><span class="line">    sleep 1</span><br><span class="line">    start</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">reload() &#123;</span><br><span class="line">    configtest || return $?</span><br><span class="line">    echo -n $&quot;Reloading $prog: &quot;</span><br><span class="line">    killproc $nginx -HUP</span><br><span class="line">    RETVAL=$?</span><br><span class="line">    echo</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">force_reload() &#123;</span><br><span class="line">    restart</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">configtest() &#123;</span><br><span class="line">  $nginx -t -c $NGINX_CONF_FILE</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">rh_status() &#123;</span><br><span class="line">    status $prog</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">rh_status_q() &#123;</span><br><span class="line">    rh_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">    start)</span><br><span class="line">        rh_status_q &amp;&amp; exit 0</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    stop)</span><br><span class="line">        rh_status_q || exit 0</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    restart|configtest)</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    reload)</span><br><span class="line">        rh_status_q || exit 7</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    force-reload)</span><br><span class="line">        force_reload</span><br><span class="line">        ;;</span><br><span class="line">    status)</span><br><span class="line">        rh_status</span><br><span class="line">        ;;</span><br><span class="line">    condrestart|try-restart)</span><br><span class="line">        rh_status_q || exit 0</span><br><span class="line">            ;;</span><br><span class="line">    *)</span><br><span class="line">        echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot;</span><br><span class="line">        exit 2</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<h4 id="（3）、修改nginx文件的执行权限"><a href="#（3）、修改nginx文件的执行权限" class="headerlink" title="（3）、修改nginx文件的执行权限"></a>（3）、修改nginx文件的执行权限</h4><blockquote>
<p>chmod +x nginx</p>
</blockquote>
<h4 id="（4）、添加该文件到系统服务中去"><a href="#（4）、添加该文件到系统服务中去" class="headerlink" title="（4）、添加该文件到系统服务中去"></a>（4）、添加该文件到系统服务中去</h4><blockquote>
<p>chkconfig –add nginx</p>
</blockquote>
<p>查看是否添加成功</p>
<blockquote>
<p>chkconfig –list nginx</p>
</blockquote>
<p>启动，停止，重新装载</p>
<blockquote>
<p>service nginx start|stop</p>
</blockquote>
<h2 id="五、Nginx配置"><a href="#五、Nginx配置" class="headerlink" title="五、Nginx配置"></a>五、Nginx配置</h2><h3 id="1、查看配置"><a href="#1、查看配置" class="headerlink" title="1、查看配置"></a>1、查看配置</h3><blockquote>
<p>cd   /usr/local/nginx/conf</p>
<p>vim   nginx.conf</p>
</blockquote>
<h3 id="2、配置解析"><a href="#2、配置解析" class="headerlink" title="2、配置解析"></a>2、配置解析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#进程数，建议设置和CPU个数一样或2倍</span><br><span class="line">worker_processes  2;</span><br><span class="line"></span><br><span class="line">#日志级别</span><br><span class="line">error_log  logs/error.log  warning;(默认error级别)</span><br><span class="line"></span><br><span class="line"># nginx 启动后的pid 存放位置</span><br><span class="line">#pid        logs/nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">	#配置每个进程的连接数，总的连接数= worker_processes * worker_connections</span><br><span class="line">    #默认1024</span><br><span class="line">    worker_connections  10240;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">#连接超时时间，单位秒</span><br><span class="line">keepalive_timeout  65;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost                 </span><br><span class="line">        #默认请求</span><br><span class="line">  		location / &#123;</span><br><span class="line">    				 root  html;   #定义服务器的默认网站根目录位置</span><br><span class="line">   				  index  index.php index.html index.htm;  #定义首页索引文件的名称</span><br><span class="line">        &#125;</span><br><span class="line">	    #定义错误提示页面</span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h3 id="3、负载均衡配置"><a href="#3、负载均衡配置" class="headerlink" title="3、负载均衡配置"></a>3、负载均衡配置</h3><p>1）安装Tomcat，参考 <code>Tomcat配置</code></p>
<h4 id="2）多负载均执行一下操作："><a href="#2）多负载均执行一下操作：" class="headerlink" title="2）多负载均执行一下操作："></a>2）多负载均执行一下操作：</h4><p> 多负载的情况下，打开指定虚拟机器</p>
<blockquote>
<p>open  node01    </p>
<p>node01  为指定虚拟机器的别名，在hosts文件中配置的</p>
</blockquote>
<p>启动Tomcat</p>
<blockquote>
<p>在Tomcat解压的目录下       ./startup.sh  </p>
</blockquote>
<p>注意： 记得关闭虚拟机器的防火墙</p>
<blockquote>
<p>service  iptables  stop</p>
</blockquote>
<p>浏览器访问</p>
<blockquote>
<p>虚拟机器IP地址：8080</p>
</blockquote>
<p>默认负载均衡配置</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt; http &#123; </span><br><span class="line">&gt;     upstream shsxt&#123;   </span><br><span class="line">&gt;     # 以下均为实际执行服务的服务器</span><br><span class="line">&gt;     #只有当hosts文件中给ip地址配置了别名，这里server后面才能用别名，</span><br><span class="line">&gt;     #否则跟IP地址</span><br><span class="line">&gt;         server node01; </span><br><span class="line">&gt;         server node02; </span><br><span class="line">&gt;         server node03; </span><br><span class="line">&gt;     &#125; </span><br><span class="line">&gt; </span><br><span class="line">&gt;     server &#123; </span><br><span class="line">&gt;     #指定访问端口为80 ，那么Tomcat服务器端的port也要改为80</span><br><span class="line">&gt;         listen 80;   </span><br><span class="line">&gt; 	    server_name  localhost;</span><br><span class="line">&gt;         location / &#123;</span><br><span class="line">&gt;             proxy_pass http://shsxt;    </span><br><span class="line">&gt;             # shsxt  是指定的代理服务器</span><br><span class="line">&gt;         &#125;</span><br><span class="line">&gt;     &#125; </span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>查看使用 80端口的程序</p>
<blockquote>
<p>netstat -anp |grep 80</p>
</blockquote>
<p>配置文件编辑结束后，启动nginx服务</p>
<blockquote>
<p>service  nginx  start</p>
</blockquote>
<h4 id="（1）、轮询负载均衡（默认）"><a href="#（1）、轮询负载均衡（默认）" class="headerlink" title="（1）、轮询负载均衡（默认）"></a>（1）、轮询负载均衡（默认）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 对应用程序服务器的请求以循环方式分发</span><br></pre></td></tr></table></figure>
<h4 id="（2）、加权负载均衡"><a href="#（2）、加权负载均衡" class="headerlink" title="（2）、加权负载均衡"></a>（2）、加权负载均衡</h4><blockquote>
<p>通过使用服务器权重，还可以进一步影响nginx负载均衡算法，</p>
<p>谁的权重越大，分发到的请求就越多。</p>
<p>权重总数：10</p>
</blockquote>
<p>在nginx.conf文件中修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">upstream shsxt &#123;</span><br><span class="line">       server srv1.example.com weight=3;//域名为在/etc/hosts文件中取的别名</span><br><span class="line">       server srv2.example.com;</span><br><span class="line">       server srv3.example.com;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>配置修改之后，重启</p>
<blockquote>
<p>service  nginx  restart</p>
</blockquote>
<h4 id="（3）、最少连接负载平衡"><a href="#（3）、最少连接负载平衡" class="headerlink" title="（3）、最少连接负载平衡"></a>（3）、最少连接负载平衡</h4><blockquote>
<p>在连接负载最少的情况下，nginx会尽量避免将过多的请求分发给繁忙的应用程序服务器，</p>
<p>而是将新请求分发给不太繁忙的服务器，避免服务器过载。</p>
</blockquote>
<p>在nginx.conf文件中修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">upstream shsxt &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server srv1.example.com;</span><br><span class="line">        server srv2.example.com;</span><br><span class="line">        server srv3.example.com;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="（4）、会话持久性——ip-hash负载平衡机制"><a href="#（4）、会话持久性——ip-hash负载平衡机制" class="headerlink" title="（4）、会话持久性——ip-hash负载平衡机制"></a>（4）、会话持久性——ip-hash负载平衡机制</h4><p><code>特点</code>：保证相同的客户端总是定向到相同的服务;</p>
<p>(此方法可确保来自同一客户端的请求将始终定向到同一台服务器，除非此服务器不可用。)</p>
<p>在nginx.conf文件中修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">upstream shsxt&#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server （IP地址|别名）;</span><br><span class="line">    server （IP地址|别名）;</span><br><span class="line">    server （IP地址|别名）;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5-、Nginx的访问控制"><a href="#5-、Nginx的访问控制" class="headerlink" title="(5)、Nginx的访问控制"></a>(5)、Nginx的访问控制</h4><blockquote>
<p>Nginx还可以对IP的访问进行控制，allow代表允许，deny代表禁止.</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">deny 192.168.2.180;</span><br><span class="line">allow 192.168.78.0/24;</span><br><span class="line">allow 10.1.1.0/16;</span><br><span class="line">allow 192.168.1.0/32;</span><br><span class="line">deny all;</span><br><span class="line">proxy_pass http://shsxt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">从上到下的顺序，匹配到了便跳出。</span><br><span class="line">如上的例子先禁止了1个，</span><br><span class="line">接下来允许了3个网段，</span><br><span class="line">其中包含了一个ipv6，</span><br><span class="line">最后未匹配的IP全部禁止访问</span><br></pre></td></tr></table></figure>
]]></content>
        
        <categories>
            
            <category> Nginx </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> 负载均衡 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Hive学习]]></title>
        <url>http://sungithup.github.io/2019/01/11/Hive%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h2 id="一、Hive是什么？"><a href="#一、Hive是什么？" class="headerlink" title="一、Hive是什么？"></a>一、Hive是什么？</h2><h3 id="1、基于-Hadoop-的一个数据仓库工具"><a href="#1、基于-Hadoop-的一个数据仓库工具" class="headerlink" title="1、基于 Hadoop 的一个数据仓库工具"></a>1、基于 Hadoop 的一个<code>数据仓库工具</code></h3><ul>
<li>可以将<code>结构化</code>的数据文件映射为一张<code>hive数据库表</code>；</li>
<li>这张Hive数据库表保存不了metadata元数据信息，而是将metadata存储在本地磁盘上的MySQL（关系型数据库）中</li>
<li>并提供简单的 sql 查询功能；</li>
<li>可以将 sql 语句转换为 MapReduce 任务进行运行。</li>
</ul>
<h3 id="2、快速实现简单的MapReduce-统计的工具"><a href="#2、快速实现简单的MapReduce-统计的工具" class="headerlink" title="2、快速实现简单的MapReduce 统计的工具"></a>2、快速实现简单的MapReduce 统计的工具</h3><ul>
<li>方便非Java编程者对HDFS的数据做mapreduce操作；</li>
<li>学习成本低，十分适合数据仓库的统计分析。</li>
</ul>
<h3 id="3、什么是数据仓库？"><a href="#3、什么是数据仓库？" class="headerlink" title="3、什么是数据仓库？"></a>3、什么是数据仓库？</h3><ul>
<li>Data Warehouse(DW 或DWH）是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。</li>
<li>单个数据存储，出于分析性报告和决策支持目的而创建。</li>
<li>为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制.</li>
<li><strong>数据仓库</strong>是用来做<strong>查询分析的数据库</strong>，<strong>基本不用来做插入，修改，删除操作</strong>。</li>
</ul>
<h3 id="4、数据处理的两大分类"><a href="#4、数据处理的两大分类" class="headerlink" title="4、数据处理的两大分类"></a>4、数据处理的两大分类</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz2wawi0ujj30h106c0tg.jpg" alt="oltp+olap"></p>
<ul>
<li><h4 id="联机事务处理OLTP（on-line-transaction-processing）"><a href="#联机事务处理OLTP（on-line-transaction-processing）" class="headerlink" title="联机事务处理OLTP（on-line transaction processing）"></a>联机事务处理OLTP（on-line transaction processing）</h4></li>
</ul>
<blockquote>
<p>OLTP是传统的关系型<a href="http://lib.csdn.net/base/mysql" target="_blank" rel="noopener">数据库</a>的主要应用，主要是基本的、日常的事务处理，例如银行交易。</p>
<p>OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作；</p>
</blockquote>
<ul>
<li><h4 id="联机分析处理OLAP（On-Line-Analytical-Processing）"><a href="#联机分析处理OLAP（On-Line-Analytical-Processing）" class="headerlink" title="联机分析处理OLAP（On-Line Analytical Processing）"></a>联机分析处理OLAP（On-Line Analytical Processing）</h4></li>
</ul>
<blockquote>
<p>OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。</p>
<p>OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。</p>
</blockquote>
<ul>
<li>数据文件按结构的分类</li>
</ul>
<blockquote>
<p>结构化数据：关系型</p>
<p>半结构化数据：K-V</p>
<p>松散型：</p>
</blockquote>
<h2 id="二、Hive架构原理"><a href="#二、Hive架构原理" class="headerlink" title="二、Hive架构原理"></a>二、Hive架构原理</h2><p>用户接口主要有三个：CLI，Client 和 WUI。</p>
<h2 id="三、Hive搭建及三种模式"><a href="#三、Hive搭建及三种模式" class="headerlink" title="三、Hive搭建及三种模式"></a>三、Hive搭建及三种模式</h2><h3 id="1、Hive的安装配置："><a href="#1、Hive的安装配置：" class="headerlink" title="1、Hive的安装配置："></a>1、Hive的安装配置：</h3><h4 id="（1）基本环境：Hadoop集群环境（至少3个节点）"><a href="#（1）基本环境：Hadoop集群环境（至少3个节点）" class="headerlink" title="（1）基本环境：Hadoop集群环境（至少3个节点）"></a>（1）基本环境：Hadoop集群环境（至少3个节点）</h4><blockquote>
<p><strong>Hive</strong>是依赖于hadoop系统的，因此在运行Hive之前需要保证已经搭建好hadoop集群环境。</p>
</blockquote>
<h4 id="（2）安装一个关系型数据mysql"><a href="#（2）安装一个关系型数据mysql" class="headerlink" title="（2）安装一个关系型数据mysql"></a>（2）安装一个关系型数据mysql</h4><blockquote>
<p>因为Hive数据仓库的元数据信息是存放在本地磁盘的关系数据库上的</p>
</blockquote>
<p><code>安装步骤</code>：详见  “Linux系统数据库MySQL安装.md”</p>
<h4 id="（3）解压安装（按需在指定节点上）"><a href="#（3）解压安装（按需在指定节点上）" class="headerlink" title="（3）解压安装（按需在指定节点上）"></a>（3）解压安装（按需在指定节点上）</h4><blockquote>
<p>解压apache-hive-1.2.1-bin.tar.gz</p>
</blockquote>
<h4 id="（4）追加配置环境变量"><a href="#（4）追加配置环境变量" class="headerlink" title="（4）追加配置环境变量"></a>（4）追加配置环境变量</h4><blockquote>
<p>vim ~/.bash_profile</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HIVE_HOME=Hive的解压路径</span><br><span class="line"></span><br><span class="line">HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure>
<h4 id="（5）替换和添加相关jar包"><a href="#（5）替换和添加相关jar包" class="headerlink" title="（5）替换和添加相关jar包"></a>（5）替换和添加相关jar包</h4><blockquote>
<ul>
<li><p>修改HADOOP_HOME/share/hadoop/yarn/lib目录下的jline-*.jar </p>
<p>将其替换成HIVE_HOME/lib下的jline-2.12.jar。 </p>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>–将如下(<code>hive连接mysql)</code>的jar包拷贝到hive解压目录的lib目录下</p>
<p>mysql-connector-java-5.1.32-bin.jar</p>
</li>
</ul>
</blockquote>
<h4 id="（6）修改配置文件（选择3种模式里哪一种）"><a href="#（6）修改配置文件（选择3种模式里哪一种）" class="headerlink" title="（6）修改配置文件（选择3种模式里哪一种）"></a>（6）修改配置文件（选择3种模式里哪一种）</h4><p><code>见三种安装模式</code></p>
<h4 id="（7）启动"><a href="#（7）启动" class="headerlink" title="（7）启动"></a>（7）启动</h4><blockquote>
<p>hive </p>
</blockquote>
<h3 id="2、三种模式"><a href="#2、三种模式" class="headerlink" title="2、三种模式"></a>2、三种模式</h3><table>
<thead>
<tr>
<th>三种模式</th>
</tr>
</thead>
<tbody>
<tr>
<td>A、内嵌模式（元数据保存在内嵌的derby中，允许一个会话链接，尝试多个会话链接时会报错）【了解】                                                                                                        B、本地模式（本地安装mysql 替代derby存储元数据）【重要】                                                                                  C、远程模式（远程安装mysql 替代derby存储元数据）【重要】</td>
</tr>
</tbody>
</table>
<h4 id="（1）内嵌Derby单用户模式（了解）"><a href="#（1）内嵌Derby单用户模式（了解）" class="headerlink" title="（1）内嵌Derby单用户模式（了解）"></a>（1）内嵌Derby单用户模式（了解）</h4><ul>
<li>元数据是内嵌在Derby数据库中的，只能允许一个会话连接，数据会存放到HDFS上。</li>
<li>存储方式简单，只需要hive-site.xml </li>
<li>注：使用 derby<br>存储方式时，运行 hive 会在当前目录生成一个 derby 文件和一个<br>metastore_db</li>
</ul>
<p>hive-site.xml ：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:derby:;databaseName=metastore_db;create=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.derby.jdbc.EmbeddedDriver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="（2）本地用户模式（重要，多用于本地开发测试）"><a href="#（2）本地用户模式（重要，多用于本地开发测试）" class="headerlink" title="（2）本地用户模式（重要，多用于本地开发测试）"></a>（2）本地用户模式（<code>重要</code>，多用于本地开发测试）</h4><table>
<thead>
<tr>
<th>与嵌入式的区别</th>
</tr>
</thead>
<tbody>
<tr>
<td><em> 不再使用内嵌的Derby作为元数据的存储介质，而是使用其他数据库比如MySQL来存储元数据且是一个多用户的模式，运行多个用户client连接到一个数据库中。这种方式一般作为公司内部同时使用Hive。                                                                                                                                                                               </em> 这里有一个前提，每一个用户必须要有对MySQL的访问权利，即每一个客户端使用者需要知道MySQL的用户名和密码才行。</td>
</tr>
</tbody>
</table>
<ul>
<li>需要在本地运行一个 mysql 服务器</li>
<li>在node00上（与MySQL在同一个节点上）解压安装Hive</li>
</ul>
<blockquote>
<p>MySQL      Hive      :  node00</p>
</blockquote>
<ul>
<li>需要将 mysql 的 jar 包（mysql-connector-java-5.1.32-bin.jar）拷贝到$HIVE_HOME/lib 目录下</li>
</ul>
<p>hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive_local/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node00/hive_remote?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="（3）远程模式（重要）"><a href="#（3）远程模式（重要）" class="headerlink" title="（3）远程模式（重要）"></a>（3）远程模式（重要）</h4><ul>
<li><h5 id="remote-一体"><a href="#remote-一体" class="headerlink" title="remote 一体"></a>remote 一体</h5></li>
</ul>
<blockquote>
<p>将Hive解压安装与MySQL不同的节点上</p>
<p>MySQL  ：node00</p>
<p>Hive  ： node02</p>
<p>需要在 Hive服务器启动 meta服务</p>
</blockquote>
<p>hive-site.xml</p>
<p>(hadoop 2.6.5)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive1/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>如果在hadoop5.X环境下还需要添加</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node01:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>注</code><strong>：</strong>这里把hive的服务端和客户端都放在同一台服务器上了。服务端和客户端可以拆开</p>
<ul>
<li><h5 id="Remote-分开-公司企业经常用"><a href="#Remote-分开-公司企业经常用" class="headerlink" title="Remote 分开(公司企业经常用)"></a>Remote 分开(公司企业经常用)</h5></li>
</ul>
<p>将 hive-site.xml 配置文件拆为如下两部分（此时不与MySQL在同一台节点上）</p>
<blockquote>
<p>MySql  ：   node00</p>
<p>服务端 ：   node02</p>
<p>客户端 ：    node01</p>
</blockquote>
<p><strong>1</strong>）、服务端配置文件（node02）</p>
<p>hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive2/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node00:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>2</strong>）、客户端配置文件（node01）</p>
<p> hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive2/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--注意这里的路径要和服务端一致---&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.local<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node02:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>服务端启动 hive 程序 hive –service metastore</strong></p>
<p><strong>客户端直接使用 hive 命令即可</strong></p>
<p>Hive常见问题总汇：<a href="http://blog.csdn.net/freedomboy319/article/details/44828337" target="_blank" rel="noopener">http://blog.csdn.net/freedomboy319/article/details/44828337</a></p>
<h2 id="四、HQL详解"><a href="#四、HQL详解" class="headerlink" title="四、HQL详解"></a>四、HQL详解</h2><p><code>Hql 就是HiveQl语句</code></p>
<h3 id="1、DDL语句（数据库定义语言）"><a href="#1、DDL语句（数据库定义语言）" class="headerlink" title="1、DDL语句（数据库定义语言）"></a>1、DDL语句（数据库定义语言）</h3><h4 id="（1）具体参见：https-cwiki-apache-org-confluence-display-Hive-LanguageManual-DDL"><a href="#（1）具体参见：https-cwiki-apache-org-confluence-display-Hive-LanguageManual-DDL" class="headerlink" title="（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL"></a>（1）具体参见：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</a></h4><p><code>Hive的数据定义语言</code> （<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener">LanguageManual DDL</a>;)）</p>
<p><strong><code>重点</code> hive 的建表语句和分区。</strong></p>
<h4 id="（2）创建-删除-修改-使用数据库"><a href="#（2）创建-删除-修改-使用数据库" class="headerlink" title="（2）创建/删除/修改/使用数据库"></a>（2）创建/删除/修改/使用数据库</h4><ul>
<li><h5 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h5></li>
</ul>
<p>（Hive搭建完毕后，会创建一个默认的数据库）</p>
<blockquote>
<p>查看    show databases；</p>
</blockquote>
<blockquote>
<p>CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment];</p>
<p>举例：</p>
<p>create database attribute;</p>
<p>create database attr;</p>
</blockquote>
<p><code>注意：</code>创建数据时，数据库名不要和系统关键字冲突，否则会报错；</p>
<p>如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">命令：</span><br><span class="line">hive&gt; create database out;</span><br><span class="line"></span><br><span class="line">报错：</span><br><span class="line">FAILED: ParseException line 1:16 Failed to recognize predicate 'out'. Failed rule: 'identifier' in create database statement</span><br><span class="line"></span><br><span class="line">原因：</span><br><span class="line">在Hive1.2.0版本开始增加了如下配置选项，默认值为true：</span><br><span class="line"></span><br><span class="line">hive.support.sql11.reserved.keywords</span><br><span class="line"></span><br><span class="line">该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。</span><br><span class="line"></span><br><span class="line">解决：</span><br><span class="line">法一：弃用这个关键字，换个名字</span><br><span class="line">法二：弃用对保留关键字的支持</span><br><span class="line">在conf下的hive-site.xml配置文件中修改配置选项：</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.sql11.reserved.keywords<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><blockquote>
<p>DROP (DATABASE|SCHEMA) [IF EXISTS] database_name;</p>
<p>举例：</p>
<p>drop database attribute;</p>
</blockquote>
</li>
</ul>
<ul>
<li>修改数据库(了解)</li>
</ul>
<blockquote>
<p>ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …);</p>
<p>ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; </p>
</blockquote>
<ul>
<li><h5 id="使用数据库-（进入某一数据库。如果没有这步操作，会进入默认default数据库）"><a href="#使用数据库-（进入某一数据库。如果没有这步操作，会进入默认default数据库）" class="headerlink" title="使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库）"></a>使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库）</h5></li>
</ul>
<blockquote>
<p>USE database_name;</p>
<p>举例：</p>
<p>use attr；</p>
</blockquote>
<h4 id="（3）创建-删除-表（重点）"><a href="#（3）创建-删除-表（重点）" class="headerlink" title="（3）创建/删除/表（重点）"></a>（3）创建/删除/表（重点）</h4><ul>
<li><h5 id="创建表（重要！）"><a href="#创建表（重要！）" class="headerlink" title="创建表（重要！）"></a>创建表（重要！）</h5></li>
</ul>
<p>数据类型：</p>
<blockquote>
<p>data_type<br>  : <code>primitive_type  原始数据类型</code><br>  | <code>array_type        数组</code><br>  | <code>map_type        map</code><br>  | struct_type<br>  | union_type  – (Note: Available in Hive 0.7.0 and later)</p>
<p><em>primitive_type</em><br>  : TINYINT<br>  | SMALLINT<br>  | <code>INT</code><br>  | <code>BIGINT</code><br>  | BOOLEAN<br>  | FLOAT<br>  | <code>DOUBLE</code><br>  | DOUBLE PRECISION<br>  | <strong>STRING  基本可以搞定一切</strong><br>  | BINARY<br>  | TIMESTAMP<br>  | DECIMAL<br>  | DECIMAL(precision, scale)<br>  | <code>DATE</code><br>  | VARCHAR<br>  | CHAR  </p>
<p><em>array_type</em><br>  : <code>ARRAY &lt; data_type &gt;</code></p>
<p><em>map_type</em><br>  : <code>MAP &lt; primitive_type, data_type &gt;</code></p>
<p><em>struct_type</em><br>  : STRUCT &lt; col_name : data_type [COMMENT col_comment], …&gt;</p>
<p><em>union_type</em><br>  : UNIONTYPE &lt; data_type, data_type, … &gt;  </p>
</blockquote>
<ul>
<li><h5 id="1、准备数据"><a href="#1、准备数据" class="headerlink" title="1、准备数据"></a>1、准备数据</h5></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai</span><br><span class="line">2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai</span><br><span class="line">3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing</span><br></pre></td></tr></table></figure>
<ul>
<li><h5 id="2、创建表"><a href="#2、创建表" class="headerlink" title="2、创建表"></a>2、创建表</h5></li>
</ul>
<p>(如果没有指定进入某一数据库，就会在默认数据库中创建)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log(</span><br><span class="line"> id int,</span><br><span class="line"> name string,</span><br><span class="line"> age int,</span><br><span class="line"> likes array&lt;string&gt;,</span><br><span class="line"> address map&lt;string,string&gt;</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by &apos;,&apos;</span><br><span class="line"> COLLECTION ITEMS TERMINATED by &apos;-&apos;</span><br><span class="line"> map keys terminated by &apos;:&apos;</span><br><span class="line"> lines terminated by &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>导入数据</strong>（属于DML但是为了演示需要在此应用）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br><span class="line"></span><br><span class="line"> [LOCAL]:从本地  |  若无，则为从HDFS</span><br><span class="line"> [OVERWRITE]  ： 会覆盖Hive表中的数据   | 若无则会追加</span><br><span class="line"> </span><br><span class="line"> [PARTITION....] ： 创建分区</span><br></pre></td></tr></table></figure>
<blockquote>
<p>将log1文件中的数据填加到log表中</p>
<p>（log1中数据的格式要和log表格式保持一致，否则会乱；若文件已存在，则会自动重命名）</p>
<ul>
<li>本地加载（相当于复制）数据到Hive的制定表中</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<ul>
<li>HDFS加载（相当于剪切）数据到Hive的制定表中</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; LOAD DATA INPATH &apos;/root/su/log1&apos; INTO TABLE log ;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
</blockquote>
<ul>
<li>查看表中数据</li>
</ul>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; 对本表查询不会产生MapReduce任务</span><br><span class="line">&gt; hive&gt; select * from log;</span><br><span class="line">&gt; 使用函数查询会产生MapReduce任务</span><br><span class="line">&gt; hive&gt; select count(*) from log;</span><br><span class="line">&gt; 查询表的字段信息</span><br><span class="line">&gt; hive&gt; desc log;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>第一个查询结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1	zshang	18	[&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;]	&#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;</span><br><span class="line">1	zhaoliu	18	[&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;]	&#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;</span><br><span class="line">2	lishi	16	[&quot;shop&quot;,&quot;boy&quot;,&quot;book&quot;]	&#123;&quot;stu_addr&quot;:&quot;hunan&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;</span><br><span class="line">3	wang2mazi	20	[&quot;fangniu&quot;,&quot;eat&quot;]	&#123;&quot;stu_addr&quot;:&quot;shanghai&quot;,&quot;work_addr&quot;:&quot;tianjing&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>第二个查询结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure>
<p><code>附加题</code></p>
<blockquote>
<p>查询表中likes字段中有girl的人</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name from log2 where likes[1]=&quot;girl&quot;;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>查询表中address字段有stu_addr为beijing的人</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  select name from log2 where address[&quot;stu_addr&quot;]=&quot;beijing&quot;;</span><br></pre></td></tr></table></figure>
<ul>
<li><h5 id="3、删除表"><a href="#3、删除表" class="headerlink" title="3、删除表"></a>3、删除表</h5></li>
</ul>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; DROP TABLE [IF EXISTS] table_name [PURGE];</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>举例：</p>
<p>（用drop命令删除表，会将表中数据一并删除，其对应在MySQl中的表的元数据信息也会随之删除；</p>
<p>​    用hdfs命令删除表对应的文件目录，表中数据也一并删除，但其元数据信息依然保存在My  SQL上，</p>
<p>​     再load数据，可恢复该表）                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </p>
</blockquote>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; drop table log1；</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; hdfs dfs -rmr /user/hive_local/warehouse/attr.db/log1</span><br><span class="line">&gt; </span><br><span class="line">&gt; hive&gt; use attr;</span><br><span class="line">&gt; hive&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;</span><br><span class="line">&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
</blockquote>
<ul>
<li><h5 id="创建外部表（重要）"><a href="#创建外部表（重要）" class="headerlink" title="创建外部表（重要）"></a>创建外部表（重要）</h5></li>
</ul>
<blockquote>
<p>外部关键字EXTERNAL允许您创建一个表,并提供一个位置,以便hive不使用这个表的默认位置。<strong>这方便如果你已经生成了数据，当删除一个外部表</strong>,<strong>表中的数据不会从文件系统中删除</strong>。外部表指向任何HDFS的存储位置,而不是存储在配置属性指定的文件夹<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.metastore.warehouse.dir&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener"> hive.metastore.warehouse.dir</a>;).中</p>
</blockquote>
<p>创建表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create EXTERNAL table log1(</span><br><span class="line"> id int,</span><br><span class="line"> name string,</span><br><span class="line"> age int,</span><br><span class="line"> likes array&lt;string&gt;,</span><br><span class="line"> address map&lt;string,string&gt;</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by &apos;,&apos;</span><br><span class="line"> COLLECTION ITEMS TERMINATED by &apos;-&apos;</span><br><span class="line"> map keys terminated by &apos;:&apos;</span><br><span class="line"> lines terminated by &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
<p>加载数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;</span><br></pre></td></tr></table></figure>
<p>删除外部表（<code>相当于删除的是表的元数据信息，而表中的数据还保存</code>）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table log1；</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<blockquote>
<p>hive&gt; show tables;</p>
<p>无log1</p>
<p>MySQl中也无此表元数据信息</p>
<p>但是，</p>
<p>在HDFS文件系统中，此表数据依然存在</p>
<p>也就是说，词表还可以恢复</p>
</blockquote>
<p>恢复表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">重新创建log1表，该表即可恢复</span><br></pre></td></tr></table></figure>
<h4 id="（4）修改表-更新，删除数据-这些很少用"><a href="#（4）修改表-更新，删除数据-这些很少用" class="headerlink" title="（4）修改表,更新，删除数据(这些很少用)"></a>（4）修改表,更新，删除数据(这些很少用)</h4><p>重命名表</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; ALTER TABLE table_name RENAME TO new_table_name;</span><br><span class="line">&gt; </span><br><span class="line">&gt; Eg: alter table meninem rename to jacke;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
</blockquote>
<p>更新数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE tablename SET column = value [, column = value ...][WHERE expression]</span><br></pre></td></tr></table></figure>
<p>删除数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DELETE FROM tablename [WHERE expression]</span><br></pre></td></tr></table></figure>
<h3 id="2、DML语句（数据库管理语言）"><a href="#2、DML语句（数据库管理语言）" class="headerlink" title="2、DML语句（数据库管理语言）"></a>2、DML语句（数据库管理语言）</h3><h4 id="（1）具体参见："><a href="#（1）具体参见：" class="headerlink" title="（1）具体参见："></a>（1）具体参见：</h4><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML</a></p>
<p>   <strong>重点是数据加载和查询插入语法</strong></p>
<p>Hive数据操作语言（<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener">LanguageManual DML</a>;)）</p>
<h4 id="（2）四种插入-导入数据-重要"><a href="#（2）四种插入-导入数据-重要" class="headerlink" title="（2）四种插入/导入数据(重要)"></a>（2）四种插入/导入数据(重要)</h4><blockquote>
<p>Hive不能很好的支持用<code>insert</code>语句一条一条的进行插入操作，不支持<code>update</code>操作。数据是以<code>load</code>的方式加载到建立好的表中。数据一旦导入就不可以修改。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table log3(</span><br><span class="line"> id int,</span><br><span class="line"> name string,</span><br><span class="line"> age int</span><br><span class="line"> )</span><br><span class="line"> row format delimited fields terminated by &apos;,&apos;</span><br><span class="line"> lines terminated by &apos;\n&apos;;</span><br></pre></td></tr></table></figure>
<h5 id="1-直接加载数据"><a href="#1-直接加载数据" class="headerlink" title="1.直接加载数据"></a>1.直接加载数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br><span class="line">load data local inpath &apos;/root/su/log1&apos; into table log1;</span><br></pre></td></tr></table></figure>
<h5 id="2-将表1查询结果插入表2"><a href="#2-将表1查询结果插入表2" class="headerlink" title="2.将表1查询结果插入表2"></a>2.将表1查询结果插入表2</h5><p><code>注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建person2表，然后从表person1查询数据导入：</span><br><span class="line">覆盖：</span><br><span class="line">INSERT OVERWRITE TABLE person2 [PARTITION(dt=&apos;2008-06-08&apos;, country)]</span><br><span class="line">       SELECT id,name, age From ppt;</span><br><span class="line">追加：</span><br><span class="line">INSERT INTO TABLE log3 </span><br><span class="line">       SELECT id,name, age From log;</span><br></pre></td></tr></table></figure>
<h5 id="3-将表1、表2查询结果插入表3、表4"><a href="#3-将表1、表2查询结果插入表3、表4" class="headerlink" title="3.将表1、表2查询结果插入表3、表4"></a>3.将表1、表2查询结果插入表3、表4</h5><p><code>注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FROM person t1</span><br><span class="line">INSERT OVERWRITE | INTO TABLE person1 [PARTITION(dt=&apos;2008-06-08&apos;, country)]</span><br><span class="line">       SELECT t1.id, t1.name, t1.age ;</span><br><span class="line">       </span><br><span class="line">FROM log t1,log1 t2 </span><br><span class="line">INSERT OVERWRITE TABLE log4  </span><br><span class="line"> SELECT t1.id,t1.name,t2.age ;</span><br><span class="line"> </span><br><span class="line"> 是否存在笛卡尔积：？？？？存在。</span><br><span class="line"> </span><br><span class="line"> 为了防止笛卡尔积：</span><br><span class="line"> FROM log t1,log1 t2 </span><br><span class="line">INSERT OVERWRITE TABLE log4  </span><br><span class="line"> SELECT t1.id,t1.name,t2.age where t1.id =t2.id;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">【from放前面好处就是后面可以插入多条语句 】</span><br><span class="line">FROM abc t1,sufei t2 </span><br><span class="line">INSERT OVERWRITE TABLE qidu  </span><br><span class="line"> SELECT t1.id,t1.name,t1.age,t2.likes,t2.address ;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM abc t1,sufei t2 </span><br><span class="line">INSERT OVERWRITE TABLE qidu  </span><br><span class="line"> SELECT t1.id,t1.name,t1.age,t1.likes,t1.address where…</span><br><span class="line">INSERT OVERWRITE TABLE wbb</span><br><span class="line"> SELECT t2.id,t2.name,t2.age,t2.likes,t2.address where…;</span><br></pre></td></tr></table></figure>
<h5 id="4-直接列出数据插入表中（大量数据时不推荐）"><a href="#4-直接列出数据插入表中（大量数据时不推荐）" class="headerlink" title="4.直接列出数据插入表中（大量数据时不推荐）"></a>4.直接列出数据插入表中（大量数据时不推荐）</h5><p><code>注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO TABLE students</span><br><span class="line">   VALUES (1,&apos;zs&apos;,18,&apos;boy&apos;,&apos;beijng&apos;),(2,&apos;wh&apos;,&apos;girl&apos;,&apos;stu_addr&apos;:shanghai&apos;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本地load数据和从HDFS上load加载数据的过程有什么<code>区别</code>？</p>
<ul>
<li><p>本地： local 会自动复制到HDFS上的hive的**目录下</p>
</li>
<li><p>Hdfs导入 后移动到hive的**目录下</p>
</li>
</ul>
</blockquote>
<h4 id="（3）查询数据并保存"><a href="#（3）查询数据并保存" class="headerlink" title="（3）查询数据并保存"></a>（3）查询数据并保存</h4><ol>
<li><ul>
<li><h5 id="保存数据到本地："><a href="#保存数据到本地：" class="headerlink" title="保存数据到本地："></a>保存数据到本地：</h5></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/opt/datas/hive_exp_emp2&apos;</span><br><span class="line">     ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;</span><br><span class="line">         select * from db_1128.emp ;</span><br><span class="line">留意两种的区别：保存的数据格式</span><br><span class="line"></span><br><span class="line">insert overwrite local directory &apos;/sun/temp/hive_save1&apos;</span><br><span class="line">     row format delimited fields terminated by &apos;,&apos;</span><br><span class="line">      COLLECTION ITEMS TERMINATED by &apos;-&apos;</span><br><span class="line">      map keys terminated by &apos;:&apos;      </span><br><span class="line">          select * from log2 ;</span><br><span class="line">          </span><br><span class="line">这里如果将 overwrite  改为into 会报错。</span><br></pre></td></tr></table></figure>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//查看数据</span><br><span class="line">!cat /sun/temp/hive_save1/000000_0;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><ul>
<li><h5 id="保存数据到HDFS上："><a href="#保存数据到HDFS上：" class="headerlink" title="保存数据到HDFS上："></a>保存数据到HDFS上：</h5></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite directory &apos;/user/beifeng/hive/hive_exp_emp&apos;</span><br><span class="line">     select * from db_1128.emp ;</span><br><span class="line"></span><br><span class="line">insert overwrite directory &apos;/sun/hive/temp/hive_save1&apos;</span><br><span class="line">      row format delimited fields terminated by &apos;,&apos;</span><br><span class="line">      COLLECTION ITEMS TERMINATED by &apos;-&apos;</span><br><span class="line">      map keys terminated by &apos;:&apos;</span><br><span class="line">      select * from log2 ;</span><br><span class="line">     </span><br><span class="line">这里如果将 overwrite  改为into 会报错。</span><br></pre></td></tr></table></figure>
</li>
<li><ul>
<li><h5 id="在外部shell中将数据重定向到文件中："><a href="#在外部shell中将数据重定向到文件中：" class="headerlink" title="在外部shell中将数据重定向到文件中："></a>在外部shell中将数据重定向到文件中：</h5></li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(注意：需要指明是哪个数据库的表)</span><br><span class="line"># hive -e &quot;select * from attr.log;&quot; &gt; /sun/hive/temp/hive_save2</span><br><span class="line"># cat /sun/hive/temp/hive_save2</span><br></pre></td></tr></table></figure>
<h4 id="（4）备份数据或还原数据（在HDFS上）"><a href="#（4）备份数据或还原数据（在HDFS上）" class="headerlink" title="（4）备份数据或还原数据（在HDFS上）"></a>（4）备份数据或还原数据（在HDFS上）</h4><ol>
<li><ul>
<li>备份数据（包括表的元数据和表中的数据）：</li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPORT TABLE log to &apos;/sun/hive/datas/export/cp1&apos;</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><ul>
<li>删除再还原数据：</li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">先删除表。</span><br><span class="line">drop table log;</span><br><span class="line">show tables ;</span><br><span class="line">再还原数据：</span><br><span class="line">IMPORT FROM &apos;/sun/hive/datas/export/cp1&apos; ;</span><br></pre></td></tr></table></figure>
<h4 id="（5）其他Hql操作"><a href="#（5）其他Hql操作" class="headerlink" title="（5）其他Hql操作"></a>（5）其他Hql操作</h4><h5 id="Hive的group-by-join-left-join-right-join等-having-sort-by-order-by等操作和MySQL没有什么大的区别："><a href="#Hive的group-by-join-left-join-right-join等-having-sort-by-order-by等操作和MySQL没有什么大的区别：" class="headerlink" title="Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别："></a>Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别：</h5><p><a href="http://www.2cto.com/kf/201609/545560.html" target="_blank" rel="noopener">http://www.2cto.com/kf/201609/545560.html</a></p>
<h3 id="3、Hive-SerDe（序列化、反序列化）"><a href="#3、Hive-SerDe（序列化、反序列化）" class="headerlink" title="3、Hive SerDe（序列化、反序列化）"></a>3、Hive SerDe（序列化、反序列化）</h3><h4 id="1-定义"><a href="#1-定义" class="headerlink" title="(1)定义"></a>(1)定义</h4><p><strong><code>Hive SerDe</code></strong> - Serializer and Deserializer  SerDe 用于做序列化和反序列化。</p>
<p>构建在数据存储和执行引擎之间，对两者实现解耦。</p>
<p>对数据实现序列化，清洗数据，使之成为有效数据并加载。</p>
<p>Hive通过ROW FORMAT DELIMITED以及SERDE进行内容的读写。</p>
<h4 id="（2）实现"><a href="#（2）实现" class="headerlink" title="（2）实现"></a>（2）实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">row_format</span><br><span class="line">: DELIMITED </span><br><span class="line">          [FIELDS TERMINATED BY char [ESCAPED BY char]] </span><br><span class="line">          [COLLECTION ITEMS TERMINATED BY char] </span><br><span class="line">          [MAP KEYS TERMINATED BY char] </span><br><span class="line">          [LINES TERMINATED BY char] </span><br><span class="line">: SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Hive正则匹配（实现数据清洗）</span><br><span class="line">创建表 logtbl：</span><br><span class="line"></span><br><span class="line"> CREATE TABLE logtbl (</span><br><span class="line">    host STRING,</span><br><span class="line">    identity STRING,</span><br><span class="line">    t_user STRING,</span><br><span class="line">    time STRING,</span><br><span class="line">    request STRING,</span><br><span class="line">    referer STRING,</span><br><span class="line">    agent STRING)</span><br><span class="line">  ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos;</span><br><span class="line">  WITH SERDEPROPERTIES (</span><br><span class="line"> &quot;input.regex&quot;=&quot;([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \&quot;(.*)\&quot; (-|[0-9]*) (-|[0-9]*)&quot;) </span><br><span class="line">  STORED AS TEXTFILE;</span><br><span class="line">  </span><br><span class="line">加载数据:</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/root/su/localhost_access_log.2016-02-29&apos; into table logtbl;</span><br><span class="line"></span><br><span class="line">查看数据：</span><br><span class="line"></span><br><span class="line">select * from logtbl;</span><br><span class="line"></span><br><span class="line">显示：</span><br><span class="line">192.168.57.4	-	-	29/Feb/2016:18:14:35 +0800	GET /bg-upper.png HTTP/1.1	304    -</span><br><span class="line">192.168.57.4	-	-	29/Feb/2016:18:14:35 +0800	GET /bg-nav.png HTTP/1.1	304    -</span><br><span class="line">192.168.57.4	-	-	29/Feb/2016:18:14:35 +0800	GET /asf-logo.png HTTP/1.1	304    -</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">(省略。。。)</span><br></pre></td></tr></table></figure>
<p><code>表数据见数据文件：localhost_access_log.2016-02-29.txt</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -</span><br><span class="line">192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -</span><br></pre></td></tr></table></figure>
<h2 id="五、Beeline和Hiveserver2（Hive的升级）"><a href="#五、Beeline和Hiveserver2（Hive的升级）" class="headerlink" title="五、Beeline和Hiveserver2（Hive的升级）"></a>五、Beeline和Hiveserver2（Hive的升级）</h2><h4 id="1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）"><a href="#1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）" class="headerlink" title="1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）"></a>1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./hiveserver2</span><br></pre></td></tr></table></figure>
<p>若已经配置环境变量则启动方式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hivesever2</span><br></pre></td></tr></table></figure>
<h4 id="2、启动-beeline（可在服务端-客户端启动，相当于客户端）"><a href="#2、启动-beeline（可在服务端-客户端启动，相当于客户端）" class="headerlink" title="2、启动 beeline（可在服务端|客户端启动，相当于客户端）"></a>2、启动 beeline（可在服务端|客户端启动，相当于客户端）</h4><blockquote>
<p>因为beeline是在Hive安装目录的/bin下，所以只要有hive包都可以启动</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># ./beeline</span><br><span class="line">beeline&gt; !connect jdbc:hive2://node00:10000 root 123456</span><br><span class="line">显示：</span><br><span class="line">Connecting to jdbc:hive2://node00:10000</span><br><span class="line">Connected to: Apache Hive (version 1.2.1)</span><br><span class="line">Driver: Hive JDBC (version 1.2.1)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">0: jdbc:hive2://node00:10000&gt;</span><br><span class="line">使用：列出数据库</span><br><span class="line">0: jdbc:hive2://node00:10000&gt; show databases;</span><br><span class="line">+----------------+--+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+--+</span><br><span class="line">| attr           |</span><br><span class="line">| attribute      |</span><br><span class="line">| default        |</span><br><span class="line">+----------------+--+</span><br><span class="line">3 rows selected (7.493 seconds)</span><br><span class="line">0: jdbc:hive2://node00:10000&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">而在服务端：</span><br><span class="line">显示：</span><br><span class="line"></span><br><span class="line">[root@node00 ~]# hiveserver2</span><br><span class="line">19/01/07 08:52:09 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not exist</span><br><span class="line">OK</span><br><span class="line">OK</span><br><span class="line">OK</span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">退出：</span><br><span class="line">服务端：ctrl + c</span><br><span class="line">客户端：！quit；  或 ctrl + c</span><br><span class="line"></span><br><span class="line">作用：</span><br><span class="line">对操作结果添加了美化。不过不太常用，耗内存，数据大的时候，还影响页面。</span><br></pre></td></tr></table></figure>
<h2 id="六、Hive的JDBC"><a href="#六、Hive的JDBC" class="headerlink" title="六、Hive的JDBC"></a>六、Hive的JDBC</h2><blockquote>
<p>一般是平台使用展示或接口，服务端启动hiveserver2后，在java代码中通过调用hive的jdbc访问默认端口10000进行连接、访问</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HivejdbcClient</span> </span>&#123;    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String driverName = <span class="string">"org.apache.hive.jdbc.HiveDriver"</span>;   </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;        </span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">        &#125;<span class="keyword">catch</span> (ClassNotFoundException)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.exit(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line"><span class="comment">// repalace "hive" here with the name of user the queries should run as</span></span><br><span class="line">        Connection con = DriverManager.getConnection(<span class="string">"jdbc:hive2://node00:10000/default"</span>,<span class="string">"root"</span>,<span class="string">"123456"</span>);</span><br><span class="line">        Statement stmt = con.createStatement();</span><br><span class="line">        String sql = <span class="string">"select * from log limit 0"</span>;</span><br><span class="line">        ResultSet rs = stmt.executeQuery(sql);</span><br><span class="line">        <span class="keyword">while</span>(rs.next())&#123;</span><br><span class="line">            System.out.println(rs.getInt(<span class="number">1</span>)+<span class="string">"-"</span>+rs.getString(<span class="string">"name"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="七、Hive分区与自定义函数UDF-UDAF-UDTF"><a href="#七、Hive分区与自定义函数UDF-UDAF-UDTF" class="headerlink" title="七、Hive分区与自定义函数UDF  UDAF UDTF"></a>七、Hive分区与自定义函数UDF  UDAF UDTF</h2><h3 id="1、Hive的分区partition（重要）"><a href="#1、Hive的分区partition（重要）" class="headerlink" title="1、Hive的分区partition（重要）"></a>1、Hive的分区partition（重要）</h3><blockquote>
<p><code>功能：</code></p>
<p>为了方便海量数据的管理和查询，可以对数据建立分区（可按日期、部门、类型等具体业务）。进行分门别类的管理。</p>
</blockquote>
<blockquote>
<p><code>注意：</code></p>
<ul>
<li><p>必须在定义表的时候创建partition分区</p>
</li>
<li><p>存储数据时，添加分区字段的数据，直接将数据按分区进行存储。<br>   添加分区时：</p>
<p>   ​             时间的格式：/   ： 存储时会乱码，用 -  不会。<br>   ​             需要指定分区<br>   ​             多个分区时，存在父子目录关系，按顺序对应，对应父子<br>   ​             创建表时，已经指定分区个数，就只能填加指定个数的字段数据</p>
</li>
</ul>
<pre><code>   删除分区时：
​            若该分区是父分区的最后一个子区，则父分区也会被删除
​            若删除父分区，其所有子分区也都会备删除
​            若删除的分区，分别在多个不同父分区中存在，则都会被删除
   重命名分区时：
​            修改之后的名字不能是已经存在的
</code></pre><ul>
<li><strong>注意：在创建 删除多分区等操作时一定要注意分区的先后顺序，他们是父子节点的关系。分区字段不要和表字段相同</strong></li>
</ul>
</blockquote>
<blockquote>
<p><code>类别：</code></p>
<ul>
<li>单分区和多分区</li>
<li>静态分区和动态分区</li>
</ul>
</blockquote>
<h4 id="（1）创建分区"><a href="#（1）创建分区" class="headerlink" title="（1）创建分区"></a>（1）创建分区</h4><ul>
<li><h5 id="单分区建表"><a href="#单分区建表" class="headerlink" title="单分区建表"></a>单分区建表</h5></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table day_table(</span><br><span class="line">id int, </span><br><span class="line">content string</span><br><span class="line">) </span><br><span class="line">partitioned by (dt string) </span><br><span class="line">row format delimited fields terminated by &apos;,&apos;;</span><br></pre></td></tr></table></figure>
<p><code>注意：</code>【单分区表，按天分区，在表结构中存在id，content，dt三列；以dt为文件夹区分】</p>
<ul>
<li><h5 id="双分区建表"><a href="#双分区建表" class="headerlink" title="双分区建表"></a>双分区建表</h5></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table day_hour_table (</span><br><span class="line">id int,</span><br><span class="line">content string</span><br><span class="line">) </span><br><span class="line">partitioned by (dt string, hour string) </span><br><span class="line">row format delimited fields terminated by &apos;,&apos;;</span><br></pre></td></tr></table></figure>
<p><code>注意：</code></p>
<p>【双分区表，按天和小时分区，在表结构中新增加了dt和hour两列；先以dt为文件夹，再以hour子文件夹区分】</p>
<h4 id="（2）添加分区表的分区"><a href="#（2）添加分区表的分区" class="headerlink" title="（2）添加分区表的分区"></a>（2）添加分区表的分区</h4><p>（表已创建，在此基础上添加分区：按什么分区）：</p>
<p><code>注意：报错</code>：此时添加，要注意分区的个数相对应，否则会报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FAILED: ValidationFailureSemanticException Partition spec &#123;dt=2008-08-08, hour=08&#125; contains non-partition columns</span><br></pre></td></tr></table></figure>
<p><code>注意：报错</code>此时添加，要注意分区的字段名要对应添加，否则会保如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FAILED: ValidationFailureSemanticException Partition spec &#123;d=2008-08-08&#125; contains non-partition columns</span><br></pre></td></tr></table></figure>
<p><code>注意：</code>一定是存在分区，才可添加</p>
<p> 添加分区：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name</span><br><span class="line">ADD partition_spec [ LOCATION &apos;location1&apos; ] partition_spec [ LOCATION &apos;location2&apos; ] ...</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">例： </span><br><span class="line">ALTER TABLE day_table ADD PARTITION (dt=&apos;2028-08-08&apos;, hour=&apos;08&apos;);</span><br><span class="line">ALTER TABLE day_table ADD PARTITION (dt=&apos;2028-08-08&apos;);</span><br></pre></td></tr></table></figure>
<h4 id="（3）删除分区"><a href="#（3）删除分区" class="headerlink" title="（3）删除分区"></a>（3）删除分区</h4><p>语法：（– 用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。）</p>
<p>删除如双分区中的子级分区时，如果仅剩一个子分区，那么父级分区也会被删除。（连坐）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name DROP partition_spec, partition_spec,...</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line">ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);</span><br><span class="line"></span><br><span class="line">ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;);</span><br></pre></td></tr></table></figure>
<h4 id="（4）数据加载进分区表中"><a href="#（4）数据加载进分区表中" class="headerlink" title="（4）数据加载进分区表中"></a>（4）数据加载进分区表中</h4><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1,partcol2=val2 ...)]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">HDFS：</span><br><span class="line">LOAD DATA INPATH &apos;/user/pv.txt&apos; INTO TABLE day_hour_table PARTITION(dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);</span><br><span class="line"></span><br><span class="line">本地：</span><br><span class="line">LOAD DATA local INPATH &apos;/user/hua/*&apos; INTO TABLE day_hour partition(dt=&apos;2010-07-07&apos;);</span><br></pre></td></tr></table></figure>
<h4 id="（5）查看表的所有分区"><a href="#（5）查看表的所有分区" class="headerlink" title="（5）查看表的所有分区"></a>（5）查看表的所有分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show partitions day_hour_table;</span><br><span class="line"></span><br><span class="line">show partitions day_table;</span><br></pre></td></tr></table></figure>
<h4 id="（6）重命名分区"><a href="#（6）重命名分区" class="headerlink" title="（6）重命名分区"></a>（6）重命名分区</h4><p>语法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">例：</span><br><span class="line">ALTER TABLE day_table PARTITION (tian=&apos;2018-05-01&apos;) RENAME TO PARTITION (tain=&apos;2018-06-01&apos;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Hive的函数课参考官网，用时查阅即可： <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</a></p>
</blockquote>
<h4 id="（7）动态分区-重要-–注意外部表"><a href="#（7）动态分区-重要-–注意外部表" class="headerlink" title="（7）动态分区(重要)–注意外部表"></a>（7）动态分区(重要)–注意外部表</h4><ol>
<li><p>在本地文件/home/grid/a.txt中写入以下4行数据</p>
<blockquote>
<p> aaa,US,CA<br> aaa,US,CB<br> bbb,CA,BB<br> bbb,CA,BC</p>
</blockquote>
</li>
<li><p>建立非分区表并加载数据 </p>
</li>
</ol>
<blockquote>
<p>创建表</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE info1 (</span><br><span class="line">      name STRING, </span><br><span class="line">      cty STRING, </span><br><span class="line">      st STRING</span><br><span class="line">) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>加载数据</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;/root/su/a&apos; INTO TABLE info1;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>查看</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM info1;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>建立外部分区表并动态加载数据  （注意删除外部表的相关事项）</li>
</ol>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; CREATE EXTERNAL TABLE info2 (</span><br><span class="line">&gt; name STRING</span><br><span class="line">&gt; ) </span><br><span class="line">&gt; PARTITIONED BY (country STRING, state STRING);   </span><br><span class="line">&gt; </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
</blockquote>
<ol start="4">
<li>这时候就需要使用动态分区来实现，使用动态分区需要注意设定以下参数：</li>
</ol>
<blockquote>
<p> <strong>hive.exec.dynamic.partition</strong></p>
<p>默认值：false</p>
<p>是否开启动态分区功能，默认false关闭。</p>
<p>使用动态分区时候，该参数必须设置成true;</p>
</blockquote>
<blockquote>
<p><strong>hive.exec.dynamic.partition.mode</strong></p>
<p>默认值：strict</p>
<p>动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。</p>
<p>一般需要设置为nonstrict</p>
</blockquote>
<blockquote>
<p> <strong>hive.exec.max.dynamic.partitions.pernode</strong></p>
<p>默认值：100</p>
<p>在每个执行MR的节点上，最大可以创建多少个动态分区。</p>
<p>该参数需要根据实际的数据来设定。</p>
<p>比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</p>
</blockquote>
<blockquote>
<p><strong>hive.exec.max.dynamic.partitions</strong></p>
<p>默认值：1000</p>
<p>在所有执行MR的节点上，最大一共可以创建多少个动态分区。</p>
<p>同上参数解释。</p>
</blockquote>
<blockquote>
<p> <strong>hive.exec.max.created.files</strong></p>
<p>默认值：100000</p>
<p>整个MR Job中，最大可以创建多少个HDFS文件。</p>
<p>一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。</p>
</blockquote>
<blockquote>
<p> <strong>hive.error.on.empty.partition</strong></p>
<p>默认值：false</p>
<p>当有空分区生成时，是否抛出异常。</p>
<p>一般不需要设置。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition=true;  </span><br><span class="line"></span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;  </span><br><span class="line"></span><br><span class="line">set hive.exec.max.dynamic.partitions.pernode=1000;  </span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; </span><br><span class="line"></span><br><span class="line">INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; </span><br><span class="line"></span><br><span class="line">SELECT * FROM info2;</span><br></pre></td></tr></table></figure>
<h3 id="2、自定义函数UDF-UDAF-UDTF"><a href="#2、自定义函数UDF-UDAF-UDTF" class="headerlink" title="2、自定义函数UDF  UDAF UDTF"></a>2、自定义函数UDF  UDAF UDTF</h3><blockquote>
<p>自定义函数包括三种 UDF、UDAF、UDTF</p>
<p>UDF：一进一出</p>
<p>UDAF：聚集函数，多进一出。如：Count/max/min</p>
<p>UDTF：一进多出，如 lateralview  explore()，（类似于mysql中的视图）</p>
<p><strong>使用方式</strong> ：在HIVE会话中add自定义函数的jar 文件，然后创建 function 继而使用函数</p>
</blockquote>
<h4 id="（1）UDF-开发（用的多一点）"><a href="#（1）UDF-开发（用的多一点）" class="headerlink" title="（1）UDF 开发（用的多一点）"></a>（1）UDF 开发（用的多一点）</h4><p><strong>1、</strong>UDF函数可以直接应用于 select 语句，对查询结构做格式化处理后，再输出内容。</p>
<p><strong>2、</strong>编写 UDF 函数的时候需要注意一下几点：</p>
<p>  a）自定义 UDF 需要<code>继承</code> org.apache.hadoop.hive.ql.<code>UDF</code>。</p>
<p>  b）需要<code>实现</code> <code>evaluate</code> 函数，evaluate 函数支持重载。</p>
<p><strong>3、</strong>步骤</p>
<p>  a）把程序打包放到目标机器上去；</p>
<p>（需要hive和hadoop，jdk 的相关jar包）</p>
<p>函数一：脱敏处理</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.hive.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TuoMing</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> Text res = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(String string)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 校验参数是否为空</span></span><br><span class="line">		<span class="keyword">if</span>(string==<span class="keyword">null</span>)&#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">		&#125;</span><br><span class="line">        <span class="comment">// 若为单个字符        </span></span><br><span class="line">		<span class="keyword">if</span>(string.length()==<span class="number">1</span>)&#123;</span><br><span class="line">			res.set(<span class="string">"*"</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">	  String str1 = string.substring(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">	  String str2 = string.substring(string.length()-<span class="number">1</span>,string.length());</span><br><span class="line">	  res.set(str1+<span class="string">"***"</span>+str2);</span><br><span class="line">	  <span class="keyword">return</span> res;</span><br><span class="line">	  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>函数二：add函数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.bigdata.hive.udf;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Add</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> Text res = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(String num1,String num2)</span> </span>&#123;</span><br><span class="line">		<span class="comment">// 校验参数是否为空</span></span><br><span class="line">		<span class="keyword">if</span>(num1==<span class="keyword">null</span>)&#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">		&#125;<span class="keyword">else</span> <span class="keyword">if</span>(num2==<span class="keyword">null</span>)&#123;</span><br><span class="line">			res.set(num1);</span><br><span class="line">			<span class="keyword">return</span> res;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">int</span> n = Integer.parseInt(num1)+Integer.parseInt(num2);</span><br><span class="line">	    String str =n+<span class="string">""</span>;</span><br><span class="line">	    res.set(str);</span><br><span class="line">	    <span class="keyword">return</span> res;</span><br><span class="line">	  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  b）进入 hive 客户端，添加 jar 包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;add jar /root/su/TuoMing.jar;</span><br><span class="line">(相当于添加到环境变量中)</span><br><span class="line">(清除缓存时记得删除jar包： delete jar /*)</span><br><span class="line">delete jar /jar/udf_test.jar;</span><br></pre></td></tr></table></figure>
<p>  c）创建临时函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;CREATE TEMPORARY FUNCTION add_example AS &apos;hive.udf.add&apos;;</span><br><span class="line">CREATE TEMPORARY FUNCTION tm_example AS &apos;com.bigdata.hive.udf.TuoMing&apos;;</span><br><span class="line">（as 后面添加的是：包名+类名）</span><br></pre></td></tr></table></figure>
<p>  d）查询 HQL 语句：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT  add_example(8,  9)  FROM  scores;</span><br><span class="line"></span><br><span class="line">SELECT  add_example(scores.math,  scores.art)  FROM  scores;</span><br><span class="line"></span><br><span class="line">SELECT  tm_example(id)  FROM  log;</span><br></pre></td></tr></table></figure>
<p>  e）销毁临时函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  DROP  TEMPORARY  FUNCTION  tm_example;</span><br></pre></td></tr></table></figure>
<h4 id="（2）UDAF自定义集函数-用的少"><a href="#（2）UDAF自定义集函数-用的少" class="headerlink" title="（2）UDAF自定义集函数(用的少)"></a>（2）UDAF自定义集函数(用的少)</h4><blockquote>
<p>多行进一行出，如 sum()、min()，用在 group  by 时</p>
</blockquote>
<p><strong>1.</strong>必须<code>继承</code>org.apache.hadoop.hive.ql.exec.<code>UDAF</code>(函数类继承)</p>
<p>org.apache.hadoop.hive.ql.exec.<code>UDAFEvaluator</code>(内部类 Eval uator 实现 UDAFEvaluator 接口)</p>
<p><strong>2.</strong>Evaluator 需要实现 <code>init、iterate、terminatePartial、merge、terminate</code> 这几个函数</p>
<blockquote>
<ul>
<li><p>init():类似于构造函数，用于 UDAF 的初始化</p>
</li>
<li><p>iterate():接收传入的参数，并进行内部的轮转，返回 boolean</p>
</li>
<li><p>terminatePartial():无参数，其为 iterate 函数轮转结束后，返回轮转数据，</p>
</li>
</ul>
<p>类似于 hadoop 的Combinermerge():接收 terminatePartial 的返回结果，进行数据 merge 操作，</p>
<p>​                                                                  其返回类型为 boolean </p>
<ul>
<li>terminate():返回最终的聚集函数结果</li>
</ul>
</blockquote>
<p>开发一个功能同：</p>
<blockquote>
<p>Oracle 的 wm_concat()函数</p>
<p>Mysql 的 group_concat()</p>
</blockquote>
<blockquote>
<p>Hive  UDF 的数据类型：</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz53mse6n1j30fe0c3jxm.jpg" alt="Hive  UDF 的数据类型："></p>
<h4 id="（3）UDTF（用的少一点）"><a href="#（3）UDTF（用的少一点）" class="headerlink" title="（3）UDTF（用的少一点）"></a>（3）UDTF（用的少一点）</h4><p>UDTF：一进多出，如 lateral  view  explode( )  返回一个数组表</p>
<blockquote>
<p><strong>Hive Lateral View</strong>   视图</p>
<p>Lateral View用于和UDTF函数（explode、split）结合来使用。</p>
<p>首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。</p>
<p><code>主要解决</code></p>
<p>在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题</p>
</blockquote>
<blockquote>
<p><code>语法：</code></p>
<p>LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias)</p>
</blockquote>
<blockquote>
<p><code>例：</code></p>
<p>统计人员表中共有多少种爱好、多少个城市?</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; select count(distinct(myCol1)), count(distinct(myCol2))，count(distinct(myCol3))from log2 </span><br><span class="line">&gt;       LATERAL VIEW explode(likes) myTable1 AS myCol1 </span><br><span class="line">&gt;       LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select myCol1, myCol2 from log2 </span><br><span class="line">      LATERAL VIEW explode(likes) myTable1 AS myCol1 </span><br><span class="line">      LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>distinct(myCol1) 表示去重</p>
</blockquote>
<blockquote>
<p>LATERAL VIEW explode(likes) myTable1   AS myCol1   </p>
<p>将likes查询结果放到mytable1表中，作为字段myCol1     </p>
</blockquote>
<blockquote>
<p> LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;</p>
<p>将address查询结果放到myTable2 表中，作为字段myCol2，myCol3，因为address是包含K-V的（两个）</p>
</blockquote>
<h2 id="八、Hive索引-知道"><a href="#八、Hive索引-知道" class="headerlink" title="八、Hive索引(知道)"></a>八、Hive索引(知道)</h2><blockquote>
<p>一个表上创建索引：</p>
<p>使用给定的列表的列作为键创建一个索引。</p>
<p>详见创建<a href="javascript:changelink(&#39;https://cwiki.apache.org/confluence/display/Hive/IndexDev#IndexDev-CREATEINDEX&#39;,&#39;EN2ZH_CN&#39;" target="_blank" rel="noopener">索引</a>;)设计文档。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CREATE INDEX index_name</span><br><span class="line">  ON TABLE base_table_name (col_name, ...)</span><br><span class="line">  AS index_type</span><br><span class="line">  [WITH DEFERRED REBUILD]</span><br><span class="line">  [IDXPROPERTIES (property_name=property_value, ...)]</span><br><span class="line">  [IN TABLE index_table_name]</span><br><span class="line">  [</span><br><span class="line">     [ ROW FORMAT ...] STORED AS ...</span><br><span class="line">     | STORED BY ...</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (...)]</span><br><span class="line">  [COMMENT &quot;index comment&quot;];</span><br></pre></td></tr></table></figure>
<h2 id="九、案例实践"><a href="#九、案例实践" class="headerlink" title="九、案例实践"></a>九、案例实践</h2><h3 id="案例一：-基站掉话率"><a href="#案例一：-基站掉话率" class="headerlink" title="案例一：(基站掉话率)"></a>案例一：(基站掉话率)</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz53p74gyrj30fe07sab1.jpg" alt="基站掉话率"></p>
<h4 id="1、创建表"><a href="#1、创建表" class="headerlink" title="1、创建表"></a>1、创建表</h4><p>cell_monitor表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table cell_monitor(</span><br><span class="line">        record_time string,</span><br><span class="line">        imei string,</span><br><span class="line">        cell string,</span><br><span class="line">        ph_num int,</span><br><span class="line">        call_num int,</span><br><span class="line">        drop_num int,</span><br><span class="line">        duration int,</span><br><span class="line">        drop_rate DOUBLE,</span><br><span class="line">        net_type string,</span><br><span class="line">        erl string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;</span><br><span class="line">STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure>
<p> 结果表cell_drop_monitor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create table cell_drop_monitor(</span><br><span class="line">imei string,</span><br><span class="line">total_call_num int,</span><br><span class="line">total_drop_num int,</span><br><span class="line">d_rate DOUBLE</span><br><span class="line">) </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure>
<h4 id="2、load数据"><a href="#2、load数据" class="headerlink" title="2、load数据"></a><strong>2</strong>、load<strong>数据</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA LOCAL INPATH &apos;/root/su/cdr_summ_imei_cell_info.csv&apos; OVERWRITE INTO TABLE cell_monitor;</span><br></pre></td></tr></table></figure>
<h4 id="3、找出掉线率最高的基站"><a href="#3、找出掉线率最高的基站" class="headerlink" title="3、找出掉线率最高的基站"></a><strong>3</strong>、找出掉线率最高的基站</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from cell_monitor cm </span><br><span class="line">insert overwrite table cell_drop_monitor  </span><br><span class="line">select cm.imei ,sum(cm.drop_num),sum(cm.duration),sum(cm.drop_num)/sum(cm.duration) d_rate </span><br><span class="line">group by cm.imei </span><br><span class="line">sort by d_rate desc;</span><br></pre></td></tr></table></figure>
<h3 id="案例二：（单词统计）"><a href="#案例二：（单词统计）" class="headerlink" title="案例二：（单词统计）"></a>案例二：（单词统计）</h3><h4 id="1、建表"><a href="#1、建表" class="headerlink" title="1、建表"></a><strong>1</strong>、建表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table docs(line string);</span><br><span class="line">create table wc(word string, totalword int);</span><br></pre></td></tr></table></figure>
<h4 id="2、加载数据"><a href="#2、加载数据" class="headerlink" title="2、加载数据"></a>2、加载数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/tmp/wc&apos; into table docs;</span><br></pre></td></tr></table></figure>
<h4 id="3、统计"><a href="#3、统计" class="headerlink" title="3、统计"></a>3、统计</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from (select explode(split(line, &apos; &apos;)) as word from docs) w </span><br><span class="line">insert into table wc </span><br><span class="line">  select word, count(1) as totalword </span><br><span class="line">  group by word </span><br><span class="line">  order by word;</span><br></pre></td></tr></table></figure>
<h4 id="4、查询结果"><a href="#4、查询结果" class="headerlink" title="4、查询结果"></a><strong>4</strong>、查询结果</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from wc;</span><br></pre></td></tr></table></figure>
<h2 id="十、分桶（重要）"><a href="#十、分桶（重要）" class="headerlink" title="十、分桶（重要）"></a>十、分桶（重要）</h2><h3 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h3><blockquote>
<ul>
<li><p>主要应用于<code>数据抽样</code>。</p>
</li>
<li><p>通过对<code>列值取哈希</code>值的方式，将不同数据放到不同的文件中存储。</p>
</li>
<li><p>对Hive中每个<code>表</code>、<code>分区</code>都可以进行分桶。</p>
</li>
<li><p>列的哈希值 /桶的个数→<code>决定</code>每条数据划分到哪个桶中</p>
</li>
</ul>
</blockquote>
<h3 id="2、开启支持分桶"><a href="#2、开启支持分桶" class="headerlink" title="2、开启支持分桶"></a>2、开启支持分桶</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.enforce.bucketing=true;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>默认：false；</p>
<p>设置为true之后，mr运行时会根据bucket的个数自动分配reduce task个数。</p>
<p>（用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用）</p>
<p><strong>一次作业产生的桶数 = reducde task数</strong></p>
</blockquote>
<h3 id="3、往分桶表中加载数据"><a href="#3、往分桶表中加载数据" class="headerlink" title="3、往分桶表中加载数据"></a>3、往分桶表中加载数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into table bucket_table select columns from tbl;</span><br><span class="line">insert overwrite table bucket_table select columns from tbl;</span><br></pre></td></tr></table></figure>
<h3 id="4、桶表"><a href="#4、桶表" class="headerlink" title="4、桶表"></a>4、桶表</h3><h3 id="抽样查询"><a href="#抽样查询" class="headerlink" title="抽样查询"></a>抽样查询</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from bucket_table tablesample(bucket 1 out of 4 on columns);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>TABLESAMPLE语法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; TABLESAMPLE(BUCKET x OUT OF y)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>x：表示从哪个bucket开始抽取数据，<code>x&lt;=y</code></p>
<p>y：必须为该表总bucket数的<code>倍数</code>或<code>因子</code></p>
<p>理解：</p>
<p>分桶表已经按age分为4桶，然后，有y个人去抽，从第(x 取模 桶数)桶中抽</p>
</blockquote>
<h3 id="5、实战"><a href="#5、实战" class="headerlink" title="5、实战"></a>5、实战</h3><p>创建普通表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE mm( </span><br><span class="line">id INT, </span><br><span class="line">name STRING, </span><br><span class="line">age INT</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure>
<p>测试数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1,tom,11</span><br><span class="line">2,cat,22</span><br><span class="line">3,dog,33</span><br><span class="line">4,hive,44</span><br><span class="line">5,hbase,55</span><br><span class="line">6,mr,66</span><br><span class="line">7,alice,77</span><br><span class="line">8,scala,88</span><br></pre></td></tr></table></figure>
<p>加载数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/root/su/mm&apos; into table mm;</span><br></pre></td></tr></table></figure>
<p><strong>创建分桶表</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE psnbucket( </span><br><span class="line">id INT, </span><br><span class="line">name STRING, </span><br><span class="line">age INT</span><br><span class="line">)</span><br><span class="line">CLUSTERED BY (age) INTO 4 BUCKETS </span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;;</span><br></pre></td></tr></table></figure>
<p><strong>加载数据：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table psnbucket select id, name, age from mm;</span><br></pre></td></tr></table></figure>
<p><strong>抽样</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select id, name, age from psnbucket tablesample(bucket 2 out of 4 on age);</span><br></pre></td></tr></table></figure>
<p><code>注意：</code></p>
<blockquote>
<p> hive&gt; select id, name, age from psnbucket tablesample(bucket 4 out of 2 on age);<br>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table psnbucket</p>
<p>denominator  : 分母</p>
</blockquote>
<h2 id="十一、运行方式"><a href="#十一、运行方式" class="headerlink" title="十一、运行方式"></a>十一、运行方式</h2><h3 id="1、Hive运行模式"><a href="#1、Hive运行模式" class="headerlink" title="1、Hive运行模式"></a>1、Hive运行模式</h3><blockquote>
<h4 id="–-命令行方式cli：控制台模式"><a href="#–-命令行方式cli：控制台模式" class="headerlink" title="– 命令行方式cli：控制台模式"></a>– 命令行方式cli：控制台模式</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--与hdfs交互</span><br><span class="line">  * 执行dfs命令</span><br><span class="line">  * 例 ：hive&gt; dfs -ls /</span><br><span class="line">  </span><br><span class="line">--与Linux交互</span><br><span class="line">   *  ！ 开头</span><br><span class="line">   *  hive&gt; !pwd</span><br></pre></td></tr></table></figure>
<blockquote>
<h4 id="–脚本运行方式：（生产中常用）"><a href="#–脚本运行方式：（生产中常用）" class="headerlink" title="–脚本运行方式：（生产中常用）"></a>–脚本运行方式：（生产中常用）</h4></blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">在外部shell中执行,指定数据库,分号可加可不加</span><br><span class="line"># hive -e &quot;select * from attr.log &quot;</span><br><span class="line"># hive -e &quot;select * from attr.log；select * from default.log2&quot;</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">将执行结果重定向到指定文件：</span><br><span class="line"># hive -e &quot;select * from attr.log &quot; &gt;&gt;log1</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">静默模式执行，不打印log日志</span><br><span class="line"># hive -S -e &quot;select * from attr.log &quot; &gt;&gt;log1</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">脚本执行</span><br><span class="line">先编辑脚本问价</span><br><span class="line"># vim file1</span><br><span class="line">编辑内容</span><br><span class="line">select * from attr.log where id = 1;</span><br><span class="line">select * from attr.log where id &lt; 3;</span><br><span class="line">执行脚本</span><br><span class="line"># hive -f file1</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">?? 使用命令文件执行hive-init.sql</span><br><span class="line">?? # hive -i /home/hive-init.sql</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">在hive cli中执行脚本文件</span><br><span class="line">hive&gt; source file1</span><br></pre></td></tr></table></figure>
<h3 id="？未解决？"><a href="#？未解决？" class="headerlink" title="？未解决？"></a>？未解决？</h3><blockquote>
<p>?? 使用命令文件执行hive-init.sql<br>?? # hive -i /home/hive-init.sql</p>
</blockquote>
<h2 id="十二、hive的GUI接口（web页面）"><a href="#十二、hive的GUI接口（web页面）" class="headerlink" title="十二、hive的GUI接口（web页面）"></a>十二、hive的GUI接口（web页面）</h2><p>Hive Web GUI接口</p>
<h3 id="web界面安装："><a href="#web界面安装：" class="headerlink" title="web界面安装："></a>web界面安装：</h3><p><strong>1、</strong>下载源码包apache-hive-1.2.1-src.tar.gz,</p>
<p><strong>2、</strong>在本地Windows系统中解压</p>
<p>并将\apache-hive-1.2.1-src\hwi\web路径中所有的文件打成war包</p>
<p><code>制作方法：</code></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5b7di85fj30rn0eodhm.jpg" alt="war包"></p>
<blockquote>
<p>1、到\apache-hive-1.2.1-src\hwi\web路径下</p>
<p>2、在路径栏输入命令：jar -cvf hive-hwi.war *</p>
<p>3、即可生成文件：hive-hwi.war</p>
</blockquote>
<p><strong>3、</strong>将hwi-war包放在$HIVE_HOME/lib/中（Linux系统）</p>
<p><strong>4、</strong>复制tools.jar(在jdk的lib目录下)到$HIVE_HOME/lib下</p>
<p><strong>5、</strong>修改hive-site.xml</p>
<p>路径：/usr/soft/apache-hive-1.2.1-bin/conf/hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.listen.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.listen.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>9999<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.war.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>lib/hive-hwi.war<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>6、</strong>启动hwi服务(端口号9999)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hwi</span><br></pre></td></tr></table></figure>
<p><strong>7、</strong>浏览器通过以下链接来访问</p>
<p><a href="http://node00:9999/hwi/" target="_blank" rel="noopener">http://node00:9999/hwi/</a></p>
<p><strong>8、</strong>登录页面：</p>
<p>USER:</p>
<p>GROUPS:</p>
<p>自已定义</p>
<h2 id="十三、权限管理"><a href="#十三、权限管理" class="headerlink" title="十三、权限管理"></a>十三、权限管理</h2><p>Hive - SQL Standards Based Authorization in  HiveServer2</p>
<h3 id="（1）三种授权模型"><a href="#（1）三种授权模型" class="headerlink" title="（1）三种授权模型"></a>（1）三种授权模型</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5bus4ks8j30kb08o3zp.jpg" alt=""></p>
<h3 id="（2）常用：基于SQL标准的完全兼容SQL的授权模型"><a href="#（2）常用：基于SQL标准的完全兼容SQL的授权模型" class="headerlink" title="（2）常用：基于SQL标准的完全兼容SQL的授权模型"></a>（2）常用：基于SQL标准的完全兼容SQL的授权模型</h3><p>特点：</p>
<ul>
<li><p>支持对于用户的授权认证</p>
</li>
<li><p>支持角色role的授权认证</p>
</li>
<li><p>role可理解为是一组权限的集合，通过role为用户授权</p>
<p> 一个用户可以具有一个或多个角色</p>
<p>​    默认包含俩种角色：public、admin</p>
</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5c0cf550j30k406tjtk.jpg" alt="限制"></p>
<h3 id="（3）操作"><a href="#（3）操作" class="headerlink" title="（3）操作"></a>（3）操作</h3><p>在<code>hive服务端</code>修改配置文件hive-site.xml添加以下配置内容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.users.in.admin.role<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authenticator.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><strong>服务端启动hiveserver2；客户端通过beeline进行连接</strong></p>
<p>角色的添加、删除、查看、设置：</p>
<p>第一次操作无权限：</p>
<p>需要：CREATE ROLE  admin；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CREATE ROLE role_name;  			 -- 创建角色</span><br><span class="line">DROP ROLE role_name;  				 -- 删除角色</span><br><span class="line">SET ROLE (role_name|ALL|NONE); 		 -- 设置角色</span><br><span class="line">SHOW CURRENT ROLES;    				 -- 查看当前具有的角色</span><br><span class="line">SHOW ROLES;    				         -- 查看所有存在的角色</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5c6wynhnj30hs07kgmv.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz5c8bvsjkj30im077wft.jpg" alt=""></p>
<p>【官网：权限】</p>
<table>
<thead>
<tr>
<th>Action</th>
<th>Select</th>
<th>Insert</th>
<th>Update</th>
<th>Delete</th>
<th>Owership</th>
<th>Admin</th>
<th>URL Privilege(RWX   Permission + Ownership)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ALTER DATABASE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>ALTER INDEX PROPERTIES</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ALTER INDEX REBUILD</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ALTER PARTITION LOCATION</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td>Y (for new partition   location)</td>
</tr>
<tr>
<td>ALTER TABLE (all of them   except the ones above)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ALTER TABLE ADD PARTITION</td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y (for partition location)</td>
</tr>
<tr>
<td>ALTER TABLE DROP PARTITION</td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ALTER TABLE LOCATION</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td>Y (for new location)</td>
</tr>
<tr>
<td>ALTER VIEW PROPERTIES</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ALTER VIEW RENAME</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ANALYZE TABLE</td>
<td>Y</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CREATE DATABASE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y (if custom location   specified)</td>
</tr>
<tr>
<td>CREATE FUNCTION</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>CREATE INDEX</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y (of table)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CREATE MACRO</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>CREATE TABLE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y (of database)</td>
<td></td>
<td>Y  (for create   external table – the location)</td>
</tr>
<tr>
<td>CREATE TABLE AS SELECT</td>
<td>Y (of input)</td>
<td></td>
<td></td>
<td></td>
<td>Y (of database)</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CREATE VIEW</td>
<td>Y + G</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DELETE</td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DESCRIBE TABLE</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DROP DATABASE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DROP FUNCTION</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>DROP INDEX</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DROP MACRO</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>DROP TABLE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DROP VIEW</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DROP VIEW PROPERTIES</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>EXPLAIN</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>INSERT</td>
<td></td>
<td>Y</td>
<td></td>
<td>Y (for OVERWRITE)</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LOAD</td>
<td></td>
<td>Y (output)</td>
<td></td>
<td>Y (output)</td>
<td></td>
<td></td>
<td>Y (input location)</td>
</tr>
<tr>
<td>MSCK (metastore check)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
</tr>
<tr>
<td>SELECT</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SHOW COLUMNS</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SHOW CREATE TABLE</td>
<td>Y+G</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SHOW PARTITIONS</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SHOW TABLE PROPERTIES</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SHOW TABLE STATUS</td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TRUNCATE TABLE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td>UPDATE</td>
<td></td>
<td></td>
<td>Y</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="十四、Hive优化（重点）"><a href="#十四、Hive优化（重点）" class="headerlink" title="十四、Hive优化（重点）"></a>十四、Hive优化（重点）</h2><p><code>详见Hive优化文档</code></p>
<p>HIve常用函数：</p>
<p><a href="https://www.cnblogs.com/kimbo/p/6288516.html" target="_blank" rel="noopener">https://www.cnblogs.com/kimbo/p/6288516.html</a></p>
<p><a href="https://www.iteblog.com/archives/2258.html#3_avg" target="_blank" rel="noopener">https://www.iteblog.com/archives/2258.html#3_avg</a></p>
<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions</a></p>
<p>MapReducde底层源码：</p>
<ol>
<li><a href="http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD</a></li>
<li><a href="http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1</a></li>
<li><a href="http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3</a></li>
<li><a href="http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4" target="_blank" rel="noopener">http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4</a></li>
</ol>
]]></content>
        
        <categories>
            
            <category> BigData </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> MySQL </tag>
            
            <tag> 分布式 </tag>
            
            <tag> Hive </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Linux系统数据库MySQL安装]]></title>
        <url>http://sungithup.github.io/2019/01/10/Linux%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E5%AE%89%E8%A3%85/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="Linux系统数据库MySQL安装"><a href="#Linux系统数据库MySQL安装" class="headerlink" title="Linux系统数据库MySQL安装"></a>Linux系统数据库MySQL安装</h1><h2 id="一、第一次安装MySQL"><a href="#一、第一次安装MySQL" class="headerlink" title="一、第一次安装MySQL"></a>一、第一次安装MySQL</h2><h3 id="1、yum安装"><a href="#1、yum安装" class="headerlink" title="1、yum安装"></a>1、yum安装</h3><blockquote>
<p>命令 ： yum -y install mysql-server mysql-devel</p>
</blockquote>
<h3 id="2、登录"><a href="#2、登录" class="headerlink" title="2、登录"></a>2、登录</h3><blockquote>
<p>命令 ： mysql -u -p</p>
</blockquote>
<p><code>显示：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure>
<h3 id="3、查看数据库-注意用‘-’结束"><a href="#3、查看数据库-注意用‘-’结束" class="headerlink" title="3、查看数据库(注意用‘ ; ’结束 )"></a>3、查看数据库(注意用‘ ; ’结束 )</h3><blockquote>
<p>命令 ： show databases;</p>
</blockquote>
<h3 id="4、退出："><a href="#4、退出：" class="headerlink" title="4、退出："></a>4、退出：</h3><blockquote>
<p>命令： quit；</p>
</blockquote>
<h3 id="5、创建用户："><a href="#5、创建用户：" class="headerlink" title="5、创建用户："></a>5、创建用户：</h3><blockquote>
<p>命令 ： mysqladmin -uroot password 123456</p>
</blockquote>
<h3 id="6、再登录："><a href="#6、再登录：" class="headerlink" title="6、再登录："></a>6、再登录：</h3><blockquote>
<p>命令 ：mysql -u root -p</p>
</blockquote>
<p><code>显示：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure>
<p>说明成功了！</p>
<h3 id="7、数据库操作："><a href="#7、数据库操作：" class="headerlink" title="7、数据库操作："></a>7、数据库操作：</h3><blockquote>
<p>命令 ： use mysql;</p>
</blockquote>
<p><code>显示：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line"></span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure>
<h3 id="8、查看用户数据表："><a href="#8、查看用户数据表：" class="headerlink" title="8、查看用户数据表："></a>8、查看用户数据表：</h3><blockquote>
<p>命令 ： show tables;</p>
</blockquote>
<h3 id="9、查询user表部分字段："><a href="#9、查询user表部分字段：" class="headerlink" title="9、查询user表部分字段："></a>9、查询user表部分字段：</h3><blockquote>
<p>命令 ： select host,user,password from user;</p>
</blockquote>
<h3 id="10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确"><a href="#10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确" class="headerlink" title="10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确"></a>10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确</h3><h4 id="（1）推荐"><a href="#（1）推荐" class="headerlink" title="（1）推荐"></a>（1）推荐</h4><p>现将user表中其他无密码的记录删除</p>
<blockquote>
<p>命令 ： delete from user where password = ‘ ‘;</p>
</blockquote>
<h4 id="2-更新有密码的记录的host字段值"><a href="#2-更新有密码的记录的host字段值" class="headerlink" title="(2)更新有密码的记录的host字段值"></a>(2)更新有密码的记录的host字段值</h4><blockquote>
<p>命令 ： update user set host = “%”;</p>
</blockquote>
<h4 id="3-刷新权限"><a href="#3-刷新权限" class="headerlink" title="(3)刷新权限"></a>(3)刷新权限</h4><blockquote>
<p> 命令 ：flush privileges;</p>
</blockquote>
<h4 id="4-退出"><a href="#4-退出" class="headerlink" title="(4)退出"></a>(4)退出</h4><blockquote>
<p>命令 ：quit;</p>
</blockquote>
<h2 id="二、Linux系统登录数据库MySQL报错"><a href="#二、Linux系统登录数据库MySQL报错" class="headerlink" title="二、Linux系统登录数据库MySQL报错"></a>二、Linux系统登录数据库MySQL报错</h2><h3 id="报错一："><a href="#报错一：" class="headerlink" title="报错一："></a>报错一：</h3><p>1、登录 </p>
<blockquote>
<p>mysqld -uroot -p123456</p>
</blockquote>
<p><code>报错：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (111)</span><br></pre></td></tr></table></figure>
<p><code>解决：</code></p>
<p>1）、先删除mysql.sock</p>
<blockquote>
<p>cd /var/lib/mysql</p>
</blockquote>
<blockquote>
<p>mv  mysql.sock  mysql.sock.bak</p>
</blockquote>
<p>2）、再次登陆</p>
<blockquote>
<p>mysql -uroot -p123456</p>
</blockquote>
<p><code>报错：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (2)</span><br></pre></td></tr></table></figure>
<p> 3）、看看mysql的状态，</p>
<blockquote>
<p> /etc/rc.d/init.d/mysqld status</p>
</blockquote>
<p> <code>显示：</code></p>
<blockquote>
<p> mysqld is stopped</p>
</blockquote>
<p>4）、看看是不是mysql的权限问题<br>在/var/lib目录下：</p>
<blockquote>
<p>ls -lt|grep mysql</p>
</blockquote>
<p><code>显示：</code></p>
<blockquote>
<p>drwxr-xr-x. 4 mysql   mysql 4096 Jan  6 11:09 mysql</p>
</blockquote>
<p>5）、说明mysql服务没有启动</p>
<p>2、启动mysql服务</p>
<blockquote>
<p>/etc/init.d/mysqld start</p>
</blockquote>
<p><code>显示：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Starting mysqld:                                           [  OK  ]</span><br></pre></td></tr></table></figure>
<p>3、再次登录：</p>
<blockquote>
<p>mysql -uroot -p123456</p>
</blockquote>
<p><code>显示：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.1.73 Source distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure>
<p>4、退出</p>
<blockquote>
<p>quit</p>
</blockquote>
<p>5、解决出现mysql.sock的问题</p>
<blockquote>
<p>  （1）、vim /etc/mycnf</p>
</blockquote>
<p>  编辑内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">skip_name_resolve=on innodb_file_per_table=on</span><br></pre></td></tr></table></figure>
<p>  按esc :wq 保存并退出<br>  （2）使用命令：</p>
<blockquote>
<p>mysql_secure_installation</p>
</blockquote>
<p>  （3）直接[ enter ] 键，输入密码，</p>
<p>(另推荐：Jakie_ZHF老师的博客)</p>
<h3 id="报错二："><a href="#报错二：" class="headerlink" title="报错二："></a>报错二：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)</span><br></pre></td></tr></table></figure>
]]></content>
        
        <categories>
            
            <category> MySQL </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> MySQL </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[大数据思想]]></title>
        <url>http://sungithup.github.io/2019/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%80%9D%E6%83%B3/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h3 id="1、大数据核心问题："><a href="#1、大数据核心问题：" class="headerlink" title="1、大数据核心问题："></a>1、大数据核心问题：</h3><p>==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）==</p>
<h3 id="2、大数据思维"><a href="#2、大数据思维" class="headerlink" title="2、大数据思维"></a>2、大数据思维</h3><p><strong>分而治之</strong></p>
<blockquote>
<p>把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q）</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9d7pkpij30pr0eut99.jpg" alt="enter description here"></p>
<h3 id="3、业务场景"><a href="#3、业务场景" class="headerlink" title="3、业务场景"></a>3、业务场景</h3><p>仓储、数牌</p>
<h4 id="业务一：找-重复行-chongfuhang"><a href="#业务一：找-重复行-chongfuhang" class="headerlink" title="业务一：找{重复行}(chongfuhang)"></a><strong>业务一：找{重复行}(chongfuhang)</strong></h4><p><em>++现有1TB的TXT文件 ;<br>格式：数字+字符 ；<br>网速：500M/s ；<br>服务器内存大小：128M ；<br>条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++</em></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9cu9psdj30ht0a1wf1.jpg" alt="enter description here"></p>
<p><strong>==方法==</strong></p>
<p> 答：共需要2次IO：2*30min=1h</p>
<p>==第一次IO==：</p>
<ol>
<li>给<em><code>每一行内容加上唯一标记（hashcode（内容），value（行号））</code></em>。<br>对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。</li>
<li><em>`对每一行的hash值进行取模运算，并放置于归类分区的小文件中</em>`。<br>由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。</li>
</ol>
<p>==第二次IO==：</p>
<ol start="3">
<li>在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。</li>
</ol>
<h4 id="业务二：快-排序-paixu"><a href="#业务二：快-排序-paixu" class="headerlink" title="业务二：快{排序}(paixu)"></a><strong>业务二：快{排序}(paixu)</strong></h4><p><em>++现有1TB的TXT文件 ;<br>格式：数字；<br>网速：500M/s ；<br>服务器内存大小：128M ；<br>条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++</em></p>
<p>两次IO，2 * 30分钟 = 1小时</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9dek3xtj30pr0eu758.jpg" alt="enter description here"></p>
<p><strong>==方法一：先全局有序后局部有序==</strong></p>
<p> 1.对全局按分区排序（由大到小）。<br>​    用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················）<br> 2.对局部进行排序（由大到小）。<br>​    对每个分区进行排序。</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fyw9dj82b6j30pr0ewdgm.jpg" alt="enter description here"></p>
<p><strong>==方法二：先局部有序后全局有序==</strong></p>
<ol>
<li>先实现局部有序(小–&gt;大)。<br>将文件划分为N个分区，在每个分区内部进行排序</li>
<li>使用归并实现全局有序。<br>每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。</li>
</ol>
<p><img src="./images/坚持_1.jpg" alt="知否"></p>
]]></content>
        
        <categories>
            
            <category> 大数据 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> BigData </tag>
            
            <tag> 头脑风暴 </tag>
            
            <tag> 分而治之 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[MapReduce学习]]></title>
        <url>http://sungithup.github.io/2019/01/05/MapReduce%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<h1 id="MapReduce学习"><a href="#MapReduce学习" class="headerlink" title="MapReduce学习"></a>MapReduce学习</h1><h2 id="一、MapReduce是什么"><a href="#一、MapReduce是什么" class="headerlink" title="一、MapReduce是什么"></a>一、MapReduce是什么</h2><p>1、概念</p>
<p>MapReduce是一种<code>分布式离线计算框架</code>，是一种编程模型，用于在分布式系统上大规模数据集(大于1TB)的并行运算。</p>
<p>分布式编程：</p>
<blockquote>
<p>借助一个集群，通过多台机器去并行处理大规模数据集，从而获得海量计算能力。</p>
</blockquote>
<p>2、理解</p>
<p><code>Map</code>(映射)</p>
<p><code>Reduce</code>(归约)</p>
<blockquote>
<p>指定一个Map(映射)函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce(归约)函数，用来保证所有映射的键值对中的每一个共享相同的键组。</p>
</blockquote>
<h2 id="二、MapReduce设计理念"><a href="#二、MapReduce设计理念" class="headerlink" title="二、MapReduce设计理念"></a>二、MapReduce设计理念</h2><p>1、分布式计算</p>
<blockquote>
<p>分布式计算将该应用分解成许多小的部分，分配给多台计算机节点进行处理。这样可以节约整体计算时间，大大提高计算效率。</p>
</blockquote>
<p><strong> 分而治之 </strong>策略：</p>
<blockquote>
<p>一个存储在分布式文件系统中的大规模数据集，</p>
<p>会被切分成许多独立的分片（split），</p>
<p>这些分片可以被</p>
<p>多个Map任务并行处理</p>
</blockquote>
<p>2、移动计算，而分移动数据</p>
<blockquote>
<p>将计算程序应用移动到具有数据的集群计算机节点之上进行计算操作；</p>
<p>将有用、准确、及时的信息提供给任何时间、任何地点的任何客户。</p>
</blockquote>
<p>3、Master/Slave架构</p>
<blockquote>
<p>包括一个Master和若干个Slave。<br>Master上运行JobTracker，Slave上运行TaskTracker</p>
</blockquote>
<h2 id="三、MapReduce计算框架的组成"><a href="#三、MapReduce计算框架的组成" class="headerlink" title="三、MapReduce计算框架的组成"></a>三、MapReduce计算框架的组成</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6guaswsqj30fd06pwgh.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6hucmi5pj30v90exn7e.jpg" alt="MR"></p>
<p>1、 Mapper负责“<strong>分</strong>”，即把得到的复杂的任务分解为若干个“简单的任务”执行。</p>
<p>​        “简单的任务”：</p>
<ul>
<li><p>数据或计算规模相对于原任务要大大缩小；</p>
</li>
<li><p>就近计算，即会被分配到存放了所需数据的节点进行计算；</p>
</li>
<li>每个map任务之间可以并行计算，不产生任何通信。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6hw158klj30f308eq7e.jpg" alt="split"></p>
<p>2、Split规则：（取三者的中间值）</p>
<p>–  max.split(100M)</p>
<p>–  min.split(10M)</p>
<p>–  block(64M)</p>
<p><strong>max(min.split,min(max.split,block))</strong></p>
<p><strong>split实际大小=block大小</strong>（2.X：128M）</p>
<p>Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6hz81714j30ea07vmzd.jpg" alt=""></p>
<p>3、Reduce详解（总·重要）</p>
<p>–  Reduce的任务是对map阶段的结果进行“<strong>汇总</strong>”并输出。</p>
<p>Reducer的数目由mapred-site.xml配置文件里的项目mapred.reduce.tasks决定。缺省值为1，用户可自定义。</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6i3l3zo1j30fe09mq62.jpg" alt=""></p>
<p>4、Shuffle详解（总·核心）</p>
<p>– 在mapper和reducer中间的一个步骤</p>
<p>   可以把mapper的输出按照某种key值重新切分和组合成n份，把key值符合某种范围的输出送到特定的reducer那里去处理。</p>
<p>–  可以简化reducer过程</p>
<p>Partitoner ： hash(key) mod R</p>
<h2 id="四、MapReduce架构"><a href="#四、MapReduce架构" class="headerlink" title="四、MapReduce架构"></a>四、MapReduce架构</h2><h3 id="1、非共享式架构"><a href="#1、非共享式架构" class="headerlink" title="1、非共享式架构"></a>1、非共享式架构</h3><p>每个节点都有自己的内存，容错性比较好。</p>
<h3 id="2、一主多从架构"><a href="#2、一主多从架构" class="headerlink" title="2、一主多从架构"></a>2、一主多从架构</h3><p>可扩展性好，硬件要求易达到。</p>
<p>–  主 JobTracker:（ResourceManager资源管理）</p>
<blockquote>
<p>负责调度分配每一个子任务task运行于TaskTracker上，</p>
<p>如果发现有失败的task就重新分配其任务到其他节点。</p>
<p>每一个hadoop集群中只一个 JobTracker, 一般它运行在Master节点上。</p>
</blockquote>
<p>–  从TaskTracker:（NodeManager）</p>
<blockquote>
<p>TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务，</p>
<p>为了减少网络带宽TaskTracker最好运行在HDFS的DataNode上。</p>
</blockquote>
<h1 id="MapReduce的体系结构"><a href="#MapReduce的体系结构" class="headerlink" title="MapReduce的体系结构"></a>MapReduce的体系结构</h1><p>MapReduce主要有以下4个部分组成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1 ）Client</span><br><span class="line">•用户编写的MapReduce程序通过Client提交到JobTracker端</span><br><span class="line">•用户可通过Client提供的一些接口查看作业运行状态</span><br><span class="line">2 ）JobTracker</span><br><span class="line">•JobTracker负责资源监控和作业调度</span><br><span class="line">•JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点</span><br><span class="line">•JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器</span><br><span class="line">（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源</span><br><span class="line">3 ）TaskTracker</span><br><span class="line">•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报</span><br><span class="line">给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、</span><br><span class="line">杀死任务等）</span><br><span class="line">•TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到</span><br><span class="line">一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分</span><br><span class="line">配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask和Reduce Task使用</span><br><span class="line">（所以最好放在DataNode上）</span><br><span class="line">4 ）Task</span><br><span class="line">Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6i591yd9j30tp0fqq4x.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6ivspvrcj30y50dujum.jpg" alt=""></p>
<h2 id="五、MapReduce搭建"><a href="#五、MapReduce搭建" class="headerlink" title="五、MapReduce搭建"></a>五、MapReduce搭建</h2><h3 id="1、节点分布情况"><a href="#1、节点分布情况" class="headerlink" title="1、节点分布情况"></a>1、节点分布情况</h3><table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">NN</th>
<th style="text-align:center">DN</th>
<th style="text-align:center">JN</th>
<th style="text-align:center">ZK</th>
<th style="text-align:center">ZKFC</th>
<th style="text-align:center">RM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">node00</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">node01</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">node02</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h3 id="2、配置文件"><a href="#2、配置文件" class="headerlink" title="2、配置文件"></a>2、配置文件</h3><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6j2l1t72j30fe0begoy.jpg" alt=""></p>
<p>修改配置文件</p>
<p>(1)<strong>mapred-site.xml:</strong>（配置mapreudce需要的框架环境）</p>
<p>路径：F:\hadoop-2.6.5\etc\hadoop\mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>（2）<strong>yarn-site.xml:</strong>（配置yarn的任务调度的计算框架）</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>因为<strong>ResourceManager</strong> <strong>和NodeManager</strong>主从结构，RM存在单点故障，要对它做HA（通过ZK）</p>
<p>修改yarn-site.xml配置文件,完整的内容如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node04<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:2181,node02:2181,node03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="六、个人理解"><a href="#六、个人理解" class="headerlink" title="六、个人理解"></a>六、个人理解</h2><blockquote>
<p>基于源码，对mapreduce的工作流程的描述：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">一个应用程序要进行大规模数据处理分析</span><br><span class="line"></span><br><span class="line">数据文件保存在HDFS中，分块存储在分布式节点上</span><br><span class="line"></span><br><span class="line">首先是将数据文件切分成许多split切片</span><br><span class="line"></span><br><span class="line">每一个split切片单独启动一个map任务，所以会启动多个map任务</span><br><span class="line"></span><br><span class="line">map阶段的输入是诸多(key,value),输出是新的（key,value）,然后被拉去到不同的reduce上并行处理操作</span><br><span class="line"></span><br><span class="line">所以每个map的输出阶段都执行分区操作，并决定reduce任务的个数</span><br><span class="line"></span><br><span class="line">然后对map输出结果进行排序、归并、合并，这个过程叫map阶段的shuffle</span><br><span class="line"></span><br><span class="line">shuffle结束后，将相应的结果分发给reduce，让reduce完成后续的工作 </span><br><span class="line"></span><br><span class="line">结束后，将结果输出给HDFS。</span><br><span class="line"></span><br><span class="line">不同的map之间不会通信，不同的reduce也不会通信，整个过程对用户透明。</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6zgfpvqnj30tw0fxace.jpg" alt="shuffle"></p>
<blockquote>
<p>MapReduce执行的各个阶段：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1、从HDFS中加载文件，加载读取由INputFormat模块来完成，对输入负责格式验证，同时，对数据进行逻辑上切分成split</span><br><span class="line"></span><br><span class="line">2、由record read具体根据分片的位置长度信息去找各个block，以（key，value）输出，作为map的输入，</span><br><span class="line"></span><br><span class="line">3、map中有用户自定义的map函数就可以进行相应的数据处理，并输出一堆（key，value），作为中间结果</span><br><span class="line"></span><br><span class="line">4、之后，是shuffle（洗牌）过程对这中间结果进行分区、排序、合并，并溢写到磁盘，</span><br><span class="line"></span><br><span class="line">5、相应的reduce任务就会来fetch对应的分区（key，value（list））</span><br><span class="line"></span><br><span class="line">6、reduce中有用户自定义的reduce函数就可以完成对数据的分析，结果以新的（key，value）输出</span><br><span class="line"></span><br><span class="line">7、输出结果借助OutputFormat模块对输出格式进行检查，以及相关目录是否存在等，最后写入到HDFS中。</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6zikp8mbj30vh0fldle.jpg" alt="split"></p>
<blockquote>
<p>关于split的切分的理解：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1、InputFormat将大的数据文件分成很多split</span><br><span class="line">2、文件在HDFS中是以很多个物理块block分布式存储不同的节点上</span><br><span class="line">3、切片是用户自定义的逻辑分片</span><br><span class="line">4、split的数量决定map任务的数量</span><br><span class="line">5、切片过多会导致map任务启动过多，map任务之间切换的时候就会耗费相关的管理资源，所以切片过多会影响执行效率</span><br><span class="line">6、 切片过少又会影响任务执行的并行度，所以理想情况用block块的大小作为切片的大小。</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz6ztmqg0oj30s00dg42p.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz85ufnyj9j30ud0b3mz0.jpg" alt=""></p>
<blockquote>
<p>关于shuffle的理解</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">map端shuffle</span><br><span class="line">1、从HDFS输入数据和执行map任务，在map任务执行之前，RecordReader阅读器还将数据变成满足Map函数所需的（K，V）形式，然后InputFormat会将其切分成若干切片（一堆（K，V））。</span><br><span class="line">2、每个切片会分配一个map任务，每个map任务会分配一个默认的缓存，一般默认缓存为100M.map的输出键值对作为中间结果先写入到缓存（直接写入磁盘会增加寻址开销，所以集中写入磁盘一次寻址就可以完成批量写入，就可以将寻址开销分摊到大量数据中，这就是缓存的作用）。</span><br><span class="line">3、当写入的内容达到缓存空间的一定比例后（溢写比，一般为0.8，就是80M的时候，为了不影响map任务的继续执行），会启动溢写进程，把缓存中相关数据写入磁盘。</span><br><span class="line">4、在溢写过程中，会执行分区（partition）、排序（sort，按照key值）和可能的合并（combine，为了减少溢写到磁盘的数据量，慎用）操作，写入磁盘，生成磁盘的溢写文件。5、在map任务运行结束前，系统会对溢写文件进行归并（merge），形成大文件（里面的键值对是分区，排序的）,文件格式为（key,value&lt;list&gt;），归并时如果溢写文件大于预定值（默认为3），会再次合并</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">reduce端shuffle</span><br><span class="line">1、reduce任务会询问JobTracker，去拉取map机器上的属于自己的分区，对来自不同机器的数据进行归并、合并，然后输入到reduce函数中进行数据的处理分析，再写入磁盘</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz87b2w9k0j30vg0fijvo.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz87chuik1j30y50g00wu.jpg" alt=""></p>
<p>我</p>
<h1 id="MapReduce应用程序执行过程"><a href="#MapReduce应用程序执行过程" class="headerlink" title="MapReduce应用程序执行过程"></a>MapReduce应用程序执行过程</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz87d53czuj30vp0fltap.jpg" alt=""></p>
]]></content>
        
        <categories>
            
            <category> Mapreduce </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> HDFS </tag>
            
            <tag> MapReduce </tag>
            
            <tag> 计算框架 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Hadoop2.X]]></title>
        <url>http://sungithup.github.io/2019/01/04/Hadoop2.X/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h2 id="一、Hadoop-2-x产生背景"><a href="#一、Hadoop-2-x产生背景" class="headerlink" title="一、Hadoop 2.x产生背景"></a>一、Hadoop 2.x产生背景</h2><h3 id="1、Hadoop-1-0存在的问题"><a href="#1、Hadoop-1-0存在的问题" class="headerlink" title="1、Hadoop 1.0存在的问题"></a>1、Hadoop 1.0存在的问题</h3><h4 id="（1）HDFS存在的问题"><a href="#（1）HDFS存在的问题" class="headerlink" title="（1）HDFS存在的问题"></a>（1）HDFS存在的问题</h4><ul>
<li>NameNode单点故障，难以应用于在线场景</li>
<li>NameNode（一个）压力过大，内存受限，影响系统扩展性</li>
</ul>
<h4 id="（2）MapReduce存在的问题"><a href="#（2）MapReduce存在的问题" class="headerlink" title="（2）MapReduce存在的问题"></a>（2）MapReduce存在的问题</h4><ul>
<li>JobTracker访问压力大，影响系统扩展性</li>
<li>难以支持MapReduce以外的计算框架，比如Spark、Storm</li>
</ul>
<h3 id="2、Hadoop-2-0分支"><a href="#2、Hadoop-2-0分支" class="headerlink" title="2、Hadoop 2.0分支"></a>2、Hadoop 2.0分支</h3><p>HDFS：分布式文件存储系统<br>MapReduce：计算框架<br>YARN：资源管理系统</p>
<h3 id="3、特点"><a href="#3、特点" class="headerlink" title="3、特点"></a>3、特点</h3><p> 1）. 解决单点故障：HDFS HA（高可用）</p>
<blockquote>
<p>  通过主备NameNode解决，如果主NameNode发生故障，就切换到备NameNode上   |</p>
</blockquote>
<p> 2).解决内存受限问题：HDFS Federation（联邦制）、HA</p>
<blockquote>
<p> HA：两个NameNode<br> (3.0就实现了一组多从：水平扩展，支持多个NameNode；每个NameNode分管一部分目录；所有NameNode共享所有DataNode资源)</p>
</blockquote>
<p> 3).仅架构上发生变化使用方式不变</p>
<h2 id="二、HDFS-HA结构及功能"><a href="#二、HDFS-HA结构及功能" class="headerlink" title="二、HDFS HA结构及功能"></a>二、HDFS HA结构及功能</h2><h3 id="HA"><a href="#HA" class="headerlink" title="**HA"></a>**HA</h3><p>DN：DataNode（数据节点）</p>
<blockquote>
<p>存放数据block块；遵循心跳机制向NN Active和NN Standby汇报block块信息，但只执行active的命令         </p>
</blockquote>
<p>主备NN：NameNode Active 和 NameNode Standby （主备名称节点）</p>
<blockquote>
<p>主NN对外提供读写服务，备NN同步主NN元数据，以待切换，所有的DN同时向两个NN汇报数据块信息</p>
<p>元数据信息加载到主NN，并写入JN（至少写两台：过半原则）；</p>
<p>备NN可以从JN中同步元数据信息；</p>
<p>解决单点故障；</p>
<p>–两种切换方式：</p>
<p>手动：通过命令实现主备切换</p>
<p>自动：基于Zookeeper实现（详情见搭建步骤）</p>
</blockquote>
<p>JN：JournalNode（至少3台）</p>
<blockquote>
<p>存储主NN元数据信息，实现主备NN间数据共享；</p>
<p>（遵循过半原则：至少有过半的数量参与投票）</p>
</blockquote>
<p>ZKFC：FailoverController（竞争锁）</p>
<blockquote>
<p>谁拿到了这个所，谁就是active NN</p>
<p>心跳机制监控主备NN状态，一旦出现一台挂机，就会释放锁，另一个NN就会立即启动竞争锁，成为active NN</p>
</blockquote>
<p>ZK：Zookeeper（至少3台）</p>
<blockquote>
<p>（实现主备NN切换）</p>
</blockquote>
<h3 id="联邦"><a href="#联邦" class="headerlink" title="**联邦"></a>**联邦</h3><blockquote>
<p>通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使到namenode/namespace可以通过增加机器来进行水平扩展</p>
</blockquote>
<blockquote>
<p>通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中。</p>
</blockquote>
<h2 id="三、YARN-资源管理"><a href="#三、YARN-资源管理" class="headerlink" title="三、YARN(资源管理)???????"></a>三、YARN(资源管理)???????</h2><p><code>详见Yarn学习.md</code></p>
<p>1、核心思想：SourceManager（资源管理）+ReplicationMaster（任务调度）</p>
<p>2.yarn的引入使得多个计算框架可以应用到一个集群中</p>
<h2 id="四、Zookeeper工作原理"><a href="#四、Zookeeper工作原理" class="headerlink" title="四、Zookeeper工作原理"></a>四、Zookeeper工作原理</h2><p><code>详见Zookeeper学习.md</code></p>
<h2 id="五、Hadoop2-X-集群搭建"><a href="#五、Hadoop2-X-集群搭建" class="headerlink" title="五、Hadoop2.X 集群搭建"></a>五、Hadoop2.X 集群搭建</h2><h3 id="1、linux环境下搭建"><a href="#1、linux环境下搭建" class="headerlink" title="1、linux环境下搭建"></a>1、linux环境下搭建</h3><table>
<thead>
<tr>
<th></th>
<th style="text-align:center">NN</th>
<th style="text-align:center">DN</th>
<th style="text-align:center">JN</th>
<th style="text-align:center">ZKFC</th>
<th style="text-align:center">ZK</th>
<th style="text-align:center">SM</th>
<th style="text-align:center">RM</th>
</tr>
</thead>
<tbody>
<tr>
<td>node00</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td>node01</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td>node02</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
</tr>
</tbody>
</table>
<p>0.在搭建环境之前的准备</p>
<p>三台虚拟机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">关闭防火墙</span><br><span class="line">安装jdk</span><br><span class="line">编辑/etc/hosts/给各个节点服务器起别名</span><br><span class="line">时间服务器：ntpdate</span><br><span class="line">     安装：yum install ntpdate -y</span><br><span class="line">     生成：ntpdate cn.ntp.org.cn</span><br><span class="line">免密登录环境准备</span><br></pre></td></tr></table></figure>
<p>在hadoop安装目录下hadoop-2.6.5/etc/hadoop/</p>
<ol>
<li>编辑hadoop-env.sh</li>
</ol>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br></pre></td></tr></table></figure>
<p>   2.编辑core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://Sunrise<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="comment">&lt;!--配置集群的名字--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:2181,node01:2181,node02:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置zookeeper：三个节点--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span><span class="comment">&lt;!--配置hadoop基础配置存放的路径--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>3.编辑hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>sxt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.Sunrise<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.Sunrise.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.Sunrise.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.Sunrise.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node00:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.Sunrise.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://node00:8485;node01:8485;node02:8485/sxt<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.Sunrise<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 配置隔离机制为ssh 防止脑裂：保证activeNN仅有一台--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定秘钥的位置 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_dsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--免密登录是生成的文件，有的是id_rsa--&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!-- 指定journalnode日志文件存储的路径 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启自动故障转移 --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="4">
<li>配置hadoop中的slaves（主从架构：datanode）</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node00</span><br><span class="line">node01</span><br><span class="line">node02</span><br></pre></td></tr></table></figure>
<p>  5.准备zookeeper</p>
<ul>
<li><p>三台zookeeper：node00，node01，node02</p>
</li>
<li><p>编辑zookeeper-3.4.13/conf/zoo.cfg</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/usr/soft/zookeeper-3.4.13/data</span><br><span class="line">dataLogDir=/usr/soft/zookeeper-3.4.13/logs</span><br><span class="line">clientPort=2181</span><br><span class="line">server.1=node00:2888:3888</span><br><span class="line">server.2=node01:2888:3888</span><br><span class="line">server.3=node02:2888:3888</span><br></pre></td></tr></table></figure>
</li>
<li><p>在dataDir目录中创建文件myid，三台节点的文件内容分别为1，2，3</p>
</li>
</ul>
<p>6.配置环境变量   </p>
<blockquote>
<p>vim ~/.bash_profile</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/soft/jdk1.8.0_191</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">HADOOP_HOME=/usr/soft/hadoop-2.6.5</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>
<blockquote>
<p>source ~/.bash_profile</p>
<p>使其成为资源文件，发送到其他节点后，也需要此操作</p>
</blockquote>
<p>7.将以上配置文件远程发送至其他节点服务器</p>
<blockquote>
<p>scp -r filename nodename:<code>pwd</code></p>
</blockquote>
<p>8.命令操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1. 启动三个zookeeper：./zkServer.sh start</span><br><span class="line">2. 启动三个JournalNode：./hadoop-daemon.sh start journalnode</span><br><span class="line">3. （生成fsimage文件）在其中一个namenode上格式化：</span><br><span class="line">    hdfs namenode -format</span><br><span class="line">4. 把刚刚格式化之后的元数据拷贝到另外一个namenode上</span><br><span class="line"> a)	启动刚刚格式化的namenode :  hadoop-daemon.sh start namenode</span><br><span class="line"> b)	（同步fsimage文件）在另一个（没有格式化的）namenode上执行：</span><br><span class="line">    hdfs namenode -bootstrapStandby</span><br><span class="line"> c)	启动没格式化的namenode：    hadoop-daemon.sh start namenode</span><br><span class="line">5. （初始化竞争锁zookeeper）在其中一个namenode上初始化zkfc：</span><br><span class="line">    hdfs zkfc -formatZK</span><br><span class="line">6. 停止上面节点：stop-dfs.sh</span><br><span class="line">7. 全面启动（三个节点）：start-dfs.sh</span><br><span class="line">8. 启动yarn资源管理器</span><br><span class="line">   yarn-daemon.sh start resourcemanager </span><br><span class="line">   (yarn resourcemanager  )</span><br></pre></td></tr></table></figure>
<h3 id="2、使用"><a href="#2、使用" class="headerlink" title="2、使用"></a>2、使用</h3><p>（启动步骤）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1)关闭防火墙：service iptables stop        （3台）</span><br><span class="line">(2)启动zookeeper:zkServer.sh start          （3台）</span><br><span class="line">(3)启动集群：start-dfs.sh |（start-all.sh   :  同时启动hdfs和yarn)</span><br><span class="line">(4)启动yarn：yarn-daemon.sh start resourcemanager （可3台）</span><br></pre></td></tr></table></figure>
<p>（关闭步骤）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)关闭yarn：yarn-daemon.sh stop resourcemanager  （开几台关几台）</span><br><span class="line">(2)关闭集群：stop-dfs.sh   |（stop-all.sh    :同时关闭hdfs和yarn） （3台）</span><br><span class="line">(3)关闭zookeeper：zkServer.sh stop                 （3台）</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">有可能会出错的地方</span><br><span class="line">1，	确认每台机器防火墙均关掉</span><br><span class="line">2，	确认每台机器的时间是一致的</span><br><span class="line">3，	确认配置文件无误，并且确认每台机器上面的配置文件一样</span><br><span class="line">4，	如果还有问题想重新格式化，那么先把所有节点的进程关掉</span><br><span class="line">5，	删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录，所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录</span><br><span class="line">6，	接上面的第6步又可以重新格式化已经启动了</span><br><span class="line">7，	最终Active Namenode停掉的时候，StandBy可以自动接管！</span><br></pre></td></tr></table></figure>
]]></content>
        
        
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[HDFS学习]]></title>
        <url>http://sungithup.github.io/2019/01/03/HDFS%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="Hadoop学习"><a href="#Hadoop学习" class="headerlink" title="Hadoop学习"></a>Hadoop学习</h1><h2 id="一、分布式文件存储系统HDFS"><a href="#一、分布式文件存储系统HDFS" class="headerlink" title="一、分布式文件存储系统HDFS"></a>一、分布式文件存储系统HDFS</h2><h3 id="1、什么是分布式？"><a href="#1、什么是分布式？" class="headerlink" title="1、什么是分布式？"></a>1、什么是分布式？</h3><blockquote>
<p>定义：将海量的数据，复杂的业务分发到不同的计算机节点和服务器上分开处理和计算。</p>
<p>特点：</p>
<ul>
<li>多副本，提高服务的容错率、安全性、高可靠性</li>
<li>适合批处理，提高服务的效率和速度，</li>
<li>减轻单台服务的压力</li>
<li>具有很好的可扩展性</li>
<li>计算向数据靠拢，安全，高效</li>
</ul>
</blockquote>
<p><code>大数据三驾马车：GFS、MapReduce、Bigtable</code></p>
<h3 id="2、什么是HDFS？"><a href="#2、什么是HDFS？" class="headerlink" title="2、什么是HDFS？"></a>2、什么是HDFS？</h3><h4 id="（1）HDFS为什么会出现？"><a href="#（1）HDFS为什么会出现？" class="headerlink" title="（1）HDFS为什么会出现？"></a>（1）HDFS为什么会出现？</h4><blockquote>
<p>主要解决大量【pb级以上】的大数据的分布式存储问题</p>
</blockquote>
<h4 id="（2）HDFS的特点"><a href="#（2）HDFS的特点" class="headerlink" title="（2）HDFS的特点"></a>（2）HDFS的特点</h4><blockquote>
<p>$$ 分布式特性：</p>
<ul>
<li>适合大数据处理：GB、TB、PB以上的数据</li>
<li>百万规模以上的文件数量:10K+ 节点</li>
<li>适合批处理：移动计算而非数据(MR),数据位置暴露给计算框架</li>
</ul>
<p>$$ 自身特性：</p>
<ul>
<li>可构建在廉价机器上</li>
<li>高可靠性：通过多副本提提高</li>
<li>高容错性：数据自动保存多个副本；副本丢失后，自动恢复,提供了恢复机制</li>
</ul>
<p>$$ 缺点：</p>
<p>—–低延迟高数据吞吐访问问题（不适合低延迟数据访问，Hbase适合）</p>
<ul>
<li>不支持毫秒级</li>
<li>吞吐量大但有限制于其延迟（瓶颈：低延迟无法突破）</li>
</ul>
<p>—–小文件存取占用NameNode大量内存(寻道时间超过读取时间,约占99%)</p>
<p>——-不支持多用户写入及任意修改文件</p>
<ul>
<li>不支持文件修改：一个文件只能有一个写者</li>
<li>文件仅支持append不支持修改</li>
<li>（其实本身是支持的，主要为了用空间换时间，节约成本）</li>
</ul>
<p>$$ 实现目标：</p>
<ul>
<li>兼容廉价的硬件设施</li>
<li>实现流数据读写</li>
<li>支持大数据集</li>
<li>支持简单的文件模型</li>
<li>强大的跨平台兼容性</li>
</ul>
</blockquote>
<h4 id="（3）HDFS架构图"><a href="#（3）HDFS架构图" class="headerlink" title="（3）HDFS架构图"></a>（3）HDFS架构图</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0lg96gkqj30hz0dsgmf.jpg" alt="HDFS架构图"></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0lq2gmmtj30f009vwfj.jpg" alt="HDFS架构图"></p>
<blockquote>
<p><code>关系型数据库：</code>安全，存储在磁盘中；如MySql、Oracle、SQlServer</p>
<p><code>非关系型数据库：</code>不安全，存储在内存中；如Redis、MemcacheDB、mongDB、Hbase</p>
</blockquote>
<h3 id="3、HDFS的功能模块及原理详解"><a href="#3、HDFS的功能模块及原理详解" class="headerlink" title="3、HDFS的功能模块及原理详解"></a>3、HDFS的功能模块及原理详解</h3><h4 id="HDFS数据存储模型（block）"><a href="#HDFS数据存储模型（block）" class="headerlink" title=" HDFS数据存储模型（block）"></a><1> HDFS数据存储模型（block）</1></h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0n3j6xvnj30fe09vdh0.jpg" alt="block"></p>
<h5 id="（1）文件被线性切分固定大小的数据块：block"><a href="#（1）文件被线性切分固定大小的数据块：block" class="headerlink" title="（1）文件被线性切分固定大小的数据块：block"></a>（1）文件被<code>线性切分</code>固定大小的数据块：block</h5><ul>
<li><p>通过偏移量offset（单位：byte）标记</p>
</li>
<li><p>默认数据块大小为64MB (hadoop1.x，hadoop2.x默认为128M）)，可自定义配置</p>
</li>
<li><p>若文件大小不到64MB ，则单独存成一个block</p>
</li>
</ul>
<h5 id="（2）一个文件存储方式"><a href="#（2）一个文件存储方式" class="headerlink" title="（2）一个文件存储方式"></a>（2）一个文件存储方式</h5><ul>
<li><p>按大小被切分成若干个block ，存储到<code>不同节点上</code></p>
</li>
<li><p>默认情况下每个block都有2个副本 共3个副本</p>
</li>
<li><p>副本数不大于节点数</p>
</li>
</ul>
<h5 id="（3）Block大小和副本数通过Client端上传文件时设置，"><a href="#（3）Block大小和副本数通过Client端上传文件时设置，" class="headerlink" title="（3）Block大小和副本数通过Client端上传文件时设置，"></a>（3）Block大小和副本数通过Client端上传文件时设置，</h5><blockquote>
<p> 文件上传成功后副本数可以变更，Block Size大小不可变更</p>
<p>块的大小远远大于普通文件系统，可以最小化寻址开销</p>
</blockquote>
<h4 id="NameNode（简称NN）"><a href="#NameNode（简称NN）" class="headerlink" title="NameNode（简称NN）"></a><2>NameNode（简称NN）</2></h4><blockquote>
<ul>
<li>存储<code>元数据</code>；</li>
<li>元数据保存在<code>内存中</code>；</li>
<li>保存<code>文件</code>、<code>block块</code>、<code>datanode</code>之间的映射关系</li>
</ul>
</blockquote>
<h5 id="1-gt-NN主要功能："><a href="#1-gt-NN主要功能：" class="headerlink" title="1&gt; NN主要功能："></a>1&gt; NN主要功能：</h5><blockquote>
<p>接收客户端的读写服务；接收DN汇报block位置关系</p>
</blockquote>
<h5 id="2-gt-NN保存metadate元信息"><a href="#2-gt-NN保存metadate元信息" class="headerlink" title="2&gt; NN保存metadate元信息"></a>2&gt; NN保存metadate元信息</h5><blockquote>
<p>基于<code>内存</code>存储，<code>不会</code>和磁盘发生交换</p>
</blockquote>
<p>​        <code>metadata</code>元数据信息包括以下</p>
<blockquote>
<ul>
<li><p>文件的归属（ownership）和权限（permission）</p>
</li>
<li><p>文件大小和写入时间</p>
</li>
<li><p>block列表【偏移量】：即一个完整文件有哪些block（b0+b1+b2+..=file）</p>
</li>
<li><p>位置信息（<code>动态</code>的）：Block每个副本保存在哪个DataNode中</p>
<p><code>*注意*</code>：位置信息是由DN启动时上报给NN ，因为它会随时变化，所以不会保存在内存和磁盘中</p>
</li>
</ul>
</blockquote>
<h5 id="3-gt-NameNode的metadate信息在启动后会加载到内存"><a href="#3-gt-NameNode的metadate信息在启动后会加载到内存" class="headerlink" title="3&gt; NameNode的metadate信息在启动后会加载到内存"></a>3&gt; NameNode的metadate信息在启动后会加载到内存</h5><blockquote>
<p>同时：</p>
<p>metadata信息也会保存fsimage文件中（fsimage文件是位于磁盘上的镜像文件）</p>
<p>对metadata的操作日志也会记录在edits 文件中（edits文件是位于磁盘上的日志文件）</p>
</blockquote>
<h4 id="SecondaryNameNode（简称SNN）"><a href="#SecondaryNameNode（简称SNN）" class="headerlink" title="SecondaryNameNode（简称SNN）"></a><3>SecondaryNameNode（简称SNN）</3></h4><h5 id="1-gt-SNN主要功能"><a href="#1-gt-SNN主要功能" class="headerlink" title="1&gt;SNN主要功能"></a>1&gt;SNN主要功能</h5><blockquote>
<p>帮助NameNode合并edits和fsimage文件，减少NN启动时间；</p>
<p>SecondaryNameNode一般是单独运行在一台机器上；</p>
<p>它不是NN的备份（但可以做备份)。</p>
</blockquote>
<h5 id="2-gt-合并流程"><a href="#2-gt-合并流程" class="headerlink" title="2&gt;合并流程"></a>2&gt;合并流程</h5><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0p1lys2zj30f009vwf2.jpg" alt="SNN合并"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SecondaryNameNode的工作情况：</span><br><span class="line">（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，</span><br><span class="line">     暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，</span><br><span class="line">     上层写日志的函数完全感觉不到差别；</span><br><span class="line">（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文</span><br><span class="line">    件，并下载到本地的相应目录下；</span><br><span class="line">（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件      中的各项更新操作，使得内存中的FsImage保持最新；</span><br><span class="line">    这个过程就是EditLog和FsImage文件合并；</span><br><span class="line">（4）SecondaryNameNode执行完（3）操作之后，</span><br><span class="line">     会通过post方式将新的FsImage文件发送到NameNode节点上</span><br><span class="line">（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，</span><br><span class="line">     同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了</span><br></pre></td></tr></table></figure>
<h5 id="3-gt-合并机制"><a href="#3-gt-合并机制" class="headerlink" title="3&gt;合并机制"></a>3&gt;合并机制</h5><blockquote>
<p> ——-SNN执行合并时间和机制</p>
<ul>
<li><p>A、根据配置文件设置的时间间隔fs.checkpoint.period 默认3600秒</p>
</li>
<li><p>B、根据配置文件设置edits log大小 fs.checkpoint.size </p>
</li>
</ul>
<p>​          规定edits文件的最大值默认是64MB</p>
</blockquote>
<h4 id="DataNode（简称DN）"><a href="#DataNode（简称DN）" class="headerlink" title="DataNode（简称DN）"></a><4>DataNode（简称DN）</4></h4><h5 id="1-gt-DN主要功能"><a href="#1-gt-DN主要功能" class="headerlink" title="1&gt;  DN主要功能"></a>1&gt;  DN主要功能</h5><blockquote>
<ul>
<li>存储<code>文件内容</code>（block）；</li>
<li>文件内容保存在<code>磁盘</code>；</li>
<li>维护了<code>block id</code> 到<code>datanode本地文件</code>的映射关系</li>
<li>启动DN线程的时候会向NameNode汇报block位置信息</li>
</ul>
</blockquote>
<h5 id="2-gt-DN工作机制"><a href="#2-gt-DN工作机制" class="headerlink" title="2&gt;   DN工作机制"></a>2&gt;   DN工作机制</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">•    数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，</span><br><span class="line">•    会根据客户端或者是名称节点的调度来进行数据的存储和检索，</span><br><span class="line">•    并且通过心跳机制向名称节点定期发送自己所存储的块的列表，保持与其联系（3秒一次）</span><br><span class="line">    （如果NN 10分钟没有收到DN的心跳，则认为其已经lost，并copy其上的block到其它DN）</span><br><span class="line">•    每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中</span><br></pre></td></tr></table></figure>
<h5 id="3-gt-block的副本放置策略"><a href="#3-gt-block的副本放置策略" class="headerlink" title="3&gt; block的副本放置策略"></a>3&gt; block的副本放置策略</h5><blockquote>
<p>–  第一个副本：放置在上传文件的DN（集群内提交）；</p>
<p>​                           如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。</p>
<p>–  第二个副本：放置在于第一个副本不同的机架的节点上。</p>
<p>–  第三个副本：与第二个副本相同机架的不同节点。</p>
<p>–  更多副本：随机节点</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ny1tfrjj30f00akaaf.jpg" alt="block块存放位置"></p>
<h3 id="4、HDFS读写流程"><a href="#4、HDFS读写流程" class="headerlink" title="4、HDFS读写流程"></a>4、HDFS读写流程</h3><h4 id="读文件过程"><a href="#读文件过程" class="headerlink" title=" 读文件过程"></a><1> 读文件过程</1></h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1o4z4nmwj30gm09vgmg.jpg" alt="read"></p>
<blockquote>
<p>1、首先<code>client端</code>调用FileSystem对象（<code>FS</code>）的<code>open方法</code>，（FS：一个DistributedFileSystem的实例）。<br>2、DistributedFileSystem通过<code>rpc</code>协议从NameNode（<code>NN</code>）获得文件的第一批block的<code>locations</code>，（同一个block按副本数会返回多个locations，因为同一文件的block<code>分布式存储</code>在不同节点上），这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面（<code>就近选择</code>）。</p>
<p>3、前两步会返回一个FSDataInputStream对象，该对象会被封装DFSInputStream对象，DFSInputStream可以方便的管理DN和NN的数据流。客户端调用<code>read方法</code>，DFSInputStream会连接离客户端最近的DN，数据从DN源源不断的流向客户端（对客户端是透明的，只能看到一个读入的Input流）。</p>
<p>4、如果第一批block都读完了， DFSInputStream就会去NN拿下一批block的locations，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ptkw9krj30q40gegpp.jpg" alt="读"></p>
<p><code>注意：</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果在读数据的时候， DFSInputStream和DN的通讯发生异常，就会尝试连接正在读的block的排序第二近的DN,并且会记录哪个DN发生错误，剩余的blocks读的时候就会直接跳过该DN。 DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到NN，然后DFSInputStream在其他的DN上读该block的镜像。</span><br><span class="line">该设计就是客户端直接连接DN来检索数据，并且NN来负责为每一个block提供最优的DN，NN仅仅处理block location的请求，这些信息都加载在NN的内存中，hdfs通过DN集群可以承受大量客户端的并发访问。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">* RPC *（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。</span><br><span class="line">RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。</span><br></pre></td></tr></table></figure>
<h4 id="写文件过程"><a href="#写文件过程" class="headerlink" title="写文件过程"></a><2>写文件过程</2></h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1o6wamv3j30ga0aawfd.jpg" alt="write"></p>
<blockquote>
<p><strong>1.</strong>客户端通过调用DistributedFileSystem的<code>create方法</code>创建新文件。</p>
<p><strong>2.</strong>DistributedFileSystem通过<code>RPC</code>调用NN去创建一个没有blocks关联的新文件，创建前，NN会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NN就会记录下新文件，否则就会抛出IO异常。</p>
<p><strong>3.</strong>前两步结束后，会返回FSDataOutputStream的对象，封装在DFSOutputStream，客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的<code>packet</code>，然后排成队列dataQuene。</p>
<p><strong>4.</strong>NN会给这个新的block分配最适合存储的几个datanode，DFSOutputStream把packet包排成一个<code>管道pipeline</code>输出。先按队列输出到管道的第一个datanode中，并将该Packet从dataQueue队列中移到ackQueue队列中，第一个datanode又把packet输出到第二个datanode中，以此类推。</p>
<p><strong>5.</strong>DFSOutputStream中的<code>ackQuene</code>，也是由packet组成，等待DN的收到响应，当pipeline中的DN都表示已经收到数据的时候，这时ackQuene才会把对应的packet包移除掉。 如果在写的过程中某个DN发生错误，会采取以下几步：</p>
<p>​      1) pipeline被关闭掉；  </p>
<p>​      2)为了防止丢包，ackQuene里的packet会<code>同步</code>到dataQuene里;新建pipeline管道接到其他正常DN上</p>
<p>​     4)剩下的部分被写到剩下的正常的datanode中； </p>
<p>​     5)NN找到另外的DN去创建这个块的复制。（对客户端透明）</p>
<p><strong>6.</strong>客户端完成写数据后调用<code>close方法</code>关闭写入流</p>
</blockquote>
<p><code>注意：</code>客户端执行write操作后，写完的block才是可见的，正在写的block对客户端是不可见的</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ptp2xlej30on0gf78r.jpg" alt="写"></p>
<h3 id="5-HDFS文件权限和安全模式"><a href="#5-HDFS文件权限和安全模式" class="headerlink" title="5.HDFS文件权限和安全模式"></a>5.HDFS文件权限和安全模式</h3><h4 id="？？HDFS文件权限？？"><a href="#？？HDFS文件权限？？" class="headerlink" title="？？HDFS文件权限？？"></a><1>？？HDFS文件权限？？</1></h4><p>– 与Linux文件权限类似 </p>
<blockquote>
<p>   • r: read;    w:write;    x:execute，权限x对于文件忽略，对于文件夹表示是否允许访问其内容 </p>
</blockquote>
<p>– 如果Linux系统用户zs使用hadoop命令创建一个文件，那么这个 文件在HDFS中owner就是zs。 </p>
<p>– HDFS的权限目的：阻止好人做错事，而不是阻止坏人做坏事。</p>
<h4 id="？？安全模式？？"><a href="#？？安全模式？？" class="headerlink" title="？？安全模式？？"></a><2>？？安全模式？？</2></h4><blockquote>
<ul>
<li>NN启动的时候，首先将映像文件(fsimage)载入内存，并执行编辑日志(edits)中的各项操作。 </li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>一旦在内存中成功建立文件系统元数据的映射，则创建一个新的fsimage文件(这个操作不需要SecondaryNameNode)和一个空的编辑日志。 </li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>此刻namenode运行在安全模式。即namenode的文件系统对于客服端来说是只读的。(显示目录，显示文件内容等。写、删除、重命名都会失败)。 </li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>在此阶段Namenode收集各个datanode的报告，当数据块达到最小副本数以上时，会被认为是“安全”的， 在一定比例（可设置）的数据块被确定为“安全”后，再过若干时间，安全模式结束 </li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>当检测到副本数不足的数据块时，该块会被复制直到达到最小副本数，系统中数据块的位置并不是由namenode维护的，而是以块列表形式存储在datanode中。</li>
</ul>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1s32tmr2j30hj04ggmr.jpg" alt="异常"></p>
<h2 id="二、完全分布式搭建及eclipse插件"><a href="#二、完全分布式搭建及eclipse插件" class="headerlink" title="二、完全分布式搭建及eclipse插件"></a>二、完全分布式搭建及eclipse插件</h2><h3 id="1、完全分布式搭建（必备）"><a href="#1、完全分布式搭建（必备）" class="headerlink" title="1、完全分布式搭建（必备）"></a>1、完全分布式搭建（必备）</h3><h4 id="1-环境的准备"><a href="#1-环境的准备" class="headerlink" title="(1)环境的准备"></a>(1)环境的准备</h4><blockquote>
<ul>
<li>Linux (前面已经安装好了)</li>
</ul>
<ul>
<li><p>JDK（前面已经安装好了）</p>
</li>
<li><p>准备至少3台机器（通过克隆虚拟机；)</p>
</li>
<li><p>(网络配置、JDK搭建、hosts配置，保证节点间能互ping通）</p>
</li>
<li><p>时间同步  (ntpdate time.nist.gov)</p>
</li>
<li><p>ssh免秘钥登录   (两两互通免秘钥)</p>
</li>
</ul>
</blockquote>
<h4 id="（2）完全分布式搭建步骤"><a href="#（2）完全分布式搭建步骤" class="headerlink" title="（2）完全分布式搭建步骤"></a>（2）完全分布式搭建步骤</h4><p><code>详情见Hadoop2.X.md文件</code></p>
<h3 id="2、HDFS命令"><a href="#2、HDFS命令" class="headerlink" title="2、HDFS命令"></a>2、HDFS命令</h3><h4 id="0-命令-：hdfs-dfs"><a href="#0-命令-：hdfs-dfs" class="headerlink" title="(0)  命令 ：hdfs dfs"></a>(0)  命令 ：hdfs dfs</h4><h4 id="1-上传文件到HDFS："><a href="#1-上传文件到HDFS：" class="headerlink" title="(1)上传文件到HDFS："></a>(1)上传文件到HDFS：</h4><blockquote>
<p> <strong>hdfs dfs -put fileName[</strong>本地文件名] <strong>PATH</strong>【hdfs的文件路劲】</p>
</blockquote>
<blockquote>
<p>上传本地文件install.log到/myhdfs目录下</p>
<p><strong>hdfs dfs -put install.log /myhdfs</strong></p>
<p>​                       （文件路径) (上传目录）    </p>
</blockquote>
<h4 id="2-创建文件夹"><a href="#2-创建文件夹" class="headerlink" title="(2)创建文件夹"></a>(2)创建文件夹</h4><blockquote>
<p><strong>hdfs dfs -mkdir[-p] <paths></paths></strong> </p>
</blockquote>
<h4 id="3-删除文件或文件夹"><a href="#3-删除文件或文件夹" class="headerlink" title="(3)删除文件或文件夹"></a>(3)<strong>删除文件或文件夹</strong></h4><blockquote>
<p><strong>hdfs dfs -rm -r /myhadoop1.0</strong>  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -du -s URI[URI ...] 显示文件(夹)大小. </span><br><span class="line"></span><br><span class="line">hdfs dfs -cp -f]URI[URI...]&lt;dest&gt;    复制文件(夹)，可以覆盖，可以保留原有权限信息</span><br><span class="line"></span><br><span class="line">hdfs dfs -count -q&lt;paths&gt;列出文件夹数量、文件数量、内容大小.</span><br><span class="line"></span><br><span class="line">hdfs dfs -chown -R[:[GROUP]]URI[URI] 修改所有者.</span><br><span class="line"></span><br><span class="line">hdfs dfs -chmod [-R]&lt;MODE[,MODE]...|OCTALMODE&gt;URI[URI ...] 修改权限.</span><br></pre></td></tr></table></figure>
<p>（4）指定block大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">产生100000条数据：</span><br><span class="line"></span><br><span class="line">for i in `seq 100000`;do  echo &quot;hello sxt $i&quot; &gt;&gt; test.txt;done</span><br><span class="line"></span><br><span class="line">上传文件test.txt到指定的Java22目录下，并指定block块的大小1M：</span><br><span class="line"></span><br><span class="line">hdfs dfs -D dfs.blocksize=1048576-put test.txt /java22</span><br><span class="line"></span><br><span class="line">-D   ----设置属性</span><br></pre></td></tr></table></figure>
<h3 id="3、eclipse插件安装配置"><a href="#3、eclipse插件安装配置" class="headerlink" title="3、eclipse插件安装配置"></a>3、eclipse插件安装配置</h3><h4 id="（1）、导入插件"><a href="#（1）、导入插件" class="headerlink" title="（1）、导入插件"></a>（1）、导入插件</h4><blockquote>
<p>将以下jar包放入eclipse的plugins文件夹中</p>
<p>​         hadoop-eclipse-plugin-2.6.0.jar</p>
</blockquote>
<p>启动eclipse：出现界面如下：</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1uhqzg9oj30fe09nt9c.jpg" alt="插件应用"></p>
<h4 id="（2）配置环境变量"><a href="#（2）配置环境变量" class="headerlink" title="（2）配置环境变量"></a>（2）配置环境变量</h4><p><strong>Eclipse</strong>插件安装完后修改windows下的用户名，然后重启Eclipse：</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1uk49nbrj30fe0770tj.jpg" alt="环境变量"></p>
<h4 id="（3）新建Java项目"><a href="#（3）新建Java项目" class="headerlink" title="（3）新建Java项目"></a>（3）新建Java项目</h4><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ut3z3r2j30et0ah0tw.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1ut7fj9xj30fe09kq3i.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1utb5r5tj30fe0brgmt.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz1uw9a82yj30g508qmxh.jpg" alt=""></p>
<h2 id="三、网盘"><a href="#三、网盘" class="headerlink" title="三、网盘"></a>三、网盘</h2><p><strong>1、代码编写</strong></p>
<p><strong>新建Java项目，导入所需要的jar包</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop中的share\hadoop\hdfs</span><br><span class="line"></span><br><span class="line">hadoop中的share\hadoop\hdfs\lib</span><br><span class="line"></span><br><span class="line">hadoop中的share\hadoop\common</span><br><span class="line"></span><br><span class="line">hadoop中的share\hadoop\common\lib下的jar包。</span><br></pre></td></tr></table></figure>
<p><strong>block</strong>底层—offset偏移量来读取字节数组</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">blk</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Path ifile = <span class="keyword">new</span> Path(<span class="string">""</span>);</span><br><span class="line">		FileStatus file = fs.getFileStatus(ifile );</span><br><span class="line"><span class="comment">//      获取block的location信息HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容</span></span><br><span class="line">		BlockLocation[] blk = fs.getFileBlockLocations(file,<span class="number">0</span>, file.getLen());</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> (BlockLocation bb : blk) &#123;</span><br><span class="line">			System.out.println(bb);</span><br><span class="line">		&#125;</span><br><span class="line">		FSDataInputStream input = fs.open(ifile);</span><br><span class="line">		System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line">		System.out.println((<span class="keyword">char</span>)input.readByte());		</span><br><span class="line"><span class="comment">//		指定从哪个offset的位置偏移量来读</span></span><br><span class="line">		input.seek(<span class="number">1048576</span>);</span><br><span class="line">		System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line">		input.seek(<span class="number">1048576</span>);</span><br><span class="line">		System.out.println((<span class="keyword">char</span>)input.readByte());</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>   private   static void blk() throws Exception {           Path ifile = new Path(“”);           FileStatus file =   fs.getFileStatus(ifile );   //      获取block的location信息    HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容           BlockLocation[] blk =   fs.getFileBlockLocations(file , 0, file.getLen());                      for (BlockLocation bb : blk) {               System.out.println(bb);           }           FSDataInputStream input =   fs.open(ifile);           System.out.println((char)input.readByte());           System.out.println((char)input.readByte());          //      指定从哪个offset的位置偏移量来读           input.seek(1048576);           System.out.println((char)input.readByte());           input.seek(1048576);           System.out.println((char)input.readByte());       }   </p>
]]></content>
        
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Zookeeper学习]]></title>
        <url>http://sungithup.github.io/2019/01/03/Zookeeper%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<p>动物园管理员</p>
<p><code>推荐图书：</code>《从Paxo到Zookeeper》</p>
<h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h2 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h2><blockquote>
<p>开源的、分布式应用程序，提供<strong><code>一致性</code></strong>服务，是Haoop （实现HA）和Hbase（和zookeeper是强依赖关系）的重要组件</p>
</blockquote>
<p>提供的功能：</p>
<ul>
<li>配置维护</li>
<li>域名维护</li>
<li>分布式的同步</li>
<li>组服务</li>
</ul>
<p>Zookeeper→提供通用分布式锁服务，用以协调分布式应用</p>
<p>Keepalived→实现节点健康检查，采用优先级监控，没有协同工作，功能单一，可扩展性差。</p>
<h2 id="2、Zookeep而角色"><a href="#2、Zookeep而角色" class="headerlink" title="2、Zookeep而角色"></a>2、Zookeep而角色</h2><p> <img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd81ntj7tj30fd06pgn1.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd828tp44j30fo07lwgl.jpg" alt=""></p>
<p>（一般很少配置Observer，因为用的少，而且配置的节点一般为奇数）</p>
<blockquote>
<p>Zookeeper需保证高可用和强一致性；</p>
<p>​    为了支持更多的客户端，需要增加更多Server；</p>
<p>​    Server增多，投票阶段延迟增大，影响性能；</p>
<p>​    权衡伸缩性和高吞吐率，引入Observer</p>
<p>​    Observer不参与投票；</p>
<p>​    Observers接受客户端的连接，并将写请求转发给leader节点；</p>
<p>​    加入更多Observer节点，提高伸缩性，同时不影响吞吐率。</p>
</blockquote>
<h2 id="3、Zookeeper特点"><a href="#3、Zookeeper特点" class="headerlink" title="3、Zookeeper特点"></a>3、Zookeeper特点</h2><table>
<thead>
<tr>
<th style="text-align:center"><strong>特点</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong><em>最终</em>一致性</strong></td>
<td>为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能（与强一致性相对）</td>
</tr>
<tr>
<td style="text-align:center"><strong>可靠性</strong></td>
<td>如果消息被到一台服务器接受，那么它将被所有的服务器接受.</td>
</tr>
<tr>
<td style="text-align:center"><strong>实时性</strong></td>
<td>Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。</td>
</tr>
<tr>
<td style="text-align:center"><strong>独立性</strong></td>
<td>各个Client之间互不干预</td>
</tr>
<tr>
<td style="text-align:center"><strong>原子性</strong></td>
<td>更新只能成功或者失败，没有中间状态。</td>
</tr>
<tr>
<td style="text-align:center"><strong>顺序性</strong></td>
<td>所有Server，同一消息发布顺序一致。</td>
</tr>
</tbody>
</table>
<h3 id="4、安装部署："><a href="#4、安装部署：" class="headerlink" title="4、安装部署："></a>4、安装部署：</h3><p><a href="https://zookeeper.apache.org/" target="_blank" rel="noopener">官网：</a></p>
<p><a href="https://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">下载：</a></p>
<p>（1）<code>修改</code>配置文件：</p>
<p>在Zokeeper的安装目录中的conf目录下，将zoo_sample.cfg文件改名为zoo.cfg</p>
<blockquote>
<p>mv zoo_sample.cfg zoo.cfg</p>
</blockquote>
<p><code>编辑：</code></p>
<blockquote>
<p>vim /usr/soft/zookeeper-3.4.13/conf/zoo.cfg</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#发送心跳的间隔时间，单位：毫秒</span><br><span class="line">tickTime=2000  </span><br><span class="line">dataDir=/usr/soft/zookeeper-3.4.13/data</span><br><span class="line">dataLogDir=/usr/soft/zookeeper-3.4.13/logs</span><br><span class="line">dataLogDir=/Users/zdandljb/zookeeper/dataLog</span><br><span class="line">#客户端连接 Zookeeper 服务器的端口，</span><br><span class="line">clientPort=2181    </span><br><span class="line">#Zookeeper 会监听这个端口，接受客户端的访问请求。</span><br><span class="line">initLimit=5</span><br><span class="line">syncLimit=2</span><br><span class="line">server.1=node01:2888:3888</span><br><span class="line">server.2=node02:2888:3888</span><br><span class="line">server.3=node03:2888:3888</span><br></pre></td></tr></table></figure>
<p><code>配置解释:</code></p>
<blockquote>
<p><code>initLimit</code>： 这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5 个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒</p>
<p>syncLimit：这个配置项标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的心跳时间长度，总的时间长度就是 2*2000=4 秒</p>
<p>server.A=B：C：D：其 中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号</p>
</blockquote>
<p>(2)<strong>创建myid文件</strong>（在上面配置文件中配置dataDir  的目录下）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">server1机器的内容为：1，</span><br><span class="line">server2机器的内容为：2，</span><br><span class="line">server3机器的内容为：3</span><br></pre></td></tr></table></figure>
<p>（3）将zookeeper包发到各个节点上</p>
<h1 id="Paxo算法"><a href="#Paxo算法" class="headerlink" title="Paxo算法"></a>Paxo算法</h1><p><a href="http://zh.wikipedia.org/zh-cn/Paxos" target="_blank" rel="noopener">官网：</a></p>
<h2 id="1、简介-1"><a href="#1、简介-1" class="headerlink" title="1、简介"></a>1、简介</h2><p>一个基于消息传递的一致性算法，广泛应用于分布式计算中，是到目前为止唯一的分布式一致性算法。</p>
<p><code>前提：</code></p>
<p>Paxos 有一个前提：没有拜占庭将军问题。就是说 Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。</p>
<h2 id="2、结合故事的对应理解"><a href="#2、结合故事的对应理解" class="headerlink" title="2、结合故事的对应理解"></a>2、结合故事的对应理解</h2><blockquote>
<p>小岛(Island)——ZK Server Cluster<br>议员(Senator)——ZK Server<br>提议(Proposal)——ZNode Change(Create/Delete/SetData…)<br>提议编号(PID)——Zxid(ZooKeeper Transaction Id)<br>正式法令——所有 ZNode 及其数据</p>
<p>总统——ZK Server Leader</p>
</blockquote>
<h1 id="zookeeper的节点及工作原理"><a href="#zookeeper的节点及工作原理" class="headerlink" title="zookeeper的节点及工作原理"></a>zookeeper的节点及工作原理</h1><h2 id="1、工作原理"><a href="#1、工作原理" class="headerlink" title="1、工作原理"></a>1、<strong>工作原理</strong></h2><blockquote>
<p>1.每个Server在内存中存储了一份数据；</p>
<p>2.Zookeeper启动时，将从实例中选举一个leader（Paxos协议）</p>
<p>3.Leader负责处理数据更新等操作</p>
<p>4.一个更新操作成功，当且仅当大多数Server在内存中成功修改数据。</p>
</blockquote>
<p>Zookeeper的核心是<strong>原子广播</strong>，这个机制保证了各个server之间的同步。实现这个机制的协议叫做<strong>Zab协议</strong>。</p>
<p>Zab协议有两种模式，它们分别是<strong>恢复模式</strong>和<strong>广播模式</strong>。</p>
<blockquote>
<p>当服务启动或者在领导者崩溃后，Zab就进入了<strong>恢复模式</strong>，当领导者被选举出来，且大多数server的完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和server具有相同的系统状态。一旦leader已经和多数的follower进行了状态同步后，他就可以开始广播消息了，即进入<strong>广播状态</strong>。这时候当一个server加入zookeeper服务中，它会在恢复模式下启动，发现leader，并和leader进行状态同步。待到同步结束，它也参与消息广播。Zookeeper服务一直维持在Broadcast状态，直到leader崩溃了或者leader失去了大部分的followers支持.</p>
<p>广播模式需要保证proposal被按顺序处理，因此zk采用了<strong>递增的事务id号(zxid)</strong>来保证。所有的提议(proposal)都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch。低32位是个递增计数。</p>
<p>当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的server都恢复到一个正确的状态。</p>
</blockquote>
<h2 id="2、Znode节点"><a href="#2、Znode节点" class="headerlink" title="2、Znode节点"></a>2、Znode节点</h2><p>（1）Znode有两种类型，<strong>短暂的（ephemeral）和持久的（persistent）</strong></p>
<p> Znode的类型在创建时确定并且之后不能再修改。</p>
<ul>
<li><p>短暂znode的客户端会话结束时，zookeeper会将该短暂znode删除，<strong>短暂znode不可以有子节点</strong></p>
</li>
<li><p>持久znode不依赖于客户端会话，只有当客户端明确要删除该持久znode时才会被删除</p>
</li>
</ul>
<p>（2）Znode有四种形式的目录节点</p>
<ul>
<li><p>PERSISTENT、持久的</p>
</li>
<li><p>EPHEMERAL、短暂的</p>
</li>
<li><p>PERSISTENT_SEQUENTIAL、持久且有序的</p>
</li>
<li><p>EPHEMERAL_SEQUENTIAL   短暂且有序的</p>
</li>
</ul>
<h2 id="3、shell操作"><a href="#3、shell操作" class="headerlink" title="3、shell操作"></a>3、shell操作</h2><p>启动服务端：./zkServer.sh start</p>
<p>停止服务：./zkServer.sh stop</p>
<p>启动客户端：./zkCli.sh -server 127.0.0.1 : 2081</p>
<p>​                                                         (localhost、node01)</p>
<p>​                     （也可连接其他节点）</p>
<p>​                      (port默认2081,可省；ip也可省)</p>
<p>退出客户端：quit</p>
<p>操作指南：help</p>
<p>查看根目录：ll /</p>
<p>​                   （ll  +路径） </p>
<p>获取具体服务内容：get /</p>
<p>​                  (get +路径+服务)可查看注册zookeeper服务的节点信息</p>
<p>（如果作为leader的namenode挂了，最新文件会相应的更换数据信息，如果没有nn，那么就没有相应的最新文件，只会有记录上一个阶段数据的文件）</p>
<p>创建服务：create /sun aabbcc</p>
<p>​               (create +路径 + 数据内容) </p>
<p>在其他节点也可启动客户端，创建服务</p>
<p>删除服务：rmr /sun</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzdcfzkia9j30fd091wgj.jpg" alt=""></p>
<h2 id="4、API操作"><a href="#4、API操作" class="headerlink" title="4、API操作"></a>4、API操作</h2><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzdcgvx18uj30fe0akwie.jpg" alt=""></p>
<p><code>见代码testzookeeper</code></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote>
<p>Zookeeper 作为 Hadoop 项目中的一个子项目，是Hadoop 集群管理的一个必不可少的模块，它<strong>主要用来控制集群中的数据</strong>，如它管理 Hadoop 集群中的NameNode，还有 Hbase 中 Master、 Server 之间状态同步等。</p>
<p>​    Zoopkeeper 提供了一套很好的分布式集群管理的机制，就是它这种基于层次型的目录树的数据结构，并对树中的节点进行有效管理，从而可以设计出多种多样的分布式的数据管理模型。</p>
</blockquote>
]]></content>
        
        <categories>
            
            <category> Zookeeper </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[YARN的入门学习]]></title>
        <url>http://sungithup.github.io/2019/01/03/Yarn%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h1><h2 id="yarn（资源管理器）"><a href="#yarn（资源管理器）" class="headerlink" title="yarn（资源管理器）"></a>yarn（资源管理器）</h2><h3 id="（1）存在背景："><a href="#（1）存在背景：" class="headerlink" title="（1）存在背景："></a>（1）存在背景：</h3><p>MR1.0存在缺陷：</p>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzczt4wiysj30x90ftq5v.jpg" alt=""></p>
<ul>
<li>单点故障：</li>
</ul>
<p>仅有一个JobTracker负责整个作业的调度、管理、监控、资源调度</p>
<p>（一个作业拿到后会分解多个任务去执行mapduce，JobTracker把任务分配给TaskTracker来具体负责执行相关map或reduce任务）</p>
<ul>
<li>JobTracker‘大包大揽’，管理事项过多</li>
</ul>
<p>（上限4000个节点）</p>
<ul>
<li><p>容易出现内存溢出</p>
</li>
<li><p>资源划分不合理</p>
<p>（强行划分slot，map资源和reduce资源不能互用，导致忙的忙死，闲的闲死）</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">既是一个计算框架，也是一个资源管理框架</span><br></pre></td></tr></table></figure>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzczujcfrmj30n50ee75d.jpg" alt=""></p>
<h3 id="（2）yarn产生"><a href="#（2）yarn产生" class="headerlink" title="（2）yarn产生"></a>（2）yarn产生</h3><ul>
<li>对JobTracker进行功能分解，将资源管理功能分给ResourceManager，将任务调度和任务监控分给ApplicationMaster，将TaskTracker的任务交给NodeManager</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">纯粹的资源管理框架</span><br><span class="line">被剥离资源管理调度功能的MapReduce就变成了MR2.0，他就是一个运行在YARN上的一个纯粹的计算框架，由YARN为其提供资源管理调度服务</span><br></pre></td></tr></table></figure>
<p><code>什么叫纯粹的计算框架？？</code></p>
<p>它提供一些计算基类，使用时，编写map类和reduce类的子类，去继承它。然后计算框架去做后台自动分片，shuffle过程。</p>
<p><code>资源管理框架？？</code></p>
<p>它专门管理CPU内存资源的分配</p>
<h1 id="二、YARN设计思路"><a href="#二、YARN设计思路" class="headerlink" title="二、YARN设计思路"></a>二、YARN设计思路</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzczzrit8qj30wm0e6wie.jpg" alt=""></p>
<h1 id="三、YARN体系结构"><a href="#三、YARN体系结构" class="headerlink" title="三、YARN体系结构"></a>三、YARN体系结构</h1><p>三大核心：</p>
<h2 id="1、RecourceManager（RM）"><a href="#1、RecourceManager（RM）" class="headerlink" title="1、RecourceManager（RM）"></a>1、RecourceManager（RM）</h2><blockquote>
<ul>
<li><p>ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主<br>要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager）</p>
</li>
<li><p>调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式<br>分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行<br>就近选择，从而实现“计算向数据靠拢”</p>
</li>
<li><p>容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、<br>磁盘等资源，从而限定每个应用程序可以使用的资源量</p>
</li>
<li><p>调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也<br>允许用户根据自己的需求重新设计调度器</p>
</li>
<li><p>应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括<br>应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状<br>态并在失败时重新启动等</p>
</li>
</ul>
</blockquote>
<h2 id="2、ApplicationMaster"><a href="#2、ApplicationMaster" class="headerlink" title="2、ApplicationMaster"></a>2、ApplicationMaster</h2><blockquote>
<p>ResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集<br>来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMaster<br>ApplicationMaster的主要功能是：<br>（1）当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，<br>ResourceManager会以容器的形式为ApplicationMaster分配资源；</p>
<p>（2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源<br>的“二次分配”；</p>
<p>（3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请<br>到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行<br>失败恢复（即重新申请资源重启任务）；</p>
<p>（4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信<br>息；</p>
<p>（5）当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。</p>
</blockquote>
<h2 id="3、NodeManager"><a href="#3、NodeManager" class="headerlink" title="3、NodeManager"></a>3、NodeManager</h2><blockquote>
<p>NodeManager是驻留在一个YARN集群中的每个节点上的代理，有所需数据的节点，主要负责：</p>
<ul>
<li><p>容器生命周期管理</p>
</li>
<li><p>监控每个容器的资源（CPU、内存等）使用情况</p>
</li>
<li><p>跟踪节点健康状况</p>
</li>
<li><p>以“心跳”的方式与ResourceManager保持通信</p>
</li>
<li><p>向ResourceManager汇报作业的资源使用情况和每个容器的运行状态</p>
</li>
<li><p>接收来自ApplicationMaster的启动/停止容器的各种请求</p>
</li>
</ul>
<p>需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事<br>情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这<br>些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与<br>NodeManager通信来掌握各个任务的执行状态</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd1ueqsbmj30xm0fjwgg.jpg" alt=""></p>
<h1 id="四、YARN-工作流程"><a href="#四、YARN-工作流程" class="headerlink" title="四、YARN 工作流程"></a>四、YARN 工作流程</h1><p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd1x7ksg6j30yl0fqjxh.jpg" alt=""></p>
<h1 id="五、YARN框架与MapReduce1-0框架的对比分析"><a href="#五、YARN框架与MapReduce1-0框架的对比分析" class="headerlink" title="五、YARN框架与MapReduce1.0框架的对比分析"></a>五、YARN框架与MapReduce1.0框架的对比分析</h1><blockquote>
<ul>
<li>从MapReduce1.0框架发展到YARN框架，客户端并没有发生变化，其大部分调用API及<br>接口都保持兼容，因此，原来针对Hadoop1.0开发的代码不用做大的改动，就可以直接放<br>到Hadoop2.0平台上运行</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><p>总体而言，YARN相对于MapReduce1.0来说具有以下优势：</p>
<p>大大减少了承担中心服务功能的ResourceManager的资源消耗</p>
<ul>
<li>ApplicationMaster来完成需要大量资源消耗的任务调度和监控</li>
<li>多个作业对应多个ApplicationMaster，实现了监控分布化</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>MapReduce1.0既是一个计算框架，又是一个资源管理调度框架，但是，只能支持<br>MapReduce编程模型。而YARN则是一个纯粹的资源调度管理框架，在它上面可以运行包<br>括MapReduce在内的不同类型的计算框架，只要编程实现相应的ApplicationMaster</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>YARN中的资源管理比MapReduce1.0更加高效<ul>
<li>以容器为单位，而不是以slot为单位</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="六、YARN-的发展目标"><a href="#六、YARN-的发展目标" class="headerlink" title="六、YARN 的发展目标"></a>六、YARN 的发展目标</h1><p><strong>YARN 的目标就是实现“一个集群多个框架”？ ，为什么？</strong></p>
<ul>
<li><p>一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架</p>
<ul>
<li><p>MapReduce实现离线批处理</p>
</li>
<li><p>使用Impala实现实时交互式查询分析</p>
</li>
<li><p>使用Storm实现流式数据实时分析</p>
</li>
<li><p>使用Spark实现迭代计算</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>这些产品通常来自不同的开发团队，具有各自的资源调度管理机制</p>
</li>
<li><p>为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分<br>别安装运行不同的计算框架，即“一个框架一个集群”</p>
</li>
</ul>
<ul>
<li><p>导致问题</p>
<ul>
<li><p>集群资源利用率低</p>
</li>
<li><p>数据无法共享</p>
</li>
<li><p>维护代价高</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源<br>调度管理框架YARN，在YARN之上可以部署其他各种计算框架</p>
</li>
<li><p>由YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架<br>的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩</p>
</li>
<li><p>可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率</p>
</li>
<li><p>不同计算框架可以共享底层存储，避免了数据集跨集群移动</p>
</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fzd5qv2p73j30r808tdgv.jpg" alt=""></p>
]]></content>
        
        <categories>
            
            <category> yarn </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> HDFS </tag>
            
            <tag> 资源管理框架 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Nginx学习]]></title>
        <url>http://sungithup.github.io/2019/01/02/Nginx%E5%AD%A6%E4%B9%A0/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h1 id="Nginx学习：大型网站高并发运行处理"><a href="#Nginx学习：大型网站高并发运行处理" class="headerlink" title="Nginx学习：大型网站高并发运行处理"></a>Nginx学习：大型网站高并发运行处理</h1><h2 id="一、Nginx使用背景"><a href="#一、Nginx使用背景" class="headerlink" title="一、Nginx使用背景"></a>一、Nginx使用背景</h2><p>1、背景</p>
<p>1）高并发（海量数据，复杂业务，大量线程）集中访问服务器</p>
<p>2)服务器资源和能力有限</p>
<p>使得服务器宕机，无法提供服务</p>
<p>2、概念理解</p>
<p>1)高并发</p>
<blockquote>
<p>海量数据访问，多个进程同时处理不同操作</p>
</blockquote>
<p>2）负载均衡</p>
<blockquote>
<p>均匀分配请求|数据到不同操作单元上</p>
</blockquote>
<p>3）常见互联网架构</p>
<blockquote>
<p>客户端层→反向代理层→站点层→服务层→数据层</p>
</blockquote>
<p><img src="https://ws1.sinaimg.cn/large/005zftzDgy1fz0gr7u5zoj31740hb42u.jpg" alt=""></p>
<h2 id="二、Nginx入门"><a href="#二、Nginx入门" class="headerlink" title="二、Nginx入门"></a>二、Nginx入门</h2><h3 id="1、了解nginx是什么"><a href="#1、了解nginx是什么" class="headerlink" title="1、了解nginx是什么"></a>1、了解nginx是什么</h3><blockquote>
<p>nginx是一款轻量级（开发方便，配置简捷）的Web 服务器/<strong>反向代理</strong>服务器及电子邮件（IMAP/POP3）代理服务器</p>
</blockquote>
<h3 id="2、特点"><a href="#2、特点" class="headerlink" title="2、特点"></a>2、特点</h3><blockquote>
<p>占有内存少，并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。</p>
<p>（使用C语言编写）</p>
</blockquote>
<h3 id="3、配置搭建Nginx（Linux系统环境下）"><a href="#3、配置搭建Nginx（Linux系统环境下）" class="headerlink" title="3、配置搭建Nginx（Linux系统环境下）"></a>3、配置搭建Nginx（Linux系统环境下）</h3><p><code>资源</code>：</p>
<p>Tengine（推荐）：<a href="http://tengine.taobao.org/download/tengine-2.2.3.tar.gz" target="_blank" rel="noopener">Tengine-2.2.3.tar.gz</a> </p>
<p>​                                 <a href="http://tengine.taobao.org/download.html" target="_blank" rel="noopener">其他版本</a></p>
<p>nginx：<a href="http://nginx.org/download/nginx-1.8.1.zip" target="_blank" rel="noopener">nginx/Windows-1.8.1</a></p>
<p>1）安装依赖</p>
<blockquote>
<p>命令：yum -y install gcc openssl-devel pcre-devel zlib-devel</p>
</blockquote>
<p>2）解压tar包</p>
<blockquote>
<p>命令：tar -zxvf Tengine-2.2.3.tar.gz</p>
</blockquote>
<p>3）configure配置：在解压后的源码目录中</p>
<p>两种方案：</p>
<blockquote>
<ul>
<li>命令： ./configure</li>
</ul>
<p>默认配置/usr/soft/nginx</p>
</blockquote>
<blockquote>
<ul>
<li>命令 : ./configure –profix==/usr/soft/nginx</li>
</ul>
<p>配置在指定路径</p>
</blockquote>
<p>4）编译并安装(默认会在/usr/local下生成nginx目录)</p>
<blockquote>
<p>make &amp;&amp; make install</p>
</blockquote>
<p>5）配置nginx服务</p>
<p>在/etc/rc.d/init.d/目录中建立文本文件nginx</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">#</span><br><span class="line"># nginx - this script starts and stops the nginx daemon</span><br><span class="line">#</span><br><span class="line"># chkconfig:   - 85 15 </span><br><span class="line"># description:  Nginx is an HTTP(S) server, HTTP(S) reverse \</span><br><span class="line">#               proxy and IMAP/POP3 proxy server</span><br><span class="line"># processname: nginx</span><br><span class="line"># config:      /etc/nginx/nginx.conf</span><br><span class="line"># config:      /etc/sysconfig/nginx</span><br><span class="line"># pidfile:     /var/run/nginx.pid</span><br><span class="line"> </span><br><span class="line"># Source function library.</span><br><span class="line">. /etc/rc.d/init.d/functions</span><br><span class="line"> </span><br><span class="line"># Source networking configuration.</span><br><span class="line">. /etc/sysconfig/network</span><br><span class="line"> </span><br><span class="line"># Check that networking is up.</span><br><span class="line">[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0</span><br><span class="line"> </span><br><span class="line">nginx=&quot;/usr/local/nginx/sbin/nginx&quot;</span><br><span class="line">prog=$(basename $nginx)</span><br><span class="line"> </span><br><span class="line">NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot;</span><br><span class="line"> </span><br><span class="line">[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx</span><br><span class="line"> </span><br><span class="line">lockfile=/var/lock/subsys/nginx</span><br><span class="line"> </span><br><span class="line">make_dirs() &#123;</span><br><span class="line">   # make required directories</span><br><span class="line">   user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -`</span><br><span class="line">   options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;`</span><br><span class="line">   for opt in $options; do</span><br><span class="line">       if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then</span><br><span class="line">           value=`echo $opt | cut -d &quot;=&quot; -f 2`</span><br><span class="line">           if [ ! -d &quot;$value&quot; ]; then</span><br><span class="line">               # echo &quot;creating&quot; $value</span><br><span class="line">               mkdir -p $value &amp;&amp; chown -R $user $value</span><br><span class="line">           fi</span><br><span class="line">       fi</span><br><span class="line">   done</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">start() &#123;</span><br><span class="line">    [ -x $nginx ] || exit 5</span><br><span class="line">    [ -f $NGINX_CONF_FILE ] || exit 6</span><br><span class="line">    make_dirs</span><br><span class="line">    echo -n $&quot;Starting $prog: &quot;</span><br><span class="line">    daemon $nginx -c $NGINX_CONF_FILE</span><br><span class="line">    retval=$?</span><br><span class="line">    echo</span><br><span class="line">    [ $retval -eq 0 ] &amp;&amp; touch $lockfile</span><br><span class="line">    return $retval</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">stop() &#123;</span><br><span class="line">    echo -n $&quot;Stopping $prog: &quot;</span><br><span class="line">    killproc $prog -QUIT</span><br><span class="line">    retval=$?</span><br><span class="line">    echo</span><br><span class="line">    [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile</span><br><span class="line">    return $retval</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">restart() &#123;</span><br><span class="line">    configtest || return $?</span><br><span class="line">    stop</span><br><span class="line">    sleep 1</span><br><span class="line">    start</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">reload() &#123;</span><br><span class="line">    configtest || return $?</span><br><span class="line">    echo -n $&quot;Reloading $prog: &quot;</span><br><span class="line">    killproc $nginx -HUP</span><br><span class="line">    RETVAL=$?</span><br><span class="line">    echo</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">force_reload() &#123;</span><br><span class="line">    restart</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">configtest() &#123;</span><br><span class="line">  $nginx -t -c $NGINX_CONF_FILE</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">rh_status() &#123;</span><br><span class="line">    status $prog</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">rh_status_q() &#123;</span><br><span class="line">    rh_status &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">case &quot;$1&quot; in</span><br><span class="line">    start)</span><br><span class="line">        rh_status_q &amp;&amp; exit 0</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    stop)</span><br><span class="line">        rh_status_q || exit 0</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    restart|configtest)</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    reload)</span><br><span class="line">        rh_status_q || exit 7</span><br><span class="line">        $1</span><br><span class="line">        ;;</span><br><span class="line">    force-reload)</span><br><span class="line">        force_reload</span><br><span class="line">        ;;</span><br><span class="line">    status)</span><br><span class="line">        rh_status</span><br><span class="line">        ;;</span><br><span class="line">    condrestart|try-restart)</span><br><span class="line">        rh_status_q || exit 0</span><br><span class="line">            ;;</span><br><span class="line">    *)</span><br><span class="line">        echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot;</span><br><span class="line">        exit 2</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>6）修改nginx文件的权限</p>
<blockquote>
<p>命令 ： chmod +x nginx</p>
</blockquote>
<p>7）将文件添加到系统服务中</p>
<blockquote>
<p>chkconfig –add nginx</p>
</blockquote>
<p>8）验证</p>
<blockquote>
<p>chkconfig –list nginx</p>
</blockquote>
<p>9）启动|停止服务</p>
<blockquote>
<p>service nginx start|stop</p>
</blockquote>
<h3 id="4、负载均衡配置"><a href="#4、负载均衡配置" class="headerlink" title="4、负载均衡配置"></a>4、负载均衡配置</h3><p>1）编辑配置文件：</p>
<blockquote>
<p>命令 ： vim /usr/local/nginx/conf/nginx.conf</p>
</blockquote>
<p>具体负载配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">（1）（默认）轮询负载</span><br><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">         &#125; </span><br><span class="line">    server &#123; </span><br><span class="line">        listen 80; </span><br><span class="line">	server_name  localhost;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://shsxt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line">（2）加权负载</span><br><span class="line">upstream shsxt&#123;</span><br><span class="line">     server sxt1.com weight=3;</span><br><span class="line">     server sxt2.com;</span><br><span class="line">&#125;</span><br><span class="line">(3)最少连接负载</span><br><span class="line">upstream shsxt&#123;</span><br><span class="line">     least_conn;</span><br><span class="line">     servcer node01;</span><br><span class="line">     server node02;</span><br><span class="line">&#125;</span><br><span class="line">(4)ip_hash负载（保持回话持久性）</span><br><span class="line">upstream shsxt&#123;</span><br><span class="line">     ip_hash;</span><br><span class="line">     server node01;</span><br><span class="line">     server node02;</span><br><span class="line">&#125;</span><br><span class="line">3. 访问控制（allow 代表允许其访问，deny 禁止其访问）</span><br><span class="line">location / &#123;</span><br><span class="line">     deny 192.168.4.29;</span><br><span class="line">     allow 192.168.198.0/24;</span><br><span class="line">     deny all;</span><br><span class="line">     proxy_pass http://shsxt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="5、session一致性问题"><a href="#5、session一致性问题" class="headerlink" title="5、session一致性问题"></a>5、session一致性问题</h3><blockquote>
<p>实现session共享即可解决这个问题</p>
<p>实现工具：memcached</p>
<p>作用：专门管理session的工具</p>
</blockquote>
<p>1）在tomcat的lib目录下导入连接memcached所需的jar包</p>
<blockquote>
<p>asm-3.2.jar</p>
<p>kryo-1.04.jar</p>
<p>kryo-serializers-0.11.jar</p>
<p>memcached-session-manager-1.7.0.jar</p>
<p>memcached-session-manager-tc7-1.8.1.jar</p>
<p>minlog-1.2.jar</p>
<p>msm-kryo-serializer-1.7.0.jar</p>
<p>reflectasm-1.01.jar</p>
<p>spymemcached-2.7.3.jar</p>
</blockquote>
<p>2）在tomcat的conf目录下编辑context.xml文件</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Manager</span> <span class="attr">className</span>=<span class="string">"de.javakaffee.web.msm.MemcachedBackupSessionManager"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">memcachedNodes</span>=<span class="string">"n1:192.168.17.9:11211"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">sticky</span>=<span class="string">"true"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">lockingMode</span>=<span class="string">"auto"</span></span></span><br><span class="line"><span class="tag">    <span class="attr">sessionBackupAsync</span>=<span class="string">"false"</span></span></span><br><span class="line"><span class="tag">   <span class="attr">requestUriIgnorePattern</span>=<span class="string">".*\.(ico|png|gif|jpg|css|js)$"</span></span></span><br><span class="line"><span class="tag"><span class="attr">sessionBackupTimeout</span>=<span class="string">"1000"</span> <span class="attr">transcoderFactoryClass</span>=<span class="string">"de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory"</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>（注意：配置memcachedNodes属性时，配置其ip和端口，默认为11211，存在多个memecacahed数据库时，用都逗号隔开）</p>
</blockquote>
<p>3）验证session：修改index.jsp（在/usr/soft/apache-tomcat-8.5.24/webapps/ROOT/index.jsp），取sessionid看一看</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SessionID:&lt;%=session.getId()%&gt;</span><br><span class="line">&lt;/br&gt;</span><br><span class="line">SessionIP:&lt;%=request.getServerName()%&gt;</span><br><span class="line">&lt;/br&gt;</span><br><span class="line">&lt;h1&gt;tomcat1&lt;/h1&gt;</span><br></pre></td></tr></table></figure>
<p>4)安装： memcached</p>
<blockquote>
<p>命令 ：yum install memcached –y</p>
</blockquote>
<p>5）启动memcached (IP地址为memcached安装的节点的IP地址)</p>
<blockquote>
<p>memcached -d -m 128m -p 11211 -l 192.168.198.128 -u root -P /tmp/</p>
</blockquote>
<p>6）在浏览器段访问服务器，默认端口 ： 80 ，对此测验，就会发现sessionID不会改变</p>
<h2 id="三、虚拟主机"><a href="#三、虚拟主机" class="headerlink" title="三、虚拟主机"></a>三、虚拟主机</h2><h3 id="1、什么是虚拟主机？"><a href="#1、什么是虚拟主机？" class="headerlink" title="1、什么是虚拟主机？"></a>1、什么是虚拟主机？</h3><p>（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。</p>
<p>（2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。</p>
<h3 id="2、虚拟主有啥特点？"><a href="#2、虚拟主有啥特点？" class="headerlink" title="2、虚拟主有啥特点？"></a>2、虚拟主有啥特点？</h3><p>（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用</p>
<p>（2）也大大简化了服务器管理的复杂性；</p>
<h3 id="3、虚拟主机有哪些类别？"><a href="#3、虚拟主机有哪些类别？" class="headerlink" title="3、虚拟主机有哪些类别？"></a>3、虚拟主机有哪些类别？</h3><p>（1）基于域名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">     &#125; </span><br><span class="line">	upstream bjsxt&#123; </span><br><span class="line">        server node03; </span><br><span class="line">     &#125; </span><br><span class="line">     </span><br><span class="line">     server &#123;    </span><br><span class="line">            listen 80; </span><br><span class="line">            //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里</span><br><span class="line">            server_name  sxt2.com;</span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://bjsxt;</span><br><span class="line">            &#125;</span><br><span class="line">      &#125; </span><br><span class="line">      server &#123; </span><br><span class="line">            listen 80; </span><br><span class="line">           //访问sxt1.com的时候，会把请求导到shsxt的服务器组里</span><br><span class="line">            server_name  sxt1.com; </span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://shsxt;</span><br><span class="line">            &#125;</span><br><span class="line">      &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
</blockquote>
<blockquote>
<p>（1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。</p>
<p>（C:\Windows\System32\drivers\etc\hosts     给IP取别名）</p>
<p>如：192.168.198.130   sxt1.com</p>
</blockquote>
<blockquote>
<p>（2）每台服务器的Tomcat的端口不与配置中的listen一致，那么windows系统浏览器访问时，需要加上Tomcat的端口，（192.168.198.128：8080）</p>
<p>​         如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80</p>
</blockquote>
<p>（2）基于端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">http &#123; </span><br><span class="line">    upstream shsxt&#123; </span><br><span class="line">        server node01; </span><br><span class="line">        server node02; </span><br><span class="line">     &#125; </span><br><span class="line">	upstream bjsxt&#123; </span><br><span class="line">        server node03</span><br><span class="line">    &#125; </span><br><span class="line"> server &#123; </span><br><span class="line">       //当访问nginx的80端口时，将请求导给bjsxt组</span><br><span class="line">        listen 8080; </span><br><span class="line">        server_name 192.168.198.128;</span><br><span class="line">        location / &#123;</span><br><span class="line">            proxy_pass http://bjsxt;</span><br><span class="line">        &#125;</span><br><span class="line">&#125; </span><br><span class="line">  server &#123; </span><br><span class="line">           //当访问nginx的81端口时，将请求导给shsxt组</span><br><span class="line">            listen 81; </span><br><span class="line">            server_name 192.168.198.128;  //nginx服务器的IP</span><br><span class="line">            location / &#123;</span><br><span class="line">                proxy_pass http://shsxt;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>（3）基于IP  ：（不常用）</p>
<h2 id="四、正向代理和反向代理"><a href="#四、正向代理和反向代理" class="headerlink" title="四、正向代理和反向代理"></a>四、正向代理和反向代理</h2><h3 id="1、正向代理"><a href="#1、正向代理" class="headerlink" title="1、正向代理"></a>1、正向代理</h3><p>理解：</p>
<blockquote>
<p>代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见）</p>
</blockquote>
<p>举例：</p>
<blockquote>
<p>国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙）</p>
<p>但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口</p>
</blockquote>
<h3 id="2、反向代理"><a href="#2、反向代理" class="headerlink" title="2、反向代理"></a>2、反向代理</h3><p>理解：</p>
<blockquote>
<p>代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器</p>
</blockquote>
<p>举例：</p>
<blockquote>
<p>如我们访问<a href="http://www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。" target="_blank" rel="noopener">www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。</a></p>
</blockquote>
<p>Nginx就是性能很好的反向代理服务器，用来作负载均衡。</p>
]]></content>
        
        <categories>
            
            <category> Nginx </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux系统环境 </tag>
            
            <tag> 负载均衡 </tag>
            
            <tag> Nginx </tag>
            
            <tag> 分布式 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[手动安装maven坐标依赖]]></title>
        <url>http://sungithup.github.io/2018/12/28/%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85maven%E5%9D%90%E6%A0%87%E4%BE%9D%E8%B5%96/</url>
        <content type="html"><![CDATA[<h1 id="手动安装maven坐标依赖"><a href="#手动安装maven坐标依赖" class="headerlink" title="手动安装maven坐标依赖"></a>手动安装maven坐标依赖</h1><h2 id="一、事件原因："><a href="#一、事件原因：" class="headerlink" title="一、事件原因："></a>一、事件原因：</h2><p>学习quartz框架时，在maven项目的pom.xml文件中添加quartz所需要的坐标依赖时，显示jar包不存在。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">提示："Dependency 'xxxx‘ not found"，	</span><br><span class="line">并且添加的如下两个坐标依赖均报红。</span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="comment">&lt;!-- 工具 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz-jobs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>分析：</p>
<table>
<thead>
<tr>
<th>1、maven项目所需要的jar包均存放在maven的F:\m2\repository(项目所需的jar包仓库)文件夹中</th>
</tr>
</thead>
<tbody>
<tr>
<td>2、在F:\apache-maven-3.5.4\conf的settings.xml文件中有如下设置：（由于使用远程仓库太慢，阿里云给我们提供了一个镜像仓库，便于我们使用，且只包含central仓库中的jar）</td>
</tr>
</tbody>
</table>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--文件中原有的配置：远程仓库---&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">id</span>&gt;</span>mirrorId<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>repositoryId<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>Human Readable Name for this Mirror.<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://my.repository.com/repo/path<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--文件中自己手动配置：阿里镜像仓库---&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span>  </span><br><span class="line">	<span class="tag">&lt;<span class="name">id</span>&gt;</span>nexus-aliyun<span class="tag">&lt;/<span class="name">id</span>&gt;</span>  </span><br><span class="line">	<span class="tag">&lt;<span class="name">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="name">mirrorOf</span>&gt;</span>    </span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>Nexus aliyun<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  </span><br><span class="line">	<span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><img src="C:\Users\Administrator\Desktop\1.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>3.可是我们在<a href="https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧" target="_blank" rel="noopener">https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧</a></th>
</tr>
</thead>
<tbody>
<tr>
<td>（如果有小伙伴有别的解决方案，还请指点一二。）</td>
</tr>
</tbody>
</table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure>
<h2 id="二、解决方案"><a href="#二、解决方案" class="headerlink" title="二、解决方案"></a>二、解决方案</h2><p>1、首先，我们需要从maven  Repository中下载我们需要的jar包（需要的两个jar包，下载原理相同）</p>
<p><img src="C:\Users\Administrator\Desktop\2.jpg" alt=""></p>
<p>2、注意我们的maven安装，需要配置环境变量，才能在dos窗口，指令安装jar包</p>
<p><img src="C:\Users\Administrator\Desktop\4.jpg" alt=""></p>
<p>因为我之前查资料时，有小伙伴说，java的环境变量配置也会影响，所以，我在这里也把java的环境变量配置也贴出来</p>
<p><img src="C:\Users\Administrator\Desktop\5.jpg" alt="1"></p>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544699916763.png" alt="1544699916763"></p>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544699989775.png" alt="1544699989775"></p>
<table>
<thead>
<tr>
<th>JAVA_HOME</th>
</tr>
</thead>
<tbody>
<tr>
<td>F:\Java\jdk1.8.0_131（  根据自己的jdk安装目录）</td>
</tr>
<tr>
<td><strong>CLASSPATH</strong></td>
</tr>
<tr>
<td>.;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar</td>
</tr>
<tr>
<td><strong>MAVEN_HOME</strong></td>
</tr>
<tr>
<td>F:\apache-maven-3.5.4（ 根据自己maven安装目录）</td>
</tr>
<tr>
<td><strong>Path</strong>（注意配置的时候，一定要和配置home时的变量名一致，如MAVEN_HOME,我配置成了%MVN_HOME%\bin;）</td>
</tr>
<tr>
<td>%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%MYSQL_HOME%\bin;%MAVEN_HOME%\bin;</td>
</tr>
</tbody>
</table>
<p>配置这些环境变量，在dos窗口才能使java  ，mvn  之类的指令可以用；</p>
<p>否则会出现如下显示。</p>
<table>
<thead>
<tr>
<th>‘mvn’ 不是内部或外部命令，也不是可运行的程序</th>
</tr>
</thead>
<tbody>
<tr>
<td>(这就是环境变量没有配成功的结果)</td>
</tr>
</tbody>
</table>
<p>3.安装</p>
<table>
<thead>
<tr>
<th>C:\Users\Administrator&gt;mvn -v</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544701045091.png" alt="1544701045091"></td>
</tr>
<tr>
<td>C:\Users\Administrator&gt;mvn install:install-file -Dfile=F:/apache-maven-3.5.4/m2/quartz-2.3.0.jar（jar包所在路径） -DgroupId=org.quartz-scheduler -DartifactId=quartz -Dversion=2.3.0 -Dpackaging=jar</td>
</tr>
<tr>
<td>（根据下面所示的配置groupId、artifactId、version）</td>
</tr>
</tbody>
</table>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.quartz-scheduler<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>quartz<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544702128551.png" alt="1544702128551"></p>
<p>如图所示，安装成功。</p>
<p><img src="C:\Users\ADMINI~1\AppData\Local\Temp\1544702179172.png" alt="1544702179172"></p>
]]></content>
        
        <categories>
            
            <category> maven </category>
            
        </categories>
        
        
        <tags>
            
            <tag> maven </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[常用Linux命令的学习（二）]]></title>
        <url>http://sungithup.github.io/2018/12/28/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
        <content type="html"><![CDATA[<p>​<br>​ </p>
<p>[TOC]</p>
<h2 id="一、磁盘指令"><a href="#一、磁盘指令" class="headerlink" title="一、磁盘指令"></a>一、磁盘指令</h2><ol>
<li>查看硬盘信息</li>
</ol>
<blockquote>
<p>命令：df</p>
</blockquote>
<p><code>（默认大小以kb显示） df -k（以kb为单位） df -m（ 以mb为单位） df –h （易于阅读）</code></p>
<ol start="2">
<li>查看文件/目录的大小</li>
</ol>
<blockquote>
<p>命令：du filename|foldername</p>
</blockquote>
<p><code>（默认单位为kb）-k    kb单位 -m    mb单位 -a 所有文件和目录  -h 更易于阅读
​    --max-depth=0    目录深度</code></p>
<h2 id="二、网络指令"><a href="#二、网络指令" class="headerlink" title="二、网络指令"></a>二、网络指令</h2><ol>
<li>查看网络配置信息</li>
</ol>
<blockquote>
<p>命令:ifconfig</p>
</blockquote>
<ol start="2">
<li>测试与目标主机的连通性</li>
</ol>
<blockquote>
<p>命令：ping remote_ip     </p>
</blockquote>
<p><code>ctrl + c :结束ping进程</code></p>
<ol start="3">
<li>显示各种网络相关信息</li>
</ol>
<blockquote>
<p>命令：netstat</p>
</blockquote>
<p><code>查看端口号（是否被占用）
(1)、lsof -i:端口号  （需要先安装lsof）
(2)、netstat -tunlp|grep 端口号</code></p>
<ol start="4">
<li>测试远程主机的网络端口</li>
</ol>
<blockquote>
<p>命令： telnet ip  port   （需要先安装telnet）</p>
</blockquote>
<p><code>测试成功后，按ctrl + ] 键，然后弹出telnet&gt;时，再按q退出</code></p>
<ol start="5">
<li>http请求模拟</li>
</ol>
<blockquote>
<p>curl -X get <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a>   模拟请求百度</p>
</blockquote>
<h2 id="三、系统管理指令"><a href="#三、系统管理指令" class="headerlink" title="三、系统管理指令"></a>三、系统管理指令</h2><ol>
<li>用户操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    操作	            命令</span><br><span class="line">创建用户	       useradd|adduser username</span><br><span class="line">修改密码	       passwd username</span><br><span class="line">删除用户	       userdel –r username</span><br><span class="line">修改用户（已下线）：	</span><br><span class="line">                 修改用户名: usermod –l new_name oldname</span><br><span class="line">                    锁定账户: usermod –L username</span><br><span class="line">                    解除账户： usermod –U username</span><br><span class="line">查看当前登录用户	仅root 用户：whoami   | cat /etc/shadow</span><br><span class="line">                 普通用户：cat /etc/pqsswd</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>用户组操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   操作	            命令</span><br><span class="line">   创建用户组	         groupadd groupname</span><br><span class="line">删除用户组	         groupdel groupname</span><br><span class="line">修改用户组	         groupmod –n new_name old_name</span><br><span class="line">查看用户组	         groups  （查看的是当前用户所在的用户组）</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>用户+用户组    </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">     操作	                命令</span><br><span class="line">   修改用户的主组	         usermod –g groupname username</span><br><span class="line">给用户追加附加组	    usermod –G groupname username</span><br><span class="line">查看用户组中用户数	   cat /etc/group</span><br><span class="line">注意：创建用户时，系统默认会创建一个和用户名字一样的主组</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>系统权限</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    操作	                     命令</span><br><span class="line"> 查看/usr下所有权限	   ll /usr</span><br><span class="line">                       权限类别	r（读取：4） w（写入：2） x（执行：1） </span><br><span class="line">                       三个为一组，无权限用 —代替</span><br><span class="line">                    UGO模型	U（User） G(Group)  O(其他)</span><br><span class="line">权限修改	    </span><br><span class="line">                       修改所有者：chown username file|folder</span><br><span class="line">	                (递归)修改所有者和所属组： chown -r username：groupname file|folder </span><br><span class="line">	                 修改所属组：chgrp groupname file|folder</span><br><span class="line">	                 修改权限：chmod ugo+rwx file|folder</span><br></pre></td></tr></table></figure>
<h2 id="四、系统配置指令"><a href="#四、系统配置指令" class="headerlink" title="四、系统配置指令"></a>四、系统配置指令</h2><p> 1.修改主机名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  编辑文件：      命令： vim /etc/sysconfig/network</span><br><span class="line">  文件内容：      HOSTNAME=node00</span><br><span class="line">（重启生效)reboot</span><br></pre></td></tr></table></figure>
<p> 2.DNS配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">编辑文件：     命令：vim /etc/resolv.conf</span><br><span class="line">文件内容：     nameserver 192.168.198.0</span><br></pre></td></tr></table></figure>
<p> 3.sudo权限配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">   操作            命令</span><br><span class="line">编辑权限配置文件：    vim /etc/sudoers</span><br><span class="line">格式：</span><br><span class="line">        授权用户 主机=[(切换到哪些用户或用户组)] [是否需要密码验证] 路径/命令</span><br><span class="line">举例：</span><br><span class="line">        test  ALL=(root)  /usr/bin/yum,/sbin/service</span><br><span class="line">解释：</span><br><span class="line">        test用户就可以用yum和servie命令，</span><br><span class="line">	   但是，使用时需要在前面加上sudo再敲命令。</span><br><span class="line">	   第一次使用需要输入用户密码,且每个十五分钟需要一次密码验证</span><br><span class="line">修改：</span><br><span class="line">       test ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service</span><br><span class="line">这样就不需要密码了</span><br><span class="line">将权限赋予某个组，%+组名</span><br><span class="line">%group ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service</span><br><span class="line"></span><br><span class="line">列出用户所有的sudo权限       sudo –l</span><br></pre></td></tr></table></figure>
<p> 4.系统时间</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">操作                 命令</span><br><span class="line">查看系统时间          date           ---查看当前时间详情</span><br><span class="line">                    cal            ---查看当前月日历</span><br><span class="line">                    cal 2018       ---查看2018年完整日历</span><br><span class="line">                    cal 12 2018    ---查看指定年月的日历       </span><br><span class="line"></span><br><span class="line">更新系统时间（推荐）   yum install ntpdate –y    ---安装ntp服务</span><br><span class="line">                    ntpdate cn.ntp.org.cn     ---到域名为cn.ntp.org.cn的时间服务器上同步时间</span><br></pre></td></tr></table></figure>
<p> 5.关于hosts配置</p>
<p>相当于给IP地址其别名，可以通过别名访问</p>
<table>
<thead>
<tr>
<th></th>
<th>路径：</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows系统</td>
<td><strong>C:/Windows/System32/drivers/etc/hosts</strong> <strong>文件</strong></td>
</tr>
<tr>
<td>Linux系统</td>
<td><strong>/etc/hosts</strong>文件：<strong>vim</strong>  +路径</td>
</tr>
<tr>
<td>统一 编辑格式</td>
<td><strong>IP</strong>地址  别名：192.168.198.128    node00</td>
</tr>
</tbody>
</table>
<p> 6.关于hostname配置</p>
<p>相当于给对应的虚拟机器起别名</p>
<table>
<thead>
<tr>
<th>Linux系统：</th>
<th><strong>vi /etc/sysconfig/network</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>编辑内容：</td>
<td><strong>HOSTNAME=node01</strong></td>
</tr>
</tbody>
</table>
<p>五、重定向与管道符</p>
<table>
<thead>
<tr>
<th>输出重定向</th>
<th>输出重定向到一个文件或设备：                                                                                     &gt;   覆盖原来的文件                                                                                                            &gt;&gt;   追加原来的文件                                                                                                              举例：                                                                                                                               ls &gt; log                           — 在log文件中列出所有项，并覆盖原文件                                                         echo   “hello”&gt;&gt;log     —将hello追加到log文件中</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入重定向</td>
<td><strong>&lt;</strong>         输入重定向到一个程序                                                                                            举例：cat <strong>&lt;</strong> log             —将log文件作为cat命令的输入，查看log文件的内容</td>
</tr>
<tr>
<td>标准   输出  重定向</td>
<td><strong>1 &gt;</strong> 或   <strong>&gt;</strong>                                                                                                                         含义：                                                                                                                             输出重定向时，只用正确的输出才会重定向到指的文件中                                        错误的则会直接打印到屏幕上</td>
</tr>
<tr>
<td>错误   输出  重定向</td>
<td><strong>2 &gt;</strong>                                                                                                                                含义：                                                                                                                              错误的输出会重定向到指定文件里，正确的日志则直接打印到屏幕上。</td>
</tr>
<tr>
<td>结合  使用</td>
<td><strong>2&gt;&amp;1</strong>                                                                                                                            含义：                                                                                                                               将无论是正确的输出还是错误的输出都重定向到指定文件</td>
</tr>
<tr>
<td>管道</td>
<td>**\</td>
<td>**                                                                                                                                      含义：                                                                                                                                  把前一个输出当做后一个输入                                                                                       grep 通过正则搜索文本，并将匹配的行打印出来                                                         netstat -anp \</td>
<td>grep 22   把netstat –anp 命令的输出 当做是grep 命令的输入</td>
</tr>
<tr>
<td>命令  执行  控制</td>
<td><strong>&amp;&amp;</strong>   前一个命令执行成功才会执行后一个命令                                                              **\</td>
<td>\</td>
<td>**      前一个命令执行失败才会执行后一个命令</td>
</tr>
</tbody>
</table>
]]></content>
        
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[常用Linux命令的学习（一）]]></title>
        <url>http://sungithup.github.io/2018/12/27/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
        <content type="html"><![CDATA[<h1 id="常用Linux命令的学习（一）"><a href="#常用Linux命令的学习（一）" class="headerlink" title="常用Linux命令的学习（一）"></a>常用Linux命令的学习（一）</h1><p>[TOC]</p>
<h2 id="一、命令指南（manual）：man"><a href="#一、命令指南（manual）：man" class="headerlink" title="一、命令指南（manual）：man"></a>一、命令指南（manual）：man</h2><blockquote>
<p>安装：yum install man –y</p>
</blockquote>
<p>（-y 表示获得允许，无需确认）</p>
<blockquote>
<p>查看ls命令指南： man ls</p>
</blockquote>
<h2 id="二、目录命令"><a href="#二、目录命令" class="headerlink" title="二、目录命令"></a>二、目录命令</h2><p><strong>切换</strong>目录：cd + 目录的路径</p>
<p>查看当前目录所在的完整路径：pwd</p>
<p><strong>新建</strong>目录：mkdir +目录名字</p>
<p><strong>查看</strong>当前目录所用有的子目录和文件：ls   ，ll等价于  ls –l</p>
<p>​        查看目录下的所有东西（包括隐藏文件）：  ls –al   等价于 ll -a<br>​<br><strong>拷贝</strong>目录或文件：cp –r install.log  install2.log</p>
<p><strong>删除</strong>目录或文件：rm  -r install.log</p>
<blockquote>
<p>(rmdir只能删除空目录)</p>
</blockquote>
<p><strong>移动</strong>目录或文件：mv + 目录/文件名字 + 其他路径</p>
<p>​         将test目录移动到  根目录/ 下 :   mv test / </p>
<blockquote>
<p>（如果移动到当前目录，用另外一个名称，则可以实现重命名的效果）</p>
</blockquote>
<p><strong>更改</strong>文件或目录的名字：mv + 旧目录名字 + 新目录名</p>
<blockquote>
<p>(     -r 用于递归的拷贝，删除，移动目录)</p>
</blockquote>
<h2 id="三、文件命令"><a href="#三、文件命令" class="headerlink" title="三、文件命令"></a>三、文件命令</h2><h3 id="1、一般文件操作"><a href="#1、一般文件操作" class="headerlink" title="1、一般文件操作"></a>1、一般文件操作</h3><p><strong>新建</strong>文件：touch  install.log<br>​        (vim install.log   编辑文件，如果文件不存在，就会新建一个对应的文件，并进入文件的编辑模式，如果按 :wq 会保存文件并退出，如果按 :q 则不保存退出)<br>​<br><strong>查看</strong>文件内容：cat +（文件名）<br>（一次性显示整个文件的内容，文件内容过多时用户体验不好）</p>
<p>一次命令显示一屏文本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  more +（文件名）</span><br><span class="line">  </span><br><span class="line">  按键         效果  </span><br><span class="line">Space         显示下一屏文本内容</span><br><span class="line">B             显示上一屏文本内容</span><br><span class="line">Enter         显示下一行文本内容</span><br><span class="line">Q             退出查看</span><br></pre></td></tr></table></figure>
<pre><code>  less+（文件名）
按键            效果
 h             显示帮助界面
 u             向后滚动半页 
 d             向前翻半页 
 e | Enter     向后翻一行文本
 space         滚动一页 
 b             向后翻一页 
 [pagedown]：  向下翻动一页 
 [pageup]：    向上翻动一页
  上下键，向上一行，向下一行
</code></pre><p>从头打印文件内容：<br>​        head  -10 +（文件名）  打印文件1到10行</p>
<p>从尾部打印文件内容<br>​        tail -10 +（文件名）打印文件最后10行</p>
<blockquote>
<p>tail -f (文件名)  常用于查看文件内容的更新变化</p>
</blockquote>
<p><strong>查找</strong>文件或目录<br>​        find +（路径名） –name +（文件名）<br>​        举例：find / -name profile<br>​             在/(根目录)目录下查找 名字为profile的文件或目录</p>
<p>​        也可利用正则：<br>​             举例： find /etc -name pro*<br>​                     在/etc目录下查找以pro开头的文件或目录</p>
<blockquote>
<p>路径越精确，查找的范围越小，速度越快 i</p>
</blockquote>
<h3 id="2、文件编辑"><a href="#2、文件编辑" class="headerlink" title="2、文件编辑"></a>2、文件编辑</h3><h4 id="vi"><a href="#vi" class="headerlink" title="vi"></a>vi</h4><p>（1） vi    进入编辑模式 —–&gt;按i   进入插入模式 ——-&gt;  按Esc 退出编辑模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi  filename   :打开或新建文件，并将光标置于第一行首 </span><br><span class="line">vi +n filename ：打开文件，并将光标置于第n行首 </span><br><span class="line">vi + filename  ：打开文件，并将光标置于最后一行首 </span><br><span class="line">vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的字符串所在的行首</span><br></pre></td></tr></table></figure>
<blockquote>
<p> filename  为文件名</p>
</blockquote>
<p>（2）在文件vi（文件编辑）模式下</p>
<ul>
<li><strong>命令行模式</strong> </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">:w      保存</span><br><span class="line">:q      退出</span><br><span class="line">:wq     保存并退出</span><br><span class="line">:q!     强制退出</span><br><span class="line">:set nu |ctrl+g    显示文本行数</span><br><span class="line">:set nonu          去除显示的行数</span><br><span class="line">:s/p1/p2/g         将当前行中所有p1均用p2替代 </span><br><span class="line">:n1,n2s/p1/p2/g    将第n1至n2行中所有p1均用p2替代 </span><br><span class="line">:g/p1/s//p2/g      将文件中所有p1均用p2替换</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>一般模式</strong> </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">按键：</span><br><span class="line">yy    复制光标所在行(常用) </span><br><span class="line">nyy   复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) </span><br><span class="line">p|P   p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)</span><br><span class="line">G     光标移至第最后一行</span><br><span class="line">nG    光标移动至第N行行首</span><br><span class="line">n+    光标下移n行 </span><br><span class="line">n-    光标上移n行 </span><br><span class="line">H     光标移至屏幕顶行 </span><br><span class="line">M     光标移至屏幕中间行 </span><br><span class="line">L     光标移至屏幕最后行 </span><br><span class="line"></span><br><span class="line">dd    删除所在行 </span><br><span class="line">x或X  删除一个字符，x删除光标后的，而X删除光标前的 </span><br><span class="line">u     撤销(常用)</span><br><span class="line"></span><br><span class="line">删除第N行到第M行：N,Md</span><br><span class="line">：,$-1d   删除当前光标到到数第一行数据</span><br><span class="line"></span><br><span class="line">按键：</span><br><span class="line">    i: 在当前光标所在字符的前面，转为输入模式；</span><br><span class="line">    a: 在当前光标所在字符的后面，转为输入模式；</span><br><span class="line">    o: 在当前光标所在行的下方，新建一行，并转为输入模式；</span><br><span class="line">    I：在当前光标所在行的行首，转换为输入模式</span><br><span class="line">    A：在当前光标所在行的行尾，转换为输入模式</span><br><span class="line">    O：在当前光标所在行的上方，新建一行，并转为输入模式；</span><br><span class="line"></span><br><span class="line">---逐字符移动：</span><br><span class="line">h: 左    l: 右</span><br><span class="line"></span><br><span class="line">j: 下	k: 上</span><br></pre></td></tr></table></figure>
<h4 id="vim"><a href="#vim" class="headerlink" title="vim"></a>vim</h4><blockquote>
<p>安装：yum install vim -y</p>
</blockquote>
<ul>
<li><em>用vim 打开/etc/profile 文件，</em></li>
<li><em>特点：编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强</em> ，其他均与vi相同</li>
</ul>
<h3 id="3、文件上传下载"><a href="#3、文件上传下载" class="headerlink" title="3、文件上传下载"></a>3、文件上传下载</h3><blockquote>
<p>安装上传下载命令：yum install lrzsz -y</p>
</blockquote>
<p>上传文件：（windows—&gt;linux）</p>
<blockquote>
<p>命令 ：rz  </p>
<p>弹出windows上传文件窗口</p>
</blockquote>
<p>下载文件：(linux—&gt;windows)</p>
<p><code>注意</code>：sz命令只能下载文件，不能下载目录，推荐将目录压缩成tar包或使用工具软件：Winscp【Xftp】</p>
<blockquote>
<p>命令：sz  （文件名）</p>
<p>弹出windows下载窗口,下载文件到指定文件目录</p>
</blockquote>
<h3 id="4、文件传输"><a href="#4、文件传输" class="headerlink" title="4、文件传输"></a>4、文件传输</h3><h4 id="1-、本地→远程"><a href="#1-、本地→远程" class="headerlink" title="(1)、本地→远程"></a>(1)、本地→远程</h4><blockquote>
<p>文件  ：  scp local_file remote_username@remote_ip:remote_folder   </p>
<p>目录 ：  scp -r local_folder remote_username@remote_ip:remote_folder</p>
</blockquote>
<h4 id="2）、远程→本地"><a href="#2）、远程→本地" class="headerlink" title="(2）、远程→本地"></a>(2）、远程→本地</h4><blockquote>
<p>文件 ： scp remote_username@remote_ip:remote_file local_folder</p>
<p>目录 ： scp remote_username@remote_ip:remote_folder local_folder   </p>
</blockquote>
]]></content>
        
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[常用Linux命令的学习（三）]]></title>
        <url>http://sungithup.github.io/2018/12/27/%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
        <content type="html"><![CDATA[<p>[TOC]</p>
<h2 id="一、服务操作"><a href="#一、服务操作" class="headerlink" title="一、服务操作"></a>一、服务操作</h2><table>
<thead>
<tr>
<th>列出  所有  服务</th>
<th>chkconfig</th>
</tr>
</thead>
<tbody>
<tr>
<td>服务  操作</td>
<td>service 服务名 start\</td>
<td>stop\</td>
<td>status\</td>
<td>restart                                                                 永久关闭\</td>
<td>打开 （启动后生效） chkconfig iptables on\</td>
<td>off</td>
</tr>
<tr>
<td>添加  服务</td>
<td>1)    编辑脚本：vim myservice.sh                                                                                                                  编辑内容：                                                                                                                                     （在最前面加一下两句）                                                                                                #chkconfig:   2345 80 90                                                            #description:auto_run                                                                                                (自己的服务脚本：开机时同步时间）                                                      result=’ntpdate cn.ntp.org.cn’                                                                                      退出编辑并保存：按esc键 按 ：wq                                                                                     在ntpdate.log文件中输出打印：echo $result &gt; /usr/ntpdate.log                                                                                   2)    修改权限，使其拥有可执行权限:   chmod 700 myservice.sh                            3)    将脚本拷贝到/etc/init.d目录：                                                                              4)    加入服务：chkconfig –add myservice.sh                                                             5)    重启服务器，验证服务是否添加成功：date                                                     6）/usr目录下产生ntpdate.log</td>
</tr>
<tr>
<td>删除  服务</td>
<td>chkconfig –del name</td>
</tr>
<tr>
<td>更改  服务初   执行  等级</td>
<td>chkconfig –level 2345 服务名 off\</td>
<td>on                                                                     chkconfig 服务名 on\</td>
<td>of f</td>
</tr>
</tbody>
</table>
<h2 id="二、定时调度"><a href="#二、定时调度" class="headerlink" title="二、定时调度"></a>二、定时调度</h2><table>
<thead>
<tr>
<th>编辑定时任务</th>
<th>crontab –e                                                                                                                    格式：minute hour day month dayofweek command</th>
</tr>
</thead>
<tbody>
<tr>
<td>举例</td>
<td><em> </em> <em> </em> *  echo  “hello”                                                                                                     每分钟打印“hello”</td>
</tr>
<tr>
<td>时间一到，执行操作命令后</td>
<td>会出现：You have new mail in /var/spool/mail/root</td>
</tr>
<tr>
<td>查看任务执行情况</td>
<td>vim /var/spool/mail/root</td>
</tr>
<tr>
<td>查看所有用户的定时任务</td>
<td>ll /var/spool/cron</td>
</tr>
<tr>
<td>查看当前用户的定时任务</td>
<td>contab –l</td>
</tr>
<tr>
<td><em>注意</em></td>
<td>“<em>”代表任意的数字, “/”代表”每隔多久”,                                                                                “-”代表从某个数字到某个数字, “,”分开几个离散的数字                                                                                                                    如：                                                                                                                                       30-40 12 </em> <em> </em> echo “hello”                                                                                                           ——–每天12点30分至40分期间，每分钟执行一次命令                                                                                      30,40                                                                                                                                                ——–每天12点30分和12点40分                                                                                           0/5                                                                                                                                               ——–每天的12点整至12点55分期间，每隔5分钟执行一次命令</td>
</tr>
</tbody>
</table>
<h2 id="三、进程操作"><a href="#三、进程操作" class="headerlink" title="三、进程操作"></a>三、进程操作</h2><table>
<thead>
<tr>
<th>查看  进程</th>
<th>ps  -aux                                                                                                                                -a 列出所有           -u 列出用户   -x 详细列出，如cpu、内存等                                                            -e 显示所有进程    -f 全格式                                                                                                              ps  - ef  \</th>
<th>grep ssh   查看所有进程里CMD是ssh 的进程信息                                                       ps -aux –sort –pcpu   根据 CPU 使用来升序排序</th>
</tr>
</thead>
<tbody>
<tr>
<td>使程序   后台运行</td>
<td>只需要在命令后添加  &amp; 符号                                                                                                                            echo “hello” &amp;   jobs –l      –列出当前连接的所有后台进程（jobs仅适用于当前端）   ps  -ef \</td>
<td>grep 进程名          —-（推荐）列出后台进程</td>
</tr>
<tr>
<td>杀死进程</td>
<td>（强制）kill -9 pid   —-pid为进程号</td>
</tr>
</tbody>
</table>
<h2 id="四、其他命令"><a href="#四、其他命令" class="headerlink" title="四、其他命令"></a>四、其他命令</h2><table>
<thead>
<tr>
<th>wget</th>
<th>1）   安装：yum install wget  –y                                                                                                        2）   用法：wget [option] 网址  -O  指定下载保存的路径                                               3）   也可用于做爬虫</th>
</tr>
</thead>
<tbody>
<tr>
<td>yum</td>
<td>1）   备份原镜像：                                                                                                                   cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOSBase.repo.backup                                                                                 2）   下载新镜像：                                                                                                                  <strong>wget</strong> -O /etc/yum.repos.d/CentOS-Base.repo      <a href="http://mirrors.aliyun.com/repo/Centos-6.repo" target="_blank" rel="noopener">http://mirrors.aliyun.com/repo/Centos-6.repo</a>                                                               3）  查看文件内容：vim /etc/yum.repos.d/CentOS-Base.repo                                            4）  生成缓存：yum makecache</td>
</tr>
<tr>
<td>rpm</td>
<td>1）   安装 rpm –ivh rpm包                                                                                                              2）   查找已安装的rpm包：rpm –q ntp                                                                                      3）   卸载：rpm –e ntp-4.2.6p5-10.el6.centos.2.x86_64（全名）</td>
</tr>
<tr>
<td>tar</td>
<td>1）   解压：tar  -zvxf  xxxx.tar.gz                                                                                               2）   压缩：tar -zcf 压缩包命名 压缩目标                                                                                           3）   例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61                                                       4）   -z   gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加                                                   -x  解压                            -c  压缩   -f   目标文件，压缩文件新命名或解压文件名                                               -v  解压缩过程信息打印</td>
</tr>
<tr>
<td>zip</td>
<td>1）安装zip：yum install zip –y                                                                                        2）压缩命令：zip   -r 包名 目标目录                                                                                          3）安装 ：unzip,yum   install unzip –y                                                                                   4）解压  ：unzip   filename</td>
</tr>
</tbody>
</table>
<h2 id="五、安装部署"><a href="#五、安装部署" class="headerlink" title="五、安装部署"></a>五、安装部署</h2><table>
<thead>
<tr>
<th>JDK   部署</th>
<th>1)     解压: tar -zxf jdk-7u80-linux-x64.tar.gz                                                                                         2)     配置环境变量                                                                                                                                                                  编辑配置文件：vim /etc/profile                                                                                                                                                   编辑内容 ：                                                                                                                                                          JAVA_HOME= /usr/soft/jdk1.7.0_75                                                                                                 PATH=$PATH:$JAVA_HOME/bin                                                                                                           3) 重新加载环境变量：source  /etc/profile                                                                                                   4) 验证: java  -version</th>
</tr>
</thead>
<tbody>
<tr>
<td>mysql部署</td>
<td>yum安装 mysql                                                                                                                                         1) yum install mysql-server -y                                                                                                                2) yum install mysql-devel -y                                                                                                                                  3) service mysqld start                                                                                                                              4) mysql -uroot -p                                                                                                                                      5) mysqladmin -u root    password 123456</td>
</tr>
</tbody>
</table>
<h2 id="六、免密登录"><a href="#六、免密登录" class="headerlink" title="六、免密登录"></a>六、免密登录</h2><p>| 法一                                                     </p>
<p>| 1）   生成公钥和密钥：ssh-keygen -t rsa ，并且回车3次                                                                                                    （在用户的根目录生成一个 “.ssh”的文件夹）                                                                                                                           2）   查看公钥和私钥：ll ~/.ssh                                                                                                                                                                    （目录中会有以下几个文件）                                                                                                                         <code>authorized_keys</code>:存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥                                                 <code>id_rsa</code> : 生成的私钥文件                                                                                                                              <code>id_rsa.pub</code> ： 生成的公钥文件                                                                                                                                        <code>know_hosts</code> : 已知的主机公钥清单                                                                                                                                                       <em>                        如果希望ssh公钥生效需满足至少下面两个条件：                                                                                                  </em>                        1&gt; .ssh目录的权限必须是700                                                                                                                                *                         2&gt; .ssh/authorized_keys文件权限必须是600                                                                                                                 3）   将A的.ssh目录下的公钥追加拷贝到B的authorized_keys文件里                                                                         scp -p ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:/root/.ssh/authorized_keys                                                                4)     验证：将文件远程拷贝到远程主机上，看是否需要密码 |</p>
<table>
<thead>
<tr>
<th>———————————————————</th>
<th>:———————————————————–</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>（<strong>此方法有待考究）</strong>法二：通过Ansible实现   批量   免密</td>
<td>1）、 <strong>将需要做免密操作的机器hosts添加到/etc/ansible/hosts下：</strong>                                                                                             <h3>[&nbsp;Avoid close]<br>　　192.168.91.132</h3></td>
</tr>
</tbody>
</table>
<p>　　192.168.91.133<br>　　192.168.91.134    <br>      2）、 <strong>执行命令进行免密操作</strong>  ：​                 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible &lt;groupname&gt; -m authorized_key -a &quot;user=root key=&apos;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;) &#125;&#125;&apos;&quot; -k</span><br></pre></td></tr></table></figure>
<p>​                                                                                                                                                                                                                                                                                                                                             |</p>
]]></content>
        
        <categories>
            
            <category> Linux </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Linux命令 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Hello World]]></title>
        <url>http://sungithup.github.io/2018/12/19/hello-world/</url>
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
        
        
    </entry>
    
    
    
    
    <entry>
        <title><![CDATA[about]]></title>
        <url>http://sungithup.github.io/about/index.html</url>
        <content type="html"><![CDATA[<p>##关于我</p>
<p>哈哈</p>
]]></content>
    </entry>
    
    <entry>
        <title><![CDATA[categories]]></title>
        <url>http://sungithup.github.io/categories/index-1.html</url>
        <content type="html"></content>
    </entry>
    
    <entry>
        <title><![CDATA[categories]]></title>
        <url>http://sungithup.github.io/categories/index.html</url>
        <content type="html"></content>
    </entry>
    
    <entry>
        <title></title>
        <url>http://sungithup.github.io/index.md/index.html</url>
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layout: page</span><br></pre></td></tr></table></figure>
]]></content>
    </entry>
    
    <entry>
        <title><![CDATA[tags]]></title>
        <url>http://sungithup.github.io/tags/index-1.html</url>
        <content type="html"></content>
    </entry>
    
    <entry>
        <title><![CDATA[tags]]></title>
        <url>http://sungithup.github.io/tags/index.html</url>
        <content type="html"></content>
    </entry>
    
    
</search>

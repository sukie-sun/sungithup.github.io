<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark学习（六）]]></title>
    <url>%2F2019%2F02%2F22%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、 SparkStreaming简介 SparkStreaming是流式处理框架，是Spark API的扩展，支持可扩展、高吞吐、容错的实时数据处理。 实时数据来源：kafka、Flume、Twitter、ZeroMQ、TCP Socket 可以使用高级功能的复杂算子来处理流数据：如 map、reduce、join、window 处理后的数据可以存放在文件系统、数据库等，方便实时展现 二、SparkStreaming 与 Storm 的区别 Storm 是纯实时的流式处理框架，SparkStreaming 是准实时的处理框架（微批处理）。因为微批处理，SparkStreaming 的吞吐量比 Storm 要高。 Storm 的事务机制要比 SparkStreaming 的要完善。 Storm 支持动态资源调度。(spark1.2 开始和之后也支持) SparkStreaming 擅长复杂的业务处理，Storm 不擅长复杂的业务处理，擅长简单的汇总型计算 SparkStreaming Storm 微批处理，准实时的流式处理框架 实时计算框架，来一条数据马上处理 支持动态调整资源 支持动态调整资源 支持事务 支持事务 支持复杂的业务场景 处理场景相对简单一些 三、SparkStreaming的详情1、运行流程 SparkStreaming会启动receive task一直接受数据，每个batchInterval的时间周期，就会把数据变成一个batch，然后进一步封装成RDD，最后变成DStream ,用户操作DStream 时，可以使用一系列算子： map、flatmap、filter。。。。。 情况： 1、batchInterval为5s ，计算这批数据的时间为3s ，则此时 0—5s，在结束数据；5—10s，一边接收数据，一边处理上一批数据；依次类推。 2、batchInterval为5s ，计算这批数据的时间为6s ，则此时0—5s，在接收数据；5—10s，一边接收第二批数据，一边处理第一批数据；10—11s,一边接收第三批数据，一边处理第一批数据，第二批数据等待计算，就会造成数据堆积，如果SparkStreaming的数据存储是仅在内存中，就会发生OOM；如果设置StorageLevel 包含 disk, 则内存存放不下的数据会溢写至 disk, 加大延迟 注意； receiver task 是 7*24 小时一直在执行 2、SparkStreaming 代码（1）关于SparkStreaming 框架我们必须要知道的几点 注意： receiver模式下接收数据，local的模拟线程必须大于等于2： 一个线程用receiver的数据接收 一个线程用于执行job Duration时间设置就是我们能接受的延迟度，需要根据集群的资源情况以及监控每一个job的执行时间来调节出最佳时间。 创建JavaStreamingContext有两种方式：SparkConf 、 SparkContext 123456&gt; // JavaSparkContext → JavaStreamingContext &gt; JavaStreamingContext jsc = &gt; new JavaStreamingContext(sc,Durations.seconds(5)); &gt; // JavaStreamingContext → JavaSparkContext&gt; final JavaSparkContext sparkContext = jsc.sparkContext();&gt; 所有的代码逻辑完成以后，必须要有一个ouput opertion类算子 JavaStreamingContext.start() ，Streaming框架便启动，之后，就不能再次添加业务逻辑 JavaStreamingContext.stop() ，无参的stop( ) 会把SparkContext一同关闭；stop(false) , 只会关闭StreamContext ,SparkContext依然存在 JavaStreamingContext.stop()停止之后不能再调用 start （2）代码举例：WordCount123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import org.apache.spark.Accumulator;import org.apache.spark.SparkConf;import org.apache.spark.SparkContext;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.broadcast.Broadcast;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.Time;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import scala.Tuple2;public class WordCountOnline &#123; @SuppressWarnings("deprecation") public static void main(String[] args) &#123; /** The master URL to connect to, such as "local" to run locally with one thread, "local[4]" to run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster. */ final SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("WordCountOnline"); /** * 在创建streamingContext的时候 设置batch Interval * 创建streamingContext有两种方式：conf， context */// final JavaStreamingContext jsc = // new JavaStreamingContext(conf, Durations.seconds(5)); JavaSparkContext sc = new JavaSparkContext(conf);//创建StreamContext，及间隔时间：每个5秒处理数据 JavaStreamingContext jsc = new JavaStreamingContext(sc,Durations.seconds(5)); // final JavaSparkContext sparkContext = jsc.sparkContext();//设置监听节点及端口，接收从这个节点的这个port输入的数据，最后封装成DStream//避免端口被占用 JavaReceiverInputDStream&lt;String&gt; lines = jsc.socketTextStream("node00", 9999);//切割每一行数据 JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(String s) &#123; return Arrays.asList(s.split(" ")); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123;//给每个单词计为1 private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s, 1); &#125; &#125;);//累加，并指定分区数 JavaPairDStream&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;,3); //outputoperator类的算子 // counts.print();counts.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String,Integer&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void call(JavaPairRDD&lt;String, Integer&gt; pairRDD) throws Exception &#123; System.out.println("=============="); pairRDD.foreach(new VoidFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void call(Tuple2&lt;String, Integer&gt; tuple)throws Exception &#123; System.out.println("tuple ---- "+tuple ); &#125; &#125;); &#125; &#125;);//框架启动必须调用start jsc.start();//等待spark程序被终止 jsc.awaitTermination();//这个期间可用于一些扫尾操作，如获取SparkContext，如果直接stop，就无法实现了 //任务执行结束 jsc.stop(); System.out.println("stop====================="); &#125;&#125; （3）代码运行在Linux系统中： 启动socket server 服务：node00 12yum install nc -ync -lk 9999 在该节点上输入输入数据 （避免端口被占用） 在Windows端运行代码，便能接收到数据，从而执行运算处理 广播黑名单：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package com.sxt.java.sparkstreaming;import java.util.ArrayList;import java.util.List;import org.apache.spark.SparkConf;import org.apache.spark.SparkContext;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.broadcast.Broadcast;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import com.google.common.base.Optional;import scala.Tuple2; // 过滤黑名单（使用广播变量）public class TransformOperator &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster("local[2]").setAppName("transform"); JavaStreamingContext jsc = new JavaStreamingContext(conf,Durations.seconds(5)); //模拟黑名单 List&lt;String&gt; blackList = new ArrayList&lt;String&gt;(); blackList.add("zhangsan"); //广播黑名单 final Broadcast&lt;List&lt;String&gt;&gt; broadcastList = jsc.sparkContext().broadcast(blackList); //接受socket数据源: 1 zhangsan 2 lisi 3 wangwu JavaReceiverInputDStream&lt;String&gt; nameList = jsc.socketTextStream("node01", 7777); JavaPairDStream&lt;String, String&gt; pairNameList = nameList.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(String s) throws Exception &#123; return new Tuple2&lt;String, String&gt;(s.split(" ")[1], s); &#125; &#125;);//对DStream使用transform算子，在算子内部实现RDD到RDD的转换 JavaDStream&lt;String&gt; transFormResult = pairNameList.transform(new Function&lt;JavaPairRDD&lt;String,String&gt;,JavaRDD&lt;String&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Overridepublic JavaRDD&lt;String&gt; call(JavaPairRDD&lt;String, String&gt; nameRDD)throws Exception &#123; JavaPairRDD&lt;String, String&gt; filter = nameRDD.filter(new Function&lt;Tuple2&lt;String,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;String, String&gt; v1) throws Exception &#123; //得到广播变量 List&lt;String&gt; blackList = broadcastList.value(); return !blackList.contains(v1._1); &#125; &#125;); return filter.map(new Function&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public String call(Tuple2&lt;String, String&gt; v1) throws Exception &#123; return v1._2; &#125; &#125;); &#125; &#125;); transFormResult.print(); jsc.start(); jsc.awaitTermination(); jsc.stop(); &#125;&#125; 统计累计值：从程序启动，到当前 ， 所有批次数据的累加值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import java.util.Arrays;import java.util.List;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import com.google.common.base.Optional;import scala.Tuple2;public class UpdateStateByKeyOperator &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf() conf.setMaster("local[2]").setAppName("UpdateStateByKeyDemo"); JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(5));//设置checkpoint目录// jsc.checkpoint("hdfs://shsxt/spark/checkpoint"); jsc.checkpoint("./checkpoint"); /* 数据格式： hello shsxt hello bjsxt*/ JavaReceiverInputDStream&lt;String&gt; lines = jsc.socketTextStream("node00", 9999); JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(String s) &#123; return Arrays.asList(s.split(",")); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s, 1); &#125; &#125;);//updateStateByKey 更新key值状态 JavaPairDStream&lt;String, Integer&gt; counts =ones.updateStateByKey( new Function2&lt;List&lt;Integer&gt;, Optional&lt;Integer&gt;, Optional&lt;Integer&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Optional&lt;Integer&gt; call( List&lt;Integer&gt; values, Optional&lt;Integer&gt; state) throws Exception &#123; /** * values:经过分组最后 这个key所对应的value [1,1,1,1,1] * state:这个key在前一个批次的状态 */ Integer updateValue = 0; if (state.isPresent()) &#123; //如果存在值，便获取 updateValue = state.get(); &#125; System.out.println(updateValue + " ======== "); for (Integer value : values) &#123; updateValue += value; &#125; return Optional.of(updateValue); &#125; &#125;); //output operator counts.print();// counts.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Integer&gt;&gt;() &#123;// @Override// public void call(JavaPairRDD&lt;String, Integer&gt; pairRDD) throws Exception &#123;// System.out.println(pairRDD.getNumPartitions());//// pairRDD.collect();// &#125;// &#125;); jsc.start(); jsc.awaitTermination(); jsc.close(); &#125;&#125; 注意： Optional 类 Java 8 引入的类 主要用于解决空指针异常的问题 从本质上说，这是一个包含有可选值的包装类，意味着Optional 类既可以包含有对象，也可以为空 3、SparkStreaming算子操作1、ouput opertion类算子 foreachRDD 参数：RDD 返回值：无 foreachRDD可以遍历得到DStream中的RDD 可以对RDD使用RDD的Transformation类算子进行转化 但是在这个算子内 必须对抽取出来的RDD执行Action类算子，代码才能执行 在这个算子内，RDD算子外执行的代码是在Driver端执行，RDD算子内的代码是在Executor中执行。 print 2、transformation类算子 transform 参数：RDD 返回：另一RDD transform算子可将DStream做RDD到RDD的任意操作 在这个算子内，RDD算子外执行的代码是在Driver端执行，RDD算子内的代码是在Executor中执行。 updateStateByKey 此算子为SparkStreaming中每一个key维护一个state，state可以是任意类型，也可以是自定义对象，更新函数也可以是自定义 与reduceByKey相似的地方就是会先按key进行分组 通过更新函数对该 key 的状态不断更新，对于每个新的 batch 而言，SparkStreaming 会在使用 updateStateByKey 的时候为已经存在的 key 进行 state 的状态更新。 如果要不断的更新每个key的state，就一定涉及到了状态的保存和容错，这个时候就需要开启checkpoint机制和功能 有何用？全面的广告点击分析，统计广告点击流量，统计这一天的车流量，统计点击量 3、注意 使用到 updateStateByKey 要开启 checkpoint 机制和功能。 12345//设置checkpoint目录: // 落地到本地磁盘 jsc.checkpoint(&quot;./checkpoint&quot;);//保存在hdfs jsc.checkpoint(&quot;hdfs://shsxt/spark/checkpoint&quot;); 多久会将内存中的数据(每一个key所对应的状态)写入到磁盘一份？ 如果batchInterval设置的时间小于10秒，那么10秒写入磁盘一份。 如果 batchInterval 设置的时间大于 10 秒，那么就会 batchInterval时间间隔写入磁盘一份。 这样做是为了防止频繁的写HDFS 4、窗口操作(1)窗口操作理解图： 假设每隔 5s 1 个 batch,上图中窗口长度为 15s，窗口滑动间隔 10s。 窗口长度和滑动间隔必须是 batchInterval 的整数倍。如果不是整数倍会检测报错。 用于计算最近一段时间的数据 (2)优化后的 window 窗口操作示意图： 优化后的 window 操作要保存状态所以要设置 checkpoint 路径，没有优化的 window 操作可以不设置 checkpoint 路径 (3)代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109import java.util.Arrays;import java.util.Iterator;import org.apache.spark.SparkConf;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import scala.Tuple2;//基于滑动窗口的热点搜索词实时统计public class WindowOperator &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf() conf.setMaster("local[2]").setAppName("WindowHotWord"); JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(5));//设置日志级别为WARN jssc.sparkContext().setLogLevel("WARN"); /** * 注意： * 没有优化的窗口函数可以不设置checkpoint目录 * 优化的窗口函数必须设置checkpoint目录 */// jssc.checkpoint("hdfs://node1:9000/spark/checkpoint"); jssc.checkpoint("./checkpoint"); JavaReceiverInputDStream&lt;String&gt; searchLogsDStream = jssc.socketTextStream("node00", 9999); //word 1 JavaDStream&lt;String&gt; searchWordsDStream = searchLogsDStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(String t) throws Exception &#123; System.out.println(t + "*************"); return Arrays.asList(t.split(" ")); &#125; &#125;); // 将搜索词映射为(searchWord, 1)的tuple格式 JavaPairDStream&lt;String, Integer&gt; searchWordPairDStream = searchWordsDStream.mapToPair( new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String searchWord)throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(searchWord, 1); &#125; &#125;); /** * 每隔10秒，计算最近60秒内的数据， * 那么这个窗口大小就是60秒，里面有12个rdd，在没有计算之前，这些rdd是不会进行计算的。 * 那么在计算的时候会将这12个rdd聚合起来，然后一起执行reduceByKeyAndWindow操作 ， * reduceByKeyAndWindow是针对窗口操作的而不是针对DStream操作的。 */// JavaPairDStream&lt;String, Integer&gt; searchWordCountsDStream =// searchWordPairDStream.reduceByKeyAndWindow(// new Function2&lt;Integer, Integer, Integer&gt;() &#123;//// private static final long serialVersionUID = 1L;// @Override// public Integer call(Integer v1, Integer v2) throws Exception &#123;// return v1 + v2;// &#125;// &#125;, Durations.minutes(30), Durations.seconds(60));//// JavaPairDStream&lt;String, Integer&gt; searchWordCountsDStream =// searchWordPairDStream.reduceByKeyAndWindow(// new Function2&lt;Integer, Integer, Integer&gt;() &#123;// @Override// public Integer call(Integer v1, Integer v2) throws Exception &#123;// System.out.println(v1 + " : " + v2);// return v1 + v2;// &#125;// &#125;,Durations.seconds(15),Durations.seconds(5)); // 窗口时间 滑块时间 //window窗口操作优化： //将划出串窗口的数据排除，将新划入窗口的数据添加 JavaPairDStream&lt;String, Integer&gt; searchWordCountsDStream = searchWordPairDStream.reduceByKeyAndWindow( new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; System.out.println("v1:" + v1 + " v2:" + v2 + " ++++++++++"); return v1 + v2; &#125; &#125;, new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; System.out.println("v1:" + v1 + " v2:" + v2 + "------------"); return v1 - v2; &#125; &#125;, Durations.seconds(15), Durations.seconds(5)); searchWordCountsDStream.print(); jssc.start(); jssc.awaitTermination(); jssc.close(); &#125;&#125; reduceByKeyAndWindow reduceByKeyAndWindow是针对窗口操作的而不是针对DStream操作的。 5. Driver HA（Standalone 或者 Mesos）代码举例： 产生文件：1234567891011121314151617181920212223242526272829303132import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.util.UUID;/** * 此复制文件的程序是模拟在data目录下动态生成相同格式的txt文件，用于给sparkstreaming 中 textFileStream提供输入流。 */public class CopyFile_data &#123;public static void main(String[] args) throws IOException, InterruptedException &#123; while(true)&#123; Thread.sleep(5000); String uuid = UUID.randomUUID().toString(); System.out.println(uuid); copyFile( new File("./data/scores.txt"),new File("./dataTest/"+uuid+"----words.txt")); &#125; &#125; public static void copyFile(File fromFile, File toFile) throws IOException &#123; FileInputStream ins = new FileInputStream(fromFile); FileOutputStream out = new FileOutputStream(toFile); byte[] b = new byte[1024*1024]; @SuppressWarnings("unused") int n = 0; while ((n = ins.read(b)) != -1) &#123; out.write(b, 0, b.length); &#125; ins.close(); out.close(); &#125;&#125; 处理数据：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import java.util.Arrays;import org.apache.spark.SparkConf;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.api.java.JavaStreamingContextFactory;import org.apache.spark.streaming.dstream.DStream;import scala.Tuple2;/** * Spark standalone or Mesos with cluster deploy mode only: * 在提交application的时候 添加 --supervise 选项 如果Driver挂掉 会自动启动一个Driver */public class SparkStreamingOnHDFS &#123; public static void main(String[] args) &#123; final SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("SparkStreamingOnHDFS"); // final String checkpointDirectory = "hdfs://shsxt/spark/SparkStreaming/CheckPoint2017"; final String checkpointDirectory = "./checkpoint"; JavaStreamingContextFactory factory = new JavaStreamingContextFactory() &#123; @Override public JavaStreamingContext create() &#123; return createContext(checkpointDirectory,conf); &#125; &#125;; //getOrCreate() 该方法会先到checkpointDirectory的文件中检查是否有checkpoint记录，如果没有就会让 factory 去调用 create() 来创建JavaStreamingContext ；如果存在就执行checkpoint的任务 JavaStreamingContext jsc = JavaStreamingContext.getOrCreate(checkpointDirectory, factory); jsc.start(); jsc.awaitTermination(); jsc.close(); &#125; @SuppressWarnings("deprecation") private static JavaStreamingContext createContext(String checkpointDirectory,SparkConf conf) &#123; // If you do not see this printed, that means the StreamingContext has been loaded// from the new checkpoint System.out.println("Creating new context"); SparkConf sparkConf = conf; // Create the context with a 1 second batch size JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(5));// ssc.sparkContext().setLogLevel("WARN"); /** * checkpoint 保存： * 1.配置信息 * 2.DStream操作逻辑 * 3.job的执行进度 * 4.offset */ ssc.checkpoint(checkpointDirectory); /** * 监控的是HDFS上的一个目录，监控文件数量的变化 文件内容如果追加监控不到。 * 只监控文件夹下新增的文件，减少的文件时监控不到的，文件的内容有改动也监控不到。 */// JavaDStream&lt;String&gt; lines = // ssc.textFileStream("hdfs://node1:9000/spark/sparkstreaming"); JavaDStream&lt;String&gt; lines = ssc.textFileStream("./dataTest"); JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(String s) &#123; return Arrays.asList(s.split(" ")); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s.trim(), 1); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;);// counts.print(); DStream&lt;Tuple2&lt;String, Integer&gt;&gt; dstream = counts.dstream(); dstream.saveAsTextFiles("./data/write/xxxxx","yyyyyy"); return ssc; &#125;&#125; 注意 因为 SparkStreaming 是 7*24 小时运行，Driver 只是一个简单的进程，有可能挂掉，所以实现 Driver 的 HA 就有必要 如果使用的 Client 模式就无法实现 Driver HA ，这里针对的是 cluster 模式 Yarn 平台 cluster 模式提交任务，AM(AplicationMaster)相当于 Driver，如果挂掉会自动启动 AM 这里所说的 DriverHA 针对的是 Spark standalone 和 Mesos 资源调度的情况下 实现 Driver 的高可用有两个步骤： 第一：提交任务层面，在提交任务的时候加上选项 –supervise,当 Driver挂掉的时候会自动重启 Driver。 第二：代码层面，使用 JavaStreamingContext.getOrCreate（checkpoint路径，JavaStreamingContextFactory） Driver 中元数据包括： 创建应用程序的配置信息。 DStream 的操作逻辑 job 中没有完成的批次数据，也就是 job 的执行进度。 6、SparkStreaming+Kafka（1）receiver 模式receiver 模式原理图 receiver 模式理解： 1.在 SparkStreaming 程序运行起来后，Executor 中会有 receivertasks 接收 kafka 推送过来的数据。数据会被持久化，默认级别为MEMORY_AND_DISK_SER_2,这个级别也可以修改。 2.receiver task对接收过来的数据进行存储和备份，这个过程会有节点之间的数据传输。 3.备份完成后去 zookeeper 中更新消费偏移量，然后向 Driver 中的 receiver tracker 汇报数据的位置。最后 Driver 根据数据本地化将 task 分发到不同节点上执行 数据本地化原则：将task分配到data所在节点 receiver 模式中存在的问题：场景一： 1、当 Driver 进程挂掉后，Driver 下的 Executor 都会被杀掉，当更新完 zookeeper 消费偏移量的时Driver 如果挂掉了，就会存在找不到数据的问题，相当于丢失数据。如何解决这个问题？ 2、开启WAL(write ahead log)预写日志机制， 在接受过来数据备份到其他节点的时候，同时备份到 HDFS 上一份（我们需要将接收来的数据的持久化级别降级到 MEMORY_AND_DISK），这样就能保证数据的安全性。 3、不过，因为写 HDFS 比较消耗性能，要在备份完数据之后才能进行更新 zookeeper 以及汇报位置等，这样会增加 job 的执行时间，这样对于任务的执行提高了延迟度 场景二： 开启了WAL机制 若数据接收完后（50~100），也将数据备份到另一节点和HDFS上，正准备更新偏移量（100）的时候，driver挂掉了，重启后，就会到HDFS上去获取未计算的数据，然后，检查偏移量（50），再根据偏移量去消费topic。这就出现了重复消费的现象 *术语解释：* SparkStreaming的receive模式能保证 at least模型，只能保证至少消费一次，不能保证仅被消费一次 exactly-once模型 能保证仅被消费一次，较理想的模型可以避免重复消费，direct模式可以实现，但是输出不能保证 receiver 的并行度设置： 1、receiver 的并行度是由 spark.streaming.blockInterval 来决定的，默认为200ms, 2、假设 batchInterval 为 5s,那么每隔 blockInterval 就会产生一个 block,这里就对应每批次产生 RDD 的 partition,这样 5 秒产生的这个 Dstream 中的这个 RDD 的 partition 为 25 个，并行度就是25。 3、如果想提高并行度，可以减少 blockInterval 的数值，但是最好不要低于 50ms。 receiver 模式代码：产生数据：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import kafka.serializer.StringEncoder;import org.apache.kafka.clients.producer.KafkaProducer;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Properties;import java.util.Random;//向kafka中生产数据public class MyProducer extends Thread &#123; // sparkstreaming storm flink 两三年后变成主流 流式处理，可能更复杂，数据处理性能要非常好 private String topic; //发送给Kafka的数据,topic private Producer&lt;Integer, String&gt; producerForKafka; public MyProducer(String topic) &#123; this.topic = topic; Properties conf = new Properties(); conf.put("metadata.broker.list", "node01:9092,node02:9092,node03:9092"); conf.put("serializer.class", StringEncoder.class.getName()); conf.put("acks",1); producerForKafka = new Producer&lt;Integer, String&gt;(new ProducerConfig(conf)); &#125; @Override public void run() &#123; int counter = 0; while (true) &#123; counter++; // String value = "shsxt" + counter; String value = "shsxt" KeyedMessage&lt;Integer, String&gt; message = new KeyedMessage&lt;&gt;(topic, value); producerForKafka.send(message); System.out.println(value + " - -- -- --- -- - -- - -");//hash partitioner 当有key时，则默认通过key 取hash后 ，对partition_number 取余数// producerForKafka.send(new KeyedMessage&lt;Integer, String&gt;(topic,22,userLog));// 每2条数据暂停2秒 if (0 == counter % 2) &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; new MyProducer("sk1").start(); new MyProducer("sk2").start(); &#125;&#125; 操作：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import java.util.Arrays;import java.util.HashMap;import java.util.Map;import kafka.serializer.StringDecoder;import org.apache.spark.SparkConf;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.storage.StorageLevel;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka.KafkaUtils;import scala.Tuple2;//receiver 模式并行度是由blockInterval决定的public class SparkStreamingOnKafkaReceiver &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf() conf.setAppName("SparkStreamingOnKafkaReceiver").setMaster("local[2]"); //开启预写日志 WAL机制 conf.set("spark.streaming.receiver.writeAheadLog.enable", "true"); JavaStreamingContext jsc = new JavaStreamingContext(conf, Durations.seconds(10)); jsc.checkpoint("./receivedata"); Map&lt;String, Integer&gt; topicConsumerConcurrency = new HashMap&lt;String, Integer&gt;(); //设置读取的topic和接受数据的线程数 topicConsumerConcurrency.put("sk1", 1); topicConsumerConcurrency.put("sk2", 1); /** * 第一个参数是StreamingContext * 第二个参数是ZooKeeper集群信息 （接受Kafka数据的时候会从Zookeeper中获得Offset等元数据信息） * 第三个参数是Consumer Group 消费者组 * 第四个参数是消费的Topic以及并发读取Topic中Partition的线程数 * * 注意： * KafkaUtils.createStream 使用五个参数的方法，设置receiver的存储级别 */// JavaPairReceiverInputDStream&lt;String,String&gt; lines = KafkaUtils.createStream(// jsc,// "node3:2181,node4:2181,node5:2181",// "MyFirstConsumerGroup", // topicConsumerConcurrency,// StorageLevel.MEMORY_AND_DISK()); JavaPairReceiverInputDStream&lt;String, String&gt; lines = KafkaUtils.createStream( jsc, "node01:2181,node02:2181,node03:2181", "MyFirstConsumerGroup", topicConsumerConcurrency); JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;Tuple2&lt;String, String&gt;, String&gt;() &#123; private static final long serialVersionUID = 1L; public Iterable&lt;String&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; return Arrays.asList(tuple._2.split("\t")); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(word, 1); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; wordsCount = pairs.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; //对相同的Key，进行Value的累计（包括Local和Reducer级别同时Reduce） private static final long serialVersionUID = 1L; public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); wordsCount.print(); jsc.start(); jsc.awaitTermination(); jsc.close(); &#125;&#125; 运行代码：Linux端 启动zookeeper：3台 1zkServer.sh start 启动kafka：3台 在kafka的解压路径下的/bin目录下 1kafka-server-start.sh ../config/server.properties 终止kafka： 1kafka-server-stop.sh （2）Driect 模式Driect 模式原理图 Direct 模式理解： SparkStreaming+kafka 的 Driect 模式就是将 kafka 看成存数据的一方，不是被动接收数据，而是主动去取数据。拉取数据后先进行计算，成功后再更新偏移量 消费者偏移量也不是用 zookeeper 来管理，而是 SparkStreaming 内部对消费者偏移量自动来维护，默认消费偏移量是在内存中，当然如果设置了checkpoint 目录，那么消费偏移量也会保存在 checkpoint 中。当然也可以实现用 zookeeper 来管理 Direct 模式并行度设置： Direct 模式的并行度是由读取的 kafka 中 topic 的 partition 数决定的。 可以在sparksteaming中使用算子改变分区数，如reartition Direct 模式代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import java.util.Arrays;import org.apache.spark.SparkConf;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.api.java.JavaStreamingContextFactory;import org.apache.spark.streaming.dstream.DStream;import scala.Tuple2;/** * * Spark standalone or Mesos with cluster deploy mode only: * 在提交application的时候 添加 --supervise 选项 如果Driver挂掉 会自动启动一个Driver * */public class SparkStreamingOnHDFS &#123; public static void main(String[] args) &#123; final SparkConf conf = new SparkConf() conf.setMaster("local[2]").setAppName("SparkStreamingOnHDFS"); // final String checkpointDirectory = "hdfs://shsxt/spark/SparkStreaming/CheckPoint2017"; final String checkpointDirectory = "./checkpoint"; JavaStreamingContextFactory factory = new JavaStreamingContextFactory() &#123; @Override public JavaStreamingContext create() &#123; return createContext(checkpointDirectory,conf); &#125; &#125;; JavaStreamingContext jsc = JavaStreamingContext.getOrCreate(checkpointDirectory, factory); jsc.start(); jsc.awaitTermination(); jsc.close(); &#125; @SuppressWarnings("deprecation") private static JavaStreamingContext createContext(String checkpointDirectory,SparkConf conf) &#123; // If you do not see this printed, that means the StreamingContext has // been loaded // from the new checkpoint System.out.println("Creating new context"); SparkConf sparkConf = conf; // Create the context with a 1 second batch size JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(5));// ssc.sparkContext().setLogLevel("WARN"); /** * checkpoint 保存： * 1.配置信息 * 2.DStream操作逻辑 * 3.job的执行进度 * 4.offset */ ssc.checkpoint(checkpointDirectory); /** * 监控的是HDFS上的一个目录，监控文件数量的变化 文件内容如果追加监控不到。 * 只监控文件夹下新增的文件，减少的文件时监控不到的，文件的内容有改动也监控不到。 */// JavaDStream&lt;String&gt; lines = ssc.textFileStream("hdfs://node1:9000/spark/sparkstreaming"); JavaDStream&lt;String&gt; lines = ssc.textFileStream("./dataTest"); JavaDStream&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(String s) &#123; return Arrays.asList(s.split(" ")); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s.trim(), 1); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;);// counts.print(); DStream&lt;Tuple2&lt;String, Integer&gt;&gt; dstream = counts.dstream(); dstream.saveAsTextFiles("./data/write/xxxxx","yyyyyy"); return ssc; &#125;&#125; 7、相关配置预写日志:（receive模式中） 1spark.streaming.receiver.writeAheadLog.enable 默认 false 没有开启 blockInterval:（receive模式中） 1spark.streaming.blockInterval 默认 200ms 反压机制: （设置自动调整每一批次的数据量的理想范围，但调整的比较慢） 1spark.streaming.backpressure.enabled 默认 false 每一批次接收数据速率:（receive模式中） 1spark.streaming.receiver.maxRate 默认没有设置 每一分区接收数据速率 :（ direct模式） 1spark.streaming.kafka.maxRatePerpartition 默认没有设置 8、如何优雅的关闭Spark Streaming作业1spark.streaming.stopGracefullyOnShutdown 设置成true 执行命令： 1kill -15/sigterm driverpid]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark框架</tag>
        <tag>SparkStreaming</tag>
        <tag>流式处理框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习（五）]]></title>
    <url>%2F2019%2F02%2F21%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Spark1、概念基于 Spark 计算框架之上且兼容 Hive 语法的 SQL 执行引擎， 2、特点 基于 Spark 的特性 由于底层的计算采用了 Spark，性能比 MapReduce 的 Hive 普遍快 2 倍以上，当数据全部 load 在内存的话，将快 10 倍以上，因此 Shark 可以作为交互式查询应用服务来使用。 基于 Hive的特性 Shark 是完全兼容 Hive的语法，表结构以及UDF函数等，已有的HiveSql可以直接进行迁移至Shark上。 Shark 底层依赖于 Hive 的解析器，查询优化器。 缺点 由于 Shark 的整体设计架构对 Hive 的依赖性太强，难以支持其长远发展，比如不能和 Spark的其他组件进行很好的集成，无法满足 Spark 的一栈式解决大数据处理的需求。 二、SparkSql1、SparkSQL介绍Hive 是 Shark 的前身，Shark 是 SparkSQL 的前身 SparkSQL 特点 其完全脱离了 Hive 的限制。 SparkSQL支持查询原生的RDD。 RDD是Spark平台的核心概念，是 Spark 能够高效的处理大数据的各种场景的基础。 能够在 Scala 中写 SQL 语句。 支持简单的 SQL 语法检查，能够在Scala中写Hive语句访问Hive数据，并将结果取回作为RDD使用。 2、Spark on Hive 和 Hive on SparkSpark on Hive： Hive 只作为储存角色，Spark 负责 sql 解析优化，执行。 数据源在hive上，解析引擎是sparksql，执行任务是spark Hive on Spark： Hive 即作为存储又负责 sql 的解析优化，Spark 负责执行。 数据源在hive上，解析引擎是hive，执行任务是spark 3、DataFrame 分布式数据容器 DataFrame 的底层封装的是 RDD，只不过 RDD 的泛型是 Row 类型。 相当于RDD+schema （数据+数据的结构信息） 与 Hive 类似，DataFrame 也支持嵌套数据类型（struct、array 和 map） 从 API 易用性的角度上 看， DataFrame API提供的是一套高层的关系操作，比函数式的 RDD API 要更加友好，门槛更低。 4、SparkSql 的数据源 JSON 类型的字符串，JDBC、Parquent、Hive，HBASE、HDFS 5、SparkSql底层架构sql——&gt;逻辑计划——&gt;优化逻辑计划——&gt;物理计划——&gt;RDD（Spark任务） 6、谓词下推sql: 1select table1.name,table2.score from table1 join table2 on table1.id=table2.id where table1.age &gt; 50 and table2.score &gt; 90 执行顺序 join:t1,t2过滤：where : t1.age&gt;50,t2.score&gt;90列裁剪：from: select: 谓词下推先各自过滤：where然后列裁剪：t1:name,id ; t2:score,idjoin 谓词下推 三、创建DataFrame的几种方式1、读取Json格式文件创建DataFrame注意： 1、json文件中不能嵌套json格式的内容 2、读取json文件格式的两种方式： 3、dataFrame.show( )默认显示前20行数据，使用dataFrame.show(行数）可显示指定行数的数据 4、将DataFrame转换成RDD： ​ Java: df.javaRDD( ) ​ Scala: df.rdd 5、显示DataFrame的Schema信息（树形的形式）：df.printSchema( ) 6、dataFrame自带API操作dataFrame ,不常用 7、使用sql查询： ​ a，将DataFrame注册临时表： df.registerTemptable(“mytable”) ​ b，使用sql： sqlContext.sql(“sql语句”) 8、df中的数据加载过之后，显示时，会默认将列按ASCII码进行排序 Java： 123456789101112131415161718192021222324252627282930313233343536373839SparkConf conf = new SparkConf();conf.setMaster("local").setAppName("jsonfile");SparkContext context = new SparkContext(conf);//创建SQLContext（实现了序列化）SQLContext sqlContext = new SQLContext(context);//文件格式：&#123;"name":"zhangsan","age": 18&#125;//读取json文件的两种方式,得到DataFrame（底层是RDD）DataFrame df = sqlContext.read().format("json").load("./data/jsonfile");//DataFrame df = sqlContext.read().json("./data/jsonfile");//显示df中的内容的两种情况（以二维表显示，空值用null代替，列自动按ASCII码排序）df.show();df.show(100);//df转换成RDD//RDD&lt;ROW&gt; rdd = df.rdd()JavaRDD&lt;Row&gt; javaRDD = df.javaRDD();//显示数据结构信息df.printSchema();//自带操作DataFrame的API//select name from tabledf.select("name").show();//select name ,age+10 as addage from tabledf.select(df.col("name"),df.col("age").plus(10).alias("addage")).show();//select name ,age from table where age&gt;19df.select(df.col("name"),df.col("age")).where(df.col("age").gt(19)).show();//select age,count(*) from table group by agedf.groupBy(df.col("age")).count().show(); //使用SQL查询//将DataFrame注册成临时的一张表，这张表相当于临时注册到内存中，是逻辑上的表，不会雾化到磁盘df.registerTempTable("table");DataFrame sqlDF = sqlContext.sql("sekect * from table where name like 'zhang&amp;'");sqlDF.show();context.stop() Scala: 123456789101112131415161718192021222324252627val conf = new SparkConf()conf.setMaster("local").setAppName("json")val context = new SparkContext(conf)val sqlContext = new SQlContext(context)//读取json文件val df = sqlContext.read.json("./data/jsonfile")//val df = sqlContext.read.format("json").load("./data/jsonfile)//将df转化成RDD//val rdd = df.rdddf.show()de.printSchema()//select * from tabledf.select(df.col("name")).show()//select name from table where age&gt;19df.select(df.col("name"),df.col("age")).where(df.col("age").gt(19)).show()//select count(*) from table group by agedf.groupBy(df.col("age")).count().show();//使用sql //注册临时表df.registerTempTable("table")val result = sqlContext.sql("select * from table")result.show()context.stop() 2、通过Json格式的RDD创建DataFrameJava 123456789101112131415161718192021222324252627282930SparkConf conf = new SparkConf();conf.setMaster("local").setAppName("jsonRdd");JavaSparkContext context = new JavaSparkContext(conf);SQLContext sqlContext = new SQLContext(context);//创建RDDJavaRDD&lt;String&gt; nameRDD = context.parallelize(Array.asList("&#123;'name','zs','age','18'&#125;","&#123;\"name\",\"ls\",\"age\",\"21\"&#125;"));JavaRDD&lt;String&gt; scoreRDD = context.parallelize(Array.asList("&#123;'name':'zs','score':'90'&#125;","&#123;\"name\":\"ls\",\"score\":\"88\"&#125;"));//将jsonRDD转换成DataFrameDataFrame namedf = sqlContext.read().json(nameRDD);DataFrame scoredf = sqlContext.read().json(scoreRDD);//为df注册临时表namedf.registerTempTable("nameTable");scoredf.registerTempTable("scoreTable");//使用sql查询DataFrame df = sqlContext.sql("select nameTable.name,nameTable.age,"+ "scoretable.score from nameTable join scoreTabel"+ "on nameTable.name = scoreTable.name ");df.show();context.stop(); Scala 12345678910111213141516171819202122232425262728val conf = new SparkConf()conf.setMaster("local").setAppName("jsonRdd")val context = new SparkContext(conf)val sqlContext = new SQLContext(context)//创建RDDval nameRDD = context.makeRDD( "&#123;'name','zs','age','18'&#125;","&#123;\"name\",\"ls\",\"age\",\"21\"&#125;")val scoreRDD = context.makeRDD( "&#123;'name':'zs','score':'90'&#125;","&#123;\"name\":\"ls\",\"score\":\"88\"&#125;")//获取dataFrameval namedf = sqlContext.read.json(nameRDD)val scoredf = sqlContext.read.json(scoreRDD)//为DataFrame指定临时表namedf.registerTempTable("nameTable")scoredf.registerTempTable("scoreTable")//使用sqlval df = sqlContext.sql("select nameTable.name,nameTable.age,"+ "scoretable.score from nameTable join scoreTabel"+ "on nameTable.name = scoreTable.name ")df.show()context.stop() 3、非Json格式的文件创建DataFrame1）通过反射的方式将非json格式的RDD转换成DataFrame（不推荐） 自定义类要实现序列化 自定义类的访问级别是public RDD转换成DataFrame后会根据映射按ASCII码排序 将DataFrame转换成RDD时，获取字段的范式有两种： 1）row.getInt(0）；df.getString(1) 通过下标获取，返回Row类型的数据，注意列顺序问题（不推荐） 2）row.getAs(“列名”) 通过列名获取对应列值（推荐） Java: 12345678910111213141516171819202122232425262728293031package com.bd.java.sql.dataframe;import java.io.Serializable;public class Person implements Serializable&#123; private static final long serialVersionUID = 1L; private String id ; private String name; private Integer age; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "Person [id=" + id + ", name=" + name + ", age=" + age + "]"; &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960SparkConf conf = new SparkConf();conf.setMaster("local").setAppName("RDD");JavaSparkContext context = new JavaSparkContext(conf);SQLContext sqlContext = new SQLContext(sc);//获取RDD（文件格式：1,zhangsan,18）JavaRDD&lt;String&gt; lineRDD = context.textFile("./data/person");//反射JavaRDD&lt;Person&gt; personRDD = lineRDD.map(new Funcation&lt;String,Person&gt;()&#123; private static final long serialVersionUID = 1L; @Override public Person call(String str) throws Exception &#123; Person person = new Person(); person.setId(str.split(",")[0]); person.setName(str.split(",")[1]); person.setAge(Integer.valueOf(str.split(",")[2])); return person; &#125; &#125;);/*传入Person.class后，sqlContext就通过反射的方式创建DataFrame因为在底层通过反射的方式可以获得Person类的所有field，再结合RDD，即可创建DataFrame*///将RDD转换成DataFramDataFrame df = sqlContext.(personRDD,Person.class);df.show();df.printSchema()df.registerTempTable("table");DataFrame sqldf = sqlContext.sql("select * from table");sqldf.show() //将DataFrame转换成RDD（两种方式）//因为排序的原因：df中列的顺序变为：age ， id ， nameJavaRDD&lt;Row&gt; javaRDD = df.javaRDD();JavaRDD&lt;Person&gt; map = javaRDD.map(new Function(Row,Person)&#123; private static final long serialVersionUID = 1L; @Override public Person call(Row row) throws Exception &#123; Person p = new Person();// p.setId(row.getString(1));// p.setName(row.getString(2));// p.setAge(row.getInt(0)); p.setId((String)row.getAs("id")); p.setName((String)row.getAs("name")); p.setAge((Integer)row.getAs("age")); return p; &#125; &#125;);map.foreach(new VoidFunction&lt;Person&gt;()&#123; private static final long serialVersionUID = 1L; @Override public void call(Person person) throws Exception &#123; System.out.println(person); &#125; &#125;);context.stop(); Scala 12345678910111213141516171819202122 val conf = new SparkConf() conf.setMaster("local").setAppName("rddreflect") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val lineRDD = sc.textFile("./data/person")//文件格式：1,zhangsan,18//将RDD转换成DataFrameval personRDD = linRDD.map&#123;x=&gt;&#123; val person = Person(x.split(",")(0),x.split(",")(1),Intger.valueOf(x.split(",")(2)) person&#125;&#125;//将personRDD转化成DataFrame val df = personRDD.toDF() df.show() //将DataFrame转换成RDDval rdd = df.rddval personRDD = rdd.map&#123;x=&gt;&#123; Person(x.getAs("id"),x.getAs("name"),x.getAs("age")) &#125;&#125; personRDD.foreach(println)context.stop() 2）动态创建Schema，将非json格式RDD转成DataFrameJava 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748SparkConf conf = new SparkConf();conf.setMaster("local").setAppName("rddStruct");JavaSparkContext sc = new JavaSparkContext(conf);SQLContext sqlContext = new SQLContext(sc);JavaRDD&lt;String&gt; lineRDD = sc.textFile("./data/person");//文件格式：1,zhangsan,18//将RDD转换成DataFrame//将RDD转成Row类型的RDDfinal JavaRDD&lt;Row&gt; rowRDD = lineRDD.map(new Function&lt;String,Row&gt;()&#123; private static final long serialVersionUID = 1L; @Override public Row call(String s) throws Exception &#123; val row = RowFactory.create( s.split(",")[0], s.split(",")[1], Integer.valueOf(s.split(",")[2]) ); return row; &#125; &#125;);//动态创建DataFrame的的元数据（Schema），字段的来源：字符串或外部数据库List&lt;StructField&gt; asList = Arrays.asList( DataTypes.createStructField("id",DataTypes.StringType,true), DataTypes.createStructField("name",DataTypes.StringType,true)， DataTypes.createStructField（"age",DataTypes.IntegerType,true));StructType schema = DataTypes.createStructType(asList);//获取DataFrameDataFrame df = sqlContext.createDataFrame(rowRDD,schema);df.printSchema();df.show();//将dataframe转换成RDD//JavaRDD&lt;Row&gt; javaRDD = df.javaRDD();// javaRDD.foreach(new VoidFunction&lt;Row&gt;() &#123;// private static final long serialVersionUID = 1L;// @Override// public void call(Row row) throws Exception &#123;// System.out.println(row.getString(0));//// System.out.println(row);// &#125;// &#125;);context.stop(); Scala 123456789101112131415161718192021val conf = new SparkConf()conf.setMaster("local").setAppName("rddStruct")val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc)val lineRDD = sc.textFile("./data/person")//文件格式：1,zhangsan,18//将RDD转换成RowRDDval rowRDD = lineRDD.map&#123;x=&gt;&#123; val split = x.split(",") RowFactory.create(split(0),split(1),Integer.valueOf(split(2))&#125;&#125;//获取schemaval schema = StructType(List(StructField("id",StringType，true),StructField("name",StringType,true),StructField("age",IntegerType,true)))val df = sqlContext.createDataFrame(rowRDD,shema)df.show()df.printSchema() context.stop() 4、读取parquet文件创建DataFrame注意： 可以将 DataFrame 存储成 parquet 文件。保存成 parquet 文件的方式有两种 12df.write().mode(SaveMode.Overwrite)format(&quot;parquet&quot;).save(&quot;./sparksql/parquet&quot;);df.write().mode(SaveMode.Overwrite).parquet(&quot;./sparksql/parquet&quot;); SaveMode 指定文件保存时的模式。 Overwrite：覆盖Append：追加ErrorIfExists：如果存在就报错Ignore：如果存在就忽略 Java 12345678910111213141516171819202122232425262728SparkConf conf = new SparkConf();conf.setMaster("local").setAppName("parquet");JavaSparkContext sc = new JavaSparkContext(conf);SQLContext sqlContext = new SQLContext(sc);JavaRDD&lt;String&gt; jsonRDD = sc.textFile("./data/json");//读取json格式的文件DataFrame df = sqlContext.read().json(jsonRDD);//sqlContext.read().format("json").load("./spark/json");df.show(); /** * 将DataFrame保存成parquet文件， * SaveMode指定存储文件时的保存模式: * Overwrite：覆盖 * Append:追加 * ErrorIfExists:如果存在就报错 * Ignore:如果存在就忽略 */// 保存成parquet文件有以下两种方式：df.write().mode(SaveMode.Overwrite).parquet("./sparksql/parquet");//df.write().mode(SaveMode.Overwrite).format("parquet").save("data/parquet"); /** * 加载parquet文件成DataFrame * 加载parquet文件有以下两种方式： */ load = sqlContext.read().parquet("data/parquet");// DataFrame load = sqlContext.read().format("parquet").load("data/parquet");load.show();sc.stop(); Scala 12345678910111213141516171819val conf = new SparkConf()conf.setMaster("local").setAppName("parquet")val sc = new SparkContext(conf)val sqlContext = new SQLContext(sc)val jsonRDD = sc.textFile("data/json")val df = sqlContext.read.json(jsonRDD)df.show()/** * 将DF保存为parquet文件 */df.write.mode(SaveMode.Overwrite).format("parquet").save("data/parquet")df.write.mode(SaveMode.Overwrite).parquet("data/parquet")/** * 读取parquet文件 */var result = sqlContext.read.parquet("data/parquet")result = sqlContext.read.format("parquet").load("data/parquet")result.show()sc.stop() 5、读取JDBC中的数据创建DataFrame（MySQL为例）两种方式创建 DataFrame Java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("mysql"); /** * 配置join或者聚合操作shuffle数据时分区的数量 */ conf.set("spark.sql.shuffle.partitions", "1"); JavaSparkContext sc = new JavaSparkContext(conf); SQLContext sqlContext = new SQLContext(sc); /** * 第一种方式读取MySql数据库表，加载为DataFrame */ Map&lt;String, String&gt; options = new HashMap&lt;String, String&gt;(); options.put("url", "jdbc:mysql://127.0.0.1:3306/spark"); options.put("driver", "com.mysql.jdbc.Driver"); options.put("user", "root"); options.put("password", "root"); options.put("dbtable", "person"); DataFrame person = sqlContext.read().format("jdbc").options(options).load(); person.show(); person.registerTempTable("person"); /** * 第二种方式读取MySql数据表加载为DataFrame */ DataFrameReader reader = sqlContext.read().format("jdbc"); reader.option("url", "jdbc:mysql://127.0.0.1:3306/spark"); reader.option("driver", "com.mysql.jdbc.Driver"); reader.option("user", "root"); reader.option("password", "root"); reader.option("dbtable", "score"); DataFrame score = reader.load(); score.show(); score.registerTempTable("score"); DataFrame result = sqlContext.sql("select person.id,person.name,person.age,score.score " + "from person,score " + "where person.name = score.name and score.score&gt; 90"); result.show(); result.registerTempTable("result");DataFrame df = sqlContext.sql("select id,name,age,score from result where ag&gt;18"); df.show(); /** * 将DataFrame结果保存到Mysql中 */ Properties properties = new Properties(); properties.setProperty("user", "root"); properties.setProperty("password", "root"); /** * SaveMode: * Overwrite：覆盖 * Append:追加 * ErrorIfExists:如果存在就报错 * Ignore:如果存在就忽略 * */ result.write().mode(SaveMode.Append).jdbc("jdbc:mysql://127.0.0.1:3306/spark", "result2", properties); System.out.println("----Finish----"); sc.stop(); Scala 123456789101112131415161718192021222324252627282930313233343536373839val conf = new SparkConf() conf.setMaster("local").setAppName("mysql") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) /** * 第一种方式读取Mysql数据库表创建DF */ val options = new HashMap[String,String](); options.put("url", "jdbc:mysql://192.168.100.111:3306/spark") options.put("driver","com.mysql.jdbc.Driver") options.put("user","root") options.put("password", "1234") options.put("dbtable","person") val person = sqlContext.read.format("jdbc").options(options).load() person.show() person.registerTempTable("person") /** * 第二种方式读取Mysql数据库表创建DF */ val reader = sqlContext.read.format("jdbc") reader.option("url", "jdbc:mysql://192.168.100.111:3306/spark") reader.option("driver","com.mysql.jdbc.Driver") reader.option("user","root") reader.option("password","1234") reader.option("dbtable", "score") val score = reader.load() score.show() score.registerTempTable("score") val result = sqlContext.sql("select person.id,person.name,score.score from person,score where person.name = score.name") result.show() /** * 将数据写入到Mysql表中 */ val properties = new Properties() properties.setProperty("user", "root") properties.setProperty("password", "1234") result.write.mode(SaveMode.Append). jdbc("jdbc:mysql://192.168.100.111:3306/spark", "result", properties) sc.stop() 6、读取Hive中的数据加载成DataFrame HiveContext 是 SQLContext 的子类，连接 Hive 建议使用HiveContext 由于本地没有 Hive 环境，要提交到集群运行，提交命令： 12345678&gt; ./spark-submit&gt; --master spark://node00:7077,node01:7077&gt; --executor-cores 1&gt; --executor-memory 1G&gt; --total-executor-cores 1&gt; --class com.bd.sparksql.dataframe.CreateDFFromHive&gt; /usr/soft/spark-test.jar&gt; 代码详情Java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950SparkConf conf = new SparkConf();conf.setMaster("local").setAppName("hive");JavaSparkContext sc = new JavaSparkContext(conf);//HiveContext是SQLContext的子类。（2.0之后就将两个类就合成一个类了）HiveContext hiveContext = new HiveContext(sc);//用于操作Hive上的数据//创建实例库hiveContext.sql("CREATE database spark");//切换实例库hiveContext.sql("USE spark");//删除已存在的表hiveContext.sql("DROP TABLE IF EXISTS student_infos");//在hive中创建student_infos表hiveContext.sql("CREATE TABLE IF NOT EXISTS student_infos (name STRING,age INT) row format delimited fields terminated by '\t' ");//从本地加载数据到表中hiveContext.sql("load data local inpath '/root/student_infos' into table student_infos"); hiveContext.sql("DROP TABLE IF EXISTS student_scores"); hiveContext.sql("CREATE TABLE IF NOT EXISTS student_scores (name STRING, score INT) row format delimited fields terminated by '\t'"); hiveContext.sql("LOAD DATA " + "LOCAL INPATH '/root/student_scores'" + "INTO TABLE student_scores"); /** * 查询表生成DataFrame */// DataFrame df = hiveContext.table("student_infos");//第二种读取Hive表加载DF方式DataFrame goodStudentsDF = hiveContext.sql("SELECT si.name, si.age, ss.score " + "FROM student_infos si " + "JOIN student_scores ss " + "ON si.name=ss.name " + "WHERE ss.score&gt;=80");//将df注册成临时表，才能使用sql goodStudentsDF.registerTempTable("goodstudent"); DataFrame result = hiveContext.sql("select * from goodstudent"); result.show(); /** * 将结果保存到hive表 good_student_infos */ hiveContext.sql("DROP TABLE IF EXISTS good_student_infos"); goodStudentsDF.write().mode(SaveMode.Overwrite).saveAsTable("good_student_infos"); DataFrame table = hiveContext.table("good_student_infos"); Row[] goodStudentRows = table.collect(); for(Row goodStudentRow : goodStudentRows) &#123; System.out.println(go odStudentRow); &#125; sc.stop(); Scala12345678910111213141516171819202122232425val conf = new SparkConf() conf.setAppName("HiveSource") val sc = new SparkContext(conf) /** * HiveContext是SQLContext的子类。 */ val hiveContext = new HiveContext(sc) hiveContext.sql("use spark") hiveContext.sql("drop table if exists student_infos") hiveContext.sql("create table if not exists student_infos (name string,age int) row format delimited fields terminated by '\t'") hiveContext.sql("load data local inpath '/root/test/student_infos' into table student_infos") hiveContext.sql("drop table if exists student_scores") hiveContext.sql("create table if not exists student_scores (name string,score int) row format delimited fields terminated by '\t'") hiveContext.sql("load data local inpath '/root/test/student_scores' into table student_scores") val df = hiveContext.sql("select si.name,si.age,ss.score from student_infos si,student_scores ss where si.name = ss.name") hiveContext.sql("drop table if exists good_student_infos") /** * 将结果写入到hive表中 */ df.write.mode(SaveMode.Overwrite).saveAsTable("good_student_infos") sc.stop() 关于序列化你要知道的！！四、Spark On Hive 的配置Hive配置：（在Linux端）（1）在Spark客户端配置Spark On Hive在Spark客户端安装包下spark-1.6.0/conf路径下创建hive-site.xml： 编辑内容：配置hive的metastore路径（即hive服务端的IP） 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://192.168.198.131:9083&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; （2）启动 zookeeper 集群，启动 HDFS 集群。12zkServer.sh start (3台)start-all.sh (任一台) 注意： 由于我们这里是使用Spark作为计算框架 所以不需要启动yarn 启动yarn是在使用MapReduce作为计算框架时 （3）启动spark服务（在spark解压目录的/sbin路径下）1./start-all.sh （4）启动mysql服务(mysql :node00 hive ：服务端：node02 ； 客户端 ： node01) 检查mysql服务是否启动： 命令： 12&gt; chkconfig&gt; 显示： mysqld 0:off 1:off 2:off 3:off 4:off 5:off 6:off 没有启动 启动mysql服务 命令： 123&gt; [root@node00 conf]# service mysqld start&gt; Starting mysqld: [ OK ]&gt; 登录mysql 1234&gt; [root@node00 conf]# mysql -u root -p&gt; Enter password: 123456&gt; mysql&gt; &gt; （5）启动 Hive服务端启动 Hive 的 metastore 服务 1234#后台启动hive服务端hive --service metastore &amp;#启动打印服务日志Start Hive MetaStore Server （6）打开hive交互式页面(在任一台)1hive 创建数据库spark 1hive&gt; create database spark; （7）启动 SparkShell读取 Hive 中的表总数，对比 hive 中查询同一表查询总数测试时间。 12345678910./spark-shell--master spark://node00:7077,node01:7077--executor-cores 1--executor-memory 1g--total-executor-cores 1import org.apache.spark.sql.hive.HiveContextval hc = new HiveContext(sc)hc.sql("show databases").showhc.sql("user default").showhc.sql("select count(*) from jizhan").show 注意 如果使用 Spark on Hive 查询数据时，出现错误： 12&gt; Cause by: java.net.UknownHostException： XXX&gt; 找不到 HDFS 集群路径，要在客户端机器 conf/spark-env.sh 中设置HDFS 的 路 径 ： 12&gt; HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop&gt; spark On hive：（在windows端）1、配置文件：在项目中新建文件夹conf（标记为资源文件）： 添加一下三个配置文件:（其中hive-site.xml文件用于连接hive 服务端， 其余两个文件用于连接hdfs） hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;Sunrise&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.Sunrise&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node00:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node00:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/var/hadoop/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.fsdataset.volume.choosing.policy&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node00:8485;node01:8485;node02:8485/shsxt&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/var/jn&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.shsxt&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.balance.bandwidthPerSec&lt;/name&gt; &lt;value&gt;10485760&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.socket.timeout&lt;/name&gt;&lt;value&gt;900000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.handler.count&lt;/name&gt;&lt;value&gt;20&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;&lt;value&gt;30&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt;&lt;value&gt;1800000&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;9 core-site.xml 12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. See accompanying LICENSE file.--&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Sunrise&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node00:2181,node01:2181,node02:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/var/hadoop&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hive-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://192.168.198.131:9083&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2、本地运行注意bug 若需要将上面类打包到Linux系统上运行时，代码中conf.setMaster(“local”）中setMaster(“local”)就不需要了 否则会报错： xxxxxx OOM(内存溢出) Edit Configurations —&gt;添加VM options的配置 1-Xms800m -Xmx800m -XX:PermSize=64M -XX:MaxNewSize=256m -XX:MaxPermSize=128m java.io.IOException: Failed to delete: C:\Users\SunRise\AppData\Local\Temp\spark-64f2b5a7-f8b8-4da4-b1af-137bb278e3a4 临时目录 删除失败，不影响程序的正常运行 org.apache.hadoop.hive.ql.metadata.HiveException: copyFiles: error while checking/creating destination directory!! 数据加载失败，远程连接拒绝：因为我把core-site.xml 、 hdfs-site.xml 这两个资源文件删除了。 配置这两个作为资源文件时，注意在使用textFile( )时要取消，因为要避免从hdfs上拿文件 3、打包在Linux上运行 项目打包 1、点击Project Structure—&gt;Artifacts—&gt; ‘+’—&gt;JAR—&gt;如图：所使用的的Spark包就不用打进去了，因为Linux中也有。 2、点击Build—&gt;Build Project ,之后就会在指定路径下生成对应的jar包 3、将生成的jar包放在Linux系统上对应的Spark客户端节点上 注意bug 如果打包项目的时候，没有将hive-site.xml文件打包进去，运行时，会报错，说数据库不存在 解决方法：将它打包进去，或者将该文件放在spark解压目录的conf路径下 启动spark， 1./start-all.sh 启动提交前提： zookeeper集群启动 hdfs集群启动 hive服务端启动 spark集群启动 启动提交（在node00上，保证要有，两个文件，+ 运行jar包） 1./spark-submit --master spark://node00:7077 --class com.bd.spark.java.sparkstream.CreateDFFromHive /usr/soft/spark-test.jar 运行结果： 五、悬而未决1、关于序列化的问题你要知道的12345678910111213测试java中以下几种情况下不被序列化的问题：1.反序列化时serializable 版本号不一致时会导致不能反序列化。2.子类中实现了serializable接口，父类中没有实现，父类中的变量不能被序列化,序列化后父类中的变量会得到null。注意：父类实现serializable接口,子类没有实现serializable接口时，子类可以正常序列化3.被关键字transient修饰的变量不能被序列化。4.静态变量不能被序列化，属于类，不属于方法和对象，所以不能被序列化。 2、储存 DataFrame 将 DataFrame 存储为 parquet 文件。 将 DataFrame 存储到 JDBC 数据库。 将 DataFrame 存储到 Hive 表。 六、自定义函数UDF和UDAF1、UDF:用户自定义函数Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445 SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("udf"); JavaSparkContext context = new JavaSparkContext(conf); SQLContext sqlContext = new SQLContext(context);JavaRDD&lt;String&gt; paraRDD = context.parallelize(Arrays.asList("zs1","ls12","ww123")); //rowRDD JavaRDD&lt;Row&gt; rowRDD = paraRDD.map(new Function&lt;String, Row&gt;() &#123; @Override public Row call(String v1) throws Exception &#123; return RowFactory.create(v1); &#125; &#125;); //schema List&lt;StructField&gt; list = new ArrayList&lt;StructField&gt;(); list.add(DataTypes.createStructField("name",DataTypes.StringType,true)); StructType schema = DataTypes.createStructType(list); //DataFrame DataFrame df = sqlContext.createDataFrame(rowRDD,schema); df.registerTempTable("names"); //udf sqlContext.udf().register("StringLen",new UDF1&lt;String,Integer&gt;()&#123; @Override public Integer call(String s) throws Exception &#123; return s.length(); &#125; &#125;,DataTypes.IntegerType); //udf2 sqlContext.udf().register("StringLens", new UDF2&lt;String, Integer, Integer&gt;() &#123; @Override public Integer call(String s, Integer s2) throws Exception &#123; System.out.println(s2.toString()); return s.length()+s2; &#125; &#125;,DataTypes.IntegerType); //使用sql sqlContext.sql("select name ,StringLens(name,100) as length from names").show(); context.stop(); Scala 123456789101112131415161718192021222324252627282930 val conf = new SparkConf() conf.setMaster("local").setAppName("udf") val context = new SparkContext(conf) val sqlContext = new SQLContext(context) val rdd = context.makeRDD(Array("zs1","ls12","ww123"))val rowRDD = rdd.map(x=&gt;&#123; RowFactory.create(x)&#125;) val field = Array(DataTypes.createStructField("name",DataTypes.StringType,true)) val schema = DataTypes.createStructType(field) val df = sqlContext.createDataFrame(rowRDD,schema) df.registerTempTable("names") sqlContext.udf.register("StringLen",(x:String)=&gt;&#123; x.length &#125;) sqlContext.udf.register("StringLens",(x:String,y:Integer)=&gt;&#123; x.length+y &#125;)sqlContext.sql("select name , StringLen(name) from names").show() sqlContext.sql("select name,StringLens(name,100)from names").show() context.stop() 2、UDAF:用户自定义聚合函数 实现 UDAF 函数如果要自定义类要实现UserDefinedAgg regateFunction 类 功能：实现统计相同值得个数 数据： * zhangsan * zhangsan * lisi * lisi * wangwu * wangwu * zhangsan * * select count(*) from user group by name Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114 SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("udaf"); JavaSparkContext context = new JavaSparkContext(conf); SQLContext sqlContext = new SQLContext(context);//指定了两个分区 JavaRDD&lt;String&gt; rdd = context.parallelize( Arrays.asList("zhangsan", "lisi", "wangwu", "zhangsan", "zhangsan","lisi", "zhangsan", "lisi", "wangwu", "zhangsan", "zhangsan", "lisi"), 2); JavaRDD&lt;Row&gt; rowRDD = rdd.map(new Function&lt;String, Row&gt;() &#123; @Override public Row call(String v1) throws Exception &#123; return RowFactory.create(v1); &#125; &#125;); List&lt;StructField&gt; field = new ArrayList&lt;&gt;(); field.add(DataTypes.createStructField("name",DataTypes.StringType,true)); StructType schema = DataTypes.createStructType(field); DataFrame df = sqlContext.createDataFrame(rowRDD, schema); df.registerTempTable("names"); sqlContext.udf().register("CountString", new UserDefinedAggregateFunction() &#123; //select name ,StringCount(name) as number from user group by name //初始化一个内部的自己定义的值,在Aggregate之前每组数据的初始化结果 @Override public void initialize(MutableAggregationBuffer buffer) &#123; //初始化buffer第0位置的元素为0 buffer.update(0,0); System.out.println("buffer initialize ----"+buffer.get(0)); &#125; /** * 更新 可以认为一个一个地将组内的字段值传递进来 实现拼接的逻辑 * buffer.getInt(0)获取的是上一次聚合后的值 * 相当于map端的combiner，combiner就是对每一个map task的处理结果进行一次小聚合 * 大聚和发生在reduce端. * 这里即是:在进行聚合的时候，每当有新的值进来，对分组后的聚合如何进行计算 */ //相当于分区内 //buffer1:表示上一次的累加值 buffer2:本次传进来的值 //将函数输入的参数理解为一行（Row） @Override public void update(MutableAggregationBuffer buffer, Row arg1) &#123;System.out.println("class buffer :"+buffer.getClass()+"-------"+buffer.hashCode());System.out.println("class arg1:"+arg1.getClass()+"-------"+arg1.hashCode()); buffer.update(0,buffer.getInt(0)+1);System.out.println("update----buffer:"+buffer.toString()+",arg1:"+arg1.toString()); &#125; /** * 合并 update操作， 可能是针对一个分组内的部分数据，在某个节点上发生的 但是可能一个分组内的数据，会分布在多个节点上处理 * 此时就要用merge操作，将各个节点上分布式拼接好的串，合并起来 * buffer1.getInt(0) : 大聚合的时候 上一次聚合后的值 * buffer2.getInt(0) : 本次计算传入进来的update的结果 * 这里即是：最后在分布式节点完成后需要进行全局级别的Merge操作 */ //相当于分区之间 @Override public void merge(MutableAggregationBuffer buffer1, Row buffer2) &#123;System.out.println("class buffer1 :"+buffer1.getClass()+"----"+buffer1.hashCode());System.out.println("class buffer2 :"+buffer2.getClass()+"----"+buffer2.hashCode()); buffer1.update(0,buffer1.getInt(0)+buffer2.getInt(0)); System.out.println("merge：b1:"+buffer1.toString()+",buffer2:"+buffer2.toString()); &#125; //指定输入字段的字段及类型 @Override public StructType inputSchema() &#123; return DataTypes.createStructType( Arrays.asList(DataTypes.createStructField("name",DataTypes.StringType,true)) ); &#125; // 在进行聚合操作的时候所要处理的数据的结果的类型 @Override public StructType bufferSchema() &#123; return DataTypes.createStructType( Arrays.asList(DataTypes.createStructField("buffer",DataTypes.IntegerType,true))); &#125; //指定UDAF函数计算后返回的结果类型 @Override public DataType dataType() &#123; return DataTypes.IntegerType; &#125; //最后返回一个和DataType的类型要一致的类型，返回UDAF最后的计算结果 @Override public Object evaluate(Row buffer) &#123; return buffer.getInt(0); &#125; //确保一致性 一般用true,用以标记针对给定的一组输入，UDAF是否总是生成相同的结果。 @Override public boolean deterministic() &#123; return true; &#125; &#125;); sqlContext.sql("select name , CountString(name) from names").show(); context.stop(); Scala 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.Rowimport org.apache.spark.sql.RowFactoryimport org.apache.spark.sql.SQLContextimport org.apache.spark.sql.expressions.MutableAggregationBufferimport org.apache.spark.sql.expressions.UserDefinedAggregateFunctionimport org.apache.spark.sql.types.DataTypeimport org.apache.spark.sql.types.DataTypesimport org.apache.spark.sql.types.IntegerTypeimport org.apache.spark.sql.types.StringTypeimport org.apache.spark.sql.types.StructTypeclass MyUDAF extends UserDefinedAggregateFunction &#123; // 为每个分组的数据执行初始化值 def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0 &#125; // 每个组，有新的值进来的时候，进行分组对应的聚合值的计算 def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getAs[Int](0)+1 &#125; // 最后merger的时候，在各个节点上的聚合值，要进行merge，也就是合并 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getAs[Int](0)+buffer2.getAs[Int](0) &#125; //输入数据的类型 def inputSchema: StructType = &#123; DataTypes.createStructType( Array(DataTypes.createStructField("input", StringType, true))) &#125; // 聚合操作时，所处理的数据的类型 def bufferSchema: StructType = &#123; DataTypes.createStructType( Array(DataTypes.createStructField("aaa", IntegerType, true))) &#125; // 最终函数返回值的类型 def dataType: DataType = &#123; DataTypes.IntegerType &#125; // 最后返回一个最终的聚合值 要和dataType的类型一一对应 def evaluate(buffer: Row): Any = &#123; buffer.getAs[Int](0) &#125; //保证数据一致性 def deterministic: Boolean = &#123; true &#125;&#125;object UDAF &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("udaf") val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val rdd = sc.makeRDD(Array("zhangsan","lisi","wangwu","zhangsan","lisi")) val rowRDD = rdd.map &#123; x =&gt; &#123;RowFactory.create(x)&#125; &#125; val schema =DataTypes.createStructType( Array(DataTypes.createStructField("name", StringType, true))) val df = sqlContext.createDataFrame(rowRDD, schema) df.show() df.registerTempTable("user") /** * 注册一个udaf函数 */ sqlContext.udf.register("StringCount", new MyUDAF()) sqlContext.sql("select name ,StringCount(name) from user group by name").show() sc.stop() &#125;&#125; 七、开窗函数注意： row_number() 开窗函数是按照某个字段分组，然后取另一字段的前几个的值，相当于 分组取 topN 如果 SQL 语句里面使用到了开窗函数，那么这个 SQL 语句必须使用HiveContext 来执行，HiveContext 默认情况下在本地无法创建。 开窗函数格式： 1row_number() over (partitin by XXX order by XXX) Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.sql.DataFrame;import org.apache.spark.sql.SaveMode;import org.apache.spark.sql.hive.HiveContext;/** * row_number()开窗函数： * 主要是按照某个字段分组，然后取另一字段的前几个的值，相当于 分组取topN group by .... order by .... limit 0, 5 ; * row_number() over (partition by xxx order by xxx desc) xxx * 注意： * 如果SQL语句里面使用到了开窗函数，那么这个SQL语句必须使用HiveContext来执行 * @author root * */public class RowNumberWindowFun &#123; //-Xms800m -Xmx800m -XX:PermSize=64M -XX:MaxNewSize=256m -XX:MaxPermSize=128m public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setAppName("windowfun").setMaster("local"); JavaSparkContext sc = new JavaSparkContext(conf); conf.set("spark.sql.shuffle.partitions", "1"); HiveContext hiveContext = new HiveContext(sc); hiveContext.sql("use spark"); hiveContext.sql("drop table if exists sales"); hiveContext.sql( "create table if not exists sales (riqi string,leibie string,jine Int) " + "row format delimited fields terminated by '\t'"); hiveContext.sql( "load data local inpath './data/sales.txt' into table sales"); /** * 开窗函数格式： * 【 row_number() over (partition by XXX order by XXX) as rank】 * 注意：rank 从1开始 */ /** * 以类别分组，按每种类别金额降序排序，显示 【日期，种类，金额】 结果，如： * * 1 A 100 * 2 B 200 * 3 A 300 * 4 B 400 * 5 A 500 * 6 B 600 * 排序后： * 5 A 500 --rank 1 * 3 A 300 --rank 2 * 1 A 100 --rank 3 * 6 B 600 --rank 1 * 4 B 400 --rank 2 * 2 B 200 --rank 3 * * 2018 A 400 1 * 2017 A 500 2 * 2016 A 550 3 * * * 2016 A 550 1 * 2017 A 500 2 * 2018 A 400 3 * *///无法取前三//hiveContext.sql("select riqi,leibie,jine,"// + "row_number() over (partition by leibie order by jine desc) rank "// + "from sales").show(); DataFrame result = hiveContext.sql("select riqi,leibie,jine,rank from ( select riqi,leibie,jine," + "row_number() over (partition by leibie order by jine desc) rank from sales) t"+ "where t.rank&lt;=3"); result.show(100); /** * 将结果保存到hive表sales_result */ result.write().mode(SaveMode.Overwrite).saveAsTable("sales_result"); sc.stop(); &#125;&#125; Scala 1234567891011121314151617181920212223242526272829import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.sql.hive.HiveContextobject RowNumberWindowFun &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setAppName("windowfun") val sc = new SparkContext(conf) val hiveContext = new HiveContext(sc) hiveContext.sql("use spark"); hiveContext.sql("drop table if exists sales"); hiveContext.sql( "create table if not exists sales (riqi string,leibie string,jine Int) " + "row format delimited fields terminated by '\t'"); hiveContext.sql( "load data local inpath '/root/test/sales' into table sales"); /** * 开窗函数格式： * 【 rou_number() over (partitin by XXX order by XXX) 】 */ val result = hiveContext.sql( "select riqi,leibie,jine from (select riqi,leibie,jine," +"row_number() over (partition by leibie order by jine desc) rank" + "from sales) t where t.rank&lt;=3"); result.show(); sc.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark框架 - SparkSql - SQL语句</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习（四）]]></title>
    <url>%2F2019%2F02%2F19%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、广播变量1、广播变量理解图 2、广播变量的使用Java: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.bd.java.core;import java.util.Arrays;import java.util.List;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.broadcast.Broadcast;/** * 广播变量： * 1.不能将一个RDD使用广播变量广播出去，因为RDD是不存数据的，可以将RDD的结果广播出去。 * 2.广播变量只能在Driver端定义，不能在Executor端定义。 * 3.在Driver端可以修改广播变量的值，在Executor端不能修改广播变量的值。 */public class BroadCast &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("broadcast"); JavaSparkContext sc = new JavaSparkContext(conf); //List中已经实现了序列化，可以用于跨网络传输 List&lt;String&gt; list = Arrays.asList("hello bjsxt"); //广播变量将list广播出去 final Broadcast&lt;List&lt;String&gt;&gt; broadCastList = sc.broadcast(list); JavaRDD&lt;String&gt; lines = sc.textFile("data/word.txt"); JavaRDD&lt;String&gt; result = lines.filter(new Function&lt;String, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(String s) throws Exception &#123; //匿名内部类中使用的变量必须使用final修饰 return broadCastList.value().contains(s); &#125; &#125;); result.foreach(new VoidFunction&lt;String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void call(String t) throws Exception &#123; System.out.println(t); &#125; &#125;); sc.close(); &#125;&#125; Scala: 1234567891011val conf = new SparkConf()conf.setMaster("local").setAppName("brocast")val sc = new SparkContext(conf)val list = List("hello xasxt")val broadCast = sc.broadcast(list)val lineRDD = sc.textFile("./words.txt")lineRDD.filter &#123; x =&gt; &#123; println(broadCast.value) broadCast.value.contains(x) &#125;.foreach &#123; println&#125;sc.stop() 3、注意事项 为什么使用广播变量 能不能将一个 RDD 使用广播变量广播出去？不能，因为RDD是不存储数据的。可以将RDD的结果广播出去。 广播变量只能在 Driver 端定义，在Executor端使用，不能在 Executor 端定义。 在 Driver 端可以修改广播变量的值，在 Executor 端无法修改广播变量的值 代码中，算子内部执行是在Executor端，其余的在Driver端 系列化：用于机器之间跨网络传输时，要将文件序列化到磁盘才可完成传输 内存大会频繁的gc（垃圾回收）就会卡顿，如果内存还不够，就会报oom（内存溢出） 二、累加器1、累加器理解图 2、累加器的使用Java： 1234567891011121314151617181920212223242526272829303132333435package com.bd.java.core;import org.apache.spark.Accumulator;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.rdd.RDD;/** * 累加器在Driver端定义赋初始值和读取，在Executor端累加。 */public class AccumulatorOperator &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("accumulator"); JavaSparkContext sc = new JavaSparkContext(conf); //获取累加器：初始值为0 final Accumulator&lt;Integer&gt; accumulator = sc.accumulator(0); sc.textFile("data/word.txt",2).foreach(new VoidFunction&lt;String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void call(String t) throws Exception &#123; accumulator.add(1); //不能再Executor端获取accumulator.value()来触发累加// System.out.println(accumulator.value()); System.out.println(accumulator); &#125; &#125;); // accumulator.value 写法只能在driver端，excutor端的task只能用accumulator的写法来查看数据 System.out.println(accumulator.value()); sc.stop(); &#125;&#125; Scala: 123456789101112131415val conf = new SparkConf()conf.setMaster("local").setAppName("accumulator")val sc = new SparkContext(conf)val accumulator = sc.accumulator(0)/*val count = 0sc.textFile("./words.txt").foreach &#123; x =&gt;&#123;count+=1println("count:"+count)&#125;&#125; //結果count打印为0 ， 因为count未能序列化，无法实现跨网络传输*/sc.textFile("./words.txt").foreach &#123; x =&gt;&#123;accumulator.add(1)&#125;&#125;println(accumulator.value)sc.stop() 注意： 累加器在Driver端定义赋初始值，累加器只能在Driver端读取，在 Excutor 端更新。 三、SparkShuffle1、SparkShuffle 概念reduceByKey 会将上一个 RDD 中的每一个 key 对应的所有 value 聚合成一个 value，然后生成一个新的 RDD，元素类型是&lt;key,value&gt;对的形式，这样每一个 key 对应一个聚合起来的 value。 问题：聚合之前，每一个 key 对应的 value 不一定都是在一个 partition中，也不太可能在同一个节点上，因为 RDD 是分布式的弹性的数据集，RDD 的 partition 极有可能分布在各个节点上。 如何聚合？ – – Shuffle Write ：上一个 stage 的每个 map task 就必须保证将自己处理的当前分区的数据相同的 key 写入一个分区文件中，可能会写入多个不同的分区文件中。 – – Shuffle Read ：reduce task 就会从上一个 stage 的所有 task 所在的机器上寻找属于自己的那些分区文件，这样就可以保证每一个 key 所对应的 value 都会汇聚到同一个节点上去处理和聚合。 Spark 中有两种 Shuffle 类型，HashShuffle 和 SortShuffle，Spark1.2之前是 HashShuffle 默认的分区器是 HashPartitioner，Spark1.2 引入SortShuffle 默认的分区器是 RangePartitioner。 2、HashShuffle1&gt; 普通机制 普通机制示意图 执行流程a) 每一个 map task 将不同结果写到不同的 buffer 中，每个buffer 的大小为 32K。buffer 起到数据缓存的作用。 b) 每个 buffer 文件最后对应一个磁盘小文件。 c) reduce task 来拉取对应的磁盘小文件。 总结① .map task 的计算结果会根据分区器（默认是hashPartitioner）来决定写入到哪一个磁盘小文件中去。ReduceTask 会去 Map 端拉取相应的磁盘小文件。② .产生的磁盘小文件的个数：M（map task 的个数）*R（reduce task 的个数） 存在的问题 产生的磁盘小文件过多，会导致以下问题： a) 在 Shuffle Write 过程中会产生很多写磁盘小文件的对象。 b) 在 Shuffle Read 过程中会产生很多读取磁盘小文件的对象。 c) 在JVM堆内存中对象过多会造成频繁的gc,gc还无法解决运行所需要的内存 的话，就会 OOM。 d) 在数据传输过程中会有频繁的网络通信，频繁的网络通信出现通信故障的可能性大大增加，一旦网络通信出现了故障会导致 shuffle file cannot find 由于这个错误导致的 task 失败，TaskScheduler 不负责重试，由 DAGScheduler 负责重试 Stage。 2&gt; 合并机制 合并机制示意图 总结产生磁盘小文件的个数：C(core 的个数)*R（reduce 的个数） 3、SortShuffle1&gt; 普通机制 普通机制示意图 2&gt;bypass 机制4、Shuffle 文件寻址1)MapOutputTracker2) BlockManager四、Spark 内存管理1、静态内存管理分布图2、统一内存管理分布图3、reduce 中 OOM 如何处理？五、Shuffle 调优1、SparkShuffle 调优配置项如何使用？]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>广播</tag>
        <tag>累加器</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习（三）]]></title>
    <url>%2F2019%2F02%2F18%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[案例一、统计网站 pv 和 uv统计0、概念理解PV 是网站分析的一个术语，用以衡量网站用户访问的网页的数量。 对于广告主，PV 值可预期它可以带来多少广告收入。一般来说，PV 与来访者的数量成正比，但是 PV 并不直接决定页面的真实来访者数量，如同一个来访者通过不断的刷新页面，也可以制造出非常高的 PV。 1、什么是 PV 值PV （page view ）即页面浏览量或点击量，是衡量一个网站或网页用户访问量。 PV 值就是所有访问者在 24 小时（0 点到 24 点）内看了某个网站多少个页面或某个网页多少次。 PV 是指页面刷新的次数，每一次页面刷新，就算做一次 PV 流量。 2、什么是UV 值UV （unique visitor ）即独立访客数，指访问某个站点或点击某个网页的不同 IP 地址的人数。 在同一天内，UV 只记录第一次进入网站的具有独立IP 的访问者，在同一天内再次访问该网站则不计数。 UV 提供了一定时间内不同观众数量的统计指标，而没有反应出网站的全面活动。 PV 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.bd.java.core;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.deploy.master.Master;import scala.Tuple2;import java.util.Iterator;import java.util.Map;public class TestPV &#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setMaster("local").setAppName("pv"); JavaSparkContext context = new JavaSparkContext(sparkConf); JavaRDD&lt;String&gt; lineRDD = context.textFile("./data/pvuvdata"); /**求每个页面 PV * 文件每一行的内容115.77.12.186 安徽 2017-10-10 1512012307084 5641635304912151098 www.suning.com */ //方法一：mapToPair().reduceByKey().foeach() lineRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String line) throws Exception &#123; String[] str = line.split("\t"); return new Tuple2&lt;&gt;(str[5],1); &#125; &#125;).reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;).foreach(new VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Integer&gt; tuple2) throws Exception &#123; System.out.println(tuple2); &#125; &#125;); //方法二: mapToPair().groupByKey().foeach() JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupByKeyRDD = lineRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String line) throws Exception &#123; String[] str = line.split("\t"); return new Tuple2&lt;&gt;(str[5], 1); &#125; &#125;).groupByKey(); groupByKeyRDD.foreach(new VoidFunction&lt;Tuple2&lt;String, Iterable&lt;Integer&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; tuple2) throws Exception &#123; int count = 0; Iterator&lt;Integer&gt; iterator = tuple2._2.iterator(); while(iterator.hasNext())&#123; count++; &#125; System.out.println("url : " + tuple2._1 + " value: " + count); &#125; &#125;); //方法三： mapToPair().countByKey()--&gt;对map遍历 Map&lt;String, Object&gt; map = lineRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String line) throws Exception &#123; String[] str = line.split("\t"); // url,1 return new Tuple2&lt;&gt;(str[5], 1); &#125; &#125;).countByKey(); for (String key :map.keySet())&#123; System.out.println("key : " + key + " value :" + map.get(key) ); &#125; &#125;&#125; UV 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.bd.java.core;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.broadcast.Broadcast;import org.apache.spark.deploy.master.Master;import org.apache.spark.scheduler.DAGScheduler;import scala.Tuple2;import java.util.HashSet;import java.util.Iterator;import java.util.Map;public class TestUV &#123; private int sum = 10; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setMaster("local").setAppName("uv"); JavaSparkContext context = new JavaSparkContext(sparkConf); JavaRDD&lt;String&gt; lineRDD = context.textFile("./data/pvuvdata"); /**求每个网站 UV： 用IP唯一标识用户 ，注意去重 * 文件每一行的内容115.77.12.186 安徽 2017-10-10 1512012307084 5641635304912151098 www.suning.com */ //方法一：mapToPair().groupByKey().foreach() JavaPairRDD&lt;String, Iterable&lt;String&gt;&gt; rdd1 = lineRDD.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123; @Override public Tuple2&lt;String, String&gt; call(String s) throws Exception &#123; String url = s.split("\t")[5]; String ip = s.split("\t")[0]; return new Tuple2&lt;&gt;(url, ip); &#125; &#125;).groupByKey(); rdd1.foreach(new VoidFunction&lt;Tuple2&lt;String, Iterable&lt;String&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Iterable&lt;String&gt;&gt; tuple2) throws Exception &#123; //set:无序，不可重复，所以它可以自动去重 HashSet&lt;Object&gt; set = new HashSet&lt;&gt;(); Iterator&lt;String&gt; iterator = tuple2._2.iterator(); while(iterator.hasNext())&#123; set.add(iterator.next()); &#125; System.out.println(tuple2._1 + " : " + set.size()); &#125; &#125;); //方法二：mapToPair().ditinct().countByKey()--&gt;遍历map Map&lt;String, Object&gt; map = lineRDD.mapToPair(new PairFunction&lt;String, String, String&gt;() &#123; @Override public Tuple2&lt;String, String&gt; call(String s) throws Exception &#123; String url = s.split("\t")[5]; String ip = s.split("\t")[0]; return new Tuple2&lt;&gt;(url, ip); &#125; &#125;).distinct().countByKey(); for (String key : map.keySet()) &#123; System.out.println("key : " + key + " value :" + map.get(key)); &#125; &#125;&#125; 1------------------------------------------------------------------------------------- 1234567891011121314151617181920212223242526272829303132package com.sxt.scala.coreimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object PVUV &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("pvuv") val sc = new SparkContext(conf) val records = sc.textFile("data/pvuvdata") //pv records.map(x =&gt; &#123; val fields = x.split("\t") (fields(5), 1) &#125;).reduceByKey(_ + _).sortBy(_._2).foreach(println) //uv val result: RDD[(String, Iterable[String])] = records.map(x =&gt; &#123; val fields = x.split("\t") (fields(5), fields(0)) &#125;).groupByKey() result.foreach(x =&gt; &#123; val key = x._1 val iteratable = x._2 println("key : " + key + " size : " + iteratable.toSet.size) &#125;) &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536package com.bd.scala.coreimport org.apache.spark.rdd.RDDimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object PVUV2 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() // "local[4]" 指定本地以及使用的核数 conf.setMaster("local[4]").setAppName("pvuv") val context = new SparkContext(conf) val linesRDD = context.textFile("data/pvuvdata") //pv //(www.jd.com,1000) linesRDD.map(x=&gt;&#123; val fields: Array[String] = x.split("\t") (fields(5),1) &#125;).reduceByKey((x,y)=&gt;&#123;x + y&#125;).sortBy(_._2,false).foreach(println) //uv //(www.taobao.com,10.20.30.18) val groupRDD: RDD[(String, Iterable[String])] = linesRDD.map(x=&gt;&#123; val fields = x.split("\t") (fields(5),fields(0)) &#125;).groupByKey() groupRDD.map(x=&gt;&#123; val key = x._1 val size = x._2.toSet.size (key,size) &#125;).sortBy(_._2,false)foreach(println) &#125;&#125; 案例二：二次排序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.bd.java.core;import java.io.Serializable;public class SecondSortKey implements Serializable , Comparable&lt;SecondSortKey&gt;&#123; private static final long serialVersionUID = 1L; private int first; private int second; public int getFirst() &#123; return first; &#125; public void setFirst(int first) &#123; this.first = first; &#125; public int getSecond() &#123; return second; &#125; public void setSecond(int second) &#123; this.second = second; &#125; public SecondSortKey(int first, int second) &#123; super(); this.first = first; this.second = second; &#125; @Override public int compareTo(SecondSortKey o1) &#123; if (getFirst() - o1.getFirst() == 0) &#123; // 5 6 // this &lt; o1 // 6 5 // this &gt; o1 return o1.getSecond() - getSecond(); /*if(getSecond() - o1.getSecond() == 0) &#123; return getThree() - o1.getThree(); &#125;else &#123; return getSecond() - o1.getSecond(); &#125;*/ &#125; else &#123; return o1.getFirst() - getFirst(); &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.bd.java.core;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import scala.Tuple2;public class SecondKeyTest &#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf(); sparkConf.setMaster("local").setAppName("SecondarySortTest"); final JavaSparkContext sc = new JavaSparkContext(sparkConf); JavaRDD&lt;String&gt; secondRDD = sc.textFile("./data/secondSort.txt"); /*文件内容格式 * 1 3 * 1 4 * 2 3 */ //maptoPair --&gt;sortByKey --&gt;foreach JavaPairRDD&lt;SecondSortKey, String&gt; secRDD = secondRDD.mapToPair(new PairFunction&lt;String, SecondSortKey, String&gt;() &#123; @Override public Tuple2&lt;SecondSortKey, String&gt; call(String line) throws Exception &#123; String[] fields = line.split(" "); SecondSortKey secondSortKey =new SecondSortKey( Integer.parseInt(fields[0]), Integer.parseInt(fields[1]) ); return new Tuple2&lt;&gt;(secondSortKey, line); &#125; &#125;); secRDD.sortByKey().foreach(new VoidFunction&lt;Tuple2&lt;SecondSortKey, String&gt;&gt;() &#123; @Override public void call(Tuple2&lt;SecondSortKey, String&gt; tuple2) throws Exception &#123; System.out.println(tuple2._2); &#125; &#125;); &#125;&#125; 1------------------------------------------------------------------------------------- 12 案例三：分组取topN1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192package com.sxt.java.core;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import scala.Tuple2;import java.util.*;public class TopN &#123; public static void main(String[] args) &#123; SparkConf conf; conf = new SparkConf().setMaster("local[5]").setAppName("TopOps"); JavaSparkContext sc = new JavaSparkContext(conf); //hdfs://shsxt/wc.txt JavaRDD&lt;String&gt; linesRDD = sc.textFile("data/scores.txt",5); final List n = new ArrayList();// linesRDD.count();/*a 86 a 58 b 78*/ JavaPairRDD&lt;String, Integer&gt; pairRDD = linesRDD.mapToPair( new PairFunction&lt;String, String, Integer&gt;() &#123; /** * */ private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String str) throws Exception &#123; String[] splited = str.split(" "); String clazzName = splited[0]; Integer score = Integer.valueOf(splited[1]); return new Tuple2&lt;String, Integer&gt;(clazzName, score); &#125; &#125;); JavaPairRDD&lt;String, Iterable&lt;Integer&gt;&gt; groupByKeyRDD =pairRDD.groupByKey(); groupByKeyRDD.foreach( new VoidFunction&lt;Tuple2&lt;String, Iterable&lt;Integer&gt;&gt;&gt;() &#123; /** * */ private static final long serialVersionUID = 1L; @Override public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; tuple) throws Exception &#123; String clazzName = tuple._1; Iterator&lt;Integer&gt; iterator = tuple._2.iterator(); System.out.println(tuple); //取前三：大的放前，小的后移 Integer[] top3 = new Integer[3]; while (iterator.hasNext()) &#123; Integer score = iterator.next(); for (int i = 0; i &lt; top3.length; i++) &#123; if (top3[i] == null) &#123; top3[i] = score; break; &#125; else if (score &gt; top3[i]) &#123; for (int j = 2; j &gt; i; j--) &#123; top3[j] = top3[j - 1]; &#125; top3[i] = score; break; &#125; &#125; &#125; System.out.println("class Name:" + clazzName); for (Integer sscore : top3) &#123; System.out.println(sscore); &#125; &#125; &#125;);// groupByKeyRDD.foreach(new VoidFunction&lt;Tuple2&lt;String, Iterable&lt;Integer&gt;&gt;&gt;() &#123;// @Override// public void call(Tuple2&lt;String, Iterable&lt;Integer&gt;&gt; tuple2) throws Exception &#123;// String key = tuple2._1;// Iterator&lt;Integer&gt; iterator = tuple2._2.iterator();// List list = IteratorUtils.toList(iterator);// Collections.sort(list);// for(int i=0;i&lt;Math.min(3,list.size());i++)&#123;// // list.size = 3 list.get(2)// System.out.println(key + " " + list.get(list.size()-i-1));// &#125;// &#125;// &#125;); &#125;&#125; 1----------------------------------------------------------------------------------- 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.sxt.scala.core/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Computes the PageRank of URLs from an input file. Input file should * be in format of: * URL neighbor URL * URL neighbor URL * URL neighbor URL * ... * where URL and their neighbors are separated by space(s). */object SparkPageRank &#123; def main(args: Array[String]) &#123; // if (args.length &lt; 1) &#123; // System.err.println("Usage: SparkPageRank &lt;file&gt; &lt;iter&gt;") // System.exit(1) // &#125; val sparkConf = new SparkConf().setAppName("PageRank").setMaster("local[1]") val iters = 20; // val iters = if (args.length &gt; 0) args(1).toInt else 10 val ctx = new SparkContext(sparkConf) val lines = ctx.textFile("page.txt") //根据边关系数据生成 邻接表 如：(1,(2,3,4,5)) (2,(1,5)).. val links = lines.map&#123; s =&gt; val parts = s.split("\\s+") (parts(0), parts(1)) &#125;.distinct().groupByKey().cache() links.foreach(println) // (1,1.0) (2,1.0).. var ranks = links.mapValues(v =&gt; 1.0) ranks.foreach(println) for (i &lt;- 1 to iters) &#123; // (1,((2,3,4,5), 1.0)) val contribs = links.join(ranks).values.flatMap&#123; case (urls, rank) =&gt; val size = urls.size urls.map(url =&gt; (url, rank / size)) &#125; ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _) &#125; val output = ranks.collect()// output.foreach(tup =&gt; println(tup._1 + " has rank: " + tup._2 + ".")) ctx.stop() &#125;&#125; 参数解释：spark-submitspark-submit -h –master （优先使用代码中的配置） –name （指定APPname） –deploy mode （默认为client，指定运行模式） –jars （可以用来为代码添加所需要的jar包依赖） IDEA代码打包：BUILD（注意避免jar包过大，可） –files （添加代码所需的文件） –conf （PROP=value） –driver-memory –executor-memory –total-executor-core （若不指明，就把所有的核均用完） –queue 资源分配： yarn ： 分配到队列中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@node00 bin]# ./spark-submit -hUsage: spark-submit [options] &lt;app jar | python file&gt; [app arguments]Usage: spark-submit --kill [submission ID] --master [spark://...]Usage: spark-submit --status [submission ID] --master [spark://...]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally("client") or on one of the worker machines inside the cluster ("cluster") (Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated (逗号分隔) list of local jars to include on the driver and executor classpaths.(Driver 和 executor 依赖的第三方 jar 包) --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by -- repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in -- packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with -- packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark- defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. --help, -h Show this help message and exit --verbose, -v Print additional debug output --version, Print the version of current Spark Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: "default"). --num-executors NUM Number of executors to launch (Default: 2). --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically.sc.textFile("hdfs://node00:8020/test.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).foreach(println) Spark Shell1、 概念：SparkShell 是 Spark 自带的一个快速原型开发工具，也可以说是Spark 的 scala REPL(Read-Eval-Print-Loop),即交互式 shell。支持使用 scala 语言来进行 Spark 的交互式编程。 2、使用:（配置从HDFS上获取文件） (1)启动HDFS，上传文件 123zkServer.sh start (3台)start-all.sh (任一台)hadoop dfs -put test.txt / (任一台：将test.txt文件上传至hdfs的根目录) (2)启动standalone集群：在/sbin路径下 1./start-all.sh (3)在客户端上启动 spark-shell: 1./spark-shell (local模式：在控制台打印) 或 1./spark-shell --master spark://node00:7077 （client模式：控制台无打印，可通过web页面查看） （4）运行 wordcount： 1sc.textFile("hdfs://Sunrise/test.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).foreach(println) 三、参数解释：spark-shell1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@node00 bin]# ./spark-shell -hUsage: ./bin/spark-shell [options]Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --deploy-mode DEPLOY_MODE Whether to launch the driver program locally ("client") or on one of the worker machines inside the cluster ("cluster")(Default: client). --class CLASS_NAME Your application's main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by -- repositories. The format for thecoordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in -- packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with -- packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark- defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. --help, -h Show this help message and exit --verbose, -v Print additional debug output --version, Print the version of current Spark Spark standalone with cluster deploy mode only: --driver-cores NUM Cores for driver (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). --queue QUEUE_NAME The YARN queue to submit to (Default: "default"). --num-executors NUM Number of executors to launch (Default: 2). --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. SparkUI1、SparkUI 界面介绍提交spark：在/bin路径下 1./spark-submit --master spark://node00:7077 --name sp --class org.apache.spark.example.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar --100 注意：–name指代的参数在代码中也有配置，所以对同一参数均有配置时，以代码中的配置为主 浏览器页面访问：node00:8080 页面显示 点击：Application ID列中的值 → Application Detail UI 会显示查看不了事件日志 Event logging is not enabledNo event logs were found for this application! To enable event logging, set spark.eventLog.enabled to true and spark.eventLog.dir to the directory to which your event logs are written. 2、配置 historyServer 临时配置，对本次提交的应用程序起作用 1234./spark-shell --master spark://node00:7077--name myapp1--conf spark.eventLog.enabled=true--conf spark.eventLog.dir=hdfs://Sunrise/spark/test 停止程序，在 Web Ui 中 Completed Applications 对应的ApplicationID 中能查看 history。 spark-default.conf 配置文件中配置 HistoryServer，对所有提交的Application 都起作用 在 客 户 端 节 点 ， 进 入 ../spark-1.6.0/conf/spark-defaults.conf 最后加入: 12345678# 开启记录事件日志的功能spark.eventLog.enabled true# 设置事件日志存储的目录spark.eventLog.dir hdfs://Sunrise/spark/test# 设置 HistoryServer 加载事件日志的位置spark.history.fs.logDirectory hdfs://Sunrise/spark/test# 日志优化选项, 压缩日志spark.eventLog.compress true 发送到其他节点（如果节点上没有以上配置，就不会有对应的作用） 在HDFS上一定要先存在路径/spark/test 12#hdfs集群一定要启动hadoop dfs -mkdir -p /spark/test 页面显示 点击：Application ID列中的值 → Application Detail UI 就会有显示内容 启动 HistoryServer：(在/sbin路径下) 1./start-history-server.sh （Sunrise在这里是HDFS集群的名字） 访问 HistoryServer： node00:18080,之后所有提交的应用程序运行状况都会被记录。 Master HA1、Master 的高可用原理Standalone 集群只有一个 Master，如果 Master 挂了就无法提交应用程序，但不影响正在执行的worker。 给 Master 进行高可用配置可以使用fileSystem(文件系统)和 zookeeper（分布式协调服务）。 fileSystem 只有存储功能，可以存储 Master 的元数据信息，用fileSystem 搭建的 Master 高可用，在 Master 失败时，需要我们手动启动另外的备用 Master，这种方式不推荐使用。 zookeeper 有选举和存储功能，可以存储 Master 的元素据信息，使用zookeeper 搭建的 Master 高可用，当 Master 挂掉时，备用的 Master会自动切换，推荐使用这种方式搭建 Master 的 HA。 2、Master 高可用搭建1) 在 Spark Master 节点上配置主 Master，配置.spark1.6.0/conf/ spark-env.sh 1234export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=node00:2181,node01:2181,node02:2181-Dspark.deploy.zookeeper.dir=/sparkmaster" 2) 发送到其他 worker 节点上 12scp spark-env.sh node01:`pwd`....... 3) 找一台节点（非主 Master 节点:node01）配置备用 Master,修改spark-env.sh 配置节点上的 MasterIP 1SPARK_MASTER_IP=192.168.198.130 4) 启动集群之前启动 zookeeper 集群： 1zkServer.sh start 5) 启动 spark Standalone 集群，启动备用 Master node00:(在/sbin路径下) 1./start-all.sh node01: 1./start-master.sh 6) 打开主 Master 和备用 Master WebUI 页面，观察状态 主master： img 1.6.0 Spark Master at spark://192.168.198.128:7077 URL: spark://192.168.198.128:7077 REST URL: spark://192.168.198.128:6066 (cluster mode) Alive Workers: 2 Cores in use: 2 Total, 0 Used Memory in use: 2.0 GB Total, 0.0 B Used Applications: 0 Running, 6 Completed Drivers: 0 Running, 0 Completed Status: ALIVE 备用master： img 1.6.0 Spark Master at spark://192.168.198.130:7077 URL: spark://192.168.198.130:7077 REST URL: spark://192.168.198.130:6066 (cluster mode) Alive Workers: 0 Cores in use: 0 Total, 0 Used Memory in use: 0.0 B Total, 0.0 B Used Applications: 0 Running, 0 Completed Drivers: 0 Running, 0 Completed Status: ALIVE 注意点  主备切换过程中不能提交 Application。  主备切换过程中不影响已经在集群中运行的 Application。因为 Spark 是粗粒度资源调度。 测试验证 提交 SparkPi 程序，kill 主 Master 观察现象。 1./spark-submit --master spark://node00:7077,node01:7077 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 10000 显示： 主备切换有时差，因为也不急 程序不受影响]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>计算框架</tag>
        <tag>参数解释</tag>
        <tag>Spark shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习（二）]]></title>
    <url>%2F2019%2F02%2F17%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、控制算子1、概念： 控制算子有三种，cache、persist、checkpoint 以上算子都可以将RDD 持久化，持久化的单位是 partition。 cache 和 persist 都是懒 执行的。 必须有一个 action 类算子触发执行。 cache 和 persist 算子的返回值可赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了 checkpoint 算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系（所有父RDD）。 错误：rdd.cache().count() 返回的不是持久化的 RDD，而是一个数值了。 2、详解 :one:​ cache默认将 RDD 的数据持久化到内存中。cache 是懒执行。 注意： chche () =persist()=persist(StorageLevel.Memory_Only) :two: persist 支持指定持久化级别 useOffHeap 使用堆外内存 disk、memory、offheap、deserialized（不序列化）、replication（副本数，默认为1） 序列化：压缩数据（节省空间，使用数据时要反序列化，会额外消耗CPU性能） none 、disk_only、disk_only_2、memeory_only 、memeory_only _ser 、 memory_and_disk 、 memory_and_disk_2 :three: checkpoint checkpoint 将 RDD 持久化到磁盘，还可以切断 RDD 之间的依赖关系。 checkpoint 的执行原理： 当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。 当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的RDD 做一个标记。 Spark 框架会自动启动一个新的 job，重新计算这个 RDD 的数据，将数据持久化到 HDFS 上。 优化： 对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS上就可以，省去了重新计算这一步。 持久化级别：如下 1234567891011121314151617181920212223242526val cocnf = new SparkConf()conf.setMaster("local").setAppname("count")val context = new SparkContext()//设置CP在HDFS上的路径context.setCheckPointDir("")val lineADD = context.textFile("./countword.txt")val time1 = System.currentTimeMillis()val c = lineADD.count()val time2 = System.currentTimeMillis()val t1 = time2 - time1//做缓存(persisit（m_o）)linelineADD = lineADD.cache()//做持久化lineADD.persisit(StorageLevel.memory_only)//checkpoint 容错,最好还有cachelineADD.checkpoint()val time3 = System.currentTimeMillis()val c = lineADD.count()val time4 = System.currentTimeMillis()val t2 = time4 - time3//t1 远大于 t2 二、算子补充transformation转换算子 12&gt; join,leftOuterJoin,rightOuterJoin,fullOuterJoin&gt; 作用在 K,V 格式的 RDD 上。根据 K 进行连接，对（K,V）join(K,W)返回（K,(V,W)） join 后的分区数与父 RDD 分区数多的那一个相同 12&gt; union&gt; 合并两个数据集。两个数据集的类型要一致。 返回新的 RDD 的分区数是合并 RDD 分区数的总和。 12&gt; intersection&gt; 取两个数据集的交集 12&gt; subtract&gt; 取两个数据集的差集 12&gt; mapPartition&gt; 与 map 类似，遍历的单位是每个 partition 上的数据。 12&gt; distinct(map+reduceByKey+map)&gt; 12&gt; cogroup&gt; 当调用类型（K,V）和（K，W）的数据上时，返回一个数据集（K，（Iterable,Iterable）） action触发算子 12&gt; foreachPartition&gt; 遍历的数据是每个 partition 的数据。 三、集群搭建及测试Standalone1、下载安装包、解压Spark历史版本下载 注意： 与Hadoop的版本保持对应。 此处使用： spark-1.6.0-bin-hadoop2.6.tgz 1tar -zvxf spark-1.6.0-bin-hadoop2.6.tgz 2、改名1mv spark-1.6.0-bin-hadoop2.6 spark-1.6.0 3、修改slaves进入安装包的conf目录下，修改slaves.template文件，添加从节点。并保存。 123#备份cp slaves.template slavesvim slaves 常驻进程：master、worker 配置slaves（与worker对应） 12node01node02 4、修改 spark-env.sh改名（备份） 1cp spark-env.sh.template spark-env.sh 配置spark-env.sh（注意与虚拟机实际配置对应） 1234567891011121314151617#locally#cluster#YARN client#standalone deploy#配置 java_home 路径JAVA_HOME=/usr/soft/jdk1.8.0_191#master 的 ipSPARK_MASTER_IP=192.168.198.128#提交任务的端口，默认是 7077SPARK_MASTER_PORT=7077#每个 worker 从节点能够支配的 core 的个数SPARK_WORKER_CORES=1#每个 worker 从节点能够支配的内存数SPARK_WORKER_MEMORY=1024m#配置yarnHADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5/etc/hadoop 5、其他节点将spark解压文件发送到其他两个节点 12[root@node00 soft]# scp -r spark-1.6.0-bin-hadoop2.6 node01:`pwd`[root@node00 soft]# scp -r spark-1.6.0-bin-hadoop2.6 node02:`pwd` 6、配置环境变量（可不配，因为bin路径中包含start-all ，该命令与hdfs中的命令会冲突） 7、启动：(node00)在spark的解压文件的/sbin 目录下 1./start-all.sh 停止 1./stop-all.sh 显示： [root@node00 sbin]# ./start-all.shstarting org.apache.spark.deploy.master.Master, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploymaster.Master-1-node00.out node01: starting org.apache.spark.deploy.worker.Worker, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spar.deploy.worker.Worker-1-node01.out node02: starting org.apache.spark.deploy.worker.Worker, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spar.deploy.worker.Worker-1-node02.out 查看三台节点的进程 node00（命令启动的节点） 12&gt; [root@node00 sbin]# jps&gt; 2343 Master2408 Jps nose01(配置的从节点) 12&gt; [root@node01 ~]# jps&gt; 2292 Jps2229 Worker node02(从节点) 12&gt; [root@node02 ~]# jps&gt; 6216 Worker6266 Jps 注意： Worker在这里不是真正干活的进程，而是相当于Yarn中的NM。 它是负责管理所在节点资源的、向Master汇报所在节点的信息（如核数、内存数） Master： 监控任务、分发任务、回收计算结果 8、搭建客户端 将 spark 安装包原封不动的拷贝到一个新的节点上，然后，在新的节点上提交任务即可。 注意：8080 是Spark WEBUI页面的端口 ； 7077 是Spark任务提交的端口 web页面访问：ip:8080 修改master的WEBUI端口， 方法一（永久）：通过修改start-master.sh 文件（在/sbin目录下） 1vim start-master.sh 找到文件内容如下的部分： 123if [ &quot;$SPARK_MASTER_WEBUI_PORT&quot; = &quot;&quot; ]; then SPARK_MASTER_WEBUI_PORT=8080fi 方法二：在 Master 节点上导入临时环境变量，只作用于当前进程，重启就无效了。 1[root@node00 sbin]# export SPARK_MASTER_WEBUI_PORT=8080 删除临时变量 1[root@node00 sbin]# export -n SPARK_MASTER_WEBUI_PORT Yarn1、步骤1。2。3。4。5。8。同standalone 不用Master和Worker，所以不用第7步，我们使用的是yarn中的RM和NM 2、配置添加 HADOOP_CONF_DIR配置 （在使用Yarn时，就能找到关于hdfs的所有配置，其中就包括IP 和Port） 方式一： 编辑spark-env.sh文件 方式二： 1export HADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5/etc/hadoop 测试：求π值Pi案例： 源码案例：路径：在spark解压路径spark-1.6.0-bin-hadoop2.6中 spark-1.6.0-bin-hadoop2.6/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala 原理：随机产生无穷多个点落入如上图形中，求落入圆中的概率：$$概率 p = πrr/(2r*2r)=π$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */// scalastyle:off printlnpackage org.apache.spark.examplesimport scala.math.randomimport org.apache.spark._/** Computes an approximation to pi */object SparkPi &#123; def main(args: Array[String]) &#123; val conf = new SparkConf("local").setAppName("Spark Pi") val spark = new SparkContext(conf) // args 运行时传入的参数 slices 分区数量 (决定task数量) val slices = if (args.length &gt; 0) args(0).toInt else 2 //MaxValue 一个无限大的数 n 随机产生的十万个的数 val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow //parallelize可以获得RDD ，将1~n个数字放到RDD中 //val count :[Int] = spark.parallelize(1 until n, slices) val count = spark.parallelize(1 until n, slices).map &#123; i =&gt; val x = random * 2 - 1 val y = random * 2 - 1 if (x*x + y*y &lt; 1) 1 else 0 &#125;.reduce(_ + _) println("Pi is roughly " + 4.0 * count / n) spark.stop() &#125;&#125;// scalastyle:on println 所需使用的jar包：spark-examples-1.6.0-hadoop2.6.0.jar 位置：解压目录的lib路径下 在任一节点的/bin路径下上执行如下命令：（node00） Standalone 提交命令:12345./spark-submit #提交spark --master spark://node1:7077 #spark主节点的地址和端口 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100 # 指明运行的jar包+路径 和 jar包中执行的包名+类名 100 为传入的参数./spark-submit --master spark://node00:7077 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 1000 显示： 提交命令的节点（node00主节点） 会显示执行日志、运算结果 12345678910111213141516171819202122232425262728293031323334&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Starting task 999.0 in stage 0.0 (TID 999, node02, partition 999,PROCESS_LOCAL, 2158 bytes)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 995.0 in stage 0.0 (TID 995) in 68 ms on node02 (996/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 997.0 in stage 0.0 (TID 997) in 131 ms on node01 (997/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 996.0 in stage 0.0 (TID 996) in 147 ms on node01 (998/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 999.0 in stage 0.0 (TID 999) in 112 ms on node02 (999/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 998.0 in stage 0.0 (TID 998) in 115 ms on node02 (1000/1000)&gt; 19/02/13 23:27:31 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 79.202 s&gt; 19/02/13 23:27:31 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool &gt; 19/02/13 23:27:31 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 82.641779 s&gt; &gt; Pi is roughly 3.14148344 #运算结果&gt; &gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/metrics/json,null&#125;&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/stages/stage/kill,null&#125;&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/api,null&#125;&gt; 。。。。。。。。。。。。。。。。。。。。。。。。。&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/jobs/json,null&#125;&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/jobs,null&#125;&gt; 19/02/13 23:27:32 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.198.128:4040&gt; 19/02/13 23:27:32 INFO cluster.SparkDeploySchedulerBackend: Shutting down all executors&gt; 19/02/13 23:27:32 INFO cluster.SparkDeploySchedulerBackend: Asking each executor to shut down&gt; 19/02/13 23:27:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&gt; 19/02/13 23:27:33 INFO storage.MemoryStore: MemoryStore cleared&gt; 19/02/13 23:27:33 INFO storage.BlockManager: BlockManager stopped&gt; 19/02/13 23:27:33 INFO storage.BlockManagerMaster: BlockManagerMaster stopped&gt; 19/02/13 23:27:33 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&gt; 19/02/13 23:27:33 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.&gt; 19/02/13 23:27:33 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.&gt; 19/02/13 23:27:34 INFO spark.SparkContext: Successfully stopped SparkContext&gt; 19/02/13 23:27:34 INFO util.ShutdownHookManager: Shutdown hook called&gt; 19/02/13 23:27:34 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7c2019e-10f4-4b31-9308-5a94603de113&gt; 19/02/13 23:27:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7c2019e-10f4-4b31-9308-5a94603de113/httpd-39b8b4b3-9b80-4247-9c7e-ed6bd2dc389f&gt; &gt; 在命令执行期间： 在三个节点敲如下命令：jps，会显示： node00： 12345&gt; [root@node00 ~]# jps&gt; 4903 Jps&gt; 2343 Master&gt; 4764 SparkSubmit #代表是提交spark的节点 (与主从无关)&gt; node01和node02： 12345&gt; [root@node01 bin]# jps&gt; 2229 Worker&gt; 5096 CoarseGrainedExecutorBackend #代表是干活的节点 （仅为从节点进程）&gt; 5167 Jps&gt; 如果提交命令的节点是从节点（node01），则在该节点上会显示执行日志、运算结果 则在提交过程中，敲命令：jps 该节点会显示 1234567&gt; [root@node01 ~]# jps&gt; 5298 CoarseGrainedExecutorBackend #代表是干活的节点 （仅为从节点进程）&gt; 2229 Worker&gt; 5323 Jps&gt; 5213 SparkSubmit #代表是提交spark的节点 (与主从无关)&gt; &gt; YARN 提交命令：基于Hadoop ： NN DN JN ZKFC ZK RM RM node00 √ √ √ √ √ √ node01 √ √ √ √ √ √ √ node02 √ √ √ √ √ 启动zookeeper ：（3台） 1zkServer.sh start 启动hdfs ：（1台） 1start-all.sh 相当于：Instead use start-dfs.sh and start-yarn.sh 启动resourcemanager ：(在RM的主节点上启动 ：1台) 1yarn-daemon.sh start resourcemanager 在任一节点的/bin路径下执行：（node01） 12345./spark-submit--master yarn #HADOOP_CONF_DIR配置使得在使用Yarn时能找到hdfs的所有配置，其中就有IP 和Port--class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 1000 显示 执行日志、计算结果会在执行提交命令的节点上显示 在命令提交过程中在三台节点上敲命令：jps 会显示 node02： [root@node02 ~]# jps3406 DataNode3491 JournalNode1681 QuorumPeerMain4133 CoarseGrainedExecutorBackend # 真正干活的进程4092 ExecutorLauncher # 启动executor3585 NodeManager3942 SparkSubmit #提交spark的进程4217 Jps 四、Standalone 模式两种提交任务方式1、Standalone-client 提交任务方式(1)命令提交 在/sbin路径下： 1./start-all.sh 提交spark 方式一： 12345./spark-submit--master spark://node00:7077--class org.apache.spark.examples.SparkPi../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 方式二： 123456./spark-submit--master spark://node1:7077--deploy-mode client--class org.apache.spark.examples.SparkPi../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 (2)执行原理图 (3)执行流程 client 模式提交任务后，会在客户端启动 Driver 进程。 Driver 会向 Master 申请启动 Application 启动的资源。 资源申请成功，Driver 端将 task 发送到 worker 端执行。 worker 将 task 执行结果返回到 Driver 端 (4)总结 client 模式适用于测试调试程序。 Driver 进程是在客户端启动的，这里的客户端就是指提交应用程序的当前节点。 在 Driver 端可以看到 task 执行的情况。生产环境下不能使用 client 模式， 是因为： 假设要提交 100 个 application 到集群运行，Driver 每次都会在client 端启动，那么就会导致客户端 100 次网卡流量暴增的问题。 2、Standalone-cluster 提交任务方式（1）命令提交 在/sbin路径下： 1./start-all.sh 提交spark 123456./spark-submit--master spark://node00:7077--deploy-mode cluster--class org.apache.spark.examples.SparkPi../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 注意： Standalone-cluster 提交方式，应用程序使用的所有 jar 包和文件，必须保证所有的 worker 节点都要有，因为此种方式，spark 不会自动上传jar包。 Standalone-client 和yarn 模式会在提交命令的时候自动uploading 实现jar包共享， 解决方式： 1、将所有的依赖包和文件各放一份在 worker 节点上。 2、将所有的依赖包和文件打到同一个包中，然后放在 hdfs 上。(路径需指定为hdfs上的路径) 1234567&gt; &gt; ./spark-submit&gt; &gt; --master spark://node00:7077&gt; &gt; --deploy-mode cluster&gt; &gt; --class org.apache.spark.examples.SparkPi&gt; &gt; hdfs://Sunrise/lib/spark-examples-1.6.0-hadoop2.6.0.jar&gt; &gt; 1000&gt; &gt; （2）执行原理图 （3）执行流程 cluster 模式提交应用程序后，会向 Master 请求启动 Driver. Master 接受请求，随机在集群一台节点启动 Driver 进程。 Driver 启动后为当前的应用程序申请资源。 Driver 端发送 task 到 worker 节点上执行。 worker 将执行情况和执行结果返回给 Driver 端。 （4）总结 Driver 进程是在集群某一台 Worker 上启动的，在客户端是无法查看 task 的执行情况的。假设要提交 100个 application 到集群运行,每次 Driver 会随机在集群中某一台 Worker 上启动，那么这 100 次网卡流量暴增的问题就散布在集群上 总结 StandaloneStandalone 两种方式提交任务，Driver 与集群的通信包括： Driver 负责应用程序资源的申请 任务的分发。 结果的回收。 监控 task 执行情况。 五、Yarn 模式两种提交任务方式1、yarn-client 提交任务方式（1）命令提交 提交spark 方式一： 12345./spark-submit--master yarn--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar100 方式二： 12345./spark-submit--master yarn–client--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar100 方式三： 123456./spark-submit--master yarn--deploy-mode client--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar100 （2）执行原理图 （3）执行流程 客户端提交一个 Application，在客户端启动一个 Driver 进程。 应用程序启动后会向 RS(ResourceManager)发送请求，启动AM(ApplicationMaster)的资源。 RS 收到请求，随机选择一台 NM(NodeManager)启动 AM。这里的 NM 相当于 Standalone 中的Worker 节点。 AM启动后，会向RS请求一批container资源，用于启动Executor. RS 会找到一批 NM 返回给 AM,用于启动 Executor。 AM 会向 NM 发送命令启动 Executor。 Executor 启动后，会反向注册给 Driver，Driver 发送 task 到Executor,执行情况和结果返回给 Driver 端。 （4）总结 Yarn-client 模式同样是适用于测试，因为 Driver 运行在本地，Driver会与 yarn 集群中的 Executor 进行大量的通信，会造成客户机网卡流量的大量增加. ApplicationMaster 的作用： 为当前的 Application 申请资源 给 NodeManager 发送消息启动 Executor。 注意： ApplicationMaster 有 launchExecutor 和申请资源的功能，并没有作业调度的功能 2、yarn-cluster 提交任务方式（1）命令提交 提交spark 方式一： 123456./spark-submit--master yarn--deploy-mode cluster--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 方式二: 12345./spark-submit--master yarn-cluster--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 （2）执行原理图 （3）执行流程 客户机提交 Application 应用程序，发送请求到RS(ResourceManager),请求启动AM(ApplicationMaster)。 RS 收到请求后随机在一台 NM(NodeManager)上启动 AM（相当于 Driver 端）。 AM 启动，AM 发送请求到 RS，请求一批 container 用于启动Excutor。 RS 返回一批 NM 节点给 AM。 AM 连接到 NM,发送请求到 NM 启动 Excutor。 Excutor 反向注册到 AM 所在的节点的 Driver。Driver 发送 task到 Excutor。 （4）总结 Yarn-Cluster 主要用于生产环境中， 因为 Driver 运行在 Yarn 集群中某一台 nodeManager 中，每次提交任务的 Driver 所在的机器都是随机的，不会产生某一台机器网卡流量激增的现象， 缺点是任务提交后不能看到日志。只能通过 yarn 查看日志。 ApplicationMaster 的作用： 为当前的 Application 申请资源 给 NodeManger 发送消息启动 Excutor。 任务调度。 停止集群任务命令：yarn application -kill applicationID 总结yarn六、术语解释 七、宽窄依赖1、窄依赖父RDD的一个partition对应子RDD一个partition 父RDD的多个partition对应子RDD一个partition 不会产生shuffle 1234mapflatmapfilterunion 2、宽依赖父RDD的一个partition对应子RDD多个partition 会产生shuffle 会划分stage 123reduceByKeyjoingroupBy 八、stage0、概念（1）Spark任务会根据RDD之间的依赖关系，形成一个DAG有向无环图，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分依据就是RDD之间的宽窄依赖关系：遇到宽依赖就划分stage （2）stageN内有一组并行的task组成，这些task将以taskSet的格式提交给TaskScheduler运行 （2）task运行时，stage之间的关系可能并行，也可能串行 1、stage 切割规则切割规则：从后往前，遇到宽依赖就切割 stage。 2、stage 计算模式pipeline 管道计算模式,pipeline 只是一种计算思想、模式。 数据在内存中流转 数据一直在管道里面什么时候数据会落地？ 对 RDD 进行持久化(cache、persisit)。 shuffle write 的时候。 什么决定task数 Stage 的 的 task 并行度是由 stage 的最后一个RDD的分区数来决定的 （partition分区数决定task数） 同一个stage中的task计算逻辑可能不同 如何改变 RDD 的分区数？ 宽依赖可改分区数；（因为此时数据已落地到磁盘） textFile(“ ”,5) reduceByKey(_ +_ , 5) GroupByKey(4) 测试验证 pipeline 计算模式 注意：textFile(“./wc.txt”)是通过文件获得RDD，parallelize()是通过转换参数内容获得RDD 12345678910111213141516&gt; val conf = new SparkConf()&gt; conf.setMaster("local").setAppName("pipeline");&gt; val sc = new SparkContext(conf)&gt; val rdd = sc.parallelize(Array(1,2,3,4))&gt; val rdd1 = rdd.map &#123; x =&gt; &#123;&gt; println("map--------"+x)&gt; x&gt; &#125;&gt; &#125;&gt; val rdd2 = rdd1.filter &#123; x =&gt; &#123;&gt; println("fliter********"+x)&gt; true&gt; &#125; &#125;&gt; rdd2.collect().foreach(print+",") //1,2,3,4&gt; sc.stop()&gt; 显示： map——–1 fliter**1 map——–2 fliter**2 map——–3 fliter**3 map——–4 fliter**4 九、Spark 资源调度和任务调度 1、概念解释 DAGScheduler是任务调度的高层调度器，是一个对象 DAGScheduler 的主要作用就是 将DAG 根据 RDD 之间的宽窄依赖关系划分为一个个的 Stage，然后将这些Stage 以 TaskSet 的形式提交给 TaskScheduler TaskScheduler 是任务调度的低层调度器 TaskSet 其实就是一个集合，里面封装的就是一个个的 task 任务,也就是 stage 中的并行度 task 任务 Application→Job→Stage→Task Spark推测执行机制 如果有运行缓慢的task,那么TaskScheduler就会启动一个新的task（在不同节点的excutor上）来执行相同的处理逻辑，两个task中哪个task先执行结束，就以那个task的执行结果为准。 在 Spark 中推测执行默认是关闭的。 推测执行可以通过 spark.speculation 属性来配置。注意： 对于 ETL 类型要入数据库的业务要关闭推测执行机制，这样就不会有重复的数据入库。 如果遇到数据倾斜的情况，开启推测执行则有可能导致一直会有task重新启动处理相同的逻辑，任务可能一直处于处理不完的状态。（这时候task慢是因为数据量过多，而不是执行性能不行） 2、Spark 资源调度和任务调度的流程： 1、启动集群后，Worker 节点会向 Master 节点汇报资源情况，Master 掌握了集群资源情况。 2、当 Spark 提交一个 Application 后，根据 RDD 之间依赖关系将 Application 形成一个 DAG 有向无环图。 3、任务提交后，Spark 会在Driver 端创建两个对象：DAGScheduler 和 TaskScheduler， DAGScheduler 将DAG 根据 RDD 之间的宽窄依赖关系划分为一个个的 Stage，然后将这些Stage 以 TaskSet 的形式提交给 TaskScheduler， TaskSchedule 会遍历TaskSet 集合，拿到每个 task 后会将 task 发送到计算节点 Executor 中去执行（其实就是发送到 Executor 中的线程池 ThreadPool 去执行）。 task 在Executor 线程池中的运行情况会向 TaskScheduler 反馈，当 task 执行失败时，则由 TaskScheduler 负责重试，将 task 重新发送给 Executor 去执行，默认重试 3 次。如果重试 3 次依然失败，那么这个 task 所在的 stage 就失败了。 stage 失败了则由 DAGScheduler 来负责重试，重新发送 TaskSet 到TaskSchdeuler，Stage 默认重试 4 次。如果重试 4 次以后依然失败，那么这个 job 就失败了。job 失败了，Application 就失败了。 TaskScheduler 不仅能重试失败的 task,还会重试 straggling&lt;落后，缓慢的&gt;task（也就是执行速度比其他 task 慢太多的 task）。如果有运行缓慢的 task那么 TaskScheduler 会启动Spark 的推测执行机制先执行完，task 的执行结果为准。 3、资源调度和任务调度的流程图 4、粗粒度资源申请和细粒度资源申请 粗粒度资源申请(Spark）在 Application 执行之前，将所有的资源申请完毕，当资源申请成功后，才会进行任务的调度，当所有的 task 执行完成后，才会释放这部分资源。 优点： 在 Application 执行之前，所有的资源都申请完毕，每一个task 直接使用资源就可以了，不需要 task 在执行前自己去申请资源，task 启动就快了，task 执行快了，stage 执行就快了，job 就快了，application 执行就快了。 缺点： 直到最后一个 task 执行完成才会释放资源，集群的资源无法充分利用。 细粒度资源申请Application 执行之前不需要先去申请资源，而是直接执行，让 job中的每一个 task 在执行前自己去申请资源，task 执行完成就释放资源。 优点： 集群的资源可以充分利用。 缺点： task 自己去申请资源，task 启动变慢，Application 的运行就响应的变慢了。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>计算框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习（一）]]></title>
    <url>%2F2019%2F02%2F16%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Spark简介1、什么是Spark？ Lightning-fast unified analytics engine Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。 用于逻辑回归算法： 快速(100倍)：能更好的的适用于数据挖掘与机器学习等需要迭代的算法（在计算结果的基础上再计算）；Job的中间结果值在内存中流转，不需要读取HDFS，屏蔽磁盘开销；DAG调度 mr：离线，（迭代时：磁盘IO，较慢） storm：流式 Spark是用Scala编写的，方便快速编程 2、与MapReduce的区别 MapReduce Spark 区别： 同：分布式计算框架 不同： Spark基于内存，MR基于HDFS Spark处理数据的能力是MR的十倍以上 Spark除了基于内存计算之外，还有DAG有向无环图来切分任务的执行顺序 Spark API 的使用语言 Scala（很好）Python(不错)Java(…) 3、Spark运行模式 local 多用于本地测试，如在 eclipse，idea 中写程序测试 standalone standalone是Spark自带的资源调度框架，它支持完全分布式 yarn Hadoop生态圈的资源调度框架，Spark也是可以基于yarn来计算的 基于yarn来进行资源调度，必须实现ApplicationMaster接口，Spark实现的这个接口，所以可以使用 mesos 资源调度框架 二、Sparkcore1、RDD（1）概念：RDD(Resilient Distributed Dateset)弹性分布式数据集 （2）五大特性 RDD 是由一系列的 partition 组成的。 函数是作用在每一个 partition（split）上的。 RDD 之间有一系列的依赖关系。 分区器是作用在 K,V 格式的 RDD 上。 RDD 提供一系列最佳的计算位置。 获取RDD的方式 parallelize() 12345&gt; &gt; //Distribute a local Scala collection to form an RDD&gt; &gt; JavaRDD&lt;T&gt; rdd = javaSparkContext.parallelize(List&lt;T&gt; list)；&gt; &gt; JavaRDD&lt;T&gt; rdd = javaSparkContext.parallelize(List&lt;T&gt; list,int numSlices)； &gt; &gt; &gt; &gt; parallelizePairs 123456&gt; &gt; //Distribute a local Scala collection to form an RDD&gt; &gt; JavaPairRDD&lt;K,V&gt; rdd = &gt; &gt; javaSparkContext.parallelizePairs(List&lt;Tuple2&lt;K, V&gt;&gt; list)； &gt; &gt; JavaPairRDD&lt;K,V&gt; rdd = &gt; &gt; javaSparkContext.parallelizePairs(List&lt;Tuple2&lt;K, V&gt;&gt; list,int numSlices)；&gt; &gt; textFile(“./xx.txt”) 也可指定分区 （3）RDD理解图 理论注解 RDD 实际上不存储数据，这里方便理解，暂时理解为存储数据。 textFile 方法底层封装的是MR 读取文件的方式(先 split,再读取文件)，默认 split 大小是一个 block 大小。 ​ RDD 提供计算最佳位置，体现了数据本地化。体现了大数据中“计算移动数据不移动”的理念。 ❔ 哪里体现 RDD 的分布式？ 👆 RDD 是由 Partition 组成，partition 是分布在不同节点上的。 ❔ 哪里体现 RDD 的弹性（容错）？ 👆 partition 数量，大小没有限制,默认和split（block）一致，体现了 RDD 的弹性。👆 RDD 之间依赖关系，可以基于上一个 RDD 重新计算出 RDD。 ❔ 什么是 K,V 格式的 RDD? 👆 如果 RDD 里面存储的数据都是二元组对象，那么这个 RDD 我们就叫做 K,V 格式的 RDD。 👆 MR有分区器（根据key值求hash，来决定数据存放在哪个分区中，所以分区器必须作用在K，V格式的RDD上） 2、Spark任务执行原理 Driver：（相当于ApplicationMaster） Worker：（相当于NodeManager） 以上图中有四个机器节点， Driver 和 Worker 是启动在节点上的进程，运行在 JVM 中的进程。 Driver 与集群节点之间有频繁的通信。 Driver：任务的调度（监控任务、 负责任务(tasks)的分发和结果的回收）。如果 task的计算结果非常大就不要回收了。会造成 oom。 Worker 是 Standalone 资源调度框架里面资源管理的从节点。也是JVM 进程。 Master 是 Standalone 资源调度框架里面资源管理的主节点。也是JVM 进程。 3、Spark代码流程以用Scala编写WordCount为例1、创建 SparkConf 对象 可以设置 Application name。 可以设置运行模式及资源需求。 123456789val conf = new SparkConf() /** * 几种运行方式： * 1.本地运行 * 2.yarn * 3.standalone * 4.mesos */ conf.setMaster("local").setAppName("wc") 2、创建 SparkContext 对象 1val context = new SparkContext(conf) 3、基于 Spark 的上下文创建一个 RDD，对 RDD 进行处理。 12345678910//获取文件中每一行数据的ADD val lineADD = context.textFile("./wc.txt")//获取每一行数据按空格切分后的ADD val wordADD = lineADD.flatMap(x=&gt;&#123;x.split(" ")&#125;)//获取每个单词加上,1 后的ADD（K,V格式） val KVADD = wordADD.map(x=&gt;&#123;(x,1)&#125;)//获取将相同key的value相加后的ADD（K,V格式），相当于Tuple2 val resultADD = KVADD.reduceByKey((x,y)=&gt;&#123;x+y&#125;)//降序排序 val sortADD = resultADD.sortBy(_._2,false) 4、应用程序中要有 Action 类算子来触发 Transformation 类算子执行。 1sortADD.foreach(println) 5、关闭 Spark 上下文对象 SparkContext。 1context.stop() 4、Transformations 转换算子（1）概念Transformations 类算子是一类算子（函数）叫做转换算子，如map,flatMap,reduceByKey 等。Transformations 算子是延迟执行，也叫懒加载执行。 有action触发算子任务才能提交，才会执行runjob 算子必须作用在RDD上 （2）Transformation 类算子 :arrow_up_small: filter过滤符合条件的记录数，true 保留，false 过滤掉。 🔼 contains 作为条件，是否包含，返回true|false :arrow_up_small:map将一个 RDD 中的每个数据项，通过 map 中的函数映射变为一个新的元素。特点：输入一条，输出一条数据。 :black_joker: mapToPair (Java) 将RDD（如lineRDD）转换成二元组 🃏 mapValues 操作（K,V）RDD中的value 返回Tuple2&lt;&gt; :arrow_up_small: flatMap先 map 后 flat。与 map 类似，每个输入项可以映射为 0 到多个输出项。 🔼 mapPartition 与 map 类似，遍历的单位是每个 partition 上的数据。一进一出 🔼mapPartitionWithIndex类似于 mapPartitions,除此之外还会携带分区的索引值。 🔼 repartition repartition（3） 增加或减少分区.会产生shuffle 🔼coalesce coalesce(3,false) 常用于减少分区，第二个参数决定减少分区时是否产生shuffle：true 为产生 shuffle，false 不产生 shuffle。默认是 false。 如果 coalesce 设置的分区数比原来的 RDD 的分区数还多的话，第二个参数设置为 false 不会起作用， 如果设置成 true，效果和 repartition 一样。即 repartition(numPartitions) = coalesce(numPartitions,true) :arrow_up_small:sample随机抽样算子，根据传进去的小数按比例进行，有放回或者无放回的抽样。 :arrow_up_small:reduceByKey对于K，V格式的RDD，将key相同的RDD，对其value值根据相应的逻辑进行处理。 🔼reduceByKeyWAndWindow (f1,f2,s1,s2) 窗口函数 :arrow_up_small:sortByKey/sortBy作用在 K,V 格式的 RDD 上，对 key 进行升序或者降序排序。 :arrow_up_small:join / leftOuterJoin / rightOuterJoin / fullOuterJoin join ：保留公共元素 （K,V） leftOutJoin ：保留左边的元素 rightOutJoin ：保留右边元素 fullOutJoin ：去重保留 （保留最大分区数） 作用在 K,V 格式的 RDD 上。根据 K 进行连接，对（K,V）join(K,W)返回（K,(V,W)） join 后的分区数与父 RDD 分区数多的那一个相同 🔼union 都保留 （保留总分区数） 合并两个数据集。两个数据集的类型要一致。 返回新的 RDD 的分区数是合并 RDD 分区数的总和。 🔼intersection 取两个数据集的交集 🔼 subtract 取两个数据集的差集 🔼 distinct(map+reduceByKey+map) 去重 🔼 cogroup 当调用类型（K,V）和（K，W）的数据上时，返回一个数据集（K，（Iterable,Iterable）） 🔼groupByKey作用在 K，V 格式的 RDD 上。根据 Key 进行分组。返回（K，Iterable ）。 🔼zip将两个 RDD 中的元素（KV 格式/非 KV 格式）变成一个 KV 格式的 RDD,两个 RDD 的个数必须相同。 🔼zipWithIndex该函数将 RDD 中的元素和这个元素在 RDD 中的索引号（从 0 开始）组合成（K,V）对。 🔼 1234567891011121314151617181920212223242526272829303132object WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("WC") val context = new SparkContext(conf) //用于了解集群 val linesRDD :RDD[String] = sc.textFile("./words.txt") // lineRDD.filter(x=&gt;&#123;// x.contains("sh")// &#125;).foreach(println)// lineRDD.sample(true,0.2).foreach(println) // lineRDD.map((_,1)).reduceByKey(_ + _).sortBy(_._2,false).foreach(println)// lineRDD.map((_,1)).sortByKey().foreach(println) val wordRDD :RDD[String] = linesRDD.flatMap&#123;lines =&gt; &#123; lines.split(" ") //匿名函数 &#125;&#125; val KVRDD:RDD[(String,Int)] = wordRDD.map&#123; x =&gt; (x,1) &#125; val result:RDD[(String,Int)] = KVRDD.reduceByKey&#123;(a,b)=&gt; &#123; println("a:"+a+",b:"+b) a+b &#125;&#125; 补充 123456789101112 val conf = new SparkConf() conf.setMaster("local").setAppName("WC") val context = new SparkContext(conf) //用于了解集群 //parallelizePairs //joinOptional.absent(0)optional.isPresent()optinal.get() 5、Action 行动算子（1）概念Action 类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count 等。 Transformations 类算子是延迟执行，Action 类算子是触发执行（立即）。 一个 application 应用程序中有几个 Action 类算子执行，就有几个 job 运行。 （2）Action 类算子 :arrow_up_small: count返回数据集中的元素数。会在结果计算完成后回收到 Driver 端。 🔼countByKey作用到 K,V 格式的 RDD 上，根据 Key 计数相同 Key 的数据集元素。 🔼countByValue根据数据集每个元素相同的内容来计数。返回相同内容的元素对应的条数。 :arrow_up_small: take(n)返回一个包含数据集前 n 个元素的集合。:arrow_up_small: firstfirst=take(1),返回数据集中的第一个元素 🔼 collect将计算结果回收到 Driver 端。 :arrow_up_small: foreach循环遍历数据集中的每个元素，运行相应的逻辑。 :arrow_up_small: foreachPartition 遍历的数据是每个 partition 的数据。所以传的参数为Iterator :arrow_up_small:reduce根据聚合逻辑聚合数据集中的每个元素。 12345678910111213141516171819 val conf = new SparkConf() conf.setMaster("local").setAppName("transf") val context = new SparkContext(conf) val lineADD = context.textFile("./wc.txt") val wordADD = lineADD.flatMap(x=&gt;&#123;x.split(" ")&#125;) // println(wordADD.count())//lineADD中数据回收 val arr= lineADD.collect() arr.foreach(println)// val takes: Array[String] = lineRDD.take(5)// takes.foreach(println)// val str: String = lineRDD.first()// println(str) 6、控制算子（1）概念： 控制算子有三种，cache、persist、checkpoint 以上算子都可以将RDD 持久化，持久化的单位是 partition。 cache 和 persist 都是懒 执行的。 必须有一个 action 类算子触发执行。 cache 和 persist 算子的返回值可赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了 checkpoint 算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系（所有父RDD）。 错误：rdd.cache().count() 返回的不是持久化的 RDD，而是一个数值了。 （2）详解 1️⃣​ cache默认将 RDD 的数据持久化到内存中。cache 是懒执行。 注意： chche () =persist()=persist(StorageLevel.Memory_Only) 2️⃣ persist 支持指定持久化级别 useOffHeap 使用堆外内存 disk、memory、offheap、deserialized（不序列化）、replication（副本数，默认为1） 序列化：压缩数据（节省空间，使用数据时要反序列化，会额外消耗CPU性能） none 、disk_only、disk_only_2、memeory_only 、memeory_only _ser 、 memory_and_disk 、 memory_and_disk_2 3️⃣ checkpoint checkpoint 将 RDD 持久化到磁盘，还可以切断 RDD 之间的依赖关系。 checkpoint 的执行原理： 当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。 当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的RDD 做一个标记。 Spark 框架会自动启动一个新的 job，重新计算这个 RDD 的数据，将数据持久化到 HDFS 上。 优化： 对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS上就可以，省去了重新计算这一步。 持久化级别：如下 1234567891011121314151617181920212223242526val cocnf = new SparkConf()conf.setMaster("local").setAppname("count")val context = new SparkContext()//设置CP在HDFS上的路径context.setCheckPointDir("")val lineADD = context.textFile("./countword.txt")val time1 = System.currentTimeMillis()val c = lineADD.count()val time2 = System.currentTimeMillis()val t1 = time2 - time1//做缓存(persisit（m_o）)linelineADD = lineADD.cache()//做持久化lineADD.persisit(StorageLevel.memory_only)//checkpoint 容错,最好还有cachelineADD.checkpoint()val time3 = System.currentTimeMillis()val c = lineADD.count()val time4 = System.currentTimeMillis()val t2 = time4 - time3//t1 远大于 t2 WordCount以用Java编写为例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.shsxt.spark.java;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.*;import scala.Tuple2;import java.util.Arrays;import java.util.List;public class WordCount &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("wc"); JavaSparkContext context = new JavaSparkContext(conf); JavaRDD&lt;String&gt; rdd = context.textFile("./wc.txt"); long count = rdd.count(); List&lt;String&gt; collect = rdd.collect(); List&lt;String&gt; take = rdd.take(5); String first = rdd.first(); JavaRDD&lt;String&gt; wordRDD = rdd.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterable&lt;String&gt; call(String line) throws Exception &#123; String[] split = line.split(" "); List&lt;String&gt; list = Arrays.asList(split); return list; &#125; &#125;);// wordRDD.map(new Function&lt;String&gt;() &#123;// @Override// public String call(String v1) throws Exception &#123;// return null;// &#125;// &#125;) JavaPairRDD&lt;String, Integer&gt; pairRDD = wordRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2(word, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; resultRDD = pairRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; reverseRDD = resultRDD.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; @Override public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; tuple2) throws Exception &#123; return new Tuple2&lt;&gt;(tuple2._2, tuple2._1); &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; sortByKey = reverseRDD.sortByKey(false); JavaPairRDD&lt;String, Integer&gt; result = sortByKey.mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; tuple2) throws Exception &#123; return new Tuple2&lt;&gt;(tuple2._2, tuple2._1); &#125; &#125;); result.foreach(new VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Integer&gt; tuple2) throws Exception &#123; System.out.println(tuple2); &#125; &#125;); &#125;&#125; Linx系统定时调度： crontab 定时调度脚本文件 脚本文件中，编辑spark 提交命令 注意： 脚本文件中的命令必须写它的完整路径，否则找不到此命令]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>计算引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set方法]]></title>
    <url>%2F2019%2F02%2F16%2FSet%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147Scala Set 常用方法下表列出了 Scala Set 常用的方法：序号 方法及描述def +(elem: A): Set[A]为集合添加新元素，x并创建一个新的集合，除非元素已存在 def -(elem: A): Set[A]移除集合中的元素，并创建一个新的集合 def contains(elem: A): Boolean如果元素在集合中存在，返回 true，否则返回 false。def &amp;(that: Set[A]): Set[A]返回两个集合的交集 def &amp;~(that: Set[A]): Set[A]返回两个集合的差集def +(elem1: A, elem2: A, elems: A*): Set[A]通过添加传入指定集合的元素创建一个新的不可变集合 def ++(elems: A): Set[A]合并两个集合def -(elem1: A, elem2: A, elems: A*): Set[A]通过移除传入指定集合的元素创建一个新的不可变集合def addString(b: StringBuilder): StringBuilder将不可变集合的所有元素添加到字符串缓冲区def addString(b: StringBuilder, sep: String): StringBuilder将不可变集合的所有元素添加到字符串缓冲区，并使用指定的分隔符def apply(elem: A)检测集合中是否包含指定元素def count(p: (A) =&gt; Boolean): Int计算满足指定条件的集合元素个数def copyToArray(xs: Array[A], start: Int, len: Int): Unit复制不可变集合元素到数组def diff(that: Set[A]): Set[A]比较两个集合的差集def drop(n: Int): Set[A]]返回丢弃前n个元素新集合 def dropRight(n: Int): Set[A]返回丢弃最后n个元素新集合def dropWhile(p: (A) =&gt; Boolean): Set[A]从左向右丢弃元素，直到条件p不成立def equals(that: Any): Booleanequals 方法可用于任意序列。用于比较系列是否相等。def exists(p: (A) =&gt; Boolean): Boolean判断不可变集合中指定条件的元素是否存在。def filter(p: (A) =&gt; Boolean): Set[A]输出符合指定条件的所有不可变集合元素。def find(p: (A) =&gt; Boolean): Option[A]查找不可变集合中满足指定条件的第一个元素 def forall(p: (A) =&gt; Boolean): Boolean查找不可变集合中满足指定条件的所有元素def foreach(f: (A) =&gt; Unit): Unit将函数应用到不可变集合的所有元素 def head: A获取不可变集合的第一个元素def init: Set[A]返回所有元素，除了最后一个def intersect(that: Set[A]): Set[A]计算两个集合的交集def isEmpty: Boolean判断集合是否为空def iterator: Iterator[A]创建一个新的迭代器来迭代元素def last: A返回最后一个元素def map[B](f: (A) =&gt; B): immutable.Set[B]通过给定的方法将所有元素重新计算def max: A查找最大元素 def min: A查找最小元素 def mkString: String集合所有元素作为字符串显示def mkString(sep: String): String使用分隔符将集合所有元素作为字符串显示def product: A返回不可变集合中数字元素的积。def size: Int返回不可变集合元素的数量def splitAt(n: Int): (Set[A], Set[A])把不可变集合拆分为两个容器，第一个由前 n 个元素组成，第二个由剩下的元素组成def subsetOf(that: Set[A]): Boolean如果集合A中含有子集B返回 true，否则返回falsedef sum: A返回不可变集合中所有数字元素之和 def tail: Set[A]返回一个不可变集合中除了第一元素之外的其他元素def take(n: Int): Set[A]返回前 n 个元素def takeRight(n: Int):Set[A]返回后 n 个元素def toArray: Array[A]将集合转换为数组def toBuffer[B &gt;: A]: Buffer[B]返回缓冲区，包含了不可变集合的所有元素def toList: List[A]返回 List，包含了不可变集合的所有元素def toMap[T, U]: Map[T, U]返回 Map，包含了不可变集合的所有元素 def toSeq: Seq[A]返回 Seq，包含了不可变集合的所有元素def toString(): String返回一个字符串，以对象来表示]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String 方法]]></title>
    <url>%2F2019%2F02%2F16%2FString%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139String 方法 char charAt(int index)返回指定位置的字符 从0开始 int compareTo(Object o)比较字符串与对象 int compareTo(String anotherString)按字典顺序比较两个字符串 int compareToIgnoreCase(String str)按字典顺序比较两个字符串，不考虑大小写 String concat(String str)将指定字符串连接到此字符串的结尾 boolean contentEquals(StringBuffer sb)将此字符串与指定的 StringBuffer 比较。 static String copyValueOf(char[] data)返回指定数组中表示该字符序列的 String static String copyValueOf(char[] data, int offset, int count)返回指定数组中表示该字符序列的 String boolean endsWith(String suffix)测试此字符串是否以指定的后缀结束 boolean equals(Object anObject)将此字符串与指定的对象比较 boolean equalsIgnoreCase(String anotherString)将此 String 与另一个 String 比较，不考虑大小写 byte getBytes()使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 byte[] getBytes(String charsetName使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin)将字符从此字符串复制到目标字符数组 int hashCode()返回此字符串的哈希码16 int indexOf(int ch)返回指定字符在此字符串中第一次出现处的索引（输入的是ascii码值） int indexOf(int ch, int fromIndex)返返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索 int indexOf(String str)返回指定子字符串在此字符串中第一次出现处的索引 int indexOf(String str, int fromIndex)返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始 String intern()返回字符串对象的规范化表示形式 int lastIndexOf(int ch)返回指定字符在此字符串中最后一次出现处的索引 int lastIndexOf(int ch, int fromIndex)返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索 int lastIndexOf(String str)返回指定子字符串在此字符串中最右边出现处的索引 int lastIndexOf(String str, int fromIndex)返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索 int length()返回此字符串的长度 boolean matches(String regex)告知此字符串是否匹配给定的正则表达式 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len)测试两个字符串区域是否相等28 boolean regionMatches(int toffset, String other, int ooffset, int len)测试两个字符串区域是否相等 String replace(char oldChar, char newChar)返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的 String replaceAll(String regex, String replacement使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串 String replaceFirst(String regex, String replacement)使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串 String[] split(String regex)根据给定正则表达式的匹配拆分此字符串 String[] split(String regex, int limit)根据匹配给定的正则表达式来拆分此字符串 boolean startsWith(String prefix)测试此字符串是否以指定的前缀开始 boolean startsWith(String prefix, int toffset)测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 CharSequence subSequence(int beginIndex, int endIndex)返回一个新的字符序列，它是此序列的一个子序列 String substring(int beginIndex)返回一个新的字符串，它是此字符串的一个子字符串 String substring(int beginIndex, int endIndex)返回一个新字符串，它是此字符串的一个子字符串 char[] toCharArray()将此字符串转换为一个新的字符数组 String toLowerCase()使用默认语言环境的规则将此 String 中的所有字符都转换为小写 String toLowerCase(Locale locale)使用给定 Locale 的规则将此 String 中的所有字符都转换为小写 String toString()返回此对象本身（它已经是一个字符串！） String toUpperCase()使用默认语言环境的规则将此 String 中的所有字符都转换为大写 String toUpperCase(Locale locale)使用给定 Locale 的规则将此 String 中的所有字符都转换为大写 String trim()删除指定字符串的首尾空白符 static String valueOf(primitive data type x)返回指定类型参数的字符串表示形式]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List方法]]></title>
    <url>%2F2019%2F02%2F16%2FList%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137def +(elem: A): List[A]前置一个元素列表def ::(x: A): List[A]在这个列表的开头添加的元素。def :::(prefix: List[A]): List[A]增加了一个给定列表中该列表前面的元素。def ::(x: A): List[A]增加了一个元素x在列表的开头def addString(b: StringBuilder): StringBuilder追加列表的一个字符串生成器的所有元素。def addString(b: StringBuilder, sep: String): StringBuilder追加列表的使用分隔字符串一个字符串生成器的所有元素。def apply(n: Int): A选择通过其在列表中索引的元素def contains(elem: Any): Boolean测试该列表中是否包含一个给定值作为元素。def copyToArray(xs: Array[A], start: Int, len: Int): Unit列表的副本元件阵列。填充给定的数组xs与此列表中最多len个元素，在位置开始。def distinct: List[A]建立从列表中没有任何重复的元素的新列表。def drop(n: Int): List[A]返回除了第n个的所有元素。def dropRight(n: Int): List[A]返回除了最后的n个的元素def dropWhile(p: (A) =&gt; Boolean): List[A]丢弃满足谓词的元素最长前缀。def endsWith[B](that: Seq[B]): Boolean测试列表是否使用给定序列结束。def equals(that: Any): Booleanequals方法的任意序列。比较该序列到某些其他对象。def exists(p: (A) =&gt; Boolean): Boolean测试谓词是否持有一些列表的元素。def filter(p: (A) =&gt; Boolean): List[A]返回列表满足谓词的所有元素。def forall(p: (A) =&gt; Boolean): Boolean测试谓词是否持有该列表中的所有元素。def foreach(f: (A) =&gt; Unit): Unit应用一个函数f以列表的所有元素。def head: A选择列表的第一个元素def indexOf(elem: A, from: Int): Int经过或在某些起始索引查找列表中的一些值第一次出现的索引。def init: List[A]返回除了最后的所有元素def intersect(that: Seq[A]): List[A]计算列表和另一序列之间的多重集交集。def isEmpty: Boolean测试列表是否为空def iterator: Iterator[A]创建一个新的迭代器中包含的可迭代对象中的所有元素def last: A返回最后一个元素def lastIndexOf(elem: A, end: Int): Int之前或在一个给定的最终指数查找的列表中的一些值最后一次出现的索引def length: Int返回列表的长度def map[B](f: (A) =&gt; B): List[B]通过应用函数以g这个列表中的所有元素构建一个新的集合def max: A查找最大的元素def min: A查找最小元素def mkString: String显示列表的字符串中的所有元素def mkString(sep: String): String显示的列表中的字符串中使用分隔串的所有元素def reverse: List[A]返回新列表，在相反的顺序元素def sorted[B &gt;: A]: List[A]根据排序对列表进行排序def startsWith[B](that: Seq[B], offset: Int): Boolean测试该列表中是否包含给定的索引处的给定的序列def sum: A概括这个集合的元素def tail: List[A]返回除了第一的所有元素def take(n: Int): List[A]返回前n个元素def takeRight(n: Int): List[A]返回最后n个元素def toArray: Array[A]列表以一个数组变换def toBuffer[B &gt;: A]: Buffer[B]列表以一个可变缓冲器转换def toMap[T, U]: Map[T, U]此列表的映射转换def toSeq: Seq[A]列表的序列转换def toSet[B &gt;: A]: Set[B]列表到集合变换def toString(): String列表转换为字符串]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试问题总结]]></title>
    <url>%2F2019%2F02%2F16%2F%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819悲观锁 + 乐观锁悲观锁（Pessimistic Lock）每次获取数据的时候，都会担心数据被修改，所以每次操作数据前都会进行加锁（读锁、写锁、行锁等），确保在自己在使用数据过程中，数据戹被其他进程修改，使用完成后再对数据进行解锁。由于数据被上了所，期间对数据进行读写的其他进程都需等待。在Java中，synchronized的思想也是悲观锁。乐观锁（Optimistic Lock）每次获取数据的时候，都不会担心数据被修改，所以每次获取数据的时候都不会进行加锁，但在更新数据的时候需要判断数据是否被别人修改。如果数据被其他线程更改，就不进行数据更新。如果数据没有被其他线程更改，就进行数据更新。因为数据没有进行加锁，期间数据可以被进行读写操作。一般会使用版本机制或CAS操作实现。 version方式：一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值与当前数据库中的version值相等，才更新，否则重试更新操作，直到更新成功SQL：update table set x=x+1, version=version+1 where id=#&#123;id&#125; and version=#&#123;version&#125;; CAS操作方式： 即compare and swap 或者 compare and set，涉及到三个操作数，数据所在的内存值，预期值，新值。当需要更新时，判断当前内存值与之前取到的值是否相等，若相等，则用新值更新，若失败则重试，一般情况下是一个自旋操作，即不断的重试。适用场景：悲观锁：适合写操作频繁的场景。如果用于读操作频繁的场景，每次读取都进行加锁，会增加大量锁的开销，降低系统吞吐率。乐观锁：适合读操作频繁的场景如果用于写操作频繁的场景，数据发生冲突的可能性就会增加，为例保证数据的一致性，应用层需要不断的重新获取数据，这样就带来了大量的查询操作，降低系统的吞吐率。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Map方法]]></title>
    <url>%2F2019%2F02%2F16%2FMap%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138Scala Map 方法下表列出了 Scala Map 常用的方法：序号 方法及描述def ++(xs: Map[(A, B)]): Map[A, B]返回一个新的 Map，新的 Map xs 组成def -(elem1: A, elem2: A, elems: A*): Map[A, B]返回一个新的 Map, 移除 key 为 elem1, elem2 或其他 elems。 def --(xs: GTO[A]): Map[A, B]返回一个新的 Map, 移除 xs 对象中对应的 keydef get(key: A): Option[B]返回指定 key 的值def iterator: Iterator[(A, B)]创建新的迭代器，并输出 key/value 对def addString(b: StringBuilder): StringBuilder将 Map 中的所有元素附加到StringBuilder，可加入分隔符def addString(b: StringBuilder, sep: String): StringBuilder将 Map 中的所有元素附加到StringBuilder，可加入分隔符 def apply(key: A): B返回指定键的值，如果不存在返回 Map 的默认方法def clone(): Map[A, B]从一个 Map 复制到另一个 Mapdef contains(key: A): Boolean如果 Map 中存在指定 key，返回 true，否则返回 false。def copyToArray(xs: Array[(A, B)]): Unit复制集合到数组def count(p: ((A, B)) =&gt; Boolean): Int计算满足指定条件的集合元素数量def default(key: A): B定义 Map 的默认值，在 key 不存在时返回。def drop(n: Int): Map[A, B]返回丢弃前n个元素新集合def dropRight(n: Int): Map[A, B]返回丢弃最后n个元素新集合def dropWhile(p: ((A, B)) =&gt; Boolean): Map[A, B]从左向右丢弃元素，直到条件p不成立def empty: Map[A, B]返回相同类型的空 Mapdef equals(that: Any): Boolean如果两个 Map 相等(key/value 均相等)，返回true，否则返回falsedef exists(p: ((A, B)) =&gt; Boolean): Boolean判断集合中指定条件的元素是否存在def filter(p: ((A, B))=&gt; Boolean): Map[A, B]返回满足指定条件的所有集合def filterKeys(p: (A) =&gt; Boolean): Map[A, B]返回符合指定条件的的不可变 Mapdef find(p: ((A, B)) =&gt; Boolean): Option[(A, B)]查找集合中满足指定条件的第一个元素def foreach(f: ((A, B)) =&gt; Unit): Unit将函数应用到集合的所有元素def init: Map[A, B]返回所有元素，除了最后一个def isEmpty: Boolean检测 Map 是否为空 def keys: Iterable[A]返回所有的key/p&gt;def last: (A, B)返回最后一个元素def max: (A, B)查找最大元素def min: (A, B)查找最小元素def mkString: String集合所有元素作为字符串显示def product: (A, B)返回集合中数字元素的积。def remove(key: A): Option[B]移除指定 keydef retain(p: (A, B) =&gt; Boolean): Map.this.type如果符合满足条件的返回 truedef size: Int返回 Map 元素的个数def sum: (A, B)返回集合中所有数字元素之和def tail: Map[A, B]返回一个集合中除了第一元素之外的其他元素def take(n: Int): Map[A, B]返回前 n 个元素def takeRight(n: Int): Map[A, B]返回后 n 个元素def takeWhile(p: ((A, B)) =&gt; Boolean): Map[A, B]返回满足指定条件的元素def toArray: Array[(A, B)]集合转数组def toBuffer[B &gt;: A]: Buffer[B]返回缓冲区，包含了 Map 的所有元素def toList: List[A]返回 List，包含了 Map 的所有元素def toSeq: Seq[A]返回 Seq，包含了 Map 的所有元素def toSet: Set[A]返回 Set，包含了 Map 的所有元素def toString(): String返回字符串对象]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组方法]]></title>
    <url>%2F2019%2F02%2F16%2F%E6%95%B0%E7%BB%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849方法和描述def apply( x: T, xs: T* ): Array[T]创建指定对象 T 的数组, T 的值可以是 Unit, Double, Float, Long, Int, Char, Short, Byte, Boolean。def concat[T]( xss: Array[T]* ): Array[T]合并数组def copy( src: AnyRef, srcPos: Int, dest: AnyRef, destPos: Int, length: Int ): Unit复制一个数组到另一个数组上。相等于 Java's System.arraycopy(src, srcPos, dest, destPos, length)。def empty[T]: Array[T]返回长度为 0 的数组def iterate[T]( start: T, len: Int )( f: (T) =&gt; T ): Array[T]返回指定长度数组，每个数组元素为指定函数的返回值。以上实例数组初始值为 0，长度为 3，计算函数为a=&gt;a+1：scala&gt; Array.iterate(0,3)(a=&gt;a+1)res1: Array[Int] = Array(0, 1, 2) def fill[T]( n: Int )(elem: =&gt; T): Array[T]返回数组，长度为第一个参数指定，同时每个元素使用第二个参数进行填充。 def fill[T]( n1: Int, n2: Int )( elem: =&gt; T ): Array[Array[T]]返回二数组，长度为第一个参数指定，同时每个元素使用第二个参数进行填充。def ofDim[T]( n1: Int ): Array[T]创建指定长度的数组def ofDim[T]( n1: Int, n2: Int ): Array[Array[T]]创建二维数组def ofDim[T]( n1: Int, n2: Int, n3: Int ): Array[Array[Array[T]]]创建三维数组 def range( start: Int, end: Int, step: Int ): Array[Int]创建指定区间内的数组，step 为每个元素间的步长def range( start: Int, end: Int ): Array[Int]创建指定区间内的数组def tabulate[T]( n: Int )(f: (Int)=&gt; T): Array[T]返回指定长度数组，每个数组元素为指定函数的返回值，默认从 0 开始。以上实例返回 3 个元素：scala&gt; Array.tabulate(3)(a =&gt; a + 5)res0: Array[Int] = Array(5, 6, 7)def tabulate[T]( n1: Int, n2: Int )( f: (Int, Int ) =&gt; T): Array[Array[T]]返回指定长度的二维数组，每个数组元素为指定函数的返回值，默认从 0 开始。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala学习]]></title>
    <url>%2F2019%2F02%2F15%2FScala%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Scala官网6个特征。简捷，快速1、Java 与Java无缝整合，运行在JVM上，编译形成.class文件 2、类型 类型自动推断:var 变量类型 val 常量类型 （var s = 1 自动推断s 为int类型 ） dos窗口运行Scala语言（cmd - &gt;scala） 3、并发 底层有actor，天生用于高并发和分布式 4、继承 trait 特征特质（Java中接口和抽象类的结合体？？？两者区别？？单继承多实现）​ 静态语言 （Java静态语言 shell 、Python动态语言） 5、匹配 模式匹配:(Java中的switch case类型必须一致)可匹配多种类型 6、高阶 高阶函数（函数式编程）函数可以作为参数传入方法中（Jdk 8 stream流，莱姆塔表达式） ​ 工具类中方法为静态的 二、Scala安装1、windows安装,配置环境变量Ø 官网下载scala2.10：（因为spark需要的是这个版本的Scala） http://www.scala-lang.org/download/2.10.4.html Ø 下载好后安装。双击msi包安装,记住安装的路径。 Ø 配置环境变量（和配置jdk一样） 新建SCALA_HOME 编辑Path变量，在后面追加如下： 1;%SCALA_HOME%\bin Ø 打开cmd,输入： 1scala - version 看是否显示版本号，确定是否安装成功 2、eclipse 配置scala插件Ø 下载插件（一定要对应eclipse版本下载）,并解压 http://scala-ide.org/download/prev-stable.html Ø 将解压目录下的features和plugins两个文件夹拷贝到eclipse安装目录中的”dropins/scala”目录下。 进入dropins，新建scala文件夹，将两个文件夹拷贝到“dropins/scala”下 3、Scala编辑器：scala ide下载网址：http://scala-ide.org/download/sdk.html 4、Idea 中配置scala插件Ø 打开idea,close项目后，点击Configure-&gt;Plugins Ø 搜索scala，点击Install安装 Ø 设置jdk，打开Project Structure,点击new 选择安装好的jdk路径 Ø 新建Scala项目 三、Scala基础语法1、数据类型 2、变量和常量的声明123456789101112 /** * 定义变量和常量 * 变量 :用 var 定义 ，可修改 * 常量 :用 val 定义，不可修改 */ var name = "zhangsan" println(name) name ="lisi" println(name) val gender = "m"// gender = "m"//错误，不能给常量再赋值// 定义变量或者常量的时候，也可以写上返回的类型，一般省略，如：val a:Int = 10 3、类和对象Ø 创建类 1234567class Person&#123; val name = "zhangsan" val age = 18 def sayName() = &#123; "my name is "+ name &#125;&#125; Ø 创建对象 1234567object Lesson_Class &#123; def main(args: Array[String]): Unit = &#123; val person = new Person() println(person.age); println(person.sayName()) &#125;&#125; Ø 伴生类和伴生对象 12345678910111213141516171819202122232425class Person(xname :String , xage :Int)&#123; var name = Person.name val age = xage var gender = "m" def this(name:String,age:Int,g:String)&#123; this(name,age) gender = g &#125; def sayName() = &#123; "my name is "+ name &#125;&#125;object Person &#123; val name = "zhangsanfeng" def main(args: Array[String]): Unit = &#123; val person = new Person("wagnwu",10,"f") println(person.age); println(person.sayName()) println(person.gender) &#125;&#125; 注意 建议类名首字母大写 ，方法首字母小写，类和方法命名建议符合驼峰命名法。 .一行结束，不需要分号。如果一行里有多个语句，则之间用分号隔开。 scala 中的object是单例对象，相当于java中的工具类，它里面的方法可以看成都是static静态的。object不可以传参数。另：Trait不可以传参数 scala中的class类默认可以传参数，默认的传参数就是默认的构造函数。 重写构造函数的时候，必须要先调用默认的构造函数。 class 类属性自带getter ，setter方法。 使用object时，不用new，使用class时要new ,并且new的时候，class中除了方法不执行，其他都执行。 如果在同一个文件中，object对象和class类的名称相同，则这个对象就是这个类的伴生对象，class称为object对象的伴生类，object 称为class类的伴生对象，他们可以直接访问对方的私有变量。 3. if else1234567891011/** * if else */val age =18 if (age &lt; 18 )&#123; println("no allow")&#125;else if (18&lt;=age&amp;&amp;age&lt;=20)&#123; println("allow with other")&#125;else&#123; println("allow self")&#125; 4. for ,while,do…while to和until 的用法（不带步长，带步长区别） 123456789101112131415 /** * to和until * 例： * 1 to 10 返回1到10的Range数组，包含10 * 1 until 10 返回1到10 Range数组 ，不包含10 */ println(1 to 10 )//打印： Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) println(1.to(10))//同上println(1 to (10 ,2))//步长为2，从1开始打印： Range(1, 3, 5, 7, 9) println(1.to(10, 2)) //同上println(1 until 10 ) //不包含最后一个数，打印1,2,3,4,5,6,7,8,9 println(1.until(10))//同上 println(1 until (10 ,3 ))//步长为2，从1开始打印，打印1,4,7 创建for循环 1234567/** * for 循环 * */ for( i &lt;- 1 to 10 )&#123; println(i) &#125; 创建多层for循环 123456789101112131415161718192021222324252627282930313233343536//可以分号隔开，写入多个list赋值的变量，构成多层for循环 //scala中 不能写count++ count-- 只能写count+ var count = 0; for(i &lt;- 1 to 10; j &lt;- 1 until 10)&#123; println("i="+ i +", j="+j) count += 1 &#125; println(count); //例子： 打印小九九 for(i &lt;- 1 until 10 ;j &lt;- 1 until 10)&#123; if(i&gt;=j)&#123; print(i +" * " + j + " = "+ i*j+" ") &#125; if(i==j )&#123; println() &#125; &#125;//九九乘法表 //方法一// for(i&lt;- 1 to 9 )&#123;// for (j &lt;- 1 to i)&#123;// print(j+"*"+i+"="+i*j+"\t")// &#125;// println()// &#125; //方法二// for(i&lt;- 1 to 9 )&#123;// for (j&lt;- 1 to 9)&#123;// if(j&lt;=i)&#123;// print(j+"*"+i+"="+i*j+"\t")// &#125;//// if(i==j)println()// &#125;// &#125; for循环中可以加条件判断，分号隔开 1234 //可以在for循环中加入条件判断 for(i&lt;- 1 to 10 ;if (i%2) == 0 ;if (i == 4) )&#123; println(i)&#125; scala中不能使用count++，count只能使用count = count+1 ，count += 1 while循环，while（）{}，do {}while() 12345678910111213141516171819 //将for中的符合条件的元素通过yield关键字返回成一个集合 val list = for(i &lt;- 1 to 10 ; if(i &gt; 5 )) yield i //&lt;- 后面是一个集合 for( w &lt;- list )&#123; println(w)&#125; /** * while 循环 */ var index = 0 while(index &lt; 100 )&#123; println("第"+index+"次while 循环") index += 1 &#125; index = 0 do&#123; index +=1 println("第"+index+"次do while 循环")&#125;while(index &lt;100 ) 四、Scala函数1、函数定义1234567def fun (a: Int , b: Int ) : Unit = &#123; println(a+b) &#125;fun(1,1) def fun1 (a : Int , b : Int)= a+b println(fun1(1,2)) 语法解释 函数定义语法 用def来定义 可以定义传入的参数，要指定传入参数的类型 scala中函数中如果返回返回值类型为Unit ，即无返回值 12345&gt; def add(x:Int=10,y:Int=11):Unit =&#123;&gt; x+y&gt; &#125;&gt; //写返回值类型是=时，一定要记得写 ：（冒号）&gt; scala中函数有返回值时，可以写return，也可以不写return： 省略return的时候，函数自动回将最后一行的表达式的值，作为返回值 123456789&gt; def max(x:Int,y:Int )=&#123;&gt; if (x&gt;y)&gt; x&gt; else&gt; y&gt; &#125;&gt; println(max(5,7))&gt; 结果显示：7&gt; * * 如果函数有retrun,则必须写返回类型。 12345678&gt; def min(m:Int,n:Int):Int=&#123;&gt; if(m&gt;n)&gt; return n&gt; else&gt; return m&gt; &#125;&gt; println(min(5,7))&gt; * scala中函数有返回值时，可以写返回值的类型，也可以省略，因为scala可以类型自动推断，有时候不能省略，必须写， * * 比如在递归函数中或者函数的返回值是函数类型的时候。 12345678&gt; def num(x: Int): Int = &#123;&gt; if (x == 1)&gt; 1&gt; else &#123;&gt; x * num(x - 1)&gt; &#125;&gt; &#125;&gt; * 函数定义的时候，如果去掉 = ，那么这个方法返回类型必定是Unit的。 这种说法无论方法体里面什么逻辑都成立，scala可以把任意类型转换为Unit。 假设，里面的逻辑最后返回了一个string，那么这个返回值会被转换成Unit，并且值会被丢弃。 则相当于，函数就将返回值去掉，即无返回值。 * {}里的代码，如果只有一行，则可以省略{} * 传递给方法的参数可以在方法中使用，并且scala规定方法的传过来的参数为val类型，不能修改，不是var。 2、递归函数1234567891011/** * 递归函数 * 5的阶乘 */ def fun2(num :Int) :Int= &#123; if(num ==1) num else num * fun2(num-1) &#125; print(fun2(5)) 3、包含参数默认值的函数12345678910/** * 包含默认参数值的函数 * 注意： * 1.默认值的函数中，如果传入的参数个数与函数定义相同，则传入的数值会覆盖默认值 * 2.如果不想覆盖默认值，传入的参数个数小于定义的函数的参数，则需要指定参数名称 */ def fun3(a :Int = 10,b:Int) = &#123; println(a+b) &#125; fun3(b=2) 4、可变参数个数的函数12345678910111213/** * 可变参数个数的函数 * 注意：多个参数逗号分开 */def fun4(elem :Int*)=&#123; var sum = 0; for(e &lt;- elem)&#123; sum += e &#125; sum&#125;println(fun4(1,2,3,4)) 5、匿名函数 有参匿名函数 无参匿名函数 有返回值的匿名函数 可以将匿名函数返回给val定义的值 匿名函数不能显式声明函数的返回类型 123456789101112131415161718//有参数匿名函数val fun1 = (a : Int ， b : Int) =&gt; &#123; println(a+b)&#125;value1(1,2)//无参数匿名函数val fun2 = ()=&gt;&#123; println("啦啦啦")&#125;fun2()//有返回值的匿名函数val fun3 = (a:Int,b:Int) =&gt;&#123; a+b&#125;println(fun3(4,4)) 6、 嵌套函数123456789101112131415/** * 嵌套函数 * 例如：嵌套函数求5的阶乘 */ def fun5(num:Int)=&#123; def fun6(a:Int,b:Int):Int=&#123; if(a == 1)&#123; b &#125;else&#123; fun6(a-1,a*b) &#125; &#125; fun6(num,1) &#125; println(fun5(5)) 7、偏应用函数123456789101112131415161718192021222324import java.util.Date /** * 偏应用函数是一种表达式 * 不需要提供函数需要的所有参数， * 只需要提供部分，或不提供所需参数。 */ def log(date :Date, s :String)= &#123; println("date is "+ date +",log is "+ s) &#125; val date = new Date() log(date ,"log1") log(date ,"log2") log(date ,"log3") //想要调用log，以上变化的是第二个参数，可以用偏应用函数处理,来优化log方法 /* 绑定第一个 date 参数， * 第二个参数使用下划线(_)替换缺失的参数列表， * 并把这个新的函数值的索引的赋给变量。 */ val logWithDate = log(date,_:String) logWithDate("log11") logWithDate("log22") logWithDate("log33") 8、高阶函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970 /** * 高阶函数:就是操作其他函数的函数 * 函数的参数是函数 * 或者函数的返回是函数 * 或者函数的参数和返回都是函数 *///函数作为参数或者返回累心时，只需指明函数中的类型 //函数的参数是函数： //f : (Int,Int) =&gt;Int （两个Int型参数、Int型返回值） def hightFun(f : (Int,Int) =&gt;Int, a:Int ) : Int = &#123; f(a,100) &#125; def f(v1 :Int,v2: Int):Int = &#123; v1+v2 &#125; println(hightFun(f, 1)) //*************** def test1(x: Int, f: (Int, Int) =&gt; Int) = &#123; var a = x + 100 a * f(1, 3) &#125; def sum(x: Int, y: Int): Int = &#123; x - y &#125; println(test6(10,sum))//------------------------------------------------ //函数的返回值类型为函数 ：(Int,Int)=&gt;Int f2 //1，2,3,4相加 def hightFun2(a : Int,b:Int) : (Int,Int)=&gt;Int = &#123; def f2 (v1: Int,v2:Int) :Int = &#123; v1+v2+a+b &#125; f2 &#125; println(hightFun2(1,2)(3,4)) //*********** def test2(x: Int): (Int) =&gt; String = &#123; def concat(y: Int): String = &#123; x + " !! " + y &#125; concat &#125; val concat: Int =&gt; String = test2(5) println(concat(7))// println(test2(5)(7)) //------------------------------------------------- //函数的参数是函数: f : (Int ,Int) =&gt; Int) //函数的返回是函数: (Int,Int) =&gt; Int def hightFun3(f : (Int ,Int) =&gt; Int) : (Int,Int) =&gt; Int = &#123; f &#125; println(hightFun3(f)(100,200)) println(hightFun3((a,b) =&gt;&#123;a+b&#125;)(200,200)) //以上这句话还可以写成这样 //如果函数的参数在方法体中只使用了一次 那么可以写成_表示 println(hightFun3(_+_)(200,200)) //********** def test3(y: Int, f: (Int) =&gt; Int): (Int) =&gt; Int = &#123; var sum = f(1) + y def sum4(x: Int) = &#123; x + sum &#125; sum4 &#125;// def f2 (x: Int) =&#123;// x + 10// &#125; val f = test3(10,(x:Int)=&gt;&#123;x + 10&#125;)// println(f(5)) 9、柯里化函数12345678910111213/** * 柯里化函数 * 可以理解为高阶函数的简化 */ def fun1(a :Int,b:Int)(c:Int,d:Int) = &#123; a+b+c+d &#125; println(fun1(1,2)(3,4)) //******* def fun(a :Int)(c:Int) = &#123; a+c &#125; println(fun(1)(3)) 五、字符串1、string | stringBuilder (可变)2、操作方法Ø 比较:equals Ø 比较忽略大小写:equalsIgnoreCase Ø indexOf：如果字符串中有传入的assci码对应的值，返回下标 12345678910111213141516171819202122232425262728293031323334353637383940 /** * String &amp;&amp; StringBuilder */ val str = "abcd" val str1 = "ABCD" println(str.indexOf(97)) println(str.indexOf("b")) println(str==str1) println(str.equals(str1)) println(str.equalsIgnoreCase(str1)) /** * compareToIgnoreCase * * 如果参数字符串等于此字符串，则返回值 0； * 如果此字符串小于字符串参数，则返回一个小于 0 的值； * 如果此字符串大于字符串参数，则返回一个大于 0 的值。 * */ println(str.compareToIgnoreCase(str1)) val strBuilder = new StringBuilder() //括号可省 strBuilder.append("abc")//一个 + 只能跟单个字符且必须用单引号，否则无效 // strBuilder.+('d') strBuilder+ 'd'// strBuilder.+=('h') strBuilder+= 'h' //两个+ 可跟多个字符且必须用双引号// strBuilder.++=("efg") strBuilder++= "efg"//StringBuilder可以追加浮点数 strBuilder.append(1.0) strBuilder.append(18f) println(strBuilder) println(strBuilder.toString()) 六、集合1、数组（1）创建一维数组1234567891011121314/** * 创建数组两种方式： * 1.new Array[String](3) * 2.直接Array */ //创建类型为Int 长度为3的数组 val arr1 = new Array[Int](3) //创建String 类型的数组，直接赋值 val arr2 = Array[String]("s100","s200","s300") //赋值 arr1(0) = 100 arr1(1) = 200 arr1(2) = 300 （2）数组遍历123456789101112131415161718/** * 遍历两种方式 * for * foreach */ for(i &lt;- arr1)&#123; println(i) &#125; arr1.foreach(i =&gt; &#123; println(i) &#125;) for(s &lt;- arr2)&#123; println(s) &#125; arr2.foreach &#123; x =&gt; println(x) &#125; （3）数组操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Array.concat：合并数组Array.fill(5)(“shsxt”)：创建初始值的定长数组def main(args: Array[String]): Unit = &#123; val a = new Array[Int](3) a(0) = 1 a(1) = 2 a(2) = 3//判断数组中符合条件的元素的个数// val n = a.count(x=&gt;&#123;// if(x&gt;2)// true// else// false// &#125;)//// println(n) val b = Array(4,5,6)//将两个数组的元素合并在一个新的数组中 val ints = Array.concat(a,b)//遍历数组 ints.foreach(println)//创建包含5个指定元素的数组 val strings = Array.fill(5)("shsxt") strings.foreach(println)//// for(i &lt;- a)&#123;// println(i)// &#125;//// val b = Array(4,5,6)//// for(i &lt;- b)&#123;// println(i)// &#125;// a.foreach(x=&gt; &#123; println(x) &#125; )// a.foreach(println(_))// a.foreach(println)// val c = new Array[Array[Int]](3)// c(0) = Array(1,2,3)// c(1) = Array(4,5,6)// c(2) = Array(7,8,9)// c.foreach(x=&gt;&#123;// x.foreach(y=&gt;&#123;// print(y + "\t")// &#125;)// println()// &#125;)// for(i&lt;-c)&#123;// for(j &lt;-i)&#123;// print(j + "\t")// &#125;// println()// &#125; &#125; （4）创建二维数组123456789101112131415161718192021222324252627282930313233343536/** * 创建二维数组和遍历 */ val arr3 = new Array[Array[String]](3) arr3(0)=Array("1","2","3") arr3(1)=Array("4","5","6") arr3(2)=Array("7","8","9") for(i &lt;- 0 until arr3.length)&#123; for(j &lt;- 0 until arr3(i).length)&#123; print(arr3(i)(j)+" ") &#125; println() &#125; var count = 0 for(arr &lt;- arr3 ;i &lt;- arr)&#123; if(count%3 == 0)&#123; println() &#125; print(i+" ") count +=1 &#125; arr3.foreach &#123; arr =&gt; &#123; arr.foreach &#123; println &#125; &#125;&#125; val arr4 = Array[Array[Int]](Array(1,2,3),Array(4,5,6)) arr4.foreach &#123; arr =&gt; &#123; arr.foreach(i =&gt; &#123; println(i) &#125;) &#125;&#125; println("-------") for(arr &lt;- arr4;i &lt;- arr)&#123; println(i) 2、list（1）创建list1val list = List(1,2,3,4) 备注 Nil 长度为0的list （2）list遍历 foreach ，for （3）list操作12345678910111213141516171819202122232425262728 //创建 val list = List(1,2,3,4,5) //遍历 list.foreach &#123; x =&gt; println(x)&#125;// list.foreach &#123; println&#125; //filter 过滤元素 val list1 = list.filter &#123; x =&gt; x&gt;3 &#125; list1.foreach &#123; println&#125; //count 计算符合条件的元素个数 val value = list1.count &#123; x =&gt; x&gt;3 &#125; println(value) //map 对元素操作 如切分 val nameList = List( "hello shsxt", "hello xasxt", "hello shsxt" ) val mapResult:List[Array[String]] = nameList.map&#123; x =&gt; x.split(" ") &#125; mapResult.foreach&#123;println&#125; //flatmap 将元素压扁平,先map再flat val flatMapResult : List[String] = nameList.flatMap&#123; x =&gt; x.split(" ") &#125; flatMapResult.foreach &#123; println &#125; 3、set注意：set集合自动去重 1234567891011121314151617181920212223242526272829303132val s = Set(1,2,3,3)val s1 = Set(1，2，3，4)//遍历时，自动去重s.foreach(println) for(x&lt;- s)&#123;println(x)&#125;//交集val inse = s.intersect(s1)val ins = s.&amp;(s1)inse.foreach(println)inse.foreach&#123;println&#125;//差集val di = s.diff(s1)val de = s.&amp;~(s1)//子集:s是不是s1的子集val boo: Boolean = s.subsetof(s1)//max ,minval max = s.maxval min = s.minprintln(max + ":" + min)//转成数组，lists.toArray.foreach(println)s.toList.foreach(println)//mkString , 元素以！分隔拼成字符串println(s.mkString)println(s.mkstring("!")) 4、map1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//map创建时，若key相同，则都会被后一个key覆盖val map =Map（"1"-&gt;"a",("3","c"),"2"-&gt;"b"）//获取map值println(map.get("1"))println(map.get("1").get)//获取指定key，若没有，就用另一个指定值代替println(map.getOrElse("5","100"))val res = map.get("8").getOrElse("800")println(res)//遍历map//x为map中的一组键值对for(x-&gt; map)&#123;println("key:"+x._1+",value:"+ x._2) &#125;map.foreach(x=&gt;&#123; println("key:"+x._1+",value:"+ x._2) &#125;)//遍历keyval keyIteratable = map.keyskeyIteratable.foreach(k=&gt;&#123;println("key"+k+",value"+map.get(k).get)&#125;)//遍历valueval valueIteratable = map.valuesvalueIteratable.foreach&#123;v=&gt;&#123; println("value"+v)&#125;&#125;//合并mapval map1 = Map（(1，"a"),(2,"b"),(3,"c")）val map2 = Map（(1,"aa"),(2,"bb"),(3,"cc")）map1.++(map2).foreach(println)map1.++:(map2).oreach(println)//map操作方法//filter:过滤，留下符合条件的元素val result = map.filter(x=&gt;&#123;if(x._1.equals("1")) trueelse false&#125;)result.foreach(println)map.filter(_._2.equals("a")).foreach(println)//count：统计符合条件的元素个数val countResult = map.count（x=&gt;&#123; x._1.equals("1")）println(countResult) //contains：判断map中是否包含某个keymap.contains("3")//exist：判断符合条件的元素是存在//若遇到条件返回结果为true，就停止迭代map.exists(x=&gt;&#123;if（x._1.equals("3"))&#123;println("存在喽")&#125;elseprintln("不存在喽")&#125;) 5、元组Tuple同：与列表类似 不同：元组可以包含不同类型的元素；元组的值是通过将单个值包含在圆括号中构成 注意：Tuple最多支持22个参数 1234567891011121314151617181920212223//元素个数取决于Tuple后面的数字val t1 = new Tuple1(1)val t2 = new Tuple2(1,2)val t3 = Tuple3(1,2,3) val t5 = (1,2,3，4,5)val t22 = new Tuple22(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22)//获取元组中的值 ：用 ._XX println（t5._4）val t = Tuple2((1,2),("zhangsan","lisi"))println(t._1._2)//元组遍历：元素迭代器val interator = t5.productorInteratorinterator.foreach(println)while(interator.hasNext)&#123; println(intrator.next())&#125;//元素翻转 swap， 只针对二元数组println(t2.swap)//转换成字符串println(t5.toString()) 七、traitScala：只有extends ，可以继承多个Trait Scala Trait(特征) 相当于 Java 的接口和抽象类的结合 可以定义属性和方法 注意： 若继承的多个trait中包含同名的属性和方法，就必须在在类中使用override 重新定义 若重新定义的属性是使用var定义的则会报错，所以必须是使用val定义的属性才可使用override trait中不能传参数 举例：trait中带属性带方法实现 1234567891011121314151617181920212223242526272829303132trait Read&#123; var name = "read" val age = 18 def read():Unit=&#123; println("read---") &#125;&#125;trait Write&#123; var name = "write" val age = 19 def write():Unit=&#123; println("write----") &#125;&#125;class Human extends Read with Write&#123; //前提name必须是用val定义的 // override var name = "person" override val age = 20 name = "person"&#125;object Trait&#123; def main(args:Array[String]):Unit = &#123; val human = new Human() println(human.name) human.read() human.write() &#125;&#125; 举例：trait中带方法不实现 12345678910111213141516171819202122232425262728293031trait Equal&#123; //抽象方法 def isEqual(p:Point):Boolean def isNoEqual(p:Point):Boolean=&#123; !isEqual(p) &#125;&#125;class Point(x:Int,y:Int) extends Equal &#123; var xx = x var yy = y override def isEqual (p:Point) = &#123; p.xx == xx &amp; p.yy ==yy &#125; def isEqule(p:Any) = &#123; p.isInstanceOf[Point] &amp;&amp; p.asInstanceOf[Point].xx==xx &#125;&#125;object Trait2&#123; def main(args:Array[String]):Unit = &#123; val p1 = new Point(3,5) val p2 = new Point(4,5) ptintln(p1.isNoEqual(p2)) ptintln(p1.isEqual(p2)) &#125;&#125; 八、模式匹配match1、概念理解 一个模式匹配包含多个备选项，且每个都以关键字case开始 每个备选项都包含一个模式和多个表达式，且用箭头符号 =&gt; 隔开了模式和表达式。 Ø 模式匹配不仅可以匹配值还可以匹配类型 Ø 从上到下顺序匹配，如果匹配到则不再往下匹配 Ø 都匹配不上时，会匹配到case_ ,相当于default Ø match 的最外面的”{ }”可以去掉看成一个语句 123456789101112131415161718192021object Match&#123; def main(args:Array[String]):Unit = &#123; val p1 =(1,2,3f,"4","abc",55d) p1.foreach(x=&gt;&#123; println(mymatch(x)) &#125;) &#125; def mymatch(x:Any)=&#123; x match &#123; case "4" =&gt;println("macth 4--") case x:String =&gt; println("match String") case 1 =&gt; println("1--") case 2 =&gt; println("2--") case 3f =&gt; println("3f--") // case x :Double =&gt; println("type is Double") case _ =&gt; println("no match--") &#125; &#125;&#125; 九、样例类1、概念理解 使用了case关键字的类定义就是样例类(case classes) 样例类是种特殊的类。实现了类构造参数的getter方法（构造参数默认被声明为val），当构造参数是声明为var类型的，它将帮你实现setter和getter方法。 Ø 样例类默认帮你实现了toString,equals，copy和hashCode等方法。 Ø 样例类可以new, 也可以不用new 2、举例：结合模式匹配的代码 1234567891011121314151617181920212223object Match2&#123; def main(args:Array[String]):Unit = &#123; val zs = new Woman(18,"zs") println(zs.age + " : "+ zs . name) val mm = Woman(19,"meimei") val nn = Woman(19,"meimei") println(mm.equals(zs)) println(mm.equals(nn)) val wo = List(ls,mm) wo.foreach(x=&gt;&#123; x match &#123; case x:Woman =&gt; println("match Woman--") case Woman(18,"ls") =&gt; println("ls") case Woman(19,"mm") =&gt; println("mm") case Woman(19,"zs") =&gt; println("zs") &#125; &#125;) &#125;&#125;case class Woman (age:Int ,name:String) 十、Actor Model\1. 概念理解 Actor Model是用来编写并行计算或分布式系统的高层次抽象（类似java中的Thread）让程序员不必为多线程模式下共享锁而烦恼,被用在Erlang 语言上, 高可用性99.9999999 % 一年只有31ms 宕机Actors将状态和行为封装在一个轻量的进程/线程中，但是不和其他Actors分享状态，每个Actors有自己的世界观，当需要和其他Actors交互时，通过发送事件和消息，发送是异步的，非堵塞的(fire-andforget)，发送消息后不必等另外Actors回复，也不必暂停，每个Actors有自己的消息队列，进来的消息按先来后到排列，这就有很好的并发策略和可伸缩性，可以建立性能很好的事件驱动系统。 Actor的特征： Ø ActorModel是消息传递模型,基本特征就是消息传递 Ø 消息发送是异步的，非阻塞的 Ø 消息一旦发送成功，不能修改 Ø Actor之间传递时，自己决定决定去检查消息，而不是一直等待，是异步非阻塞的 什么是Akka Akka 是一个用 Scala 编写的库，用于简化编写容错的、高可伸缩性的 Java 和Scala 的 Actor 模型应用，底层实现就是Actor,Akka是一个开发库和运行环境，可以用于构建高并发、分布式、可容错、事件驱动的基于JVM的应用。使构建高并发的分布式应用更加容易。 spark1.6版本之前，spark分布式节点之间的消息传递使用的就是Akka，底层也就是actor实现的。1.6之后使用的netty传输。 12345678910111213141516171819202122class MyActor extends Actor&#123;override def act():Unit=&#123; while(true)&#123; receive &#123; case "hello"=&gt;println("hello") case x:String =&gt; println("save String ="+ x) case x:Int =&gt; println("save Int") case _ =&gt; println("save default") &#125; &#125;&#125;object Actor&#123; def main(args:Array[String]):Unit = &#123; val actor = new MyActor() //启动 actor.start() //发送消息 actor ! "hello" &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445case class Msg(actor)()class WoActor() extends Actor&#123; oevrride def act():Unit =&#123; while(true)&#123; receive &#123; case msg:Msg =&gt;&#123; msg.mes //收到的消息 msg.actor //回复的消息 println("too") msg.actor ! "hahaha" &#125; &#125; &#125; &#125;&#125;class HuActor(wo:WoActor) extends Actor&#123; oevrride def act():Unit =&#123; while(true)&#123; receive &#123; case "hello" =&gt;&#123; println("too") val msg = Msg(this,"rrr") wo ! msg &#125; case "rrr" =&gt;&#123; println("uuu") val msg = Msg(this,"heheh") wo ! msg &#125; &#125; &#125; &#125;&#125;object Actor2&#123; def main(args:Array[String]):Unit = &#123; val woman = new WoActor() val human = new HuActor(woman) human.start() woman.start() human ! "hello" &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243case class Message(actor:Actor,msg:Any)class Actor1 extends Actor&#123; def act()&#123; while(true)&#123; receive&#123; case msg :Message =&gt; &#123; println("i sava msg! = "+ msg.msg) msg.actor!"i love you too !" &#125; case msg :String =&gt; println(msg) case _ =&gt; println("default msg!") &#125; &#125; &#125;&#125;class Actor2(actor :Actor) extends Actor&#123; actor ! Message(this,"i love you !") def act()&#123; while(true)&#123; receive&#123; case msg :String =&gt; &#123; if(msg.equals("i love you too !"))&#123; println(msg) actor! "could we have a date !" &#125; &#125; case _ =&gt; println("default msg!") &#125; &#125; &#125;&#125;object Lesson_Actor2 &#123; def main(args: Array[String]): Unit = &#123; val actor1 = new Actor1() actor1.start() val actor2 = new Actor2(actor1) actor2.start() &#125;&#125; 十一、WordCount1234567891011121314151617181920212223242526272829303132333435363738//在lib文件中添加spark的jar包 ，并addLibimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.rdd.RDDimport org.apache.spark.rdd.RDD.rddToPairRDDFunctionsobject WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("WC") val sc = new SparkContext(conf) //用于了解集群 //RDD:集合 ，抽象的 val lines :RDD[String] = sc.textFile("./words.txt") //方式一： val word :RDD[String] = lines.flatMap&#123;lines =&gt; &#123; lines.split(" ") //匿名函数 &#125;&#125; val pairs:RDD[(String,Int)] = word.map&#123; x =&gt; (x,1) &#125; //map :一进一出 flatmap：一进多出 x：每个单词 val result:RDD[(String,Int)] = pairs.reduceByKey&#123;(a,b)=&gt; &#123; println("a:"+a+",b:"+b) a+b //相当于 a = a + b用于下一个统计 &#125;&#125; //优化：排序 false（降序） 第一个_ 代表每个元素 result.sortBy(_._2,false).foreach(println) //方式二：简化写法 lines.flatMap &#123; _.split(" ")&#125;.map &#123; (_,1)&#125;.reduceByKey(_+_).foreach(println) &#125;&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>静态</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习]]></title>
    <url>%2F2019%2F02%2F14%2FRedis%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Redis的简介1、前情提要磁盘数据库： MySQL（关系型数据库） Hbase 内存数据库： Redis memcached （成本高、性能好、读写速度快、数据安全性低、直接把值放到内存里面，内存数据库就直接把值取到） 2、用作数据库、缓存和消息中间件 二八法则：80%是经常被查询，使用内存数据库做缓存中间件，读取性能高 二、Redis的特点1、数据结构丰富Redis虽然是键值对数据库，但他支持多种类型的数据结构（主要是不同类型的value） 字符串、散列（hashes），列表（lists），集合（sets），有序集合（sorted sets） 2、数据的持久化Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载到内存进行使用 3、数据的备份Redis 支持数据的备份，即 master-slave 模式的数据备份。 三、Redis的安装1、下载安装包redis-3.2.9.tar.gz 网址：http://www.redis.cn/download.html 2、依赖软件安装1yum install gcc tcl -y 3、解压 redis并进入解压目录 1tar -zvxf redis-3.2.9.tar.gz 4、 执行 编译命令（注意：当前路径下需包含 makefile文件 ： 用于手动编译） 1make &amp;&amp; make install 5、修改 redis 的配置文件 redis.config ( 先 备份一个原厂配置文件) 1vim redis.config 修改运行模式为后台运行（如果为no，启动redis-server后，控制台就会卡在启动状态）daemonize守护进程 daemonize yes 指定redis数据持久化的路径(在执行redis-cli命令的当前路径，会生成dump.rdb文件) dir ./ 使用默认的 redis.conf 文件，redis 默认只能通过 127.0.0.1:6379 这个地址访问，这样就只能在本机上操作了 想要远程操作，需要修改redis.conf 这个配置文件，在配置文件中添加相应的 ip 地址， 在bind 127.0.0.1 后面追加 bind 127.0.0.1 192.168.198.128 6、启动server服务（如在redis的解压目录下） 命令： redis-server 配置文件的地址 1redis-server redis-conf 终止服务：（查询redis进程号，然后，kill该进程） 12ps -ef | grep rediskill 进程号 7、启动客户端服务1redis-cli 显示： 127.0.0.1:6379&gt; 四、Redis的使用0、前情提要Redis的key 值是二进制安全的，这意味着可以用任何二进制序列作为 key值。 1、切换数据库（实例） select databaseid 默认共有 16 个实例库， 登录时是 ID 为0 的数据库，总共有 16 个 1select 0 2、Key操作：（前提：是针对已经存在key） KEYS pattern查找所有符合给定模式 pattern 的 key 。 12&gt; keys * &gt; 列出所有Key EXISTS key检查给定 key 是否存在。 12&gt; EXISTS name&gt; 若显示0，则不存在；若显示1，则存在 EXPIRE key seconds为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 12&gt; EXPIRE age 1&gt; 若设置成功，显示1；显示0 ，则为失败，可能是该key不存在 TTL key以秒为单位，返回给定 key 的剩余生存时间 MOVE key db 将当前数据库的 key 移动到给定的数据库 db 当中。如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果 12&gt; move name 1&gt; name 是数据库0中已存在的key，移动到数据库1中后，0中就不存在该key TYPE key 返回 key 所储存的值的类型。 12&gt; DEL key [key ...]&gt; 删除给定的一个或多个 key 。不存在的 key 会被忽略 3、String 操作字符串是一种最基本的 Redis 值类型。Redis 字符串是二进制安全的，这意味着一个 Redis 字符串能包含任意类型的数据 12&gt; SET key value [EX seconds][PX milliseconds] [NX|XX]&gt;  EX 设置过期时间，秒，等同于 SETEX key seconds value PX 设置过期时间，毫秒，等同于 PSETEX key milliseconds value NX 键不存在，才能设置，等同于 SETNX key value XX 键存在时，才能设置 注意 将字符串值 value 关联到 key 。如果 key 已经持有其他值， SET 就覆写旧值，无视类型。对于某个原本带有生存时间（TTL）的键来说， 当 SET 命令成功在这个键上执行时， 这个键原有的 TTL 将被清除。 12&gt; GET key&gt; 返回 key 所关联的字符串值。如果 key 不存在那么返回特殊值 nil 。假如 key 储存的值不是字符串类型，返回一个错误，因为 GET 只能用于处理字符串值 12&gt; APPEND key value&gt; 如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾。如果 key 不存在， APPEND 就简单地将给定 key 设为 value ，就像执行 SET key value 一样。 12&gt; STRLEN key&gt; 返回 key 所储存的字符串值的长度。当 key 储存的不是字符串值时，返回一个错误。 12&gt; INCR key&gt; 将 key 中储存的数字值增一，并显示。如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCR 操作。如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。 错误： (error) ERR value is not an integer or out of range 12&gt; INCRBY key increment&gt; 将 key 所储存的值加上增量 increment ，并显示。如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCRBY 命令。如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。 12&gt; DECR key&gt; 将 key 中储存的数字值减一。 12&gt; DECRBY key decrement&gt; 将 key 所储存的值减去减量 decrement 。 12&gt; GETRANGE key start end&gt; 返回 key 中字符串值的子字符串，字符串的截取范围由 start 和end 两个偏移量决定(包括 start 和 end 在内)。负数偏移量表示从字符串最后开始计数， -1 表示最后一个字符， -2 表示倒数第二个，以此类推 12&gt; SETRANGE key offset value&gt; 用 value 参数覆写(overwrite)给定 key 所储存的字符串值，从偏移量 offset 开始。不存在的 key 当作空白字符串处理。 12&gt; SETEX key seconds value&gt; 将值 value 关联到 key ，并将 key 的生存时间设为 seconds如果 key 已经存在， SETEX 命令将覆写旧值。这个命令类似于以下两个命令： 123&gt; SET key value&gt; EXPIRE key seconds # 设置生存时间&gt; 不同之处是， SETEX 是一个原子性(atomic)操作，关联值和设置生存时间两个动作会在同一时间内完成，该命令在 Redis 用作缓存时，非常实用。 12&gt; SETNX key value&gt; 将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 12&gt; MGET key [key ...]&gt; 返回所有(一个或多个)给定 key 的值。如果给定的 key 里面，有某个 key 不存在，那么这个 key 返回特殊值 nil 。因此，该命令永不失败 12&gt; MSET key value [key value ...]&gt; 同时设置一个或多个 key-value 对。如果某个给定 key 已经存在，那么 MSET 会用新值覆盖原来的旧值，如果这不是你所希望的效果，请考虑使用 MSETNX 命令：它只会在所有给定 key 都不存在的情况下进行设置操作。 12&gt; MSETNX key value [key value ...]&gt; 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。即使只有一个给定 key 已存在， MSETNX 也会拒绝执行所有给定 key 的设置操作。MSETNX 是原子性的，因此它可以用作设置多个不同 key 表示不同字段(field)的唯一性逻辑对象(unique logic object)，所有字段要么全被设置，要么全不被设置。 4、 List 操作 12&gt; LPUSH key value [value ...]&gt; 将一个或多个值 value 插入到列表 key 的表头。（从左边插入）如果有多个 value 值，那么各个 value 值按从左到右的顺序依次 插 入 到 表 头 ： 比 如 说 ， 对 空 列 表 mylist 执 行 命令 LPUSH mylist a b c ，列表的值将是 c b a ， 这等同于原 子 性 地 执行 LPUSH mylist a 、 LPUSH mylist b 和 LPUSH mylistc 三个命令。 12&gt; RPUSH key value [value ...]&gt; 将一个或多个值 value 插入到列表 key 的表尾。（从右边插入）如果有多个 value 值，那么各个 value 值按从左到右的顺序依次 插 入 到 表 尾 ： 比 如 对 一 个 空 列 表 mylist 执行 RPUSH mylist a b c ，得出的结果列表为 a b c ， 等同于 执 行 命令 RPUSH mylist a 、 RPUSH mylist b 、 RPUSH mylist c 。 12&gt; LRANGE key start stop&gt; 返 回 列 表 key 中 指 定 区 间 内 的 元 素 ， 区 间 以 偏 移量 start 和 stop 指定。（从左至右）下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。-1 表示最后一个元素 12&gt; LPOP key&gt; 移除并返回列表 key 的头元素。（从列表key的左边开始弹出元素） 12&gt; RPOP key&gt; 移除并返回列表 key 的尾元素。（从列表key的右边开始弹出元素） 备注： 同向为栈，异向为队列栈（lpush lpop ； rpush rpop） 队列（lpush rpop ； rpush lpop ） ArrayList : 数组（查找快，增删慢）LinkList : 链表（查找慢，增删快） 12&gt; LINDEX key index&gt; 返回列表 key 中，下标为 index 的元素。下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推 12&gt; LLEN key&gt; 返回列表 key 的长度。如果 key 不存在，则 key 被解释为一个空列表，返回 0 . 12&gt; LREM key count value&gt; 根据参数 count 的值，移除列表中与参数 value 相等的元素。count 的值可以是以下几种：count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。count = 0 : 移除表中所有与 value 相等的值。 12&gt; LTRIM key start stop&gt; 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素， 不在指定区间之内的元素都将被删除。举 个 例 子 ， 执 行 命 令 LTRIM list 0 2 ， 表 示 只 保 留 列表 list 的前三个元素，其余元素全部删除 12&gt; RPOPLPUSH source destination&gt; 命令 RPOPLPUSH 在一个原子时间内，执行以下两个动作：将列表 source 中的最后一个元素(尾元素)弹出，并返回给客户端。将 source 弹 出 的 元 素 插 入 到 列 表 destination ， 作为 destination 列表的的头元素。举 个 例 子 ， 你 有 两 个 列表 source 和 destination ， source 列 表 有 元素 a, b, c ， destination 列表有元素 x, y, z ，执行 RPOPLPUSH sourcedestination 之后， source 列表包含元素 a, b ， destination 列表包含元素 c, x, y, z ，并且元素 c 会被返回给客户端。 12&gt; LSET key index value&gt; 将列表 key 下标为 index 的元素的值设置为 value 。当 index 参数超出范围，或对一个空列表( key 不存在)进行 LSET 时，返回一个错误。 12&gt; LINSERT key BEFORE|AFTER pivot value&gt; 将值 value 插入到列表 key 当中，位于值 pivot 之前或之后。当 pivot 不存在于列表 key 时，不执行任何操作。当 key 不存在时， key 被视为空列表，不执行任何操作。 5、Set操作 12&gt; SADD key member [member ...]&gt; 将一个或多个 member 元素加入到集合 key 当中（无序），已经存在于集合的 member 元素将被忽略。假如 key 不存在，则创建一个只包含 member 元素作成员的集合。 12&gt; SMEMBERS key&gt; 返回集合 key 中的所有成员。 不存在的 key 被视为空集合。 12&gt; SISMEMBER key member&gt; 判断 member 元素是否是集合 key 的成员。 12&gt; SCARD key&gt; 返回集合 key 的基数(集合中元素的数量)。 12&gt; SREM key member [member ...]&gt; 移 除 集 合 key 中 的 一 个 或 多 个 member 元 素 ， 不 存 在的 member 元素会被忽略。 12&gt; SPOP key （抽奖场景）&gt; 移除并返回集合中的一个随机元素。如果只想获取一个随机元素，但不想该元素从集合中被移除的话，可以使用 SRANDMEMBER 命令。 12&gt; SMOVE source destination member&gt; 将 member 元素从 source 集合移动到 destination 集合。SMOVE 是原子性操作。如果 source 集合不存在或不包含指定的 member 元素，则 SMOVE 命令不执行任何操作，仅返回 0 。否则， member 元素从 source 集合中被移除，并添加到 destination 集合中去。当 destination 集合已经包含 member 元素时， SMOVE 命令只是简单地将 source 集合中的 member 元素删除。当 source 或 destination 不是集合类型时，返回一个错误。 12&gt; SDIFF key [key ...]&gt; 求差集：从第一个 key 的集合中去除其他集合和自己的交集部分 sdiff求两个set的差集，有先后顺序，以靠前的为基准，列出前一个set有的，而后一个set没有的元素 12&gt; SINTER key [key ...] （微博求共同关注场景）&gt; 返回一个集合的全部成员，该集合是所有给定集合的交集。不存在的 key 被视为空集。当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。 12&gt; SUNION key [key ...]&gt; 返回一个集合的全部成员，该集合是所有给定集合的并集。不存在的 key 被视为空集。 6、Sorted set操作类似 Sets,但是每个字符串元素都关联到一个叫 score 浮动数值。里面的元素总是通过 score 进行着排序，所以不同的是，它是可以检索的一系列元素。 注意： 在 set 基础上，加上 score 值， 之前 set 是key value1 value2….现在 Zset 是 key score1 value1 score2 value2 12&gt; ZADD key score member [[score member][score member] ...]&gt; 将 一 个 或 多 个 member 元 素 及 其 score 值 加 入 到 有 序集 key 当中。 显示 redis&gt; ZADD page_rank 9 baidu.com 8 bing.com 10 google.com(integer) 2redis&gt; ZRANGE page_rank 0 -1 WITHSCORES 1) “bing.com”2) “8”3) “baidu.com”4) “9”5) “google.com”6) “10” 12&gt; ZRANGE key start stop [WITHSCORES]&gt; 返回有序集 key 中，指定区间内的成员。其中成员的位置按 score 值递增(从小到大)来排序。具有相同 score 值的成员按字典序(lexicographical order )来排列。如果你需要成员按 score 值递减(从大到小)来排列，请使用 ZREVRANGE 命令。下标参数 start 和 stop 都以 0 为底， 也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。超出范围的下标并不会引起错误。比 如 说 ， 当 start 的 值 比 有 序 集 的 最 大 下 标 还 要 大 ， 或是 start &gt; stop 时， ZRANGE 命令只是简单地返回一个空列表。另一方面，假如 stop 参数的值比有序集的最大下标还要大，那么Redis 将 stop 当作最大下标来处理。可以通过使用 WITHSCORES 选项，来让成员和它的 score 值一并返回， 返回列表以 value1,score1, …, valueN,scoreN 的格式表示。 12&gt; ZREVRANGE key start stop [WITHSCORES]( ( 音乐排行榜场景) )&gt; 返回有序集 key 中，指定区间内的成员。其中成员的位置按 score 值递减(从大到小)来排列。 12&gt; ZREM key member [member ...]&gt; 移除有序集 key 中的一个或多个成员，不存在的成员将被忽略。 12&gt; ZREMRANGEBYSCORE key min max&gt; 移除有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。 12&gt; ZSCORE key member&gt; 返回有序集 key 中，成员 member 的 score 值。 12&gt; ZCARD key&gt; 返回有序集 key 的基数（包含的元素的个数）。 12&gt; ZCOUNT key min max&gt; 返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。 12&gt; ZRANK key member&gt; 返回有序集 key 中，成员 member 的 score 值。ZCARD key返回有序集 key 的基数（包含的元素的个数）。ZCOUNT key min max返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。ZRANK key member 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。 7、 Hash 操作 （散列）KV 模式不变，但是 V 是一个键值对 12&gt; HSET key field value&gt; 将哈希表 key 中的域 field 的值设为 value 。 12&gt; HGET key field&gt; 返回哈希表 key 中给定域 field 的值。 12&gt; HMSET key field value [field value ...]&gt; 同时将多个 field-value (域-值)对设置到哈希表 key 中。此命令会覆盖哈希表中已存在的域。 12&gt; HMGET key field [field ...]&gt; 返回哈希表 key 中，一个或多个给定域的值。 12&gt; HGETALL key&gt; 返回哈希表 key 中，所有的域和值。 12&gt; HKEYS key&gt; 返回哈希表 key 中的所有域。 12&gt; HVALS key&gt; 返回哈希表 key 中所有域的值。 12&gt; HSETNX key field value&gt; 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在。若域 field 已经存在，该操作无效。 12&gt; HEXISTS key field&gt; 查看哈希表 key 中，给定域 field 是否存在。 12&gt; HDEL key field [field ...]&gt; 删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略。 12&gt; HINCRBY key field increment&gt; 为哈希表 key 中的域 field 的值加上增量 increment 。增量也可以为负数，相当于对给定域进行减法操作。 12&gt; HINCRBYFLOAT key field increment&gt; 增加浮点数场景：用户维度统计统计数包括：关注数、粉丝数、喜欢商品数、发帖数用户为 Key，不同维度为 Field，Value 为统计数 五、Redis 的持久化1、Redis 持久化方式 RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。 2、Rdb:（1）在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的 snapshot 快照，它恢复时就是将快照文件直接读到内存里。 Rdb 保存的是 dump.rdb 文件 （2）如何触发 RDB 快照 Save：save 时只管保存，其他不管，全部阻塞。Bgsave：redis 会在后台进行快照操作，快照操作的同时还可以响应客户端的请求，可以通过 lastsave 命令获取最后一次成功执行快照的时间。 （3）如何停止 静态停止：将配置文件里的 RDB 保存规则改为 save “”动态停止 ： config set save “ ” 3、AOF(Append Only File) （1）以日志的形式来记录每个写操作，将 redis 执行过的所有写指令记录下来(读操作不记录)。只许追加文件但不可以改写文件，redis 启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次一完成数据恢复工作。======APPEND ONLY MODE======开启 aof ：appendonly yes (默认是 no) （2）Aof 策略 Appendfsync 参数： Always ：同步持久化 每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性较好。Everysec： 出厂默认推荐，异步操作，每秒记录，如果一秒宕机，有数据丢失No：从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。 （3）Rewrite 概念：AOF 采用文件追加方式，文件会越来越来大为避免出现此种情况，新增了重写机制，aof 文件的大小超过所设定的阈值时，redis 就会自动将 aof文件的内容压缩，只保留可以恢复数据的最小指令集，可以使用命令bgrewirteaof。 触发机制：redis 会记录上次重写的 aof 的大小，默认的配置当 aof 文件大小为上次 rewrite 后大小的一倍且文件大于 64M 触发。 重写原理：aof 文件持续增长而大时，会 fork 出一条新进程来将文件重写(也就是先写临时文件最后再 rename)，遍历新进程的内存中的数据，每条记录有一条 set 语句，重写 aof 文件的操作，并没有读取旧的的 aof 文件，而是将整个内存的数据库内容用命令的方式重写了一个新的 aof 文件，这点和快照有点类似。 no-appendfsync-on-rewrite no : 重写时是否可以运用 Appendfsync 用默认 no 即可，保证数据安全 auto-aof-rewrite-percentage 倍数 设置基准值auto-aof-rewrite-min-size 设置基准值大小 （4）flushall刷新内存中的数据，即清空数据可通过删除aof文件中该操作的记录来恢复数据rdb文件无法实现数据恢复 （5）aof 特点 aof优点：可用于数据恢复缺点：体积大，速度慢 aof文件体积会越来越大 （6）aof文件优化重写：当文件大小达到一定阈值，启动压缩文件 netstat -anpt查看所有端口的使用情况 4、备份 Redis 数据建议：创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。 六、Redis 主从复制0、前情提要Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(masterserver)的精确复制品 。 1、关于 Redis 复制功能的几个重要方面： Redis 使用异步复制。 一个主服务器可以有多个从服务器。 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器，多个从服务器之间可以构成一个图状结构。 复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次同步， 主服务器也可以继续处理命令请求。不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。 复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT 命令可以交给附属节点去运行。 2、从服务器配置方式一：编辑配置文件 （永久生效） 添加主服务器的IP slaveof 192.168.198.128 6379 方式二：调用 SLAVEOF 命令，输入主服务器的 IP 和端口，然后同步就会开始 前提：主从服务器的redis-server均启动了 （仅适用于当前线程的服务，同步时，会将自己原来的key值数据清空，并且在主服务器页面会显示关于从服务器的日志信息） 127.0.0.1:6379&gt; SLAVEOF 192.168.198.128 6379 若想取消该服务器的主从关系，使用如下命令，且还会保存当前数据 127.0.0.1:6379&gt; SLAVEOF no one 连接远程节点的redis服务（端口默认为6379） redis-cli -h 192.168.198.128 3、只读服务器从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。 只读模式配置 方式一：由 redis.conf 文件中的 slave-read-only 选项控制 slave-read-only yes 方式二：通过 CONFIG SET 命令来开启或关闭这个模式  只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。  另外，对一个从属服务器执行命令 SLAVEOF NO ONE 将使得这个从属服务器关闭复制功能，并从从属服务器转变回主服务器，原来同步所得的数据集不会被丢弃。  利用『 SLAVEOF NO ONE 不会丢弃同步所得数据集』这个特性，可以在主服务器失败的时候，将从属服务器用作新的主服务器，从而实现无间断运行。 4、从服务器相关配置：设置密码： ​ 主服务器： 通过 requirepass 选项设置密码 ​ 从服务器： 方式一： 服务器正在运行，使用客户端输入以下命令：config set masterauth 方式二： 将它加入到配置文件中：masterauth 5、主服务器配置只在有至少 N 个从服务器的情况下，才执行写操作 从 Redis 2.8 开始， 为了保证数据的安全性，可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。 min-slaves-to-write min-slaves-max-lag 至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag 秒， 那么主服务器就会执行客户端请求的写操作。 七、Redis-sentinel( 哨兵)0、前情提要Redis 的 Sentinel 系统用于管理多个 Redis 服务器，该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时，集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 是一个分布式系统，你可以在一个架构中运行多个 Sentinel 进程， 这些进程使用流言协议（gossipprotocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移，以及选择哪个从服务器作为新的主服务器。 1、启动 Sentinel方式一： 对于 redis-sentinel 程序， 可以用以下命令来启动Sentinel 系统： redis-sentinel /path/to/sentinel.conf 方式二： 对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器： redis- - server /path/to/sentinel.conf – sentinel 注意： 启动 Sentinel 实例必须指定相应的配置文件， 系统会使用配置文件来保存 Sentinel 的当前状态， 并在 Sentinel 重启时通过载入配置文件来进行状态还原。如果启动 Sentinel 时没有指定相应的配置文件， 或者指定的配置文件不可写（not writable）， 那么 Sentinel 会拒绝启动。 2 、配置 SentinelRedis 源码中包含了一个名为 sentinel.conf 的文件，这个文件是一个带有详细注释的 Sentinel 配置文件示例。运行一个 Sentinel 所需的最少配置如下所示： 12345sentinel monitor mymaster 192.168.198.128 6379 2 sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1protected-mode no / bind 本机 IP 注意：主服务器无密码时，记得在 sentinel 配置里配上bind 本机 ip ，或者关掉保护模式 protected-mode no 配置解释： 第一行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个Sentinel 同意 （只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行）。 down-after-milliseconds 选项 指定了 Sentinel 认为服务器已经断线所需的毫秒数。 如果服务器在给定的毫秒数之内， 没有返回 Sentinel发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。]]></content>
      <categories>
        <category>内存数据库</category>
      </categories>
      <tags>
        <tag>Linux系统环境</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm学习]]></title>
    <url>%2F2019%2F01%2F29%2FStorm%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Storm简介官网：Storm 1、Storm ：流式计算处理框架 2、 特点： 实时 分布式 高容错 storm常驻内存（7*24h:意味着永久运行，需人为关闭） 在Web UI 界面点击kill 命令kill 1bin/storm kill &lt;topologyName&gt; 数据不经过磁盘，在内存中处理 高可靠性：异常处理、消息可靠性保障机制 可维护性：StormUI图形化监控接口 3、应用（1）流式处理：（异步） ​ 客户端提交数据进行计算，并不会等待计算结果 举例： 逐条处理：ETL（用于数据清洗） 统计分析 例：计算PV、UV、访问热点 以及 某些数据的聚合、 加和、平均等等 客户端提交数据以后，计算完成结果存储到redis 、 hbase 、 MySQL或其他MQ中 客户端并不关心最终结果是多少 （2）实时请求应答服务：（同步） 客户端提交数据后，等待取得计算结果并返回给客户端 Drpc机制 举例： 二、Storm架构（从进程角度） 1、nimbus : (主)接收客户端提交的请求； 资源调度、任务分配、接收jar包 2、supervisor：(从)– 接收nimbus分配的任务、启停worker（当前supervisor上worker数量由配置文件设定） – 默认配置4个work进程 3、worker– 运行具体处理运算组件的进程（每个Worker对应执行一个Topology的子集）– worker任务类型，即spout任务、bolt任务两种– 启动executor（executor即worker JVM进程中的一个java线程，一般默认每个executor负责执行一个task任务） – 与_ack 个数一致 4、zookeeper– 负责主从的通信 – 存储心跳信息、任务信息 – 实现主从的解耦，使nimbus对备份要求不高 三、Storm编程模型 1、Topology（DAG有向无环图的实现） 是对Storm实时计算逻辑的封装，由一系列通过数据流相互关联的Spout、Bolt组成的拓扑结构 生命周期： 此拓扑结构只要启动，就会在集群中一直运行，直到手动将其kill，否则不会终止。 （与MapReduce中Job的区别，MR中Job在计算完成后就会终止） 2、spout（数据源：发送数据） 一般会从指定的数据源读取元祖（Tuple）发送到拓扑（Topology）中 一个Spout可以发送多个Stream （通过OutputFieldDeclare的declare方法声明不同的数据流，发送数据时通过SpoutOutputCollector中的emit方法指定 StreamId将数据发送出去） Spout中的核心方法是nextTuple，该方法会被Storm线程不断调用、主动从数据源拉取数据、再通过emit方法将数据生成元祖（Tuple）发送给之后的Bolt计算 3、bolt（数据处理组件：计算数据，个数不限） 单个Bolt可实现简单的任务或数据流转换 复杂的场景需要多个Bolt分多个步骤完成 一个Bolt可以发送多个Stream （通过OutputFieldDeclare的declare方法声明不同的数据流，发送数据时通过SpoutOutputCollector中的emit方法指定 StreamId将数据发送出去） Bolt中的核心方法是execute，该方法通过接收一个元组数据、实现核心业务逻辑 4、tuple（Stream中最小的数据组成单位） 5、Stream（数据流） 从Spout中传递数据给Bolt、上一个Bolt传递数据给下一个Bolt，这样形成的数据通道叫做Stream Stream声明时需要给其指定Id（默认为Default，实际开发中多使用单一数据流，无需指定StreamId） 6、Stream Grouping– 数据流分组（即数据分发策略） 7、 数据传输– ZMQ​ – ZeroMQ 开源的消息传递框架，并不是一个MessageQueue– Netty​ – Netty是基于NIO的网络框架，更加高效。（之所以Storm 0.9版本之后使用Netty，是因为ZMQ的license和Storm的license不兼容。） 四、Storm安装部署完全分布式环境：zookeeper集群（node00，node01，node02） 1、解压安装 版本：apache-storm-0.10.0.tar.gz 1tar -zvxf apache-storm-0.10.0.tar.gz 2、在storm目录中创建logs目录 1mkdir logs 3、编辑配置文件解压路径/config/storm.yaml 1234567891011storm.zookeeper.servers:- &quot;node00&quot;- &quot;node01&quot;- &quot;node02&quot;storm.local.dir: &quot;/opt/storm&quot;nimbus.host: “node00&quot;supervisor.slots.ports:- 6700- 6701- 6702- 6703 将安装包发送到其他节点 4、启动服务 （1）启动zookeeper‘ 1zkServer.sh start （2）启动nimbus（在node00上：storm安装目录下） 1nohup ./bin/storm nimbus &gt;&gt; ./logs/nimbus.out 2&gt;&amp;1 &amp; nohup:防止被操作系统意外挂起 2&gt;&amp;1：标准错误输出重定向 &amp;：后台运行 （3）启动supervisor（在node01 ，node02上：storm安装目录下） 1./bin/storm supervisor&gt;&gt; ./logs/supervisor.out 2&gt;&amp;1 &amp; （4） 启动Logviewer 1./bin/storm logviewer &amp; UI界面查看： 查看日志：http://node00:8000/log?file=wc-1-1523947796-worker-6703.log （5）查看进程 ：jps 显示： node00： [root@node00 apache-storm-0.10.0]# jps8869 QuorumPeerMain9093 Jps9083 config_value node01 、 node02 [root@node01 apache-storm-0.10.0]# jps7280 config_value7290 Jps7230 QuorumPeerMain 且三个节点的logs目录下也相应的生成了指定的日志文件 5、Storm UI（从浏览器访问，在前台页面上查看详情） （1）启动服务（可在node00节点：storm安装目录） 1./bin/storm ui &gt;&gt; ./logs/ui.out 2&gt;&amp;1 &amp; （2）浏览器访问（Storm UI在哪台节点启动，就访问哪台节点） http://node00:8080 6、运行jar 123456##查看帮助bin/storm -h##查看指定命令的帮助bin/storm help jar##运行jar包bin/storm jar &lt;jar包所在路径&gt; &lt;包名+类名&gt; &lt;参数：topologyName&gt; 五、Storm重点概念详解1、Storm Grouping – 数据流分组（即数据分发策略）• 1. Shuffle Grouping– 随机分组，随机派发stream里面的tuple，保证每个bolt task接收到的tuple数目大致相同。– 轮询，平均分配 • 2. Fields Grouping– 按字段分组，比如，按”user-id”这个字段来分组，那么具有同样”user-id”的 tuple 会被分到相同的Bolt里的一个task， 而不同的”user-id”则可能会被分配到不同的task。 • 3. All Grouping– 广播发送，对于每一个tuple，所有的bolts都会收到 • 4. Global Grouping– 全局分组，把tuple分配给task id最低的task 。 123456789101112131415161718 TopologyBuilder builder = new TopologyBuilder(); builder.setSpout("spout", new MySpout(), 2); // shuffleGrouping其实就是随机往下游去发,不自觉的做到了负载均衡// builder.setBolt("bolt", new MyBolt(),2).shuffleGrouping("spout"); // fieldsGrouping其实就是MapReduce里面理解的Shuffle,根据fields求hash来取模// builder.setBolt("bolt", new MyBolt(), 2).fieldsGrouping("spout", new Fields("session_id")); // 只往一个里面发,往taskId小的那个里面去发送 builder.setBolt("bolt", new MyBolt(), 2).globalGrouping("spout"); // 等于shuffleGrouping// builder.setBolt("bolt", new MyBolt(), 2).noneGrouping("spout"); // 广播下游的所有task都能收到数据 builder.setBolt("bolt", new MyBolt(), 5).allGrouping("spout"); ———————————————————-以上为常用grouping策略————————————————————– • 5. None Grouping– 不分组，这个分组的意思是说stream不关心到底怎样分组。目前这种分组和Shuffle grouping是一样的效果。有一点不同的是storm会把使用none grouping的这个bolt放到这个bolt的订阅者同一个线程里面去执行（未来Storm如果可能的话会这样设计）。 • 6. Direct Grouping（很少用）– 指向型分组， 这是一种比较特别的分组方法，用这种分组意味着消息（tuple）的发送者指定由消息接收者的那个task处理这个消息。只有被声明为 Direct Stream 的消息流可以声明这种分组方法。而且这种消息tuple必须使用 emitDirect 方法来发射。消息处理者可以通过 TopologyContext 来获取处理它的消息的task的id(OutputCollector.emit方法也会返回task的id) • 7. Local or shuffle grouping– 本地或随机分组。如果目标bolt有一个或者多个task与源bolt的task在同一个工作进程中，tuple将会被随机发送给这些同进程中的tasks。否则，和普通的Shuffle Grouping行为一致 • 8.customGrouping– 自定义，相当于mapreduce那里自己去实现一个partition一样。 2、并发机制1、 Worker processes Worker – 进程 一个Topology拓扑会包含一个或多个Worker（每个Worker进程只能从属于一个特定的Topology）、 这些Worker进程会并行跑在集群中不同的服务器上，即一个Topology拓扑其实是由并行运行在Storm集群中多台服务器上的进程所组成 2、Executors (threads) Executor – 线程 Executor是由Worker进程中生成的一个线程 每个Worker进程中会运行拓扑当中的一个或多个Executor线程 一个Executor线程中可以执行一个或多个Task任务（默认每个Executor只执行一个Task任务），但是这些Task任务都是对应着同一个组件（Spout、Bolt）。 3、Task Tasks – 任务 实际执行数据处理的最小单元 每个task即为一个Spout或者一个Bolt Task数量在整个Topology生命周期中保持不变，Executor数量可以变化或手动调整 （默认情况下，Task数量和Executor是相同的，即每个Executor线程中默认运行一个Task任务） 4、 设置Worker进程数1Config.setNumWorkers(int workers) 5、设置Executor线程数123TopologyBuilder.setSpout(String id, IRichSpout spout, Number parallelism_hint)TopologyBuilder.setBolt(String id, IRichBolt bolt, Number parallelism_hint)//其中， parallelism_hint即为executor线程数 6、 设置Task数量123456789ComponentConfigurationDeclarer.setNumTasks(Number val)// 例：共2 worker ， 3 excutor ， 5 taskConfig conf = new Config() ;conf.setNumWorkers(2);//2 个work进程TopologyBuilder topologyBuilder = new TopologyBuilder();topologyBuilder.setSpout("spout", new MySpout(), 1); // 1 个excutor线程 ， 1个 tasktopologyBuilder.setBolt("green-bolt", new GreenBolt(), 2) // 2 个excutor线程 ， .setNumTasks(4) // 4 个task.shuffleGrouping("blue-spout); 7、 Rebalance – 再平衡– 即，动态调整Topology拓扑的Worker进程数量、以及Executor线程数量• 支持两种调整方式：– 1、通过Storm UI– 2、通过Storm CLI 通过Storm CLI动态调整：– 例： 1234bin/storm rebalance mytopology -n 5 -e blue-spout=3 -e yellow-bolt=10##将mytopology拓扑worker进程数量调整为5个##“ blue-spout ” 所使用的线程数量调整为3个##“ yellow-bolt ”所使用的线程数量调整为10个 3、通信机制 – Worker内部的消息传递机制• Worker进程间的数据通信– ZMQ– ZeroMQ 开源的消息传递框架，并不是一个MessageQueue– Netty– Netty是基于NIO的网络框架，更加高效。（之所以Storm 0.9版本之后使用Netty，是因为ZMQ的license和Storm的license不兼容。） • Worker内部的数据通信– Disruptor– 实现了“队列”的功能。– 可以理解为一种事件监听或者消息处理机制，即在队列当中一边由生产者放入消息数据，另一边消费者并行取出消息数据处理。 4、容错机制1、集群节点宕机– Nimbus服务器• 单点故障？– 非Nimbus服务器• 故障时，该节点上所有Task任务都会超时，Nimbus会将这些Task任务重新分配到其他服务器上运行 2、进程挂掉– Worker• 挂掉时，Supervisor会重新启动这个进程。如果启动过程中仍然一直失败，并且无法向Nimbus发送心跳，Nimbus会将该Worker重新分配到其他服务器上 – Supervisor• 无状态（所有的状态信息都存放在Zookeeper中来管理），不影响已经在运行的worker，但是在当前节点worker如果挂掉就无法重启，可以在另一台supervisor节点重启worker• 快速失败（每当遇到任何异常情况，都会自动毁灭） – Nimbus• 无状态（所有的状态信息都存放在Zookeeper中来管理）• 快速失败（每当遇到任何异常情况，都会自动毁灭） 3、消息的完整性– 从Spout中发出的Tuple，以及基于他所产生Tuple（例如上个例子当中Spout发出的句子，以及句子当中单词的tuple等）– 由这些消息就构成了一棵tuple树– 当这棵tuple树发送完成，并且树当中每一条消息都被正确处理，就表明spout发送消息被“完整处理”，即消息的完整性 4、 Acker – 消息完整性的实现机制– Storm的拓扑当中特殊的一些任务– 负责跟踪每个Spout发出的Tuple的DAG（有向无环图） 5、与MapReduce的区别 Storm MapReduce 流式处理 批处理 （毫）秒级 分钟级 DAG模型 Map+Reduce模型 常驻内存运行 反复启停 进程、线程常驻内存运行，数据不进入磁盘，数据通过网络传递 为TB、PB级别数据设计的批处理计算框架 6、和Spark Streaming的区别 Storm Spark Streaming 流式处理 微批处理 （毫）秒级 秒级 成熟稳定 稳定性改进中 独立系统，专为流式计算设计 Spark核心的一种计算模型能与其他组件很好的结合 数据传输模式更为简单，很多地方也更为高效 将RDD做的很小来用小的批处理来接近流式处理 并不是不能做批处理，它也可以来做微批处理，来提高吞吐 基于内存和DAG 小记 1、storm源码中包含后缀名为.clj的文件，这是一种Clojure编程语言，它是一种运行在JVM上的Lisp方言。而Lisp是一种以表达性和功能强大著称的编程语言。 2、阿里巴巴在Storm的基础上使用Java代码并做了相关的改进，开发了JStorm，和Storm一样都是开源的。（反哺行为，包括将Flink→Blink） 7、Storm 架构设计与Hadoop架构对比 Storm Hadoop 主节点 Nimbus ResourceManager 从节点 Supervisor NodeManager 应用程序 Topology Job 工作进程 Child Worker 计算模型 Map/Reduce Spout/Bolt 六、Storm API （数据累加）MyTopology.class 123456789101112131415161718192021import backtype.storm.Config;import backtype.storm.LocalCluster;import backtype.storm.generated.StormTopology;import backtype.storm.topology.TopologyBuilder;import java.util.HashMap;public class MyTopology &#123; public static void main(String[] args) &#123; //数据累加... spout bolt TopologyBuilder topologyBuilder = new TopologyBuilder(); topologyBuilder.setSpout("myspout",new MySpout()); //shuffleGrouping（）表示将前一个bolt和后一个spout连接 topologyBuilder.setBolt("mybolt",new MyBolt()).shuffleGrouping("myspout"); StormTopology topology = topologyBuilder.createTopology(); Config config = new Config(); LocalCluster localCluster = new LocalCluster(); localCluster.submitTopology("sum",config,topology); &#125;&#125; MySpout.class 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import backtype.storm.spout.SpoutOutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IRichSpout;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichSpout;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import java.util.Map;public class MySpout extends BaseRichSpout &#123; SpoutOutputCollector collector; int i =0; /** * 初始化方法。。框架在执行任务的时候，会先执行此方法 * @param conf 可以得到spout的配置 * @param context 上下文环境 * @param collector 往下游发送数据... */ @Override public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; /*** * 此方法是spout的核心方法。 * 框架会一直（无限）调用这个方法，每当调用此方法时，我们应该往下游发送数据 * * mysout = new Myspout() * * mysout.open(conf,context,collector) * * * while(ture)&#123; * mysout.nextTuple() * &#125; */ @Override public void nextTuple() &#123; i++; /*与下面的“number”对应 *若new Values(i，"zs") *那么就是"number","name" */ collector.emit(new Values(i)); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("spout 发送.." + i); &#125; /** * 当需要往下游发送数据时，就要声明字段个数和字段名字。 * @param declarer */ @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("number")); &#125;&#125; MyBolt.class 12345678910111213141516171819202122232425262728293031323334353637383940import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IRichBolt;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Tuple;import java.util.Map;public class MyBolt extends BaseRichBolt &#123; int sum; /** * bolt 初始化方法。。 * @param stormConf * @param context * @param collector */ @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; &#125; /** * bolt 中 最核心的方法 * 框架会一直调用此方法，每次调用就传一个数据进来。 * @param input */ @Override public void execute(Tuple input) &#123; Integer integer = input.getInteger(0);// input.getIntegerByField("number"); sum +=integer; System.out.println("excute : " + integer + " sum : " + sum); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 七、Storm API （单词统计） WordCountToplogy.class 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import backtype.storm.Config;import backtype.storm.LocalCluster;import backtype.storm.StormSubmitter;import backtype.storm.generated.AlreadyAliveException;import backtype.storm.generated.InvalidTopologyException;import backtype.storm.topology.TopologyBuilder;import backtype.storm.tuple.Fields;public class WordCountToplogy &#123; /** * 一对一 线程与task * thread0 = new Thread(new bolt0) * thread0.start() * run()&#123; * while(true)&#123; * bolt0.excute(tuple) * &#125; * &#125; * thread1 = new Thread ( new bolt1) * * run()&#123; * while(true)&#123; * bolt1.excute(tuple) * &#125; * &#125; * new Thread (new bolt2) * @param args */ public static void main(String[] args) &#123; TopologyBuilder topologyBuilder = new TopologyBuilder(); topologyBuilder.setSpout("wcspout",new WordCountSpout());//5 在这里指并行度，即task个数//shuffleGrouping() 将bolt 连接至指定的spout之后 topologyBuilder.setBolt("splitbolt", new SplitBolt(),5).shuffleGrouping("wcspout");//并行度 6 //fieldsGrouping() 将bolt 连接至指定 的bolt之后，按指定字段grouping topologyBuilder.setBolt("countbolt", new CountBolt(),6).fieldsGrouping("splitbolt",new Fields("word")); Config config = new Config(); config.setNumWorkers(3); if (args.length &gt; 0) &#123; try &#123; StormSubmitter.submitTopology(args[0], config, topologyBuilder.createTopology()); &#125; catch (AlreadyAliveException e) &#123; e.printStackTrace(); &#125; catch (InvalidTopologyException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster localCluster = new LocalCluster(); localCluster.submitTopology("mytopology", config, topologyBuilder.createTopology()); &#125; &#125;&#125; WordCountSpout.class 1234567891011121314151617181920212223242526272829303132333435363738394041import backtype.storm.spout.SpoutOutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichSpout;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import java.util.Map;import java.util.Random;public class WordCountSpout extends BaseRichSpout &#123; SpoutOutputCollector collector; String[] lines = new String[]&#123; "i love learning", "i miss you ", "sxt is good", "good good study day day up" &#125;; Random random = new Random(); @Override public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; @Override public void nextTuple() &#123; int index = random.nextInt(lines.length); String line = lines[index]; collector.emit(new Values(line)); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("line")); &#125;&#125; SplitBolt.class 1234567891011121314151617181920212223242526272829303132import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Tuple;import backtype.storm.tuple.Values;import java.util.Map;public class SplitBolt extends BaseRichBolt &#123; OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; System.err.println("split ----- " + this); &#125; @Override public void execute(Tuple input) &#123; String line = input.getString(0); String[] words = line.split(" "); for(String word : words)&#123; collector.emit(new Values(word)); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word")); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Tuple;import java.util.HashMap;import java.util.Map;public class CountBolt extends BaseRichBolt &#123; Map&lt;String,Integer&gt; resultMap = new HashMap&lt;&gt;(); @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; System.err.println("countbolt ---- " + this); &#125; @Override public void execute(Tuple input) &#123; String word = input.getStringByField("word"); if(resultMap.containsKey(word))&#123; Integer integer = resultMap.get(word); integer++; resultMap.put(word,integer); &#125;else&#123; resultMap.put(word,1); &#125; System.err.println(this + " -- " + word); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125;]]></content>
      <categories>
        <category>分布式框架</category>
      </categories>
      <tags>
        <tag>Storm，流式处理框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka学习]]></title>
    <url>%2F2019%2F01%2F28%2FKafka%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Kafka简介Kafka是一个高吞吐量、低延迟分布式的消息队列系统。特点：每秒钟可以处理几十万条消息，他的低延迟最低只有几毫秒。官网：https://kafka.apache.org/ 底层使用Scala语言实现。 注意： 1、A streaming platform has three key capabilities: Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system. Store streams of records in a fault-tolerant durable way. Process streams of records as they occur. 2、Kafka is generally used for two broad classes of applications: Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data 3、First a few concepts: Kafka is run as a cluster on one or more servers that can span multiple datacenters. The Kafka cluster stores streams of records in categories called topics. Each record consists of a key, a value, and a timestamp. 4、Kafka has four core APIs: The Producer API allows an application to publish a stream of records to one or more Kafka topics. The Consumer API allows an application to subscribe to one or more topics and process the stream of records produced to them. The Streams API allows an application to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams. The Connector API allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table. 5、其他 Kafka Cluster 中有多个Broker服务器，每个类型的消息被定义为topic` 同一个topic内部的消息按照一定的key 和算法被（partition）分区存储到不同的Broker上 Producer 和consumer 可以在不同的Broker上生产或消费topic 6、概念理解 Topics and Logs： Topic 即为每条发布到 Kafka 集群的消息都有一个类别，topic在 Kafka 中可以由多个消费者订阅、消费。 每个 topic 包含一个或多个 partition（分区），partition 数量可以在创建 topic 时指定，每个分区日志中记录了该分区的数据以及索引信息。 Kafka 只保证一个分区内的消息有序，不能保证一个主题的不同分区之间的消息有序。为一个主题分配一个分区，才能保证所有消息绝对有序。 分区会给每个消息记录分配一个顺序 ID 号（偏移量）， 能够唯一地标识该分区中的每个记录。Kafka 集群保留所有发布的记录，不管这个记录有没有被消费过，Kafka 提供相应策略通过配置从而对旧数据处理。 每个消费者唯一保存的元数据信息就是消费者当前消费日志的位移位置。位移位置是由消费者控制，消费者可以通过修改偏移量读取任何位置的数据。 Producers – 生产者指定 topic 来发送消息到 Kafka Broker Consumers – 消费者根据 topic 消费相应的消息 Topic – 消息主题（类型） 一个 topic 可以有多个 partition，分布在不同的 broker server 上 7、注意： consumer自己维护消费消息的offset 每一个consumer都有对应的group group内是queue消费模型 每个consumer消费不同的partition 一个消息被一个group消费一次 group间是publish—subscribe消费模型 每个group独立消费，互补影响 一个消息被各个group消费一次 8、Kafka使用场景（允许数据丢失） 日志收集：收集各log ， 开放给各个consumer ， 如hbase， hadoop ， solr 消息系统： 群发消息 用户活动跟踪： 记录用户行为发布到topic中，提供给consumer做实时监控分析，或装载到hadoop，数仓中做离线分析 运营指标 ： 记录运营监控数据 流式处理 ： SparkStreaming ， storm 二、Kafka集群的部署和安装1、集群规划：zookeeper ： 三台（Kafka是分布式消息队列 ， 依赖zookeeper） kafka : 三台 node1、node2、node3 2、安装安装zookeeper （详见zookeeper学习.md） 安装Kafka 下载压缩包（官网地址：http://kafka.apache.org/downloads.html） 解压：1tar -zxvf kafka_2.10-0.9.0.1.tgz 修改配置文件：config/server.properties 12345## broker.id broker集群中唯一标识id，0、1、2、3 依次增长（broker即 Kafka 集群中的一台服务器）## 注：当前Kafka 集群共三台节点，分别为：node1、node2、node3。对应的 broker.id 分别为 0、1、2。 broker.id=0 ## zookeeper.connect: zk 集群地址列表 zookeeper.connect=node1:2181、node2:2181、node3:2181 将当前 node1 服务器上的 Kafka 目录同步到其他 node2、node3 服务器上。 3、启动kafka集群A、启动 Zookeeper 集群。 1zkServer.sh start B、启动 Kafka 集群。分别在三台服务器上执行以下命令启动： 1bin/kafka-server-start.sh config/server.properties 4、测试创建话题（kafka-topics.sh –help 查看帮助手册） 1、创建 topic： 123456bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 2 --partitions 3 --topic test#参数说明#--replication-factor ：指定每个分区的复制因子个数，默认 1 个## 副本有主从之分 ， 且副本分别放在不同的broker节点上#--partitions ：指定当前创建的 kafka 分区数量，默认为 1 个#--topic ：指定新建 topic 的名称 2、查看 topic 列表： 1bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --list 3、查看“test”topic 描述： 1bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --describe --topic test –Isr （ in_synchronized_replication ）: 代表数据同步的节点 4、创建消费者： 1bin/kafka-console-producer.sh --broker-list node1:9092,node2:9092,node3:9092 --topic test 然后，在当前节点的控制台输入任何内容，表作为生产的topic 5、创建消费者：（另选一台节点） 1bin/kafka-console-consumer.sh --zookeeper node1:2181,node2:2181,node3:2181 --from-beginning --topic test 此时，在控制台会打印出消费的topic 消费的消息的offset存放在zookeeper中，使用get + 路径 命令 获取对应分区的offset 注：查看帮助手册： 1bin/kafka-console-consumer.sh help 三、 Flume &amp; &amp; Kafka的结合1、Flume 安装（详见Flume学习.md） 2、Flume + KafkaA、启动 Kafka 集群。 1bin/kafka-server-start.sh config/server.properties B、配置 Flume 集群，并启动 Flume 集群。 1bin/flume-ng agent -n a1 -c conf -f conf/fk.conf -Dflume.root.logger=DEBUG,console Flume 配置文件 fk.conf 内容如下： 1234567891011121314151617181920212223242526a1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = node1a1.sources.r1.port = 41414# Describe the sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.topic = testflumea1.sinks.k1.brokerList = node1:9092,node2:9092,node3:9092a1.sinks.k1.requiredAcks = 1a1.sinks.k1.batchSize = 20a1.sinks.k1.channel = c1# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000000a1.channels.c1.transactionCapacity = 10000# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 3 、测试 分别启动 Zookeeper、Kafka、Flume 集群。 创建 topic： 1bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 --create --replication-factor 2 --partitions 3 --topic testflume 启动消费者： 1bin/kafka-console-consumer.sh --zookeeper node1:2181,node2:2181,node3:2181 --from-beginning --topic testflume 运行“RpcClientDemo”代码，通过 rpc 请求发送数据到 Flume 集群。Flume 中 source 类型为 AVRO 类型，此时通过 Java 发送 rpc 请求，测试数据是否传入 Kafka 其中，Java 发送 Rpc 请求 Flume 代码示例如下：（参考 Flume 官方文档：http://flume.apache.org/FlumeDeveloperGuide.html） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.api.RpcClient;import org.apache.flume.api.RpcClientFactory;import org.apache.flume.event.EventBuilder;import java.nio.charset.Charset;/*** Flume官网案例* http://flume.apache.org/FlumeDeveloperGuide.html* @author root*/public class RpcClientDemo &#123;public static void main(String[] args) &#123;MyRpcClientFacade client = new MyRpcClientFacade();// Initialize client with the remote Flume agent's host and portclient.init("node1", 41414);// Send 10 events to the remote Flume agent. That agent should be// configured to listen with an AvroSource.String sampleData = "Hello Flume!";for (int i = 0; i &lt; 10; i++) &#123;client.sendDataToFlume(sampleData);System.out.println("发送数据：" + sampleData);&#125;client.cleanUp();&#125;&#125;class MyRpcClientFacade &#123;private RpcClient client;private String hostname;private int port;public void init(String hostname, int port) &#123;// Setup the RPC connectionthis.hostname = hostname;this.port = port;this.client = RpcClientFactory.getDefaultInstance(hostname,port);// Use the following method to create a thrift client (instead of the// above line):// this.client = RpcClientFactory.getThriftInstance(hostname,port);&#125;public void sendDataToFlume(String data) &#123;// Create a Flume Event object that encapsulates the sample dataEvent event = EventBuilder.withBody(data,Charset.forName("UTF-8"));// Send the eventtry &#123;client.append(event);&#125; catch (EventDeliveryException e) &#123;// clean up and recreate the clientclient.close();client = null;client = RpcClientFactory.getDefaultInstance(hostname,port);// Use the following method to create a thrift client (instead of// the above line):// this.client =RpcClientFactory.getThriftInstance(hostname, port);&#125;&#125;public void cleanUp() &#123;// Close the RPC connectionclient.close();&#125;&#125; 四、Kafka数据丢失问题和重复消费问题1、为什么会丢失？Kafka ， 高吞吐 ， 一次能处理几十万条数据， （1）生产数据时： 因为服务器（生产者）发送数据给Kafka后，kafka 将数据写入内存后，就直接返回操作成功的消息（ack机制 : 1（默认值）而ack机制 : 0 时，不用管是否操作成功，就发第二条），然后再发第二条，避免的磁盘I/O带来的延迟，可是，这样不安全，万一此时该节点宕机，数据就丢失了。 而为了解决数据丢失，可以在数据写入内存时，备份到其他节点,再返回操作成功的消息（ack机制 : -1）。 （2）消费数据时： Client消费数据过程中，（频率很短）先更新了消费offset， 再处理数据（如100），结果宕机，那么重启后就会从下一个offset（如101）开始消费消息，那么100这条数据就丢失了。 解决方案：关闭自动提交 ， 改为 ， 手动提交，保证数据处理完毕后再提交消费offset。但是，解决了数据丢失，提高了性能消耗 2、数据重复消费问题因为Client（消费者）设置定时（频率很长）向zookeeper更新消费消息的offset，（如100 ， 120） 如果在没达到定的时间（如120），client就宕机了，重启后会重新去zookeeper上查询offset， 那么在定的时间之前的消息offset（100到120之间）就不存在，Client就会重新（从100）开始消费，就造成了重复消费问题。 解决方案：关闭自动定时提交 ， 改为 ， 手动提交，保证数据处理完毕后再提交消费offset。但是，解决重复消费，提高了性能消耗。 3、注意使用解决方案时，要注意业务的要求，是否能允许数据丢失和重复消费问题 4、API​ high level api​ 简单，不灵活​ simple api​ 复杂，但灵活]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>消息队列系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch学习]]></title>
    <url>%2F2019%2F01%2F25%2FElasticsearch%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Elasticsearch是什么https://ideas.spkcn.com/software/os/windows/687.html 1、简介 一个基于Lucene的、实时的、分布式搜索和分析引擎 应用于云计算中。 实时搜索、稳定、可靠、快速 安装方便 基于Restful接口 2、和Lucene的关系 Lucene 是一个库。必须使用Java开发。工作原理复杂 Elasticsearch使用Java开发，以Lucene为核心实现索引和搜索功能。通过简单的Restful API隐藏Lucene的复杂性，简化了全文搜索。 3、和SOLR对比 热度逐渐远高于solr 平均查询速度快于solr倍 ES的优势： a）Elasticsearch是分布式的。不需要其他组件，分发是实时的，被叫做”Push replication”。 b）Elasticsearch 完全支持 Apache Lucene 的接近实时的搜索。 处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级设置。 c）Elasticsearch 采用 Gateway 的概念，使得备份更加简单。 d）各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。 4、与关系型数据库对比 结构相似 database（数据库） index（索引库） table（表） type（类型） row（行） document（文档） column（列） field（字段） 一个ES集群可以有多个索引库。每个索引库包含很多种类型，类型中又包含了很多文档，每个文档又包含很多字段 传统数据库为特定列增加一个索引。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的 倒排索引： 源于实际应用中需要根据属性的值来查找记录。 种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。 不是由记录来确定属性值，而是由属性值来确定记录的位置 ​ 二、安装与部署环境要求：JDK版本为1.7及以上 下载位置：系列产品 1、在安装目录下的config 目录下：编辑elasticsearch.yml文件 编辑内容： (注意要顶格写，冒号后面要加一个空格) 12345678a) Cluster.name: shsxt (同一集群要一样)b) Node.name：node-1 (同一集群要不一样)c) Network.Host: 192.168.1.194 (这里不能写127.0.0.1)d) 防止脑裂的配置discovery.zen.ping.multicast.enabled: falsediscovery.zen.ping_timeout: 120sclient.transport.ping_timeout: 60sdiscovery.zen.ping.unicast.hosts: [&quot;192.168.1.191&quot;,&quot;192.168.1.192&quot;, &quot;192.168.1.193&quot;] 然后，将安装包发送到其他节点，再根据所在节点，进行相应的配置 2、启动 （开几台起几台） ES_HOME/bin/elasticsearch （ctrl + C 结束服务） ES_HOME/bin/elasticsearch -d(后台运行) （前提页面结束服务或者kill 杀死进程） 启动权限问题 1、启动后会报错，说不能在root用户下执行，所以我们需要添加新用户来执行启动 （分别在3台节点上） 12345678useradd espasswd（设置密码）chown —R es:es filename(路径/ES的解压文件)（修改ES安装文件的权限为用户es）su es（切换用户为es）再执行启动命令 显示：（在启动的所有节点上都会显示出所选举的master节点，此处为node0） 12019-01-26 03:22:40,594cluster.service detected_master &#123;node0&#125;&#123;GOmO6SISRHexhlbqcddx9w&#125;&#123;192.168.198.128&#125;&#123;192.168.198.128:9300&#125;, added &#123;&#123;node0&#125;&#123;GOmO6SISRHexhlbqcddx9w&#125;&#123;192.168.198.128&#125;&#123;192.168.198.128:9300&#125;,&#123;node2&#125;&#123;blKR0BxaQ92QjmPuMlPaRw&#125;&#123;192.168.198.131&#125;&#123;192.168.198.131:9300&#125;,&#125;, reason: zen-disco-receive(from master [&#123;node0&#125;&#123;GOmO6SISRHexhlbqcddx9w&#125;&#123;192.168.198.128&#125;&#123;192.168.198.128:9300&#125;]) 4.访问 1浏览器访问 http://localhost:9200 注意 9200 : 是HTTP协议所访问的端口，即从浏览器端访问的port 9300 ：是Java API访问端口 三、REST风格1、简介表现层状态转换（英语：Representational State Transfer，缩写：REST） 12345一种万维网软件架构风格，目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。表现层状态转换是根基于超文本传输协议(HTTP)之上而确定的一组约束和属性，是一种设计提供万维网络服务的软件构建风格。匹配或兼容于这种架构风格(简称为 REST 或 RESTful)的网络服务，允许客户端发出以统一资源标识符访问和操作网络资源的请求，而与预先定义好的无状态操作集一致化。因此表现层状态转换提供了在互联网络的计算系统之间，彼此资源可交互使用的协作性质(interoperability)。相对于其它种类的网络服务，例如 SOAP服务则是以本身所定义的操作集，来访问网络上的资源。 要点： 1234567需要注意的是，REST是设计风格而不是标准。REST通常基于使用HTTP，URI，和XML以及HTML这些现有的广泛流行的协议和标准。* 资源是由URI来指定。* 对资源的操作包括获取、创建、修改和删除资源，这些操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法。* 通过操作资源的表现形式来操作资源。* 资源的表现形式则是XML或者HTML，取决于读者是机器还是人，是消费web服务的客户软件还是web浏览器。当然也可以是任何其他的格式，例如JSON。 URI ： 统一资源标识符 URL ： 全球资源定位器 2、Rest操作REST的操作分为以下几种： – GET：获取对象的当前状态； – PUT：改变对象的状态； – POST：创建对象； – DELETE：删除对象； – HEAD：获取头信息。 3、ES内置的REST接口 四、CURL命令-X 指定http请求的方法 -HEAD GET POST PUT DELETE -d 指定要传输的数据 1、索引库的创建与删除创建索引库：（PUT/POST都可以） 1curl -XPUT http://192.168.198.128:9200/sukie/ 显示：（成功了） [root@node00 ~]# curl -XPUT http://192.168.198.128:9200/sukie/{“acknowledged”:true}[root@node00 ~]# 删除索引库： 1curl -XDELETE http://192.168.198.128:9200/sukie/ 2、创建document123456789(注意格式：JSON （英文状态下）) curl -XPUT http://192.168.198.128:9200/sukie/employee/2?pretty -d &apos;&#123; &quot;first_name&quot; : &quot;john&quot;, &quot;last_name&quot; : &quot;smith&quot;, &quot;age&quot; : 25, &quot;love&quot; : &quot;I love to go rock climbing&quot;, &quot;address&quot;: &quot;shanghai&quot;&#125;&apos; ==employee== ： 在此处为type（类型） ==1== ： 在此处为id ==pretty==： 表示以良好格式显示结果 显示： 1234567891011121314151617181920212223&gt; [root@node00 ~]# curl -XPUT http://192.168.198.128:9200/sukie/employee/2?pretty -d &apos;&#123; &gt; &gt; &quot;first_name&quot; : &quot;john&quot;, &gt; &quot;last_name&quot; : &quot;smith&quot;, &gt; &quot;age&quot; : 25, &gt; &quot;love&quot; : &quot;I love to go rock climbing&quot;, &gt; &quot;address&quot;: &quot;shanghai&quot;&gt; &#125;&apos;&gt; &gt; &#123;&gt; &quot;_index&quot; : &quot;sukie&quot;,&gt; &quot;_type&quot; : &quot;employee&quot;,&gt; &quot;_id&quot; : &quot;2&quot;,&gt; &quot;_version&quot; : 1,&gt; &quot;_shards&quot; : &#123;&gt; &quot;total&quot; : 2,&gt; &quot;successful&quot; : 2,&gt; &quot;failed&quot; : 0&gt; &#125;,&gt; &quot;created&quot; : true&gt; &#125;&gt; [root@node00 ~]# &gt; 123456curl -XPOST http://192.168.198.128:9200/sukie/employee -d &apos;&#123; &quot;first_name&quot; : &quot;john&quot;, &quot;last_name&quot; : &quot;smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;&#125;&apos; ==未指定id== 如：（必须使用POST） 12命令：curl -XPOST http://localhost:9200/sukie/employee -d &apos;&#123;&quot;first_name&quot; : &quot;John&quot;&#125;&apos; 若：（使用PUT会报错） 123命令：curl -XPUT http://localhost:9200/sukie/employee -d &apos;&#123;&quot;first_name&quot; : &quot;John&quot;&#125;&apos; 会报错 创建索引注意事项 索引库名称必须要全部小写，不能以下划线开头，也不能包含逗号 如果没有明确指定索引数据的ID，那么es会自动生成一个随机的ID，这时需要使用POST方式，PUT方式会出错 3、更新document12345678curl -XPUT http://192.168.198.128:9200/sukie/employee/2 -d &apos;&#123; &quot;first_name&quot; : &quot;god bin&quot;, &quot;last_name&quot; : &quot;pang&quot;, &quot;age&quot; : 38, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;address&quot;: &quot;shanghai&quot;&#125;&apos; 1234567curl -XPOST http://192.168.198.128:9200/sukie/employee?pretty -d &apos;&#123; &quot;first_name&quot; : &quot;john&quot;, &quot;last_name&quot; : &quot;smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;&#125;&apos; 显示： 12345678910111213141516171819[root@node00 ~]# curl -XPOST http://192.168.198.128:9200/sukie/employee?pretty -d &apos;&#123; &gt; &quot;first_name&quot; : &quot;john&quot;, &gt; &quot;last_name&quot; : &quot;smith&quot;, &gt; &quot;age&quot; : 25, &gt; &quot;about&quot; : &quot;I love to go rock climbing&quot;&gt; &#125;&apos;&#123; &quot;_index&quot; : &quot;sukie&quot;, &quot;_type&quot; : &quot;employee&quot;, &quot;_id&quot; : &quot;AWiFmf347KNgqTe_uJ4-&quot;, ==自动生成的id== &quot;_version&quot; : 1, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;failed&quot; : 0 &#125;, &quot;created&quot; : true&#125;[root@node00 ~]# put ： 必须指定id ，若id存在，这更新数据（全局更新）；若id不存在，则新增数据 1curl -XPUT http://localhost:9200/sukie/employee/1 -d &apos;&#123;&quot;city&quot;:&quot;beijing&quot;,&quot;car&quot;:&quot;BMW&quot;&#125;&apos; 注意;执行更新操作时： – ES首先将旧的文档标记为删除状态 – 然后添加新的文档 – 旧的文档不会立即消失，但是你也无法访问 – ES会在你继续添加更多数据的时候在后台清理已经标记为删除状态的文档 post ： id若不指定，会自动生成随机的id ​ id若指定，就实现局部更新操作（添加新字段或更新已有字段） 1234567 curl -XPOST http://localhost:9200/sukie/employee/1/_update -d &apos;&#123;&quot;doc&quot;:&#123;&quot;city&quot;:&quot;beijing&quot;, “sex”:”male”&#125;&#125;&apos; （同一个索引库，会默认创建5个分片，用以实现分布式搜索； 每个分片均另外还有一个副本分布在不同的另一个节点上，用以提高可靠性和查询速率） ==注意== 同一个索引库中不同的文档之间若用相同的字段，则这个字段的数据类型必须是一致的； 字段的数据类型是由第一次推送数据是确定 4、普通查询索引12345678910111213– 根据员工id查询 curl -XGET http://localhost:9200/sukie/employee/1?pretty– 在任意的查询字符串中添加pretty参数，es可以得到易于识别的json结果。 – curl后添加-i 参数，这样你就能得到反馈头文件curl -i XGET http://localhost:9200/sukie/employee/1?pretty– 检索文档中的一部分，如果只需要显示指定字段curl -XGET http://localhost:9200/sukie/employee/1?_source=name,age 如果只需要source的数据 curl -XGET http://localhost:9200/sukie/employee/1/_source?pretty– 查询所有curl -XGET http://localhost:9200/sukie/employee/_search?pretty – 根据条件进行查询 curl -XGET http://localhost:9200/sukie/employee/_search?q=last_name:smith 5.DSL查询DSL查询 •Domain Specific Language – 领域特定语言 123456curl -XGET http://localhost:9200/shsxt/employee/_search?pretty -d &apos;&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;&quot;last_name&quot;:&quot;smith&quot;&#125;&#125;&#125;&apos; #对多个field发起查询：multi_match 12345678910curl -XGET http://localhost:9200/shsxt/employee/_search?pretty -d &apos;&#123; &quot;query&quot;: &#123;&quot;multi_match&quot;: &#123; &quot;query&quot;:&quot;bin&quot;, &quot;fields&quot;:[&quot;last_name&quot;,&quot;first_name&quot;] &#125; &#125;&#125;&apos; 复合查询，must，must_not, should must： AND must_not：NOT should：OR 12345678910111213141516curl -XGET http://192.168.78.101:9200/shsxt/employee/_search?pretty -d &apos;&#123; &quot;query&quot;: &#123;&quot;bool&quot; : &#123; &quot;must&quot; : &#123;&quot;match&quot;: &#123;&quot;first_name&quot;:&quot;bin&quot;&#125; &#125;, &quot;must&quot; : &#123;&quot;match&quot;: &#123;&quot;age&quot;:37&#125; &#125; &#125; &#125;&#125;&apos; 查询first_name=bin的，并且年龄不在20岁到30岁之间的 123456789101112131415161718curl -XGET http://192.168.78.101:9200/shsxt/employee/_search -d &apos;&#123; &quot;query&quot;: &#123;&quot;bool&quot; : &#123; &quot;must&quot; : &#123;&quot;term&quot; : &#123; &quot;first_name&quot; : &quot;bin&quot; &#125; &#125; , &quot;must&quot; : &#123;&quot;range&quot;: &#123;&quot;age&quot; : &#123; &quot;from&quot; : 30, &quot;to&quot; : 40 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 6.删除索引1curl -XDELETE http://localhost:9200/shsxt/employee/1?pretty • 如果文档存在，es会返回200 ok的状态码，found属性值为 true，_version属性的值+1 • found属性值为false，但是_version属性的值依然会+1，这个就是内部管理的一部分，它保证了我们在多个节点间的不同操作的顺序都被正确标记了 • 注意：删除一个文档也不会立即生效，它只是被标记成已删除。 Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理 五、Elasticsearch插件安装1、head（至少一台） 方式一： 在bin目录下执行 1./plugin install mobz/elasticsearch-head 来安装head插件 方式二： 使用elasticsearch-head-master.zip文件安装 在bin目录下执行 1./plugin install file:/usr/soft/elasticsearch-head-master.zip 来安装head插件 方式三： 将elasticsearch-head-master.zip挤压解压安装后的包拷贝到elasticsearch安装目录的plugins目录下 安装后启动elasticsearch（至少2台） 访问http://ip:9200/_plugin/head 2.Kibana（1台） 它是一个基于浏览器页面的ES前端展示工具，是为ES提供日志分析的web接口，可用它对日志进行高效的搜索、可视化、分析等操作。 解压安装,然后修改配置文件config/kibana.yml的elasticsearch.url属性即可 3、Marvel Marvel插件可以帮助使用者监控elasticsearch的运行状态，不过这个插件需要license。安装完license后可以安装marvel的agent，agent会收集elasticsearch的运行状态 Step 1: Install Marvel into Elasticsearch:(3台es都装) Es_home/bin/plugin install license Es_home/bin/plugin install marvel-agent （注意：Es_home/plugins 目录的权限问题，是否为当前用户的） Step 2: Install Marvel into Kibana(在kibana机器上安) Kibana_home/bin/kibana plugin –install elasticsearch/marvel/latest Step 3: Start Elasticsearch and Kibana bin/elasticsearch bin/kibana Step 4: 浏览器访问 http://node00:5601/app/marvel ==注意：多台节点的时间同步== 4、分词器安装从地址https://github.com/medcl/elasticsearch-analysis-ik 下载elasticsearch中文分词器 （1）在安装好的elasticsearch中在plugins目录下新建ik目录 （2）将此zip包拷贝到ik目录下 （3）将权限修改为elasticsearch启动用户的权限 （4）过unzip命令解压缩： 1unzip elasticsearch-analysis-ik-1.8.0.zip （5）每台机器都这样操作，重新启动elasticsearch集群 例子： a. 创建索引库 1curl -XPUT http://localhost:9200/ik b. 设置mapping 12345678910curl -XPOST http://localhost:9200/ik/ikType/_mapping -d&apos;&#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;:&quot;analyzed&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125;&#125;&apos; c. 插入数据 1234567891011curl -XPOST http://localhost:9200/ik/ikType/1 -d&apos;&#123;&quot;content&quot;:&quot;美国留给伊拉克的是个烂摊子吗&quot;&#125;&apos;curl -XPOST http://localhost:9200/ik/ikType/2 -d&apos;&#123;&quot;content&quot;:&quot;公安部：各地校车将享最高路权&quot;&#125;&apos;curl -XPOST http://localhost:9200/ik/ikType/3 -d&apos;&#123;&quot;content&quot;:&quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot;&#125;&apos;curl -XPOST http://localhost:9200/ik/ikType/4 -d&apos;&#123;&quot;content&quot;:&quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot;&#125;&apos; d. 查询 123curl -XGET http://localhost:9200/ik/ikType/_search?pretty -d&apos;&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;content&quot; : &quot;中国&quot; &#125;&#125; &#125;&apos; 本地分片 只在主分片 优先主分片 只在某个节点分片 指定几个节点分片 优先指定分片 脑裂： 集群中出现两个master；1、负载过高时，master所在的节点负责管理和检索，忙不过来了，slaves节点就选出了另一个master；（功能解耦，用两台节点分别负责一个模块；一个放master，一个放data）； 2、网络波动，节点间的通信出现问题，超时连接，另一台master就又被选出来了;(解决：异地服务器）； 优化： 系统最大文件打开数量（默认1024个） ES JVM内存大小（256m，1g，最好设为相同值） 【256m用满了，启动垃圾回收机制，然后扩容；弹性伸缩，因为GC会影响性能。】 设置mlockall来锁定物理进程true 【Linux：swap交换区（磁盘空间：存放不用的内存）】 分片数要合理 单个分片存储：20g~30g 个数=数据总量/20g 再建一个索引库，因为支持多个索引库检索 副本：数据迁移时，先设置为0，网络和磁盘IO可以降低。 segment分片存储时的片段设为1 删除文档时，添加del标记，到客户端时再过滤。 hash取余再放到对应的分片]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Linux系统环境</tag>
        <tag>分布式搜索和分析引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CDH部署操作]]></title>
    <url>%2F2019%2F01%2F18%2FCDH%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[报错：1、Error:JAVA_HOME is not set and Java could not fund.Cloudera Manager requires Java 1.6 or later .NOTE：This script will find Oracle Java whether you install using the binary or the RPM based installer 原因：它运行时会默认到（ /usr/java/default）这个路径下找jdk 在 /usr/java/default目录下创建jdk访问的软连接， 12&gt; ln -s /home/jdk1.8.0_191 /usr/java/default&gt;]]></content>
      <categories>
        <category>CDH</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume学习]]></title>
    <url>%2F2019%2F01%2F18%2FFlume%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、理论理解1、官网:：http://flume.apache.org/ 2、概念： ​ Flume是一个分布式、可扩展、可靠、高可用的海量日志有效聚合及移动的框架。 ​ 它通常用于log数据，支持在系统中定制各类数据发送方，用于收集数据。它具有可靠性和容错可调机制和许多故障转移和恢复机制 3、Flume1.0X —-FlumeNG flume1.0x版本中flume只有agent,由3个部分组成 FlumeNG 4、架构解释 Agent ：将数据源的数据发送给collector ，Agent由source、channel、sink三大组件组成。 Source： 从Client收集数据，传递给Channel。可以接收外部源发送过来的数据。 不同的 source，可以接受不同的数据格式。 比如有目录池(spooling directory)数据源，可以监控指定文件夹中的新文件变化，如果目录中有文件产生，就会立刻读取其内容。 Channel 是一个存储地，接收source的输出，直到有sink消费掉channel中的数据，Channel中的数据直到进入到下一个channel中或者进入终端才会被删除； 当sink写入失败后，可以自动重启，不会造成数据丢失，因此很可靠。 Sink 用于数据输出 4、Flume使用原理图 Flume使用Agent内部原理图 二、特点A、数据可靠性（内部实现）​ 当节点出现故障时，日志能够被传送到其他节点上而不会丢失。 Flume提供了三种级别的可靠性保障,从强到弱依次分别为： ​ 1、end-to-end：收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。 ​ 2、Store on failure：当数据接收方crash时 ​ 3、Best effort：数据发送到接收方后，不会进行确认。(udp) B、自身可扩展性 Flume采用了三层架构，分别为agent，collector和storage，每一层均可以水平扩展。所有agent和 collector由master统一管理，使得系统容易监控和维护。master允许有多个（使用ZooKeeper进行管理和负载均衡），避免单点故障问题。 【1.0自身agent实现扩展】 C、功能可扩展性 用户可以根据需要添加自己的agent。 Flume自带了很多组件，包括各种agent（file，syslog，HDFS等） 三、Flume安装​ 1)将下载的flume包，解压 2)修改 flume-env.sh 配置文件,主要是JAVA_HOME设置[可选局部环境变量设置] ​ 3)验证是否安装成功 flume-ng version telnet 相关安装： ​ yum list telnet* 查看telnet相关的安装包 ​ 直接yum –y install telnet 就OK ​ yum -y install telnet-server 安装telnet服务 ​ yum -y install telnet-client 安装telnet客户端(大部分系统默认安装) 四、分类123456789Flume 关于Event的笔记 在Flume中使用Event对象来作为传递数据的格式。 Sources端在flume-ng-core子项目中的org.apache.flume.serialization包下，有一个名为LineDeserializer的类，这个类负责把数据按行来读取，每一行封装成一个Event（实现方式：按字节读取，当遇到&quot;\n&quot;时封装成Event返回，下一次获取Event时继续获取下一字节并判断）。然后按用户设置的批量传输的值来封装List&lt;Event&gt;备注：capacity：默认该通道中最大的可以存储的event数量是1000trasactionCapacity：每次最大可以source中拿到或者送到sink中的event数量也是100 exec： Unix等操作系统执行命令行，如tail ，cat 。可监听文件 netcat 监听一个指定端口，并将接收到的数据的每一行转换为一个event事件 avro 序列化的一种，实现RPC（一种远程过程调用协议）。 监听AVRO端口来接收外部AVRO客户端事件流 1、 netcat（监听端口，在本地控制台打印）（1） vim netcat_logger1234567891011121314151617181920212223# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 （2）命令操作 （在会话1端） 在node00节点的控制台输入启动命令： (方式一：指定配置文件的路径+文件名) 12&gt; flume-ng agent --conf-file /root/flume/netcat_logger --name a1 -Dflume.root.logger=INFO,console&gt; （方式二：配置文件所在当前目录） 12&gt; flume-ng agent --conf ./ --conf-file netcat_logger --name a1 -Dflume.root.logger=INFO,console&gt; 特别注意： #####官网方式######### 12&gt; flume-ng agent --conf conf --conf-file netcat_logger --name a1 -Dflume.root.logger=INFO,console&gt; 解释：此命令适用于将配置文件放在flume解压安装目录的conf中（不常用） 控制台显示: 1​````` 19/01/18 12:27:31 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 12:27:31 INFO node.Application: Starting Sink k119/01/18 12:27:31 INFO node.Application: Starting Source r119/01/18 12:27:31 INFO source.NetcatSource: Source starting19/01/18 12:27:31 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]1234567891011* (在会话2端)&gt; 在node00节点的控制台输入命令：&gt;&gt; 1、在节点上安装telnet&gt;&gt; ```shell&gt; yum install -y telnet&gt; yum -y install telnet-server&gt; 2、启动： 12&gt; telnet localhost 44444 &gt; 注意：： 12&gt; a1.sources.r1.bind = localhost&gt; 前提是/etc/hosts中已经配置 如果此处配置localhost 那么启动时，localhost 或127.0.0.1都可以，node00就不行 如果此处配置node00那么启动时，node00或ip都可以，localhost就不行 3、在会话2控制台输入任何内容; 都会在会话1端显示，且会话1端（ctrl+c）退出服务，会话2端也自动结束 1234yum list telnet* 查看telnet相关的安装包 直接yum –y install telnet 就OK yum -y install telnet-server 安装telnet服务 yum -y install telnet-client 安装telnet客户端(大部分系统默认安装) 2、avro（监听远程发送文件，在本地控制台打印）（1）vim avro_logger1234567891011121314151617181920212223#test avro sources##使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示##当前flume节点执行：#flume-ng agent --conf ./ --conf-file avro_loggers --name a1 -Dflume.root.logger=INFO,console##其他flume节点执行：#flume-ng avro-client --conf ./ -H 192.168.198.128 -p 55555 -F ./logsa1.sources=r1a1.channels=c1a1.sinks=k1a1.sources.r1.type = avro a1.sources.r1.bind=192.168.198.128a1.sources.r1.port=55555a1.sinks.k1.type=loggera1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels=c1a1.sinks.k1.channel=c1 实现功能： 使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示 （2）命令操作启动（在会话1端） 在node00上 当前flume节点执行（配置文件在当前目录）： 1flume-ng agent --conf ./ --conf-file avro_logger --name a1 -Dflume.root.logger=INFO,console 显示： 123456719/01/18 13:53:16 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 13:53:16 INFO node.Application: Starting Sink k119/01/18 13:53:16 INFO node.Application: Starting Source r119/01/18 13:53:16 INFO source.AvroSource: Starting Avro source r1: &#123; bindAddress: 192.168.198.128, port: 55555 &#125;...19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 13:53:17 INFO source.AvroSource: Avro source r1 started. 发送(在会话2端) 在node00上发送文件到node00 启动 可在本地和其他flume节点执行（配置文件在当前目录）： 1flume-ng avro-client --conf ./ -H 192.168.198.128 -p 55555 -F ./flume.log (在会话1端) 12319/01/18 14:12:57 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata &#125; 时刻监听传输文件的内容 注意 该过程也可应用于不同节点之间 3、exec（监听某一命令，在本地控制台打印）（1）vim exec_logger12345678910111213141516171819202122232425262728#单节点flume配置# example.conf: A single-node Flume configuration #给agent三大结构命名# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1#描述source的配置：类型、命令（监听/root/flume.log文件）# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /root/flume.log#描述sink的配置：类型# Describe the sinka1.sinks.k1.type = logger#在内存中使用一个channel缓存事件# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100#将source和sink绑定到channel上# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 （xshell会话1：） 在node00上：启动 在exec_logger文件所在的目录下 命令： 12&gt; flume-ng agent --conf-file exec_logger --name a1 -Dflume.root.logger=INFO,console&gt; r1 启动 （复制会话：会话2） 在node00上： 在root目录下 命令： 12&gt; echo hello bigdata &gt;&gt;flume.log&gt; 之后在会话1上 logger本地控制台打印： 1234&gt; 19/01/18 12:03:23 INFO sink.LoggerSink: &gt; Event: &gt; &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata &#125;&gt; 4、netcat–hdfs(监听数据，传到hdfs上)（1）vim netcat_hdfs12345678910111213141516171819202122# a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1a1.sources.r1.type = netcata1.sources.r1.bind = node00a1.sources.r1.port = 41414a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://Sunrise/myflume/%y-%m-%da1.sinks.k1.hdfs.useLocalTimeStamp=true# Define a memory channel called c1 on a1a1.channels.c1.type = memory#默认值，可省#a1.channels.c1.capacity = 1000#a1.channels.c1.transactionCapacity = 100# Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c1 (2)操作在node00的会话1上 启动 在node00上： 在netcat_hdfs文件所在的目录下 命令： 12&gt; flume-ng agent --conf-file netcat_hdfs --name a1 -Dflume.root.logger=INFO,console&gt; 显示： 123456719/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 14:34:44 INFO node.Application: Starting Sink k119/01/18 14:34:44 INFO node.Application: Starting Source r119/01/18 14:34:44 INFO source.NetcatSource: Source starting19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started19/01/18 14:34:44 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/192.168.198.128:41414] 在node00的会话2上 启动 1telnet node00 41414 显示 123Trying 192.168.198.128...Connected to node00.Escape character is &apos;^]&apos;. 输入任意内容 在node00会话1端会显示 1234519/01/18 14:36:50 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false19/01/18 14:36:51 INFO hdfs.BucketWriter: Creating hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Closing hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Renaming hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp to hdfs://Sunrise/myflume/19-01-18/FlumeData.154782221025919/01/18 14:37:29 INFO hdfs.HDFSEventSink: Writer callback called. 在HDF分布式系统上会显示，生成的文件 注意： 这种情况会在hdfs上生成很多小文件， 在官网 HDFS Sink：文档 有很多关于文件生成过程中的配置 Name Default Description channel – type – The component type name, needs to be hdfs hdfs.path – HDFS directory path (eg hdfs://namenode/flume/webdata/) hdfs.filePrefix FlumeData Name prefixed to files created by Flume in hdfs directory hdfs.fileSuffix – Suffix to append to file (eg .avro - NOTE: period is not automatically added) hdfs.inUsePrefix – Prefix that is used for temporal files that flume actively writes into hdfs.inUseSuffix .tmp Suffix that is used for temporal files that flume actively writes into hdfs.rollInterval 30 Number of seconds to wait before rolling current file (0 = never roll based on time interval) hdfs.rollSize 1024 File size to trigger roll, in bytes (0: never roll based on file size) hdfs.rollCount 10 Number of events written to file before it rolled (0 = never roll based on number of events) hdfs.idleTimeout 0 Timeout after which inactive files get closed (0 = disable automatic closing of idle files) hdfs.batchSize 100 number of events written to file before it is flushed to HDFS hdfs.codeC – Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy hdfs.fileType SequenceFile File format: currently SequenceFile, DataStream or CompressedStream (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC hdfs.maxOpenFiles 5000 Allow only this number of open files. If this number is exceeded, the oldest file is closed. hdfs.minBlockReplicas – Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath. hdfs.writeFormat Writable Format for sequence file records. One of Text or Writable. Set to Text before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive. hdfs.callTimeout 10000 Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring. hdfs.threadsPoolSize 10 Number of threads per HDFS sink for HDFS IO ops (open, write, etc.) hdfs.rollTimerPoolSize 1 Number of threads per HDFS sink for scheduling timed file rolling hdfs.kerberosPrincipal – Kerberos user principal for accessing secure HDFS hdfs.kerberosKeytab – Kerberos keytab for accessing secure HDFS hdfs.proxyUser hdfs.round false Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) hdfs.roundValue 1 Rounded down to the highest multiple of this (in the unit configured using hdfs.roundUnit), less than current time. hdfs.roundUnit second The unit of the round down value - second, minute or hour. hdfs.timeZone Local Time Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles. hdfs.useLocalTimeStamp false Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. hdfs.closeTries 0 Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart. hdfs.retryInterval 180 Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension. serializer TEXT Other possible options include avro_event or the fully-qualified class name of an implementation of the EventSerializer.Builder interface. avro-hdfs（配置方式二） 1234567891011121314151617181920# a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1a1.sources.r1.type = avroa1.sources.r1.bind=node01a1.sources.r1.port=55555a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://shsxt/hdfsflume# Define a memory channel called c1 on a1a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100# Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c1 ##当前flume节点执行： #flume-ng agent –conf ./ –conf-file avro_loggers –name a1 -Dflume.root.logger=INFO,console ##其他flume节点执行： #flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./logs 5、结合版（netcat-avro）（1）编辑配置文件（node00：vim netcat_avro1） 123456789101112131415161718192021222324252627282930313233# example.conf: A single-node Flume configuration#flume-ng agent --conf ./ --conf-file netcat_avro1 --name a1 -Dflume.root.logger=INFO,console#flume-ng --conf conf --conf-file /root/flume_test/netcat_hdfs -n a1 -Dflume.root.logger=INFO,console#telnet 192.168.235.15 44444# Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = node00 a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = node01 a1.sinks.k1.port = 60000 # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1#---------------------------#flume-ng agent --conf-file etect2_logger --name a1 -#Dflume.root.logger=INFO,console#flume-ng agent --conf conf --conf-file netcat_logger --name a1 -#Dflume.root.logger=INFO,console （node01：netcat_avro2） 12345678910111213141516171819#flume-ng agent --conf ./ --conf-file avro2 -n a1 a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = avroa1.sources.r1.bind = node01a1.sources.r1.port = 60000a1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 (2)操作 先启动后面的flume节点node01 ，在启动node00，最后启动node02 在node01上 启动 1flume-ng agent --conf conf --conf-file netcat_avro1 -n a1 -Dflume.root.logger=INFO,console 显示 12345678919/01/18 23:22:27 INFO node.Application: Starting Channel c119/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 23:22:28 INFO node.Application: Starting Sink k119/01/18 23:22:28 INFO node.Application: Starting Source r119/01/18 23:22:28 INFO source.AvroSource: Starting Avro source r1: &#123; bindAddress: node01, port: 60000 &#125;...19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 23:22:30 INFO source.AvroSource: Avro source r1 started. 在node00上： 启动 1flume-ng agent --conf ./ --conf-file netcat_avro2 --name a1 -Dflume.root.logger=INFO,console 在node02上 启动 telnet node00 44444 然后输入数据文件 最后在 node01节点上 显示文件信息 119/01/18 23:33:01 INFO sink.LoggerSink: Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 0D hello world. &#125;]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase性能优化]]></title>
    <url>%2F2019%2F01%2F17%2FHBase%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[HBase性能优化方案（一）、表的设计一、Pre-Creating Regions 预分区 详情参见：Table Creation: Pre-Creating Regions 解决海量导入数据时的热点问题 背景： 在创建HBase表的时候默认一张表只有一个region， 所有的put操作都会往这一个region中填充数据， 当这一个region过大时就会进行split。 如果在创建HBase的时候就进行预分区 则会减少当数据量猛增时由于region split带来的资源消耗。 注意： Hbase表的预分区需要紧密结合业务场景来选择分区的key值。 每个region都有一个startKey和一个endKey来表示该region存储的rowKey范围。 1&gt; create &apos;t1&apos;, &apos;cf&apos;, SPLITS =&gt; [&apos;20150501000000000&apos;, &apos;20150515000000000&apos;, &apos;20150601000000000&apos;] 或者 123456&gt; create &apos;t2&apos;, &apos;cf&apos;, SPLITS_FILE =&gt; &apos;/home/hadoop/splitfile.txt&apos; /home/hadoop/splitfile.txt中存储内容如下： 201505010000000002015051500000000020150601000000000 该语句会创建4个region： Hbase的Web UI中可以查看到表的分区情况： 其中，每个region的命名方式如下：[table],[region start key],[region id] 二、row key 1、特性 在Hbase中 rowKey 可以是任意字符串，最大长度为64KB ， 一般为10—100bytes ,存储在bytes[ ]字节数组中，一般设计为定长。 rowKey是按字典排序 Rowkey规则： 1、 定长 越小越好 2、 Rowkey的设计是要根据实际业务来 3、 散列性 a) 反转 001 002 100 200 b) Hash 2、HBase中row key用来检索表中的记录，支持以下三种方式： · 通过单个row key访问：即按照某个row key键值进行get操作； · 通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描；过滤器 · 全表扫描：即直接扫描整张表中所有行记录。 三、column family 个数限定在2~3个 原因： 因为某个column family 在flush会，他临近的column family也会因关联效应被触发flush，最终导致系统会产生更多的I/O。 四、参数设置 In Memory 创建表时，HColumnDescriptor.setInMemory(true)将表放到RegionServer的缓存中，保证在读取的时候被cache命中。 Max Version 创建表时，HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。 Time To Live 创建表时，HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 24 60 * 60)。 五、Compact &amp; Split 六、高表和宽表的选择 资源链接： http://www.cnblogs.com/rocky24/p/3372ad2a037a73daf0ff4ed4a9f43625.html https://yq.aliyun.com/articles/213705 1、分类 Hbase表设计： 高表：行多列少； 宽表：行少列多。 2、根据KeyValue信息的筛选粒度，用户应尽量将需要查询的维度和信息存储在行键中，才能达到更好的数据筛选效率。 在Hbase中，数据操作具有行级原子性，按行分片。根据用户是否批量修改Value内容来决定高表和宽表的选择，宽表每一行存储的数据信息量多，易超过最大HFile的限制，若用户不存在全局value操作的需求，宽表比较适合。 （二）、写表操作一、多HTable客户端并发写 创建多个HTable客户端用于写操作，提高写数据的吞吐量。 1234567static final Configuration conf = HBaseConfiguration.create();static final String table_log_name = “user_log”;wTableLog = new HTable[tableN];for (int i = 0; i &lt; tableN; i++) &#123; wTableLog[i] = new HTable(conf, table_log_name); wTableLog[i].setWriteBufferSize(5 * 1024 * 1024); //5MB wTableLog[i].setAutoFlush(false); 二、HTable参数设置 Auto Flush 通过调用HTable.setAutoFlush(false)方法可以将HTable写客户端的自动flush关闭，这样可以批量写入数据到HBase，而不是有一条put就执行一次更新，只有当put填满客户端写缓存时，才实际向HBase服务端发起写请求。默认情况下auto flush是开启的 Write Buffer 三、批量写 四、多线程并发写 （三）、读表操作一、多HTable客户端并发读 创建多个HTable客户端用于读操作，提高读数据的吞吐量。 1234567static final Configuration conf = HBaseConfiguration.create();static final String table_log_name = “user_log”;rTableLog = new HTable[tableN];for (int i = 0; i &lt; tableN; i++) &#123; rTableLog[i] = new HTable(conf, table_log_name); rTableLog[i].setScannerCaching(50);&#125; 二、HTable参数设置 三、批量读 四、多线程并发读 五、缓存查询结果 六、 Blockcache]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase学习]]></title>
    <url>%2F2019%2F01%2F15%2FHBase%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[非关系型数据库 官网 一、对HBase数据库的 基本了解 1、简介 基于Hadoop 的分布式数据库 特点： 1、高可靠性 2、高性能 （以上两点：基于分布式的特点） 3、面向列 （以（K,V）存储，有唯一标记的rowkey，value包含的是数据库中的列值） 4、可伸缩 （搭建在集群上） 5、实时读写 （用时间戳唯一标记每一版本的数据记录） 2、工作结构 1,利用Hadoop的HDFS作为其文件存储系统 2,利用Hadoop的MapReduce来计算处理HBase中的海量数据 3,利用Zookeeper作为其分布式协同服务 4,主要用来存储非结构化和半结构化的松散数据（NoSQL非关系型数据库有redis、MongoDB等 3、关系型数据库 1、定义 关系模型指的就是二维表格模型； 而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织 2、三大优点 容易理解 使用方便 易于维护 3、三大瓶颈 高并发读写需求 硬盘I/O是一个很大的瓶颈，并且很难能做到数据的强一致性。 海量数据的读写性能低 在一张包含海量数据的表中查询，效率是非常低的。 ​ 扩展性和可用性差 丰富的完整性使得横向扩展把难度加大了 ACID特性 ACID，指数据库事务正确执行的四个基本要素的缩写; 原子性（Atomicity）:事务不可再分割 整个事务中的所有操作，要么全部完成，要么全部不完成，不可能停滞在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性（Consistency）:事务前后数据保持一致 事务必须始终保持系统处于一致的状态，不管在任何给定的时间并发事务有多少。如果事务是并发多个，系统也必须如同串行事务一样操作。 隔离性（Isolation）：串行化，使得同一时间仅有一个请求用于同一数据。 事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。 持久性（Durability）： 在事务完成以后，该事务对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 4、非关系型数据库 1、存储格式：key value键值对，文档，图片等等，结构不固定 2、可以减少一些时间和空间的开销，仅需要根据id取出相应的value就可以完成查询。 3、一般不支持ACID特性，无需经过SQL解析，读写性能高 4、不提供where字段条件过滤 5、难以体现设计的完整性，只适合存储一些较为简单的数据 二、对HBase的基本里了解1、数据结构组成（ 1）Row key : 唯一标记决定一行数据按照字典排序最大只能存储64KB的字节数据设计非常关键 （2）Column Family列族 &amp; qualifier列 列族 必须作为表模式(schema)定义的一部分预先给出， 表中的每个列都归属于某个列族； 权限控制、存储以及调优都是在列族层面进行的； 列名 以列族作为前缀，每个“列族”都可以有多个列成员(column)； 新的列可以随后按需、动态加入； （3）Cell单元格 由行和列的坐标交叉决定； 单元格是有版本的（有时间戳决定）； 单元格的内容是未解析的字节数组；cell中的数据是没有类型的，全部是字节码形式存贮。 12&gt; 由&#123;rowkey， column( =&lt;family&gt; +&lt;qualifier&gt;)， version&#125; 唯一确定的单元。&gt; （4）Timestamp时间戳 在HBase每个cell存储单元对同一份数据有多个版本， 根据唯一的时间戳来区分每个版本之间的差异， 不同版本的数据按照时间倒序排序，最新的数据版本排在最前面 时间戳的类型是64位整型。 时间戳可以由HBase(在数据写入时自动)赋值，精确到毫秒 时间戳也可以由客户显式赋值，但必须唯一性 （5）HLog(WAL log) HLog文件就是一个普通的Hadoop SequenceFile HLog Sequence File的Key是HLogKey对象 HLogKey中记录了写入数据的归属信息，包括table和region名字，sequence number（起始值为0或是最近一次存入文件系统中sequence number）和timestamp（写入时间） HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue 存储hbase表的操作记录，(K，V)数据信息 2、体系架构 （1）Client 包含访问HBase的接口并在缓存中维护着已经访问过的Region位置信息来加快对HBase的访问。 （2）Zookeeper 保证任何时候，集群中只有一个master； 存贮所有Region的寻址入口。 实时监控Region server的上线和下线信息，并实时通知Master 存储HBase的schema和table元数据 Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等 （3）Master 为Region server分配region； 负责Region server的负载均衡； 发现失效的Region server并将其上的region重新分配； 在Region分裂或合并后，负责重新调整Region的分布 管理用户对table的增删改操作； （4）RegionServer 维护region，处理对这些region的IO请求 负责切分在运行过程中变得过大的region （5）Region 保存一个表里面某段连续的数据，每个表一开始只有一个region； 随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变） （HBase自动把表水平划分成多个区域(region)） 当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上 （6）Memstore与storefile 一个region由2-3store组成，一个store对应一个CF（列族） store包括位于内存中的memstore和位于磁盘的storefile。 写操作先写入memstore，当memstore中的数据达到某个阈值（默认64M），hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile； 当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major，compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile 当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡 客户端检索数据，先在memstore找，找不到再找storefile HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。 每个Strore又由一个memStore和0至多个StoreFile组成,StoreFile以HFile格式保存在HDFS上(HFile)。 三、Hbase 安装部署完全分布式搭建 1、安装包准备 Hbase（本文使用0.98版本） 将tar上传至Linux系统，进行解压安装 2、修改配置文件hbase-env.sh（在Hbase的解压目录的conf目录中） 修添加JAVA_HOME环境变量 123# The java implementation to use. Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/export JAVA_HOME=/usr/soft/jdk1.8.0_191 不使用HBase的默认zookeeper配置，（使用自己的）： 12# Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not. export HBASE_MANAGES_ZK=false 3、修改配置hbase-site.xml（在Hbase的解压目录的conf目录中） 123456789101112131415161718192021&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;!--Hdfs配置时的集群名--&gt;&lt;value&gt;hdfs://Sunrise:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--zookeeper的三台节点--&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;node00,node01,node02&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--配置http访问的port---&gt;&lt;name&gt;hbase.master.info.port&lt;/name&gt;&lt;value&gt;60010&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 注意：（会出bug的地方）： 1、问题描述： HBase启动时，警告：Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 解决方案： 原因：由于使用了JDK8 ，需要在HBase的配置文件中hbase-env.sh，注释掉两行。 123# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+#export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"#export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m" 重新启动HBase。 2、问题描述： 配置好HBase后，各项服务正常，但想从浏览器通过端口60010看下节点情况，但是提示拒绝访问 检测：在服务器上netstat -natl|grep 60010 发现并没有60010端口 原因：HBase 1.0 之后的版本都需要在hbase-site.xml中配置端口，如下 1234&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt;&lt;/property&gt; 重新启动HBase,在浏览器再次访问，就ok了 4、添加配置regionservers 文件（在Hbase的解压目录的conf目录中） 添加配置的regionservers 的主机名 regionservers 123node00node01node02 5、添加配置backup-masters 添加配置的master备份的主机名（在Hbase的解压目录的conf目录中） backup-masters 1node02 6、将Hadoop安装解压目录/etc/hadoop目录下的hdfs-site.xml文件 拷贝到Hbase的解压目录的conf目录中 7、配置环境变量 ~/.bash_profile 123456789101112JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/binHIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/binSQOOP_HOME=/usr/soft/sqoop-1.4.6export PATH=$PATH:$SQOOP_HOME/binHBASE_HOME=/usr/soft/hbase-1.2.9export PATH=$PATH:$HBASE_HOME/bin source ~/.bash_profile 8、将如上配置远程发送至其他节点（Hbase 、 ./bash_profile） 9、各个节点注意要做时间同步 1ntpdate cn.ntp.org.cn 10、启动HDFS集群： 12zkServer.sh startstart-hdfs.sh 11、启动： 1start-hbase.sh 显示： 12345starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node00.outnode02: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node02.outnode01: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node01.outnode00: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node00.outnode02: starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node02.out 12、查看进程： 1jps 13、浏览器访问：node00:60010 14、关闭： 1stop-hbase.sh 四、通过hbase shell命令进入HBase 命令行接口1hbase shell 进入hbase交互式界面。 通过help可查看所有命令的支持以及帮助手册 帮助创建 hbase(main):007:0&gt; help create 名称 Shell命令 举例 创建表 create ‘表名’, ‘列族名1’[,…] create ‘t1’，‘cf1’ 列出所有表 list list 添加记录 put ‘表名’, ‘RowKey’, ‘列族名称:列名’, ‘值’ put ‘t1’,‘rk_00101’,‘cf1:name’,‘zs’ 查看记录 get ‘表名’, ‘RowKey’, ‘列族名称:列名’ get ‘t1’,‘rk_00101’ get ‘t1’,‘rk_00101’,‘cf1:name’ 查看所有记录 count ‘表名’ count ‘t1’ 删除记录 delete ‘表名’ , ‘RowKey’, ‘列族名称:列名’ delete ‘t1’,‘rk_00101’,‘cf1:name’ 删除一张表 先要屏蔽该表，才能对该表进行删除。 第一步 disable ‘表名称’ 第二步 drop ‘表名称’ disable ‘t1’ drop ‘t1’ 查看所有记录 scan ‘表名 ‘ scan ‘t1’ create ‘t2’, {NAME =&gt; ‘cf1’, VERSIONS =&gt; 2}, METADATA =&gt; { ‘mykey’ =&gt; ‘myvalue’ } 查看未加工数据中指定版本记录 scan ‘t1’, {RAW =&gt; true, VERSIONS =&gt; 3} raw 未加工的 3 查看保存版本记录 scan ‘t1’, {VERSIONS =&gt; 2} 2 五、HBase优化详见HBase性能优化文档 六、Hive和Hbase的整合hive和hbase同步https://cwiki.apache.org/confluence/display/Hive/HBaseIntegration 1、拷贝jar包把hive-hbase-handler-1.2.1.jar cp到hbase/lib 下​ 同时把hbase中的所有的jar，cp到hive/lib 注意： hive-hbase-handler-1.2.1.jar在Hive的lib目录下 2、在hive的配置文件增加属性：1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node01,node02,node03&lt;/value&gt;&lt;/property&gt; 3、在hive中创建临时表123456789101112(注意：需要先在Hbase中创建t_order表，列族为order：create 't_order' 'order')CREATE EXTERNAL TABLE tmp_tbl(key string, id string, user_id string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,order:order_id,order:user_id") TBLPROPERTIES ("hbase.table.name" = "t_tbl"，"hbase.mapred.output.outputtable" = "t_tbl");（确保xyz没有在Hbase中存在）CREATE TABLE hbasetbl(key int, name string, age string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:name,cf1:age")TBLPROPERTIES ("hbase.table.name" = "xyz", "hbase.mapred.output.outputtable" = "xyz");]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop学习]]></title>
    <url>%2F2019%2F01%2F13%2FSqoop%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Sqoop简介官网 是将关系数据库（oracle、mysql、postgresql等）数据与hadoop数据进行转换的工具。 可以将一个关系型数据库(例如MySQL、Oracle)中的数据导入到Hadoop(例如HDFS、Hive、Hbase)中，也可以将Hadoop(例如HDFS、Hive、Hbase)中的数据导入到关系型数据库(例如Mysql、Oracle)中。 版本：(两个版本完全不兼容) sqoop1：1.4.x （推荐） sqoop2：1.99.x 二、sqoop 架构hadoop生态系统的架构最简单的框架。 sqoop1由client端直接接入hadoop，任务通过解析生成对应的mapreduce执行 三、Sqoop安装1、安装包解压：Sqoop1 : 1.4.7 (推荐) Sqoop2 : 1.99.7 2、配置环境变量（追加） vim ~/.bash_profile 12345678910JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/binHIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/binSQOOP_HOME=/usr/soft/sqoop-1.4.6export PATH=$PATH:$SQOOP_HOME/bin (编辑结束后，保存并退出，然后在控制台输入下面的命令，从而是环境变量生效) 链接资源： source /etc/profile 3、添加数据库驱动包 在Sqoop的安装解压目录下的lib目录下添加jar包 mysql-connector-java-5.1.10.jar 用以连接Mysql 4、重命名配置文件在Sqoop的解压安装目录下的conf目录下 1mv sqoop-env-template.sh sqoop-env.sh 编辑sqoop-env.sh 123456789101112131415#Set path to where bin/hadoop is availableexport HADOOP_COMMON_HOME=/usr/soft/hadoop-2.6.5#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/usr/soft/hadoop-2.6.5#set the path to where bin/hbase is availableexport HBASE_HOME=/usr/soft/hbase-1.2.9#Set the path to where bin/hive is availableexport HIVE_HOME=/usr/soft/apache-hive-1.2.1-bin#Set the path for where zookeper config dir is#export ZOOCFGDIR=/usr/soft/zookeeper-3.4.13export ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13 5、修改配置configure-sqoop在Sqoop的解压安装目录的bin目录下 注释掉未安装服务相关内容 例如（HBase、HCatalog、Accumulo）： 12345678910111213141516171819#if [ -z &quot;$&#123;HCAT_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/hive-hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hive-hcatalog# elif [ -d &quot;/usr/lib/hcatalog&quot; ]; then# HCAT_HOME=/usr/lib/hcatalog# else# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hive-hcatalog# if [ ! -d $&#123;HCAT_HOME&#125; ]; then# HCAT_HOME=$&#123;SQOOP_HOME&#125;/../hcatalog# fi# fi#fi#if [ -z &quot;$&#123;ACCUMULO_HOME&#125;&quot; ]; then# if [ -d &quot;/usr/lib/accumulo&quot; ]; then# ACCUMULO_HOME=/usr/lib/accumulo# else# ACCUMULO_HOME=$&#123;SQOOP_HOME&#125;/../accumulo# fi#fi 6、运行sqoop1sqoop version 前提: MySQL运行正常，且服务启动 1service mysqld start 启动sqoop连接mysql 12345sqoop list–databases --connect jdbc:mysql://node00:3306/ -username root -password 123456或sqoop list-tables --connect jdbc:mysql://192.168.198.128:3306/mysql --username root --password 123456 警告： 关于zookeeper环境变量配置的问题： 1234567[root@node00 conf]# sqoop versionWarning: /usr/soft/sqoop-1.4.6/bin/../../zookeeper does not exist! Accumulo imports will fail.Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.19/01/18 17:02:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6Sqoop 1.4.6git commit id c0c5a81723759fa575844a0a1eae8f510fa32c25Compiled by root on Mon Apr 27 14:38:36 CST 2015 解决方案： 在sqoop解压安装目录的conf目录下，在sqoop-env.sh文件中 12 添加本地ZOOKEEPER_HOME的配置 1export ZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13 四、Sqoop导入导出选项1、导入工具import：123456789101112131415161718192021222324252627282930313233343536373839404142434445 选项 含义说明 --append 将数据追加到HDFS上一个已存在的数据集上--as-avrodatafile 将数据导入到Avro数据文件--as-sequencefile 将数据导入到SequenceFile--as-textfile 将数据导入到普通文本文件（默认）--boundary-query &lt;statement&gt; 边界查询，用于创建分片（InputSplit）--columns &lt;col,col,col…&gt; 从表中导出指定的一组列的数据--delete-target-dir 如果指定目录存在，则先删除掉--direct 使用直接导入模式（优化导入速度）--direct-split-size &lt;n&gt; 分割输入stream的字节大小（在直接导入模式下）--fetch-size &lt;n&gt; 从数据库中批量读取记录数--inline-lob-limit &lt;n&gt; 设置内联的LOB对象的大小-m,--num-mappers &lt;n&gt; 使用n个map任务并行导入数据-e,--query &lt;statement&gt; 导入的查询语句--split-by &lt;column-name&gt; 指定按照哪个列去分割数据--table &lt;table-name&gt; 导入的源表表名--target-dir &lt;dir&gt; 导入HDFS的目标路径--warehouse-dir &lt;dir&gt; HDFS存放表的根路径--where &lt;where clause&gt; 指定导出时所使用的查询条件-z,--compress 启用压缩--compression-codec &lt;c&gt; 指定Hadoop的codec方式（默认gzip）--null-string &lt;null-string&gt; 如果指定列为字符串类型，使用指定字符串替换值为null的该类 列的值--null-non-string &lt;null-string&gt; 如果指定列为非字符串类型，使用指定字符串替换值为null的该 类列的值 2、导出工具export：1234567891011121314151617181920212223242526272829 选项 含义说明--validate &lt;class-name&gt; 启用数据副本验证功能，仅支持单表拷贝，可以指定验证使用的实现类--validation-threshold &lt;class-name&gt; 指定验证门限所使用的类--direct 使用直接导出模式（优化速度）--export-dir &lt;dir&gt; 导出过程中HDFS源路径--m,--num-mappers &lt;n&gt; 使用n个map任务并行导出--table &lt;table-name&gt; 导出的目的表名称--call &lt;stored-proc-name&gt; 导出数据调用的指定存储过程名--update-key &lt;col-name&gt; 更新参考的列名称，多个列名使用逗号分隔--update-mode &lt;mode&gt; 指定更新策略，包括：updateonly（默认）、allowinsert--input-null-string &lt;null-string&gt; 使用指定字符串，替换字符串类型值为null的列--input-null-non-string &lt;null-string&gt; 使用指定字符串，替换非字符串类型值为null的列--staging-table &lt;staging-table-name&gt; 在数据导出到数据库之前，数据临时存放的表名称--clear-staging-table 清除工作区中临时存放的数据--batch 使用批量模式导出 四、Sqoop导入导出操作1、导入 123456789sqoop ##sqoop命令import ##表示导入--connect jdbc:mysql://ip:3306/sqoop ##告诉jdbc，连接mysql的url--username root ##连接mysql的用户名--password 123456 ##连接mysql的密码--table myuser ##从mysql到出的表名-m 1 ##使用1个map任务进行导出--hive-import ##把mysql表数据导入到hive中，如果不适用该选项意味着导入到hdfs中--target-dir &lt;dir&gt; ##HDFS destination dir 1、将MySQL中的数据导入到HDFS/Hive/Hbase12MySQL--&gt; HDFS：sqoop import --connect jdbc:mysql://node00:3306/test --username root --password 123456 --table myuser -m 1 -target-dir hdfs://Sunrise/my02 123MySQL--&gt; Hive：sqoop import --connect jdbc:mysql://node00:3306/test --username root --password root --table myuser --hive-import -m 1##由于使用Sqoop从MySQL导入数据到Hive需要指定target-dir，因此导入的是普通表而不能为外部表。 1234567MySQL--&gt; HBase:sqoop import --connect jdbc:mysql://node00:3306/test --username root --password 1234 --table mysqoop --hbase-create-table --hbase-table sukie --hbase-row-key name --column-family cf1 -m 1##选项解释--column-family ##指定列族名--hbase-row-key ##指定rowkey对应的mysql中的键--hbase-create-table ##自动在Hbase中创建表 2、导出 12345678sqoopexport ##表示如hive数据导出到mysql--connect jdbc:mysql://ip:3306/test --username root --password 123 --table mysqoop ##mysql中的表（必须已存在）--export-dir /root/hive ## hive中导出的文件目录--fields-terminated-by '\t' ##表示如hive导出文件中的行的字段分隔符 2、使用Sqoop将HDFS/Hive/HBase中的数据导出到MySQL12HDFS--&gt;MySQL:sqoop export --connect jdbc:mysql://192.168.198.128:3306/test --username root --password 123 --table my --export-dir /root/my 将HDFS/Hive/HBase中的数据导出到MySQL操作都基本大同小异 12Hive--&gt;MySQL:sqoop export --connect jdbc:mysql://192.168.198.128:3306/test --username root --password 123 --table testa --export-dir /user/hive/warehouse/testa --input-fields-terminated-by '\001’ HBase–&gt;mysql: 目前没有直接的命令将HBase的数据导出到mysql，但可以先将数据导出到hdfs，再导出到mysql 12sqoop export --connect jdbc:mysql://192.168.198.128:3306/mysql --username root --password 123456 --table bb --export-dir '/mysql_data/part-m-00000' 也可以通过Hive建立2个表，一个外部表是基于这个Hbase表的，另一个是单纯的基于hdfs的hive原生表，然后把外部表的数据导入到原生表（临时），然后通过hive将临时表里面的数据导出到mysql 1、mysql建立空表 1234567CREATETABLE `employee` ( `rowkey` int(11) NOT NULL, `id` int(11) NOT NULL, `name` varchar(20) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8; 2、Hbase建立employee表,并加载数据 123456789create 'employee','info'put 'employee',1,'info:id',1put 'employee',1,'info:name','peter'put 'employee',2,'info:id',2put 'employee',2,'info:name','paul' 3、建立Hive外部表 hive 有分为原生表和外部表，原生表是以简单文件方式存储在hdfs里面，外部表依赖别的框架，比如Hbase，我们现在建立一个依赖于我们刚刚建立的employee hbase表的hive 外部表 1234CREATE EXTERNAL TABLE h_employee(key int, id int, name string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key, info:id,info:name")TBLPROPERTIES ("hbase.table.name" = "employee"); 1234hive&gt; select * from h_employee;OK1 1 peter2 2 paul 4、建立Hive原生表 这个hive原生表只是用于导出的时候临时使用的，所以取名叫 h_employee_export，字段之间的分隔符用逗号 123456CREATE TABLE h_employee_export( key INT, id INT, name STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\054'; 5、源Hive表导入数据到临时表 将数据从 h_employee(基于Hbase的外部表)导入到 h_employee_export(原生Hive表) 1hive&gt; insert overwrite table h_employee_export select * from h_employee; 1234hive&gt; select * from h_employee_export;OK1 1 peter2 2 paul 我们去看下实际存储的文本文件是什么样子的 1234&gt; $ hdfs dfs -cat /user/hive/warehouse/h_employee_export/000000_0&gt; 1,1,peter&gt; 2,2,paul&gt; 6、从Hive导出数据到mysql 1$ sqoop export --connect jdbc:mysql://localhost:3306/sqoop_test --username root --password root --table employee -m 1 --export-dir /user/hive/warehouse/h_employee_export/ 7、注意 在这段日志中有这样一句话 1`14/12/05 08:49:46 INFO mapreduce.Job: The url ``to` `track the job: https://hadoop01:8088/proxy/application_1406097234796_0037/` 意思是你可以用 浏览器 访问这个地址去看下任务的执行情况，如果你的任务长时间卡主没结束就是出错了，可以去这个地址查看详细的错误日志 8、查询结果 123456789mysql&gt; select * from employee;+--------+----+-------+| rowkey | id | name |+--------+----+-------+| 1 | 1 | peter || 2 | 2 | paul |+--------+----+-------+2 rows in set (0.00 sec) mysql&gt; 123456789101112131415161718191、Sqoop增量导入sqoop import -D sqoop.hbase.add.row.key=true --connect jdbc:mysql://node00:3306/spider --username root --password root --table TEST_GOODS --columns ID,GOODS_NAME,GOODS_PRICE --hbase-create-table --hbase-table t_goods --column-family cf --hbase-row-key ID --incremental lastmodified --check-column U_DATE --last-value '2017-06-27' --split-by U_DATE--incremental lastmodified 增量导入支持两种模式 append 递增的列；lastmodified时间戳。--check-column 增量导入时参考的列--last-value 最小值，这个例子中表示导入2017-06-27到今天的值 123456789101112131415161718192、Sqoop job： sqoop job --create testjob01 --import --connect jdbc:mysql://node00:3306/spider --username root --password root --table TEST_GOODS --columns ID,GOODS_NAME,GOODS_PRICE --hbase-create-table --hbase-table t_goods --column-family cf --hbase-row-key ID -m 1 设置定时执行以上sqoop job 使用linux定时器：crontab -e 例如每天执行 0 0 * * * /opt/local/sqoop-1.4.6/bin/sqoop job …. --exec testjob01]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中常用的UDF函数总结]]></title>
    <url>%2F2019%2F01%2F12%2FHive%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84UDF%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、网络资源 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371、类型转换cast(expr as &lt;type&gt;) 如： cast('1' as BIGINT) 字符串转换为数字2、if语句if(boolean testCondition, T valueTrue, T valueFalseOrNull)如果 testCondition 为 true 返回 valueTrue， 否则返回 valueFalse 或 Null如： if(1 == 1, 1, 2) 结果为13、case语句CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END如：case when a == b then b when a == c then c else d end4、字符串连接concat(string1, string2, ...)如：concat('hello', ' word') 结果为 hello word5、计算字符串长度length(string)如：length('hello') 结果为56、查找子串的位置locate(string substr, string str[, int pos])如：locate('%', '100%') 返回37、聚合某一列数据collect_set(col) 会去重collect_list(col) 不会去重这两个函数均会返回一个索引数组将数组转换为分割符分割的字符串，如下concat_ws(' ', collect_set(tblsecondtagmap.tag_name)) 8、将数组或者map类型的数据分成多行explode(ARRAY&lt;T&gt; a)explode(MAP&lt;Tkey,Tvalue&gt; m)如：select explode(array('A','B','C')); 对应abc三行ABCselect explode(map('A',10,'B',20,'C',30));对应键值对三行A 10B 20C 309、解析json数据get_json_object(string json_string, string path)path在不同的hive版本中支持情况不同$ : json对象的根. : 子对象的操作符[] : 数组类型的下标形式* : 通配符，结合 [] 一起使用如：get_json_object('&#123;"name":"bob"&#125;', '$.name') 返回bob get_json_object('&#123;"name":["own","one"]&#125;','$.name[]') 返回 ["own","one"] get_json_object('&#123;"name":["own","one"]&#125;','$.name[0]') 返回 own10、支持的复杂数据类型array 数组类型，类比索引数组map map类型， 类比关联数组11、支持rlike语句rlike支持正则表达式。如：title rlike '^.*?医.*?(公司|院|网|中心|会|联盟|所|门诊|店|厂|门户|集团|美容|整型).*?$'12、字母大小写转换upper(string A) ucase(string A) 将字符串转换为大写字母lower(string A) lcase(string A) 将字符串转换为小写字母13、时间戳与时间的转换from_unixtime(bigint unixtime[, string format]) 将时间戳转换为时间，形如“2008-10-07 03:28:54”这种的形式unix_timestamp(string date) 将时间转换为时间戳，将形如“2008-10-07 03:28:54”这种形式的时间转换为时间戳14、获取时间或者日期year(string date) 年 month(string date) 月day(string date) 日hour(string date) 小时minute(string date) 分钟second(string date) 秒--PS--前三个函数支持‘2008-10-07 03:28:54’ ‘2008-10-07’ 这两种形式--后三个函数支持‘2008-10-07 03:28:54’ ‘03:28:54’ 这两种形式]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive优化]]></title>
    <url>%2F2019%2F01%2F12%2FHive%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、核心思想： 把Hive SQL 当做MapReduce程序进行优化 注意：以下不能HQL转化为Mapreduce任务运行 —select 仅查询本表字段 —where 仅对本表字段做条件过滤 12--比如select * from table； 二、explain 用以显示任务执行计划 格式： EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query 语法解释 从语法组成可以看出来是一个“explain ”+三个可选参数+查询语句。大家可以积极尝试一下，后面两个显示内容很简单的，我介绍一下第一个 extended 这个可以显示hql语句的语法树 其次，执行计划一共有三个部分： 这个语句的抽象语法树 这个计划不同阶段之间的依赖关系 对于每个阶段的详细描述 例子： 12&gt; hive&gt; explain select * from log;&gt; 拓展课下查询MySQl的执行计划。 三、Hive运行方式集群模式：12执行hql：hive&gt; select count(*) from log; 结论： 函数（如count）是在reduce阶段进行默认提交到yarn所在的节点上运行， 优化一:设置 本地模式（运行速度加快。但对加载文件有限制） 1234hive&gt;set hive.exec.mode.local.auto=true;查看：hive&gt;set hive.exec.mode.local.auto 但是如果加载文件的最大值大于配置（默认配置为100M），仍会使用集群模式运行（在yarn所在的节点） 12345查看最大加载文件hive&gt; set hive.exec.mode.local.auto.inputbytes.max;显示：hive.exec.mode.local.auto.inputbytes.max=134217728 优化二：设置 严格模式: 123通过设置以下参数开启严格模式[防止误操作]：hive&gt; set hive.mapred.mode=strict;（默认为：nonstrict非严格模式） 但是存在查询限制: ​ 可以防止用户执行那些可能产生不好的效果的查询。即某些查询在严格模式下无法执行。 1、对分区表查询时，必须添加where对于分区字段的条件过滤； 就是用户不允许扫描所有的分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。如果没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表 12&gt; hive&gt; select * from day_table where dt='2019-01-13';&gt; 2、order by语句必须包含limit输出限制； 因为orderby为了执行排序过程会讲所有的结果分发到同一个reducer中进行处理，强烈要求用户增加这个limit语句可以防止reducer额外执行很长一段时间。 12345&gt; hive&gt; select * from log order by id limit 1;&gt; 这里的1， 表示显示前多少条记录，只能设一个数字&gt; 和Mysql（可以从0 开始）不同的是，它只能从1开始&gt; mysql可以有两个数字，表示从第几条开始，显示几条&gt; 3、限制执行笛卡尔积的查询； 因为在关系型数据库中，可以使用where充当on，但是在hive数据仓库中，必须使用on，否则，查询会出此案不可控的情况。 imit 优化三：设置并行计算: 1234--通过设置一下参数设置并行模式set hive.exec.parallel=true--通过以下设置一次SQL计算中允许并行执行的job个数的最大值set hive.exec.parallel.thread.number 执行sql： 123select t1.cf1,t2.cf2 from(select count(id) as cf1 from table) t1,(select count(id) as cf2 from table) t2; 四、Hive排序1、Order By— 对于查询结果做全局排序，只允许有一个reduce处理（当数据量较大时，reduce数量有限，应慎用。 ​ 严格模式下，必须结合limit来使用） 1select * from log order by id limit 9; （结果有序） 显示： Time taken: 102.065 seconds, Fetched: 7 row(s) 2、Sort By– 对于单个reduce的数据进行排序 –局部（单个reduce）有序，全局无序 1可以通过设置mapred.reduce.tasks的值来控制reduce的数，然后对reduce输出的结果做二次排序 案例 1select * from log sort by id; (结果无序) 显示 Time taken: 147.077 seconds, Fetched: 7 row(s) 3、Distribute By– 分区排序，经常和 Sort By 结合使用 全局有序，局部无序 1select * from log distribute by id; （结果无序） Time taken: 144.708 seconds, Fetched: 7 row(s) 注意：hive要求DISTRIBUTE BY语句出现在SORT BY语句之前 Distribute By可以将Map阶段输出的数据按指定的字段划分到不同的reduce文件中，然后，sort by 对reduce阶段的输出数据做排序。 情况一、(无序) 1select * from table distrubute by class sort by acore; 情况二、（？？） 1select * from (select * from log distribute by id ) t2 sort by t2.id asc; 4、Cluster By– 相当于 Sort By + Distribute By（Cluster By不能通过asc、desc的方式指定排序规则；可通过 distribute by column sort by column asc|desc 的方式） 123select a.* from (select * from log cluster by id ) a order by a.id limit 9 ; (结果有序)9 在这里是表中数据记录的总条数 显示： Time taken: 234.593 seconds, Fetched: 7 row(s) 1select * from (select * from log cluster by id) a； 五、==Hive Join （重难点）==1、Join 连接时，将小表（驱动表）放在join的左边2、Map Join ： 因为Map Join 是在Map端且在内存中进行的，所以不需要启动Reduce任务，也没有shuffle阶段，从而在一定程度上节省资源，提高Join效率。 方式：（两种）1、SQL方式：​ 在HQl语句中添加MapJoin标记（mapjoin）(将小表加入到内存，注意小表的大小) ​ 语法： 12SELECT /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value FROM smallTable JOIN bigTable ON smallTable.key = bigTable.key; 案例： 12SELECT /*+ MAPJOIN(log1) */ log.id,log1.name FROM log JOIN log1 ON log.id = log1.id; 2、自动的MapJoin​ 通过修改以下配置启用自动的mapjoin： 1hive&gt; set hive.auto.convert.join = true; ​ （ 该参数为true时，Hive自动对左边的表统计数据量，如果是小表就加入内存，即对小表使用Map join） 其他相关配置参数： 1hive&gt; set hive.mapjoin.smalltable.filesize; （默认：大表小表判断的阈值25MB左右，如果表的大小小于该值则会被加载到内存中运行，可自定义） 1hive&gt; set hive.ignore.mapjoin.hint; （默认值：true；是否忽略mapjoin hint 即mapjoin标记；如果为false，这则需要添加-MapJoin标记，mapjoin（smalltable）） 1hive&gt; set hive.auto.convert.join.noconditionaltask; （默认值：true；将普通的join转化为普通的mapjoin时，是否将多个mapjoin转化为一个mapjoin） 1hive&gt; set hive.auto.convert.join.noconditionaltask.size; （默认：10M；将多个mapjoin转化为一个mapjoin时，其表的最大值为10M，可自定义） 六、Map-Side聚合 相当于聚合函数：count（） 设置参数，开启在Map端的聚合(相当于combiner) 1set hive.map.aggr=true; 相关配置参数： 1set hive.groupby.mapaggr.checkinterval； （默认为：100000，表示 map端group by执行聚合时处理的多少行数据） 1set hive.map.aggr.hash.min.reduction； （默认为：0.5，进行聚合的最小比例，预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合） 1set hive.map.aggr.hash.percentmemory; （默认： 0.5 ，map端聚合使用的内存的最大值） 1set hive.map.aggr.hash.force.flush.memory.threshold; （默认为：0.9，map端做聚合操作是hash表的最大可用内容，大于该值则会触发flush 1set hive.groupby.skewindata； （默认为：false，是否对GroupBy产生的数据倾斜做优化） 附加： 数据倾斜问题解决：多种方式（使用MapJoin、使用MapSide） 参考 http://www.sohu.com/a/224276626_543508 七、控制Hive中Map和Reduce的数量1、Map数量相关的参数1set mapred.max.split.size; （默认为：256M，一个split的最大值，即每个map处理文件的最大值） 1set mapred.min.split.size.per.node; (一个节点上最小split数：1个) 1set mapred.min.split.size.per.rack; (一个机架上最小split数：1个) 2、Reduce数量相关的参数1set mapred.reduce.tasks; (默认为：-1，强制指定reduce任务的数量。-1，是未定义，不发挥作用。如果指定了，就会按指定的数量执行) 1set hive.exec.reducers.bytes.per.reducer; （默认为：256M ，每个reduce任务处理的数据量） 1set hive.exec.reducers.max; （默认为：1009个，每个任务最大的reduce数 [Map数量 &gt;= Reduce数量 ]） 八、Hive - JVM重用适用场景：1、小文件个数过多2、task个数过多 原理： hadoop默认配置是使用派生JVM来执行map和reduce任务的，JVM重用可以使得JVM实例在同一个JOB中重新使用N次 1set mapred.job.reuse.jvm.num.tasks; (默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM) 缺点： 设置开启之后，task插槽会一直占用资源，不论是否有task运行，直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习]]></title>
    <url>%2F2019%2F01%2F11%2FHive%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Hive是什么？1、基于 Hadoop 的一个数据仓库工具 可以将结构化的数据文件映射为一张hive数据库表； 这张Hive数据库表保存不了metadata元数据信息，而是将metadata存储在本地磁盘上的MySQL（关系型数据库）中 并提供简单的 sql 查询功能； 可以将 sql 语句转换为 MapReduce 任务进行运行。 2、快速实现简单的MapReduce 统计的工具 方便非Java编程者对HDFS的数据做mapreduce操作； 学习成本低，十分适合数据仓库的统计分析。 3、什么是数据仓库？ Data Warehouse(DW 或DWH）是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。 单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制. 数据仓库是用来做查询分析的数据库，基本不用来做插入，修改，删除操作。 4、数据处理的两大分类oltp+olap 联机事务处理OLTP（on-line transaction processing） OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作； 联机分析处理OLAP（On-Line Analytical Processing） OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。 数据文件按结构的分类 结构化数据：关系型 半结构化数据：K-V 松散型： 原理： Hive包括：解释器、编译器、优化器 其中，编译器将一个HiveSQL 转换为操作符，操作符是Hive的最小的处理单位，每一个操作符代表HDFS的一个操作或一个MapReduce作业。 二、Hive架构原理Hive架构图 1、架构图解释： Hive通过用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口 2、用户接口 主要有三个：Client CLI(hive shell 命令行)，JDBC/ODBC(java访问hive)，WEBUI(浏览器访问hive) ​ 其中最常用的是CLI命令行，Cli启动的时候，会同时启动一个Hive副本；Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。 3、元数据:Metastore 元数据包括: 表名,表所属数据库(默认是default) ,表的拥有者,列/分区字段,表的类型(是否是外部表),表的数据所在目录等； 默认存储在自带的derby数据库中,推荐使用MySQL存储Metastore 4、任务运行 Hive 使用HDFS进行存储,使用MapReduce进行计算 (0)驱动器:Driver (1)解析器(SQL Parser):将SQL字符转换成抽象语法树AST,这一步一般使用都是第三方工具库完成,比如antlr,对AST进行语法分析,比如表是否存在,字段是否存在,SQL语句是否有误 (2)编译器(Physical Plan):将AST编译生成逻辑执行计划 (3)优化器(Query Optimizer):对逻辑执行计划进行优化 (4)执行器(Execution):把逻辑执行计划转换成可以运行的物理计划,对于Hive来说,就是MR/Spark 其中(select * 不会产生MR任务) 三、Hive搭建及三种模式1、Hive的安装配置：（1）基本环境：Hadoop集群环境（至少3个节点） Hive是依赖于hadoop系统的，因此在运行Hive之前需要保证已经搭建好hadoop集群环境。 （2）安装一个关系型数据mysql 因为Hive数据仓库的元数据信息是存放在本地磁盘的关系数据库上的 安装步骤：详见 “Linux系统数据库MySQL安装.md” （3）解压安装（按需在指定节点上） 解压apache-hive-1.2.1-bin.tar.gz （4）追加配置环境变量1vim ~/.bash_profile 1234HIVE_HOME=Hive的解压路径HIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/bin （5）替换和添加相关jar包 修改HADOOP_HOME/share/hadoop/yarn/lib目录下的jline-*.jar 将其替换成HIVE_HOME/lib下的jline-2.12.jar。 –将如下(hive连接mysql)的jar包拷贝到hive解压目录的lib目录下 mysql-connector-java-5.1.32-bin.jar （6）修改配置文件（选择3种模式里哪一种）见三种安装模式 （7）启动 12&gt; hive &gt; 启动hive交互式界面 2、三种模式 三种模式 A、内嵌模式（元数据保存在内嵌的derby中，允许一个会话链接，尝试多个会话链接时会报错）【了解】 B、本地模式（本地安装mysql 替代derby存储元数据）【重要】 C、远程模式（远程安装mysql 替代derby存储元数据）【重要】 （1）内嵌Derby单用户模式（了解） 元数据是内嵌在Derby数据库中的，只能允许一个会话连接，数据会存放到HDFS上。 存储方式简单，只需要hive-site.xml 注：使用 derby存储方式时，运行 hive 会在当前目录生成一个 derby 文件和一个metastore_db hive-site.xml ： 12345678910111213141516171819&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.local&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; （2）本地用户模式（重要，多用于本地开发测试） 与嵌入式的区别 不再使用内嵌的Derby作为元数据的存储介质，而是使用其他数据库比如MySQL来存储元数据且是一个多用户的模式，运行多个用户client连接到一个数据库中。这种方式一般作为公司内部同时使用Hive。 这里有一个前提，每一个用户必须要有对MySQL的访问权利，即每一个客户端使用者需要知道MySQL的用户名和密码才行。 需要在本地运行一个 mysql 服务器 在node00上（与MySQL在同一个节点上）解压安装Hive MySQL， Hive : node00 需要将 连接mysql 的 jar 包（mysql-connector-java-5.1.32-bin.jar）拷贝到$HIVE_HOME/lib 目录下 hive-site.xml 12345678910111213141516171819202122232425262728&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_local/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node00:3306/hive_local?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 注意：需要实现在mysql中创建数据库：hive_local （3）远程模式（重要） remote 一体 将Hive解压安装与MySQL不同的节点上 MySQL ：node00 Hive ： node02 需要在 Hive服务器启动 meta服务 Hive的服务端和客户端在同一台节点 配置hive-site.xml（在 node02节点上） (hadoop 2.6.5) 12345678910111213141516171819202122232425262728&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_remote/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://node02:3306/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 如果在hadoop2.5.X环境下还需要添加 1234&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node02:9083&lt;/value&gt;&lt;/property&gt; 注：这里把hive的服务端和客户端都放在同一台服务器上了。服务端和客户端可以拆开 Remote 分开(公司企业经常用) 将 hive-site.xml 配置文件拆为如下两部分（此时不与MySQL在同一台节点上） MySql ： node00 服务端 ： node02 客户端 ： node01 1）、服务端配置文件（node02） 配置hive-site.xml 123456789101112131415161718192021222324&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive2/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://node00:3306/hive2?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2）、客户端配置文件（node01） 配置hive-site.xml 1234567891011121314151617&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive2/warehouse&lt;/value&gt; &lt;!--注意这里的路径要和服务端一致---&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node02:9083&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动 hive 服务端程序 1hive --service metastore &amp; 客户端启动 1hive Hive常见问题总汇： http://blog.csdn.net/freedomboy319/article/details/44828337 https://gengqi88.iteye.com/blog/1983492 如果报错： org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083. 查看进程： 1jps 将启动命令的节点上所以Runjar 进程执行如下kill 命令 1kill -9 pid 四、HQL详解Hql 就是HiveSQl语句 1、DDL语句（数据库定义语言）（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDLHive的数据定义语言 （LanguageManual DDL;)） 重点 hive 的建表语句和分区。 （2）创建/删除/修改/使用数据库 创建数据库 （Hive搭建完毕后，会创建一个默认的数据库） 查看 show databases； 创建 12&gt; CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment];&gt; 举例： create database attribute; create database attr; 注意：创建数据时，数据库名不要和系统关键字冲突，否则会报错； 如下： 123456789101112131415161718192021命令：hive&gt; create database out;报错：FAILED: ParseException line 1:16 Failed to recognize predicate 'out'. Failed rule: 'identifier' in create database statement原因：在Hive1.2.0版本开始增加了如下配置选项，默认值为true：hive.support.sql11.reserved.keywords该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。解决：法一：弃用这个关键字，换个名字法二：弃用对保留关键字的支持在conf下的hive-site.xml配置文件中修改配置选项：&lt;property&gt; &lt;name&gt;hive.support.sql11.reserved.keywords&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 删除数据库 12&gt; DROP (DATABASE|SCHEMA) [IF EXISTS] database_name;&gt; 举例： drop database attribute; 修改数据库(了解) ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …); ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; 使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库） 12&gt; USE database_name;&gt; 举例： use attr； （3）创建/删除/表（重点） ==创建表（重要！）== 数据类型： data_type : primitive_type 原始数据类型 | array_type 数组 | map_type map | struct_type | union_type – (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION | STRING 基本可以搞定一切 | BINARY | TIMESTAMP | DECIMAL | DECIMAL(precision, scale) | DATE | VARCHAR | CHAR array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], …&gt; union_type : UNIONTYPE &lt; data_type, data_type, … &gt; 1、准备数据 1231,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing 2、创建表 (如果没有指定进入某一数据库，就会在默认数据库中创建) 1234567891011create table log( id int, name string, age int, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by ',' COLLECTION ITEMS TERMINATED by '-' map keys terminated by ':' lines terminated by '\n'; 导入数据（属于DML但是为了演示需要在此应用） 123456LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [LOCAL]:从本地 | 若无，则为从HDFS [OVERWRITE] ： 会覆盖Hive表中的数据 | 若无则会追加 [PARTITION....] ： 创建分区 将log1文件中的数据加载到log表中 （log1中数据的格式要和log表格式保持一致，否则会乱；若文件已存在，则会自动重命名） 本地加载（相当于复制）数据到Hive的制定表中 12&gt; LOAD DATA LOCAL INPATH '/root/su/log1' INTO TABLE log;&gt; HDFS加载（相当于剪切）数据到Hive的制定表中 12&gt; LOAD DATA INPATH '/root/su/log1' INTO TABLE log ;&gt; 查看表中数据 1234567&gt; 对本表查询不会产生MapReduce任务&gt; hive&gt; select * from log;&gt; 使用函数查询会产生MapReduce任务&gt; hive&gt; select count(*) from log;&gt; 查询表的字段信息：描述字段类型&gt; hive&gt; desc log;&gt; 第一个查询结果： 12341 zshang 18 [&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;1 zhaoliu 18 [&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;2 lishi 16 [&quot;shop&quot;,&quot;boy&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;hunan&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;3 wang2mazi 20 [&quot;fangniu&quot;,&quot;eat&quot;] &#123;&quot;stu_addr&quot;:&quot;shanghai&quot;,&quot;work_addr&quot;:&quot;tianjing&quot;&#125; 第二个查询结果： 14 附加题 查询表中likes字段中有girl的人 1hive&gt; select name from log2 where likes[1]=&quot;girl&quot;; 查询表中address字段有stu_addr为beijing的人 1hive&gt; select name from log2 where address[&quot;stu_addr&quot;]=&quot;beijing&quot;; 3、删除表 12&gt; DROP TABLE [IF EXISTS] table_name [PURGE];&gt; 举例： （用drop命令删除表，会将表中数据一并删除，其对应在MySQl中的表的元数据信息也会随之删除； ​ 用hdfs命令删除表对应的文件目录，表中数据也一并删除，但其元数据信息依然保存在My SQL上， ​ 再load数据，可恢复该表） 12&gt; drop table log1；&gt; 123456&gt; hdfs dfs -rmr /user/hive_local/warehouse/attr.db/log1&gt; &gt; hive&gt; use attr;&gt; hive&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;&gt; &gt; 创建外部表（重要） 外部关键字EXTERNAL允许您创建一个表,并提供一个位置,以便hive不使用这个表的默认位置。这方便如果你已经生成了数据，当删除一个外部表,表中的数据不会从文件系统中删除。外部表指向任何HDFS的存储位置,而不是存储在配置属性指定的文件夹 hive.metastore.warehouse.dir;)中 创建表： 1234567891011create EXTERNAL table log1( id int, name string, age int, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; lines terminated by &apos;\n&apos;; 加载数据： 1LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1; 删除外部表（相当于删除的是表的元数据信息，而表中的数据还保存） 1drop table log1； 结果： hive&gt; show tables; 无log1 MySQl中也无此表元数据信息 但是， 在HDFS文件系统中，此表数据依然存在 也就是说，此表还可以恢复 恢复表： 1重新创建log1表，该表即可恢复 （4）修改表,更新，删除数据(这些很少用)重命名表 1234&gt; ALTER TABLE table_name RENAME TO new_table_name;&gt; &gt; Eg: alter table meninem rename to jacke;&gt; 更新数据 1UPDATE tablename SET column = value [, column = value ...][WHERE expression] 删除数据 1DELETE FROM tablename [WHERE expression] 2、DML语句（数据库管理语言）（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML 重点是数据加载和查询插入语法 Hive数据操作语言（LanguageManual DML;)） （2）四种插入/导入数据(重要) Hive不能很好的支持用insert语句一条一条的进行插入操作，不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。 1234567create table log3( id int, name string, age int ) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos;; 1.直接加载数据12LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]load data local inpath '/root/su/log1' into table log1; 2.将表1查询结果插入表2注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 1234567创建person2表，然后从表person1查询数据导入：覆盖：INSERT OVERWRITE TABLE person2 [PARTITION(dt=&apos;2008-06-08&apos;, country)] SELECT id,name, age From ppt;追加：INSERT INTO TABLE log3 SELECT id,name, age From log; 3.将表1、表2查询结果插入表3、表4注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 1234567891011121314FROM person t1INSERT OVERWRITE | INTO TABLE person1 [PARTITION(dt=&apos;2008-06-08&apos;, country)] SELECT t1.id, t1.name, t1.age ; FROM log t1,log1 t2 INSERT OVERWRITE TABLE log4 SELECT t1.id,t1.name,t2.age ; 是否存在笛卡尔积：？？？？存在。 为了防止笛卡尔积： FROM log t1,log1 t2 INSERT OVERWRITE TABLE log4 SELECT t1.id,t1.name,t2.age where t1.id =t2.id; 1234【from放前面好处就是后面可以插入多条语句 】FROM abc t1,sun t2 INSERT OVERWRITE TABLE qiniu SELECT t1.id,t1.name,t1.age,t2.likes,t2.address ; 12345FROM abc t1,sun t2 INSERT OVERWRITE TABLE qiniu SELECT t1.id,t1.name,t1.age,t1.likes,t1.address where…INSERT OVERWRITE TABLE wbb SELECT t2.id,t2.name,t2.age,t2.likes,t2.address where…; 4.直接列出数据插入表中（大量数据时不推荐）注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 12INSERT INTO TABLE students VALUES (1,&apos;zs&apos;,18,&apos;boy&apos;,&apos;beijng&apos;),(2,&apos;wh&apos;,&apos;girl&apos;,&apos;stu_addr&apos;:shanghai&apos;); 本地load数据和从HDFS上load加载数据的过程有什么区别？ 本地： local 会自动复制到HDFS上的hive的**目录下 Hdfs导入 后移动到hive的**目录下 （3）查询数据并保存 保存数据到本地： 123456789101112insert overwrite local directory '/opt/datas/hive_exp_emp2' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' select * from db_1128.emp ;留意两种的区别：保存的数据格式insert overwrite local directory '/sun/temp/hive_save1' row format delimited fields terminated by ',' COLLECTION ITEMS TERMINATED by '-' map keys terminated by ':' select * from log2 ; 这里如果将 overwrite 改为into 会报错。 12//查看数据!cat /sun/temp/hive_save1/000000_0; 保存数据到HDFS上： 12345678910insert overwrite directory &apos;/user/beifeng/hive/hive_exp_emp&apos; select * from db_1128.emp ;insert overwrite directory &apos;/sun/hive/temp/hive_save1&apos; row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; select * from log2 ; 这里如果将 overwrite 改为into 会报错。 在外部shell中将数据重定向到文件中： 123(注意：需要指明是哪个数据库的表)# hive -e &quot;select * from attr.log;&quot; &gt; /sun/hive/temp/hive_save2# cat /sun/hive/temp/hive_save2 （4）备份数据或还原数据（在HDFS上） 备份数据（包括表的元数据和表中的数据）： 1EXPORT TABLE log to &apos;/sun/hive/datas/export/cp1&apos; 删除再还原数据： 12345先删除表。drop table log;show tables ;再还原数据：IMPORT FROM &apos;/sun/hive/datas/export/cp1&apos; ; （5）其他Hql操作Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别：http://www.2cto.com/kf/201609/545560.html 3、Hive SerDe（序列化、反序列化）(1)定义Hive SerDe - Serializer and Deserializer SerDe 用于做序列化和反序列化。 构建在数据存储和执行引擎之间，对两者实现解耦。 对数据实现序列化，清洗数据，使之成为有效数据并加载。 Hive通过ROW FORMAT DELIMITED以及SERDE进行内容的读写。 （2）实现1234567row_format: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] : SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 12345678910111213141516171819202122232425262728293031323334Hive正则匹配（实现数据清洗）创建表 logtbl： CREATE TABLE logtbl ( host STRING, identity STRING, t_user STRING, time STRING, request STRING, referer STRING, agent STRING) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos; WITH SERDEPROPERTIES ( &quot;input.regex&quot;=&quot;([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \&quot;(.*)\&quot; (-|[0-9]*) (-|[0-9]*)&quot;) STORED AS TEXTFILE; 加载数据:load data local inpath &apos;/root/su/localhost_access_log.2016-02-29&apos; into table logtbl;查看数据：select * from logtbl;显示：//192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /bg-upper.png HTTP/1.1 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /bg-nav.png HTTP/1.1 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /asf-logo.png HTTP/1.1 304 -...(省略。。。) 表数据--见数据文件：localhost_access_log.2016-02-29.txt 12345678910111213141516171819202122192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 - 五、Beeline和Hiveserver2（Hive的升级）1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）1# ./hiveserver2 若已经配置环境变量则启动方式为： 1# hivesever2 2、启动 beeline（可在服务端|客户端启动，相当于客户端） 因为beeline是在Hive安装目录的/bin下，所以只要有hive包都可以启动 1234567891011121314151617181920212223242526272829303132333435363738# ./beelinebeeline&gt; !connect jdbc:hive2://node00:10000 root 123456显示：Connecting to jdbc:hive2://node00:10000Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://node00:10000&gt;使用：列出数据库0: jdbc:hive2://node00:10000&gt; show databases;+----------------+--+| database_name |+----------------+--+| attr || attribute || default |+----------------+--+3 rows selected (7.493 seconds)0: jdbc:hive2://node00:10000&gt;而在服务端：显示：[root@node00 ~]# hiveserver219/01/07 08:52:09 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not existOKOKOKOK退出：服务端：ctrl + c客户端：！quit； 或 ctrl + c作用：对操作结果添加了美化。不过不太常用，耗内存，数据大的时候，还影响页面。 六、Hive的JDBC 一般是平台使用展示或接口，服务端启动hiveserver2后，在java代码中通过调用hive的jdbc访问默认端口10000进行连接、访问 12345678910111213141516171819public class HivejdbcClient &#123; private static String driverName = "org.apache.hive.jdbc.HiveDriver"; public static void main(String[] args)&#123; try&#123; Class.forName(driverName); &#125;catch (ClassNotFoundException)&#123; e.printStackTrace(); System.exit(1); &#125;// repalace "hive" here with the name of user the queries should run as Connection con = DriverManager.getConnection("jdbc:hive2://node00:10000/default","root","123456"); Statement stmt = con.createStatement(); String sql = "select * from log limit 0"; ResultSet rs = stmt.executeQuery(sql); while(rs.next())&#123; System.out.println(rs.getInt(1)+"-"+rs.getString("name")); &#125; &#125;&#125; 七、==Hive分区与自定义函数UDF UDAF UDTF==1、==Hive的分区partition（重要）== 功能： 为了方便海量数据的管理和查询，可以对数据建立分区（可按日期、部门、类型等具体业务）。进行分门别类的管理。 注意： 必须在定义表的时候创建partition分区 存储数据时，添加分区字段的数据，直接将数据按分区进行存储。 添加分区时： ​ 时间的格式：/ ： 存储时会乱码，用 - 不会。 ​ 需要指定分区 ​ 多个分区时，存在父子目录关系，按顺序对应，对应父子 ​ 创建表时，已经指定分区个数，就只能填加指定个数的字段数据 删除分区时： ​ 若该分区是父分区的最后一个子区，则父分区也会被删除 ​ 若删除父分区，其所有子分区也都会被删除 ​ 若删除的分区，分别在多个不同父分区中存在，则都会被删除 重命名分区时： ​ 修改之后的名字不能是已经存在的 注意：在创建 删除多分区等操作时一定要注意分区的先后顺序，他们是父子节点的关系。分区字段不要和表字段相同 类别： 单分区和多分区 静态分区和动态分区 （1）创建分区 单分区建表 123456create table day_table(id int, content string) partitioned by (dt string) row format delimited fields terminated by ','; 注意：【单分区表，按天分区，在表结构中存在id，content，dt三列；以dt为文件夹区分】 双分区建表 123456create table day_hour_table (id int,content string) partitioned by (dt string, hour string) row format delimited fields terminated by ','; 注意： 【双分区表，按天和小时分区，在表结构中新增加了dt和hour两列；先以dt为文件夹，再以hour子文件夹区分】 （2）添加分区表的分区（表已创建，在此基础上添加分区：按什么分区）： 注意：报错：此时添加，要注意分区的个数相对应，否则会报错： 1FAILED: ValidationFailureSemanticException Partition spec &#123;dt=2008-08-08, hour=08&#125; contains non-partition columns 注意：报错此时添加，要注意分区的字段名要对应添加，否则会报如下错误： 1FAILED: ValidationFailureSemanticException Partition spec &#123;d=2008-08-08&#125; contains non-partition columns 注意：一定是存在分区，才可添加 添加分区： 12ALTER TABLE table_nameADD partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ... 123例： ALTER TABLE day_table ADD PARTITION (dt='2028-08-08', hour='08');ALTER TABLE day_table ADD PARTITION (dt='2028-08-08'); （3）删除分区语法：（– 用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。） 删除如双分区中的子级分区时，如果仅剩一个子分区，那么父级分区也会被删除。（连坐） 1ALTER TABLE table_name DROP partition_spec, partition_spec,... 1234例：ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;); （4）数据加载进分区表中语法： 1LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1,partcol2=val2 ...)] 12345HDFS：LOAD DATA INPATH '/user/pv.txt' INTO TABLE day_hour_table PARTITION(dt='2008-08-08', hour='08');本地：LOAD DATA local INPATH '/user/hua/*' INTO TABLE day_hour partition(dt='2010-07-07'); （5）查看表的所有分区123hive&gt; show partitions day_hour_table;show partitions day_table; （6）重命名分区语法： 1ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec1; 123例：ALTER TABLE day_table PARTITION (tian='2018-05-01') RENAME TO PARTITION (tain='2018-06-01'); （7）==动态分区(重要)–注意外部表== 在本地文件/home/grid/a 中写入以下4行数据 aaa,US,CA aaa,US,CB bbb,CA,BB bbb,CA,BC 建立非分区表并加载数据 创建表 info1 123456CREATE TABLE ( name STRING, cty STRING, st STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 加载数据 1LOAD DATA LOCAL INPATH '/root/su/a' INTO TABLE info1; 查看 1SELECT * FROM info1; 建立外部分区表 info2 , 并动态加载数据 （注意删除外部表的相关事项） 1234CREATE EXTERNAL TABLE info2 (name STRING) PARTITIONED BY (country STRING, state STRING); 实现动态分区 1234567891011set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.max.dynamic.partitions.pernode=1000; INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; --两次插入数据，会有两份相同的数据SELECT * FROM info2; 使用动态分区需要注意设定以下参数： hive.exec.dynamic.partition 默认值：false 是否开启动态分区功能，默认false关闭。 使用动态分区时候，该参数必须设置成true; hive.exec.dynamic.partition.mode 默认值：strict 动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。 一般需要设置为nonstrict hive.exec.max.dynamic.partitions.pernode 默认值：100 在每个执行MR的节点上，最大可以创建多少个动态分区。 该参数需要根据实际的数据来设定。 比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。 hive.exec.max.dynamic.partitions 默认值：1000 在所有执行MR的节点上，最大一共可以创建多少个动态分区。 同上参数解释。 hive.exec.max.created.files 默认值：100000 整个MR Job中，最大可以创建多少个HDFS文件。 一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。 hive.error.on.empty.partition 默认值：false 当有空分区生成时，是否抛出异常。 一般不需要设置。 2、自定义函数UDF UDAF UDTF 自定义函数包括三种 UDF、UDAF、UDTF UDF：一进一出 UDAF：聚集函数，多进一出。如：Count/max/min UDTF：一进多出，如 lateralview explore()，（类似于mysql中的视图） 使用方式 ：在HIVE会话中add自定义函数的jar 文件，然后创建 function 继而使用函数 （1）UDF 开发（用的多一点）Hive的函数课参考官网，用时查阅即可： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF 1、UDF函数可以直接应用于 select 语句，对查询结构做格式化处理后，再输出内容。 2、编写 UDF 函数的时候需要注意一下几点： a）自定义 UDF 需要继承 org.apache.hadoop.hive.ql.UDF。 b）需要实现 evaluate 函数，evaluate 函数支持重载。 3、步骤 a）把程序打包放到目标机器上去； （需要hive和hadoop，jdk 的相关jar包） 函数一：脱敏处理 123456789101112131415161718192021222324package com.bigdata.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public class TuoMing extends UDF &#123; private Text res = new Text(); public Text evaluate(String string) &#123; // 校验参数是否为空 if(string==null)&#123; return null; &#125; // 若为单个字符 if(string.length()==1)&#123; res.set("*"); &#125; String str1 = string.substring(0,1); String str2 = string.substring(string.length()-1,string.length()); res.set(str1+"***"+str2); return res; &#125; &#125; 函数二：add函数 12345678910111213141516171819202122package com.bigdata.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public class Add extends UDF &#123; private Text res = new Text(); public Text evaluate(String num1,String num2) &#123; // 校验参数是否为空 if(num1==null)&#123; return null; &#125;else if(num2==null)&#123; res.set(num1); return res; &#125; int n = Integer.parseInt(num1)+Integer.parseInt(num2); String str =n+""; res.set(str); return res; &#125; &#125; b）进入 hive 客户端，添加 jar 包 1234hive&gt;add jar /root/su/TuoMing.jar;(相当于添加到环境变量中)(清除缓存时记得删除jar包： delete jar /*)delete jar /jar/udf_test.jar; c）创建临时函数： 123hive&gt;CREATE TEMPORARY FUNCTION add_example AS 'hive.udf.add';CREATE TEMPORARY FUNCTION tm_example AS 'com.bigdata.hive.udf.TuoMing';（as 后面添加的是：包名+类名） d）查询 HQL 语句： 12345SELECT add_example(8,9) FROM scores;SELECT add_example(scores.math,scores.art) FROM scores;SELECT tm_example(id) FROM log; e）销毁临时函数： 1hive&gt; DROP TEMPORARY FUNCTION tm_example; （2）UDAF自定义聚集函数(用的少) 多行进一行出，如 sum()、min()，用在 group by 时 1.必须继承org.apache.hadoop.hive.ql.exec.UDAF(函数类继承) org.apache.hadoop.hive.ql.exec.UDAFEvaluator(内部类 Evaluator 实现 UDAFEvaluator 接口) 2.Evaluator 需要实现 init、iterate、terminatePartial、merge、terminate 这几个函数 init():类似于构造函数，用于 UDAF 的初始化 iterate():接收传入的参数，并进行内部的轮转，返回 boolean terminatePartial():无参数，其为 iterate 函数轮转结束后，返回轮转数据， 类似于 hadoop 的Combinermerge()：接收 terminatePartial 的返回结果，进行数据 merge 操作， ​ 其返回类型为 boolean terminate():返回最终的聚集函数结果 开发一个功能同： Oracle 的 wm_concat()函数 Mysql 的 group_concat() Hive UDF 的数据类型： Hive UDF 的数据类型： （3）UDTF（用的少一点）UDTF：一进多出，如 lateral view explode( ) 返回一个数组表 Hive Lateral View 视图 Lateral View用于和UDTF函数（explode、split）结合来使用。 首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。 主要解决 在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题 语法： LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias) 例： 统计人员表中共有多少种爱好、多少个城市? 1234&gt; select count(distinct(myCol1)), count(distinct(myCol2))，count(distinct(myCol3))from log2 &gt; LATERAL VIEW explode(likes) myTable1 AS myCol1 &gt; LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;&gt; 123select myCol1, myCol2 from log2 LATERAL VIEW explode(likes) myTable1 AS myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; distinct(myCol1) 表示去重 LATERAL VIEW explode(likes) myTable1 AS myCol1 将likes查询结果放到mytable1表中，作为字段myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; 将address查询结果放到myTable2 表中，作为字段myCol2，myCol3，因为address是包含K-V的（两个） 八、Hive索引(知道) 一个表上创建索引： 使用给定的列表的列作为键创建一个索引。 详见创建索引;)设计文档。 12345678910111213CREATE INDEX index_name ON TABLE base_table_name (col_name, ...) AS index_type [WITH DEFERRED REBUILD] [IDXPROPERTIES (property_name=property_value, ...)] [IN TABLE index_table_name] [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] [LOCATION hdfs_path] [TBLPROPERTIES (...)] [COMMENT "index comment"]; 九、案例实践案例一：(基站掉话率)基站掉话率 1、创建表cell_monitor表 1234567891011121314create table cell_monitor( record_time string, imei string, cell string, ph_num int, call_num int, drop_num int, duration int, drop_rate DOUBLE, net_type string, erl string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;STORED AS TEXTFILE; 结果表cell_drop_monitor 12345678create table cell_drop_monitor(imei string,total_call_num int,total_drop_num int,d_rate DOUBLE) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;STORED AS TEXTFILE; 2、load数据1LOAD DATA LOCAL INPATH &apos;/root/su/cdr_summ_imei_cell_info.csv&apos; OVERWRITE INTO TABLE cell_monitor; 3、找出掉线率最高的基站12345from cell_monitor cm insert overwrite table cell_drop_monitor select cm.imei ,sum(cm.drop_num),sum(cm.duration),sum(cm.drop_num)/sum(cm.duration) d_rate group by cm.imei sort by d_rate desc; 案例二：（单词统计）1、建表12create table docs(line string);create table wc(word string, totalword int); 2、加载数据1load data local inpath &apos;/tmp/wc&apos; into table docs; 3、统计12345from (select explode(split(line, &apos; &apos;)) as word from docs) w insert into table wc select word, count(1) as totalword group by word order by word; 4、查询结果1select * from wc; 十、==分桶（重要）==1、概念 主要应用于数据抽样。 通过对列值取哈希值的方式，将不同数据放到不同的文件中存储。 对Hive中每个表、分区都可以进行分桶。 列的哈希值 /桶的个数→决定每条数据划分到哪个桶中 2、开启支持分桶1hive&gt; set hive.enforce.bucketing=true; 默认：false； 设置为true之后，mr运行时会根据bucket的个数自动分配reduce task个数。 （用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用） 一次作业产生的桶数 = reducde task数 3、往分桶表中加载数据12insert into table bucket_table select columns from tbl;insert overwrite table bucket_table select columns from tbl; 4、桶表抽样查询1select * from bucket_table tablesample(bucket 1 out of 4 on columns); TABLESAMPLE语法： 12&gt; TABLESAMPLE(BUCKET x OUT OF y)&gt; x：表示从哪个bucket开始抽取数据，x&lt;=y y：必须为该表总bucket数的倍数或因子 理解： 分桶表已经按age分为4桶，然后，有y个人去抽，从第(x 取模 桶数)桶中抽 5、实战创建普通表 123456CREATE TABLE mm( id INT, name STRING, age INT)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 测试数据 123456781,tom,112,cat,223,dog,334,hive,445,hbase,556,mr,667,alice,778,scala,88 加载数据： 1load data local inpath '/root/su/mm' into table mm; 创建分桶表 1234567CREATE TABLE psnbucket( id INT, name STRING, age INT)CLUSTERED BY (age) INTO 4 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 加载数据： 1insert into table psnbucket select id, name, age from mm; 抽样 1select id, name, age from psnbucket tablesample(bucket 2 out of 4 on age); 注意： hive&gt; select id, name, age from psnbucket tablesample(bucket 4 out of 2 on age);FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table psnbucket denominator : 分母 十一、运行方式1、Hive运行模式 – 命令行方式cli：控制台模式 1234567--与hdfs交互 * 执行dfs命令 * 例 ：hive&gt; dfs -ls / --与Linux交互 * ！ 开头 * hive&gt; !pwd –脚本运行方式：（生产中常用） 123456789101112131415161718192021222324在外部shell中执行,指定数据库,分号可加可不加# hive -e &quot;select * from attr.log &quot;# hive -e &quot;select * from attr.log；select * from default.log2&quot;--------------------------------------------------------------将执行结果重定向到指定文件：# hive -e &quot;select * from attr.log &quot; &gt;&gt;log1--------------------------------------------------------------静默模式执行，不打印log日志# hive -S -e &quot;select * from attr.log &quot; &gt;&gt;log1--------------------------------------------------------------脚本执行先编辑脚本问价# vim file1编辑内容select * from attr.log where id = 1;select * from attr.log where id &lt; 3;执行脚本# hive -f file1--------------------------------------------------------------?? 使用命令文件执行hive-init.sql?? # hive -i /home/hive-init.sql--------------------------------------------------------------在hive cli中执行脚本文件hive&gt; source file1 ？未解决？ ?? 使用命令文件执行hive-init.sql?? # hive -i /home/hive-init.sql 十二、hive的GUI接口（web页面）Hive Web GUI接口 web界面安装：1、下载源码包apache-hive-1.2.1-src.tar.gz, 2、在本地Windows系统中解压 并将\apache-hive-1.2.1-src\hwi\web路径中所有的文件打成war包 制作方法： war包 1、到\apache-hive-1.2.1-src\hwi\web路径下 2、在路径栏输入命令：jar -cvf hive-hwi.war * 3、即可生成文件：hive-hwi.war 3、将hwi-war包放在$HIVE_HOME/lib/中（Linux系统） 4、复制tools.jar(在jdk的lib目录下)到$HIVE_HOME/lib下 5、修改hive-site.xml 路径：/usr/soft/apache-hive-1.2.1-bin/conf/hive-site.xml 123456789101112&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi.war&lt;/value&gt; &lt;/property&gt; 6、启动hwi服务(端口号9999) 1hive --service hwi 7、浏览器通过以下链接来访问 http://node00:9999/hwi/ 8、登录页面： USER: GROUPS: 自已定义 十三、权限管理Hive - SQL Standards Based Authorization in HiveServer2 （1）三种授权模型 （2）常用：基于SQL标准的完全兼容SQL的授权模型特点： 支持对于用户的授权认证 支持角色role的授权认证 role可理解为是一组权限的集合，通过role为用户授权 一个用户可以具有一个或多个角色 ​ 默认包含俩种角色：public、admin 限制 （3）操作在hive服务端修改配置文件hive-site.xml添加以下配置内容： 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.users.in.admin.role&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator&lt;/value&gt;&lt;/property&gt; 服务端启动hiveserver2；客户端通过beeline进行连接 角色的添加、删除、查看、设置： 第一次操作无权限： 需要：CREATE ROLE admin； 12345CREATE ROLE role_name; -- 创建角色DROP ROLE role_name; -- 删除角色SET ROLE (role_name|ALL|NONE); -- 设置角色SHOW CURRENT ROLES; -- 查看当前具有的角色SHOW ROLES; -- 查看所有存在的角色 【官网：权限】 Action Select Insert Update Delete Owership Admin URL Privilege(RWX Permission + Ownership) ALTER DATABASE Y ALTER INDEX PROPERTIES Y ALTER INDEX REBUILD Y ALTER PARTITION LOCATION Y Y (for new partition location) ALTER TABLE (all of them except the ones above) Y ALTER TABLE ADD PARTITION Y Y (for partition location) ALTER TABLE DROP PARTITION Y ALTER TABLE LOCATION Y Y (for new location) ALTER VIEW PROPERTIES Y ALTER VIEW RENAME Y ANALYZE TABLE Y Y CREATE DATABASE Y (if custom location specified) CREATE FUNCTION Y CREATE INDEX Y (of table) CREATE MACRO Y CREATE TABLE Y (of database) Y (for create external table – the location) CREATE TABLE AS SELECT Y (of input) Y (of database) CREATE VIEW Y + G DELETE Y DESCRIBE TABLE Y DROP DATABASE Y DROP FUNCTION Y DROP INDEX Y DROP MACRO Y DROP TABLE Y DROP VIEW Y DROP VIEW PROPERTIES Y EXPLAIN Y INSERT Y Y (for OVERWRITE) LOAD Y (output) Y (output) Y (input location) MSCK (metastore check) Y SELECT Y SHOW COLUMNS Y SHOW CREATE TABLE Y+G SHOW PARTITIONS Y SHOW TABLE PROPERTIES Y SHOW TABLE STATUS Y TRUNCATE TABLE Y UPDATE Y 十四、==Hive优化（重点）==详见Hive优化文档 hive 参数与变量1、hive当中的参数、变量hive当中的参数、变量，都是以命名空间开头 命名空间 读写权限 含义 hiveconf 可读写 hive-site.xml当中的各配置变量例：hive –hiveconf hive.cli.print.header=true system 可读写 系统变量，包含JVM运行参数例：system:user.name=root env 只读 环境变量例：env:JAVA_HOME hivevar 可读写 例：hive -d val=key 通过${}方式进行引用，其中system、env下的变量必须以前缀开头 2、hive 参数设置方式1、修改配置文件 ${HIVE_HOME}/conf/hive-site.xml 2、启动hive cli时，通过–hiveconf key=value的方式进行设置 例： 1hive --hiveconf hive.cli.print.header=true 3、进入cli之后，通过使用set命令设置 3、hive set命令- 在hive CLI控制台可以通过set对hive中的参数进行查询、设置 - set设置： 1set hive.cli.print.header=true; - set查看 1set hive.cli.print.header - hive参数初始化配置 当前用户家目录下的.hiverc文件 如: 1~/.hiverc 如果没有，可直接创建该文件，将需要设置的参数写到该文件中，hive启动运行时，会加载改文件中的配置。 - hive历史操作命令集 1~/.hivehistory Hive常用函数： https://www.cnblogs.com/kimbo/p/6288516.html https://www.iteblog.com/archives/2258.html#3_avg https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions MapReducde底层源码： http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1 http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3 http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce源码分析]]></title>
    <url>%2F2019%2F01%2F08%2FMapReduce%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce案例实践]]></title>
    <url>%2F2019%2F01%2F07%2FMapReduce%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[端口统计： Port 使用框架 50070 hdfs的http端口 9000 hadoop1.X的rpc端口 8020 hadoop2.X的rpc端口 8088 YARN的http端口 一、单词字数统计Job类 新建Configuration ： ​ 设置hdfs和yarn的配置 获取Job ： ​ 设置Job类和JobName ​ 设置Map端和Reduce端的类 ​ 设置Map端输出的key和value的类 调用FileInputFormat类添加输入的文件 调用FileOutputFormat类添加计算结果存放的路径 Map类 将输入的（K，V）转换成新的（K,V）：每个单词计数为1 通过Context写出 Reduce类 遍历获取的value-list ， 实现累加 通过Context写出 二、二度人脉推荐Job类 新建Configuration ： ​ 设置hdfs和yarn的配置 获取Job ： ​ 设置Job类和JobName ​ 设置Map端和Reduce端的类 ​ 设置Map端输出的key和value的类 调用FileInputFormat类添加输入的文件（在hdfs上） 调用FileOutputFormat类添加计算结果存放的路径（在hdfs上） 如上配置两个Job任务。 Map01类 根据一度好友，建立排序后的某一用户与好友对应关系，作为key ，用0作为value 同样 ，建立排序后某一用户好友的好友之间的对应关系，作为key ， 用 1 作为value 用context写出 Reduce01类 排除好友对应关系中value为 0 的 key ， 统计好友关系中value 为非0 的key的个数 ， 并将该好友关系拼接个数，作为key，用context写出，null为value Map02类 切分输入的key ， 分别获取好友关系以及个数，写出时，根据个数排序， Reduce02类 再次合并，输出二度好友关系，按热度排序 三、天气统计每月Top存在二次排序：需要定义两个比较器 分组—排序 排序–再按温度 Map类 将输入的数据的格式，转换成所需的文件格式对象，并以（K,V）格式写出 在写出之前，已经按月分组，并按温度排序 Reduce类 将对象转换成字符串，再以新的（K,V）格式写出 写出之前只取前两个温度最高的 四、tf-idf ：微博热词重要性搜索分成三个Job Job1 基本配置以及指定输入输出文件路径 Map1 计算词频 ， 分词器ik ， 得到的单词拼接微博Id ，作为key ， 以1 为value 以count为key ， 以 1 为value ， 用来对微博计数 Reduce1 对分词累加 ， 对微博数累加 ， 按分区以新的（K,V） 写出 Job 2 以Job1 的输出作为输入文件，再指定输出文件路径 Map2 获取所有输入的split ， 对所有单词 ， 以单词为key ， 以 1 为value Reduce2 对所有单词进行统计， Job3 以Job1 的输出文件作为输入文件 Map3]]></content>
      <categories>
        <category>计算框架</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
        <tag>分布式离线计算框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce学习]]></title>
    <url>%2F2019%2F01%2F05%2FMapReduce%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、MapReduce是什么1、概念 MapReduce是一种分布式离线计算框架，是一种编程模型，用于在分布式系统上大规模数据集(大于1TB)的并行运算。 分布式编程： 借助一个集群，通过多台机器去并行处理大规模数据集，从而获得海量计算能力。 2、理解 Map(映射) Reduce(归约) 指定一个Map(映射)函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce(归约)函数，用来保证所有映射的键值对中的每一个共享相同的键组。 二、MapReduce设计理念1、分布式计算 分布式计算将该应用分解成许多小的部分，分配给多台计算机节点进行处理。这样可以节约整体计算时间，大大提高计算效率。 分而治之策略： 一个存储在分布式文件系统中的大规模数据集， 会被切分成许多独立的分片（split）， 这些分片可以被 多个Map任务并行处理 2、移动计算，而非移动数据 将计算程序应用移动到具有数据的集群计算机节点之上进行计算操作； 将有用、准确、及时的信息提供给任何时间、任何地点的任何客户。 3、Master/Slave架构 包括一个Master和若干个Slave。Master上运行JobTracker，Slave上运行TaskTracker 三、MapReduce计算框架的组成 MR 1、 Mapper负责“分”，即把得到的复杂的任务分解为若干个“简单的任务”执行。 ​ “简单的任务”： 数据或计算规模相对于原任务要大大缩小； 就近计算，即会被分配到存放了所需数据的节点进行计算； 每个map任务之间可以并行计算，不产生任何通信。 split 2、Split规则：（取三者的中间值） – max.split(100M) – min.split(10M) – block(64M) max(min.split,min(max.split,block)) split实际大小=block大小（2.X：128M） Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数 3、Reduce详解（总·重要） – Reduce的任务是对map阶段的结果进行“汇总”并输出。 Reducer的数目由mapred-site.xml配置文件里的项目mapred.reduce.tasks决定。缺省值为1，用户可自定义。 4、Shuffle详解（总·核心） – 在mapper和reducer中间的一个步骤 可以把mapper的输出按照某种key值重新切分和组合成n份，把key值符合某种范围的输出送到特定的reducer那里去处理。 – 可以简化reducer过程 Partitoner ： hash(key) mod R 四、MapReduce架构1、非共享式架构每个节点都有自己的内存，容错性比较好。 2、一主多从架构可扩展性好，硬件要求易达到。 – 主 JobTracker:（ResourceManager资源管理） 负责调度分配每一个子任务task运行于TaskTracker上， 如果发现有失败的task就重新分配其任务到其他节点。 每一个hadoop集群中只一个 JobTracker, 一般它运行在Master节点上。 – 从TaskTracker:（NodeManager） TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务， 为了减少网络带宽TaskTracker最好运行在HDFS的DataNode上。 MapReduce的体系结构MapReduce主要有以下4个部分组成 1234567891011121314151617181 ）Client•用户编写的MapReduce程序通过Client提交到JobTracker端•用户可通过Client提供的一些接口查看作业运行状态2 ）JobTracker•JobTracker负责资源监控和作业调度•JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点•JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源3 ）TaskTracker•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）•TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask和Reduce Task使用（所以最好放在DataNode上）4 ）TaskTask 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 五、MapReduce搭建1、节点分布情况 NN DN JN ZK ZKFC RM NM node00 √ √ √ √ √ √ node01 √ √ √ √ √ √ √ node02 √ √ √ √ √ 2、配置文件 修改配置文件 (1)mapred-site.xml:（配置mapreudce需要的框架环境） 路径：F:\hadoop-2.6.5\etc\hadoop\mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （2）yarn-site.xml:（配置yarn的任务调度的计算框架） 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 因为ResourceManager 和NodeManager主从结构，RM存在单点故障，要对它做HA（通过ZK） 修改yarn-site.xml配置文件,完整的内容如下： 12345678910111213141516171819202122232425262728 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;Sunrise&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node02&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; 六、个人理解 基于源码，对mapreduce的工作流程的描述： 12345678910111213141516171819一个应用程序要进行大规模数据处理分析数据文件保存在HDFS中，分块存储在分布式节点上首先是将数据文件切分成许多split切片每一个split切片单独启动一个map任务，所以会启动多个map任务map阶段的输入是诸多(key,value),输出是新的（key,value）,然后被拉去到不同的reduce上并行处理操作所以每个map的输出阶段都执行分区操作，并决定reduce任务的个数然后对map输出结果进行分区、排序、归并、合并，这个过程叫map阶段的shuffleshuffle结束后，将相应的结果分发给reduce，让reduce完成后续的工作 结束后，将结果输出给HDFS。不同的map之间并行计算，不会通信；不同的reduce也不会通信，整个过程对用户透明。 shuffle MapReduce执行的各个阶段： 123456789101112131、从HDFS中加载文件，加载读取由InputFormat模块来完成，对输入负责格式验证，同时，对数据进行逻辑上切分成split2、由record read具体根据分片的位置长度信息去找各个block，以（key，value）输出，作为map的输入，3、map中有用户自定义的map函数就可以进行相应的数据处理，并输出一堆（key，value），作为中间结果4、之后，是shuffle（洗牌）过程对这中间结果进行分区、排序、合并，并溢写到磁盘，5、相应的reduce任务就会来fetch对应的分区（key，value-list）6、reduce中有用户自定义的reduce函数就可以完成对数据的分析，结果以新的（key，value）输出7、输出结果借助OutputFormat模块对输出格式进行检查，以及相关目录是否存在等，最后写入到HDFS中。 split 关于split的切分的理解： 1234561、InputFormat将大的数据文件分成很多split2、文件在HDFS中是以很多个物理块block分布式存储不同的节点上3、切片是用户自定义的逻辑分片4、split的数量决定map任务的数量5、切片过多会导致map任务启动过多，map任务之间切换的时候就会耗费相关的管理资源，所以切片过多会影响执行效率6、 切片过少又会影响任务执行的并行度，所以理想情况用block块的大小作为切片的大小。 关于shuffle的理解 12345678910map端shuffle1、从HDFS输入数据和执行map任务，在map任务执行之前，RecordReader阅读器还将数据变成满足Map函数所需的（K，V）形式，然后InputFormat会将其切分成若干切片（一堆（K，V））。2、每个切片会分配一个map任务，每个map任务会分配一个默认的缓存，一般默认缓存为100M.map的输出键值对作为中间结果先写入到缓存（直接写入磁盘会增加寻址开销，所以集中写入磁盘一次寻址就可以完成批量写入，就可以将寻址开销分摊到大量数据中，这就是缓存的作用）。3、当写入的内容达到缓存空间的一定比例后（溢写比，一般为0.8，就是80M的时候，为了不影响map任务的继续执行），会启动溢写进程，把缓存中相关数据写入磁盘。4、在溢写过程中，会执行分区（partition）、排序（sort，按照key值）和可能的合并（combine，为了减少溢写到磁盘的数据量，慎用）操作，写入磁盘，生成磁盘的溢写文件。5、在map任务运行结束前，系统会对溢写文件进行归并（merge），形成大文件（里面的键值对是分区，排序的）,文件格式为（key,value&lt;list&gt;），归并时如果溢写文件大于预定值（默认为3），会再次合并reduce端shuffle1、reduce任务会询问JobTracker，去拉取map机器上的属于自己的分区，对来自不同机器的数据进行归并、合并，然后输入到reduce函数中进行数据的处理分析，再写入磁盘 我 MapReduce应用程序执行过程]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper学习]]></title>
    <url>%2F2019%2F01%2F04%2FZookeeper%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 动物园管理员 推荐图书：《从Paxo到Zookeeper》 Zookeeper1、简介 开源的、分布式应用程序，提供一致性服务，是Haoop （实现HA）和Hbase（和zookeeper是强依赖关系）的重要组件 提供的功能： 配置维护 域名维护 分布式的同步 组服务 Zookeeper→提供通用分布式锁服务，用以协调分布式应用 Keepalived→实现节点健康检查，采用优先级监控，没有协同工作，功能单一，可扩展性差。 2、Zookeep的角色 （一般很少配置Observer，因为用的少，而且配置的节点一般为奇数） Zookeeper需保证高可用和强一致性； ​ 为了支持更多的客户端，需要增加更多Server； ​ Server增多，投票阶段延迟增大，影响性能； ​ 权衡伸缩性和高吞吐率，引入Observer ​ Observer不参与投票； ​ Observers接受客户端的连接，并将写请求转发给leader节点； ​ 加入更多Observer节点，提高伸缩性，同时不影响吞吐率。 3、Zookeeper特点 特点 说明 最终一致性 为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能（与强一致性相对） 可靠性 如果消息被到一台服务器接受，那么它将被所有的服务器接受. 实时性 Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 独立性 各个Client之间互不干预 原子性 更新只能成功或者失败，没有中间状态。 顺序性 所有Server，同一消息发布顺序一致。 4、安装部署：官网： 下载： （1）修改配置文件： 在Zokeeper的安装目录中的conf目录下，将zoo_sample.cfg文件改名为zoo.cfg 12&gt; mv zoo_sample.cfg zoo.cfg&gt; 编辑： 12&gt; vim /usr/soft/zookeeper-3.4.13/conf/zoo.cfg&gt; 12345678910111213#发送心跳的间隔时间，单位：毫秒 ; 2秒tickTime=2000 dataDir=/usr/soft/zookeeper-3.4.13/datadataLogDir=/usr/soft/zookeeper-3.4.13/logsdataLogDir=/Users/zdandljb/zookeeper/dataLog#客户端连接 Zookeeper 服务器的端口，clientPort=2181 #Zookeeper 会监听这个端口，接受客户端的访问请求。initLimit=5syncLimit=2server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888 配置解释: initLimit： 这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5 个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒 syncLimit：这个配置项标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的心跳时间长度，总的时间长度就是 2*2000=4 秒 server.A=B：C：D：其 中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号 (2)创建myid文件（在上面配置文件中配置dataDir 的目录下） 123server1机器的内容为：1，server2机器的内容为：2，server3机器的内容为：3 （3）将zookeeper包发到各个节点上 Paxo算法官网 1、简介一种基于消息传递且具有高度容错特性的一致性算法，广泛应用于分布式计算中，是到目前为止唯一的分布式一致性算法。 前提： Paxos 有一个前提：没有拜占庭将军问题。就是说 Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。 2、结合故事的对应理解 小岛(Island)——ZK Server Cluster议员(Senator)——ZK Server提议(Proposal)——ZNode Change(Create/Delete/SetData…)提议编号(PID)——Zxid(ZooKeeper Transaction Id)正式法令——所有 ZNode 及其数据 总统——ZK Server Leader zookeeper的节点及工作原理1、工作原理 1.每个Server在内存中存储了一份数据； 2.Zookeeper启动时，将从实例中选举一个leader（Paxos协议） 3.Leader负责处理数据更新等操作 4.一个更新操作成功，当且仅当大多数Server在内存中成功修改数据。 Zookeeper的核心是原子广播，这个机制保证了各个server之间的同步。实现这个机制的协议叫做Zab协议。 Zab协议有两种模式，它们分别是恢复模式和广播模式。 当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数server的完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和server具有相同的系统状态。一旦leader已经和多数的follower进行了状态同步后，他就可以开始广播消息了，即进入广播状态。这时候当一个server加入zookeeper服务中，它会在恢复模式下启动，发现leader，并和leader进行状态同步。待到同步结束，它也参与消息广播。Zookeeper服务一直维持在Broadcast状态，直到leader崩溃了或者leader失去了大部分的followers支持. 广播模式需要保证proposal被按顺序处理，因此zk采用了递增的事务id号(zxid)来保证。所有的提议(proposal)都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch。低32位是个递增计数。 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的server都恢复到一个正确的状态。 2、Znode节点（1）Znode有两种类型，短暂的（ephemeral）和持久的（persistent） Znode的类型在创建时确定并且之后不能再修改。 短暂znode的客户端会话结束时，zookeeper会将该短暂znode删除，短暂znode不可以有子节点 持久znode不依赖于客户端会话，只有当客户端明确要删除该持久znode时才会被删除 （2）Znode有四种形式的目录节点 PERSISTENT、持久的 EPHEMERAL、短暂的 PERSISTENT_SEQUENTIAL、持久且有序的 EPHEMERAL_SEQUENTIAL 短暂且有序的 3、shell操作启动服务端： 1./zkServer.sh start 停止服务： 1./zkServer.sh stop 启动客户端： 1./zkCli.sh -server 127.0.0.1 : 2081 ​ （localhost | node01） ​ （也可连接其他节点） ​ (port默认2081,可省；ip也可省) 退出客户端： 1quit 操作指南： 1help 查看根目录： 1ll / ​ （ll +路径） 获取具体服务内容： 12get /(get +路径+服务)可查看注册zookeeper服务的节点信息 （如果作为leader的namenode挂了，最新文件会相应的更换数据信息，如果没有nn，那么就没有相应的最新文件，只会有记录上一个阶段数据的文件） 创建服务： 1create /sun aabbcc ​ (create +路径 + 数据内容) 在其他节点也可启动客户端，创建服务 删除服务： 1rmr /sun 4、API操作 见代码testzookeeper 总结 Zookeeper 作为 Hadoop 项目中的一个子项目，是Hadoop 集群管理的一个必不可少的模块，它主要用来控制集群中的数据，如它管理 Hadoop 集群中的NameNode，还有 Hbase 中 Master、 Server 之间状态同步等。 ​ Zoopkeeper 提供了一套很好的分布式集群管理的机制，就是它这种基于层次型的目录树的数据结构，并对树中的节点进行有效管理，从而可以设计出多种多样的分布式的数据管理模型。]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN的入门学习]]></title>
    <url>%2F2019%2F01%2F04%2FYarn%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 一、简介yarn（资源管理器）（1）存在背景：MR1.0存在缺陷： 单点故障： 仅有一个JobTracker负责整个作业的调度、管理、监控、资源调度 （一个作业拿到后会分解多个任务去执行mapduce，JobTracker把任务分配给TaskTracker来具体负责执行相关map或reduce任务） JobTracker‘大包大揽’，管理事项过多 （上限4000个节点） 容易出现内存溢出 资源划分不合理 （强行划分slot，map资源和reduce资源不能互用，导致忙的忙死，闲的闲死） 1既是一个计算框架，也是一个资源管理框架 （2）yarn产生 对JobTracker进行功能分解，将资源管理功能分给ResourceManager，将任务调度和任务监控分给ApplicationMaster，将TaskTracker的任务交给NodeManager 12纯粹的资源管理框架被剥离资源管理调度功能的MapReduce就变成了MR2.0，他就是一个运行在YARN上的一个纯粹的计算框架，由YARN为其提供资源管理调度服务 什么叫纯粹的计算框架？？ 它提供一些计算基类，使用时，编写map类和reduce类的子类，去继承它。然后计算框架去做后台自动分片，shuffle过程。 资源管理框架？？ 它专门管理CPU内存资源的分配 二、YARN设计思路 YARN资源管理，任务调度流程： 流程大致如下： · client客户端向yarn集群(resourcemanager)提交任务 · resourcemanager选择一个node创建appmaster · appmaster根据任务向rm申请资源 · rm返回资源申请的结果 · appmaster去对应的node上创建任务需要的资源（container形式，包括内存和CPU） · appmaster负责与nodemanager进行沟通，监控任务运行 · 最后任务运行成功，汇总结果。 其中Resourcemanager里面一个很重要的东西，就是调度器Scheduler，调度规则可以使用官方提供的，也可以自定义。 三、YARN体系结构三大核心： 1、RecourceManager（RM） ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager） 调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行就近选择，从而实现“计算向数据靠拢” 容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量 调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也允许用户根据自己的需求重新设计调度器 应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等 2、ApplicationMaster ResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMasterApplicationMaster的主要功能是：（1）当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，ResourceManager会以容器的形式为ApplicationMaster分配资源； （2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源的“二次分配”； （3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行失败恢复（即重新申请资源重启任务）； （4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信息； （5）当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。 3、NodeManager NodeManager是驻留在一个YARN集群中的每个节点上的代理，有所需数据的节点，主要负责： 容器生命周期管理 监控每个容器的资源（CPU、内存等）使用情况 跟踪节点健康状况 以“心跳”的方式与ResourceManager保持通信 向ResourceManager汇报作业的资源使用情况和每个容器的运行状态 接收来自ApplicationMaster的启动/停止容器的各种请求 需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与NodeManager通信来掌握各个任务的执行状态 四、YARN 工作流程 五、YARN框架与MapReduce1.0框架1、对比分析 从MapReduce1.0框架发展到YARN框架，客户端并没有发生变化，其大部分调用API及接口都保持兼容，因此，原来针对Hadoop1.0开发的代码不用做大的改动，就可以直接放到Hadoop2.0平台上运行 总体而言，YARN相对于MapReduce1.0来说具有以下优势： 大大减少了承担中心服务功能的ResourceManager的资源消耗 ApplicationMaster来完成需要大量资源消耗的任务调度和监控 多个作业对应多个ApplicationMaster，实现了监控分布化 MapReduce1.0既是一个计算框架，又是一个资源管理调度框架，但是，只能支持MapReduce编程模型。而YARN则是一个纯粹的资源调度管理框架，在它上面可以运行包括MapReduce在内的不同类型的计算框架，只要编程实现相应的ApplicationMaster YARN中的资源管理比MapReduce1.0更加高效 以容器为单位，而不是以slot为单位 2、 MapReduce On YARN：MRv2 将MapReduce作业直接运行在YARN上，而不是由JobTracker和TaskTracker构建的MRv1系统中 基本功能模块 ​ YARN：负责资源管理和调度 ​ MRAppMaster：负责任务切分、任务调度、任务监控和容错等 ​ MapTask/ReduceTask：任务驱动引擎，与MRv1一致 每个MapRduce作业对应一个MRAppMaster任务调度 ​ YARN将资源分配给MRAppMaster ​ MRAppMaster进一步将资源分配给内部的任务 MRAppMaster容错 ​ 失败后，由YARN重新启动 ​ 任务失败后，MRAppMaster重新申请资源 六、YARN 的发展目标YARN 的目标就是实现“一个集群多个框架”？ ，为什么？ 一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架 MapReduce实现离线批处理 使用Impala实现实时交互式查询分析 使用Storm实现流式数据实时分析 使用Spark实现迭代计算 这些产品通常来自不同的开发团队，具有各自的资源调度管理机制 为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分别安装运行不同的计算框架，即“一个框架一个集群” 导致问题 集群资源利用率低 数据无法共享 维护代价高 YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源调度管理框架YARN，在YARN之上可以部署其他各种计算框架 由YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩 可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率 不同计算框架可以共享底层存储，避免了数据集跨集群移动]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2.X]]></title>
    <url>%2F2019%2F01%2F03%2FHadoop2.X%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Hadoop 2.x产生背景1、Hadoop 1.0存在的问题 （1）HDFS存在的问题 NameNode单点故障，难以应用于在线场景 NameNode（一个）压力过大，内存受限，影响系统扩展性 （2）MapReduce存在的问题 JobTracker访问压力大，影响系统扩展性 难以支持MapReduce以外的计算框架，比如Spark、Storm 2、Hadoop 2.0分支 HDFS：分布式文件存储系统MapReduce：计算框架YARN：资源管理系统 3、特点 1）. 解决单点故障：HDFS HA（高可用） 通过主备NameNode解决，如果主NameNode发生故障，就切换到备NameNode上 | 2).解决内存受限问题：HDFS Federation（联邦制）、HA HA：两个NameNode (3.0就实现了一组多从：水平扩展，支持多个NameNode；每个NameNode分管一部分目录；所有NameNode共享所有DataNode资源) 3).仅架构上发生变化使用方式不变 二、HDFS HA结构及功能 HADN：DataNode（数据节点） 存放数据block块；遵循心跳机制向NN Active和NN Standby汇报block块信息，但只执行active的命令 主备NN：NameNode Active 和 NameNode Standby （主备名称节点） 主NN对外提供读写服务，备NN同步主NN元数据，以待切换，所有的DN同时向两个NN汇报数据块信息 元数据信息加载到主NN，并写入JN（至少写两台：过半原则）； 备NN可以从JN中同步元数据信息； 解决单点故障； ———两种切换方式： 手动：通过命令实现主备切换 12&gt; hdfs haadmin -transitionToActive nn2&gt; ​ 这时nn1如果还存活则变成不可写状态，需要重启，重启后自动成为standby nn 自动：基于Zookeeper实现（详情见搭建步骤） JN：JournalNode（至少3台） 存储主NN元数据信息，实现主备NN间数据共享； （遵循过半原则：至少有过半的数量参与投票） ZKFC：FailoverController（竞争锁） 谁拿到了这个所，谁就是active NN 心跳机制监控主备NN状态，一旦出现一台挂机，就会释放锁，另一个NN就会立即启动竞争锁，成为active NN ZK：Zookeeper（至少3台） （实现主备NN切换） 联邦 通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使到namenode/namespace可以通过增加机器来进行水平扩展 通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中。 三、==！！YARN(资源管理)！！==详见Yarn学习.md 1、核心思想：ResourceManager（资源管理）+ApplicationMaster（任务调度） 2、yarn的引入使得多个计算框架可以应用到一个集群中 ResourceManager： 负责整个集群的资源管理和`调度。 ApplicationMaster： 负责应用程序相关的事务，比如任务调度、任务监控和容错等。 ​ 每个应用程序对应一个ApplicationMaster（应用程序控制-主人） 目前多个计算框架可以运行在YARN上，比如MapReduce、Spark、Storm等。 四、==！！Zookeeper工作原理！！==详见Zookeeper学习.md 五、Hadoop2.X 集群搭建1、linux环境下搭建 NN DN JN ZKFC ZK RM NM node00 √ √ √ √ √ √ √ node01 √ √ √ √ √ √ √ node02 √ √ √ √ Zookeeper Failover Controller： Failover ：失效备援（为系统备援能力的一种，当系统中其中一项设备失效而无法运作时，另一项设备即可自动接手原失效系统所执行的工作） 监控NameNode健康状态，并向Zookeeper注册NameNode，NameNode挂掉后，ZKFC为NameNode竞争锁，获得ZKFC锁的NameNode变为active。 0&gt;在搭建环境之前的准备三台虚拟机： 1234567关闭防火墙安装jdk编辑/etc/hosts/给各个节点服务器起别名时间服务器：ntpdate 安装：yum install ntpdate -y 生成：ntpdate cn.ntp.org.cn免密登录环境准备 在hadoop安装目录下hadoop-2.6.5/etc/hadoop/ 1&gt;编辑hadoop-env.sh1export JAVA_HOME=/usr/soft/jdk1.8.0_191 2&gt;编辑core-site.xml注意： fs.defaultFS 默认的服务端口是 NameNode URI hadoop.tmp.dir是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果在hdfs-site.xml中没有配置namenode 和datanode的存放位置，默认及存放在这个路径中。 123456789101112131415&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Sunrise&lt;/value&gt;&lt;!--配置集群的名字--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node00:2181,node01:2181,node02:2181&lt;/value&gt; &lt;!--配置zookeeper：三个节点--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop&lt;/value&gt;&lt;!--配置hadoop基础配置存放的路径--&gt;&lt;/property&gt;&lt;/configuration&gt; 3&gt;编辑hdfs-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;Sunrise&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.Sunrise&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node01:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node00:8485;node01:8485;node02:8485/Sunrise&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt;&lt;name&gt;dfs.client.failover.proxy.provider.Sunrise&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 配置隔离机制为ssh 防止脑裂：保证activeNN仅有一台--&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 指定秘钥的位置 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt; &lt;!--免密登录是生成的文件，有的是id_rsa--&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定journalnode日志文件存储的路径 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 开启自动故障转移 --&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 4&gt;配置hadoop中的slaves（主从架构：datanode） 123node00node01node02 5&gt;准备zookeeper 三台zookeeper：node00，node01，node02 编辑zookeeper-3.4.13/conf/zoo.cfg 12345678910tickTime=2000initLimit=10syncLimit=5dataDir=/usr/soft/zookeeper-3.4.13/datadataLogDir=/usr/soft/zookeeper-3.4.13/logsclientPort=2181#用于投票选举server.1=node00:2888:3888server.2=node01:2888:3888server.3=node02:2888:3888 在dataDir目录中创建文件myid，三台节点的文件内容分别为1，2，3 6&gt;配置环境变量1vim ~/.bash_profile 编辑内容： 123456JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/bin 12&gt; source ~/.bash_profile&gt; 使其成为资源文件，发送到其他节点后，也需要此操作 7&gt;将以上配置文件远程发送至其他节点服务器 12&gt; scp -r filename nodename:`pwd`&gt; 8&gt;命令操作：12345678910111213141516171819201. 启动三个zookeeper：./zkServer.sh start2. 启动三个JournalNode：./hadoop-daemon.sh start journalnode3. （生成fsimage文件）在其中一个namenode上格式化： hdfs namenode -format4. 把刚刚格式化之后的元数据拷贝到另外一个namenode上 a) 启动刚刚格式化的namenode : hadoop-daemon.sh start namenode b) （同步fsimage文件）在另一个（没有格式化的）namenode上执行： hdfs namenode -bootstrapStandby c) 启动没格式化的namenode： hadoop-daemon.sh start namenode5. （初始化竞争锁zookeeper）在其中一个namenode上初始化zkfc： hdfs zkfc -formatZK6. 停止上面节点： stop-dfs.sh7. 全面启动（三个节点）： start-dfs.sh8. 启动yarn资源管理器 yarn-daemon.sh start resourcemanager (yarn resourcemanager ) 2、使用（启动步骤） 1234(1)关闭防火墙：service iptables stop （3台）(2)启动zookeeper:zkServer.sh start （3台）(3)启动集群：start-dfs.sh |（start-all.sh : 同时启动hdfs和yarn 的nodemanager)(4)启动yarn：yarn-daemon.sh start resourcemanager （可3台） （关闭步骤） 123(1)关闭yarn：yarn-daemon.sh stop resourcemanager （开几台关几台）(2)关闭集群：stop-dfs.sh |（stop-all.sh :同时关闭hdfs和yarn） （3台）(3)关闭zookeeper：zkServer.sh stop （3台） 12345678有可能会出错的地方1， 确认每台机器防火墙均关掉2， 确认每台机器的时间是一致的3， 确认配置文件无误，并且确认每台机器上面的配置文件一样4， 如果还有问题想重新格式化，那么先把所有节点的进程关掉5， 删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录，所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录6， 接上面的第6步又可以重新格式化已经启动了7， 最终Active Namenode停掉的时候，StandBy可以自动接管！ 3、关于脑裂脑裂（brain-split）) 1&gt;定义： 是指在主备切换时，由于切换不彻底或其他原因，导致客户端和slave误以为出现两个active master，最终使得整个集群处于混乱状态。 2&gt;解决脑裂问题： 通常采用隔离(Fencing))机制 包括三个方面： 共享fencing ： 确保只有一个Master 往共享存储中写数据； 客户端fencing ：确保只有一个Master可以相应客户端的请求； Slave fencing ： 确保只有一个Master可以向Slave下发命令 ​ Hadoop公共库中对外提供了两种fenching实现，分别是sshfence和shellfence（缺省实现），其中sshfence是指通过ssh登陆目标Master节点上，使用命令fuser将进程杀死（通过tcp端口号定位进程pid，该方法比jps命令更准确），shellfence是指执行一个用户事先定义的shell命令（脚本）完成隔离。]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS学习]]></title>
    <url>%2F2019%2F01%2F03%2FHDFS%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] ==hadoop理解== 狭义： hadoop1 = hdfs1 + MR1 hadoop2 = hdfs2 + MR2 + YARN 广义： Hadoop生态 用于解决海量数据的处理和数据存储，数据级别为GB，TB，PB。 一、分布式文件存储系统HDFS1、什么是分布式？ 定义：将海量的数据，复杂的业务分发到不同的计算机节点和服务器上分开处理和计算。 特点： 多副本，提高服务的容错率、安全性、高可靠性 适合批处理，提高服务的效率和速度， 减轻单台服务器的压力 具有很好的可扩展性 计算向数据靠拢，安全，高效 大数据三驾马车：GFS、MapReduce、Bigtable====HDFS、MR、HBASE 2、什么是HDFS？（1）HDFS为什么会出现？ 主要解决大量【pb级以上】的大数据的分布式存储问题 （2）==HDFS的特点== $$ 分布式特性： 适合大数据处理：GB、TB、PB以上的数据 百万规模以上的文件数量:10K+ 节点 适合批处理：移动计算而非数据(MR),数据位置暴露给计算框架 $$ 自身特性： 可构建在廉价机器上 高可靠性：通过多副本实现 高容错性：数据自动保存多个副本；副本丢失后，自动恢复,提供了恢复机制 $$ 缺点： —–低延迟高数据吞吐访问问题（不适合低延迟数据访问，Hbase适合） 不支持毫秒级 吞吐量大但有限制于其延迟（瓶颈：低延迟无法突破） —–小文件存取占用NameNode大量内存(寻道时间超过读取时间,约占99%) ——-不支持多用户写入及任意修改文件 不支持文件修改：一个文件只能有一个写者 文件仅支持append不支持修改 （其实本身是支持的，主要为了用空间换时间，节约成本） $$ 实现目标： 兼容廉价的硬件设施 实现流数据读写 支持大数据集 支持简单的文件模型 强大的跨平台兼容性 （3）==HDFS架构图==HDFS架构图 HDFS架构图 关系型数据库：安全，存储在磁盘中；如MySql、Oracle、SQlServer 非关系型数据库：不安全，存储在内存中；如Redis、MemcacheDB、mongoDB、Hbase 3、==HDFS的功能模块及原理详解== HDFS数据存储模型（block）block （1）文件被线性切分固定大小的数据块：block 通过偏移量offset（单位：byte）标记 默认数据块大小为64MB (hadoop1.x，hadoop2.x默认为128M）)，可自定义配置 若文件大小不到64MB ，则单独存成一个block （2）一个文件存储方式 按大小被切分成若干个block ，存储到不同节点上 默认情况下每个block共有3个副本 副本数不大于节点数 （3）Block大小和副本数通过Client端上传文件时设置， 文件上传成功后副本数可以变更，Block Size大小不可变更 块的大小远远大于普通文件系统，可以最小化寻址开销 NameNode（简称NN） 存储元数据； 元数据保存在内存中； 保存文件、block块、datanode之间的映射关系 1&gt; NN主要功能： 接收客户端的读写服务；接收DN汇报block位置关系 2&gt; NN保存metadate元信息 基于内存存储，不会和磁盘发生交换 ​ metadata元数据信息包括以下 文件的归属（ownership）和权限（permission） 文件大小和写入时间 block列表【偏移量】：即一个完整文件有哪些block（b0+b1+b2+..=file） 位置信息（动态的）：Block每个副本保存在哪个DataNode中 *注意*：位置信息是由DN启动时上报给NN ，因为它会随时变化，所以不会保存在内存和磁盘中 3&gt; NameNode的metadate信息在启动后会加载到内存 同时： metadata信息也会保存fsimage文件中（fsimage文件是位于磁盘上的镜像文件） 对metadata的操作日志也会记录在edits 文件中（edits文件是位于磁盘上的日志文件） SecondaryNameNode（简称SNN）1&gt;SNN主要功能 帮助NameNode合并edits和fsimage文件，减少NN启动时间； SecondaryNameNode一般是单独运行在一台机器上； 它不是NN的备份（但可以做备份)。 2&gt;合并流程SNN合并 123456789101112SecondaryNameNode的工作情况：（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件， 暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成， 上层写日志的函数完全感觉不到差别；（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文 件，并下载到本地的相应目录下；（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新； 这个过程就是EditLog和FsImage文件合并；（4）SecondaryNameNode执行完（3）操作之后， 会通过post方式将新的FsImage文件发送到NameNode节点上（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件， 同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了 3&gt;合并机制 ——-SNN执行合并时间和机制 A、根据配置文件设置指定连续两次检查点的最大时间间隔 fs.checkpoint.period 默认3600秒（1小时） B、根据配置文件设置edits log文件大小 fs.checkpoint.size 默认最大值64M 配置文件：core-site.xml DataNode（简称DN）1&gt; DN主要功能 存储文件内容（block）； 文件内容保存在磁盘； 维护了block id 到datanode本地文件的映射关系 启动DN线程的时候会向NameNode汇报block位置信息 2&gt; DN工作机制12345• 数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，• 会根据客户端或者是名称节点的调度来进行数据的存储和检索，• 并且通过心跳机制向名称节点定期发送自己所存储的块的列表，保持与其联系（3秒一次） （如果NN 10分钟没有收到DN的心跳，则认为其已经lost，并copy其上的block到其它DN）• 每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中 3&gt; block的副本放置策略 – 第一个副本：放置在上传文件的DN（集群内提交）； ​ 如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。 – 第二个副本：放置在于第一个副本不同的机架的节点上。 – 第三个副本：与第二个副本相同机架的不同节点。 – 更多副本：随机节点 block块存放位置 4、HDFS读写流程 读文件过程read 1、首先client端调用DistributedFileSystem对象（DFS）的open方法，（DFS：一个DistributedFileSystem的实例）。2、DistributedFileSystem通过rpc协议从NameNode（NN）获得文件的第一批block的locations，（同一个block按副本数会返回多个locations，因为同一文件的block分布式存储在不同节点上），这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面（就近选择）。 3、前两步会返回一个FSDataInputStream对象，该对象会被封装DFSInputStream对象，DFSInputStream可以方便的管理DN和NN的数据流。客户端调用read方法，DFSInputStream会连接离客户端最近的DN，数据从DN源源不断的流向客户端（对客户端是透明的，只能看到一个读入的Input流）。 4、如果第一批block都读完了， DFSInputStream就会去NN拿下一批block的locations，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流 读 注意： 1234567 如果在读数据的时候,DFSInputStream和DN的通讯发生异常，就会尝试连接正在读的block的排序第二近的DN,并且会记录哪个DN发生错误，剩余的blocks读的时候就会直接跳过该DN。 DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到NN，然后DFSInputStream在其他的DN上读该block的镜像。 该设计就是客户端直接连接DN来检索数据，并且NN来负责为每一个block提供最优的DN，NN仅仅处理block location的请求，这些信息都加载在NN的内存中，hdfs通过DN集群可以承受大量客户端的并发访问。 * RPC *（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。 写文件过程write3 1.客户端通过调用DistributedFileSystem的create方法创建新文件。 2.DistributedFileSystem通过RPC调用NN去创建一个没有blocks关联的新文件，创建前，NN会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NN就会记录下新文件，否则就会抛出IO异常。 3.前两步结束后，会返回FSDataOutputStream的对象，封装在DFSOutputStream，客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的packet，然后排成队列dataQuene。 4.NN会给这个新的block分配最适合存储的几个datanode，DFSOutputStream把packet包排成一个管道pipeline输出。先按队列输出到管道的第一个datanode中，并将该Packet从dataQueue队列中移到ackQueue队列中，第一个datanode又把packet输出到第二个datanode中，以此类推。 5.DFSOutputStream中的ackQuene，也是由packet组成，等待DN的收到响应，当pipeline中的DN都表示已经收到数据的时候，这时ackQuene才会把对应的packet包移除掉。 如果在写的过程中某个DN发生错误，会采取以下几步： ​ 1) pipeline被关闭掉； ​ 2)为了防止丢包，ackQuene里的packet会同步到dataQuene里;新建pipeline管道接到其他正常DN上 ​ 4)剩下的部分被写到剩下的正常的datanode中； ​ 5)NN找到另外的DN去创建这个块的复制。（对客户端透明） 6.客户端完成写数据后调用close方法关闭写入流 注意：客户端执行write操作后，写完的block才是可见的，正在写的block对客户端是不可见的 写 5.HDFS文件权限和安全模式？？HDFS文件权限？？– 与Linux文件权限类似 • r: read; w:write; x:execute，权限x对于文件忽略，对于文件夹表示是否允许访问其内容 – 如果Linux系统用户zs使用hadoop命令创建一个文件，那么这个 文件在HDFS中owner就是zs。 – HDFS的权限目的：阻止好人做错事，而不是阻止坏人做坏事。 ？？安全模式？？ NN启动的时候，首先将映像文件(fsimage)载入内存，并执行编辑日志(edits)中的各项操作。 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的fsimage文件(这个操作不需要SecondaryNameNode)和一个空的编辑日志。 此刻namenode运行在安全模式。即namenode的文件系统对于客服端来说是只读的。(显示目录，显示文件内容等。写、删除、重命名都会失败)。 在此阶段Namenode收集各个datanode的报告，当数据块达到最小副本数以上时，会被认为是“安全”的， 在一定比例（可设置）的数据块被确定为“安全”后，再过若干时间，安全模式结束 当检测到副本数不足的数据块时，该块会被复制直到达到最小副本数，系统中数据块的位置并不是由namenode维护的，而是以块列表形式存储在datanode中。 异常 手动退出安全模式： 1hdfs namenode -safemode leave 二、完全分布式搭建及eclipse插件1、==完全分布式搭建（必备）==（1）环境的准备 Linux (前面已经安装好了) JDK（前面已经安装好了） 准备至少3台机器（通过克隆虚拟机；) (网络配置、JDK搭建、hosts配置，保证节点间能互ping通） 时间同步 （推荐）ntpdate cn.ntp.org.cn (ntpdate time.nist.gov) ssh免秘钥登录 (两两互通免秘钥) （2）完全分布式搭建步骤Hadoop 1.X1、下载解压缩Hadoop2、配置hadoop/hadoop-env.sh1export JAVA_HOME=/usr/java/latest 3、配置core-site.xml: fs.defaultFS 默认的服务端口NameNode URI hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这个路径中 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node01:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6.1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4、配置hdfs-site.xml:dfs.datanode.https.address https服务的端口，浏览器访问端口 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;!--默认为3个副本，若指定，则以指定的为准--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--http访问：SecondaryNameNode的服务器的ip地址别名,端口号50090--&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;node02:50090&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;!--https访问：SecondaryNameNode的服务器的ip地址别名,端口号50090--&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;node02:50091&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5、Masters:master 可以做主备的SNN在/home/hadoop-2.6.5/etc/hadoop/新建masters文件 写上SNN节点名： node02（IP地址别名） 6、Slaves: slave 奴隶 苦干；拼命工作在/home/hadoop-2.5.1/etc/hadoop/slaves文件中填写DN 节点名：node02 node03 [注意：每行写一个 写成3行] 7、最后将配置好的Hadoop通过SCP命令发送都其他节点 配置Hadoop的环境变量 8、vim ~/.bash_profile (最好手敲输入 粘贴有时候会出错) 12export HADOOP_HOME/home/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 9、记得一定要 source ~/.bash_profile10、回到跟目录下对NN进行格式化 hdfs namenode -format 11、启动HDFS： start-dfs.sh 12、关闭防火墙：service iptables stop在浏览器输入 node01:50070 ==!!Hadoop 2.X!!==详情见Hadoop2.X.md文件 2、HDFS命令(0) 命令 ：hdfs dfs(1)上传文件到HDFS： hdfs dfs -put 本地路径/fileName PATH [hdfs的文件路径] 上传本地文件/root/install.log到/myhdfs目录下 12&gt; hdfs dfs -put /root/install.log /myhdfs&gt; ​ （文件路径) (上传目录） (2)创建文件夹 12&gt; hdfs dfs -mkdir[-p] &lt;paths&gt;&gt; -p 穿透，用于创建多级文件夹 (3)删除文件或文件夹 12&gt; hdfs dfs -rm -r /myhadoop1.0 &gt; 删除多个文件夹 12&gt; hdfs dfs -rm -r /input /logs&gt; -r 递归，用于删除文件夹以及下级文件和文件夹 123456789hdfs dfs -du [-s] [-h] URI[URI ...] 显示文件(夹)大小. hdfs dfs -cp [-f] [-p] URI[URI...]&lt;dest&gt; 复制文件(夹)，可以覆盖，可以保留原有权限信息hdfs dfs -count [-q] [-h] &lt;paths&gt;列出文件夹数量、文件数量、内容大小.hdfs dfs -chown [-R] [OWNER] [:[GROUP]] URI[URI] 修改所有者.hdfs dfs -chmod [-R]&lt;MODE[,MODE]...|OCTALMODE&gt; URI[URI ...] 修改权限. （4）指定block大小其中副本数是在在配置文件中配置 123456789产生100000条数据：for i in `seq 100000`;do echo "hello sxt $i" &gt;&gt; test.txt;done上传文件test.txt到指定的Java22目录下，并指定block块的大小1M：hdfs dfs -D dfs.blocksize=1048576 -put test.txt /java22-D ----设置属性 3、eclipse插件安装配置（1）、导入插件 将以下jar包放入eclipse的plugins文件夹中 ​ hadoop-eclipse-plugin-2.6.0.jar 启动eclipse：出现界面如下： 插件应用 （2）配置环境变量Eclipse插件安装完后修改windows下的用户名，然后重启Eclipse： 【注意：改成Windows下用户的用户名root（重启生效）或改Linux文件的用户】 环境变量 （3）新建Java项目 三、网盘1、代码编写 新建Java项目，导入所需要的jar包 1234567hadoop中的share\hadoop\hdfshadoop中的share\hadoop\hdfs\libhadoop中的share\hadoop\commonhadoop中的share\hadoop\common\lib下的jar包。 block底层—offset偏移量来读取字节数组 123456789101112131415161718private static void blk() throws Exception &#123; Path ifile = new Path(""); FileStatus file = fs.getFileStatus(ifile );// 获取block的location信息HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容 BlockLocation[] blk = fs.getFileBlockLocations(file,0, file.getLen()); for (BlockLocation bb : blk) &#123; System.out.println(bb); &#125; FSDataInputStream input = fs.open(ifile); System.out.println((char)input.readByte()); System.out.println((char)input.readByte()); // 指定从哪个offset的位置偏移量来读 input.seek(1048576); System.out.println((char)input.readByte()); input.seek(1048576); System.out.println((char)input.readByte()); &#125; HDFS读取合并的小文件]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx入门学习（二）]]></title>
    <url>%2F2019%2F01%2F02%2FNginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A02%2F</url>
    <content type="text"><![CDATA[[TOC] 一、虚拟主机1、什么是虚拟主机？（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。 （2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。 2、虚拟主有啥特点？（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用 （2）也大大简化了服务器管理的复杂性； 3、虚拟主机有哪些类别？（1）基于域名 1234567891011121314151617181920212223242526http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03; &#125; server &#123; listen 80; //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里 server_name sxt2.com; location / &#123; proxy_pass http://bjsxt; &#125; &#125; server &#123; listen 80; //访问sxt1.com的时候，会把请求导到shsxt的服务器组里 server_name sxt1.com; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; 注意： （1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。 （C:\Windows\System32\drivers\etc\hosts 给IP取别名） 如：192.168.198.130 sxt1.com （2）每台服务器的Tomcat的端口不与配置的listen一致，那么windows系统浏览器访问时，需要加上TOmcat的端口，（192.168.198.128：8080） ​ 如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80 （2）基于端口 12345678910111213141516171819202122232425http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03 &#125; server &#123; //当访问nginx的80端口时，将请求导给bjsxt组 listen 8080; server_name 192.168.198.128; location / &#123; proxy_pass http://bjsxt; &#125;&#125; server &#123; //当访问nginx的81端口时，将请求导给shsxt组 listen 81; server_name 192.168.198.128; //nginx服务器的IP location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; （3）基于IP ：（不常用） 二、正向代理和反向代理1、正向代理理解： 代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见） 举例： 国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙） 但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口 2、反向代理理解： 代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器 举例： 如我们访问www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。 Nginx就是性能很好的反向代理服务器，用来作负载均衡。 三、Nginx的session一致性问题1、背景：http协议是无状态的，多次访问如果是不同服务器响应请求，就会出现上次访问留下的session或cookie失效。这就引发了session共享的问题。 2、Session一致性解决方案（1）–session复制 tomcat 本身带有复制session的功能。 （2）-共享session 需要专门管理session的软件， memcached 缓存服务，可以和tomcat整合，帮助tomcat共享管理session。 3、搭建memcachedmemcached （同redis一样）是基于内存的数据库 1、安装memcached 12&gt; yum –y install memcache&gt; 验证本机11211端口是否可用: 12&gt; telnet localhost 11211&gt; 2、启动memcached(IP地址为memcached安装的节点的IP地址) 12&gt; memcached -d -m 128m -p 11211 -l 192.168.198.128 -u root -P /tmp/&gt; 3、拷贝memcached所需jar包 将web服务器连接memcached的jar包拷贝到tomcat的lib目录下 访问Tomcat服务器期间产生的session通过相关jar包，才能写入到memcached数据库中 asm-3.2.jar kryo-1.04.jar kryo-serializers-0.11.jar memcached-session-manager-1.7.0.jar memcached-session-manager-tc7-1.8.1.jar minlog-1.2.jar msm-kryo-serializer-1.7.0.jar reflectasm-1.01.jar spymemcached-2.7.3.jar 4、配置tomcat的conf目录下的context.xml 1234567&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.198.128:11211" sticky="true" lockingMode="auto" sessionBackupAsync="false" requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; 配置memcachedNodes属性， 配置memcached数据库的ip和端口，默认11211，多个的话用逗号隔开. 目的是为了让tomcat服务器从memcached缓存里面拿session或者是放session 5、将配置完成的context.xml发送到其他虚拟机器上 scp -r context.xml root@node01:pwd 或 scp -r context.xml node01:pwd 或 scp -r context.xml root@192.168.198.130:pwd 6、修改tomcat安装目录中的webapps/ROOT下的 index.jsp，取sessionid看一看 12345678&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;html lang="en"&gt;SessionID:&lt;%=session.getId()%&gt;&lt;/br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;/br&gt;&lt;h1&gt;tomcat1&lt;/h1&gt;&lt;/html&gt; 7、在浏览器段访问服务器，默认端口 ： 80 ，对此测验，就会发现sessionID不会改变]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx入门学习（一）]]></title>
    <url>%2F2019%2F01%2F02%2FNginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A01%2F</url>
    <content type="text"><![CDATA[[TOC] 大型网站高并发运行处理一、Nginx使用背景[^ 开发者]: 由俄罗斯的程序设计师Igor Sysoev所开发 1、背景1）高并发（海量数据，复杂业务，大量线程）集中访问服务器 2)单台服务器资源和能力有限 引发服务器宕机，无法提供服务 2、概念理解1)高并发 海量数据请求访问（高），多个线程或者多个进程同时处理（并发）不同操作 2）负载均衡（Load Balance） 均匀分配请求|数据到不同操作单元上 其中，【均匀】是分布式系统架构设计中必须考虑的关键因素之一 3）常见互联网分布式架构 客户端层→反向代理层→站点层→服务层→数据层 只需要实现“将请求/数据均匀分摊到多个操作单元上执行”，就能实现负载均衡 二、Nginx入门1、了解nginx是什么 nginx是一款轻量级（开发方便，配置简捷）的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器 2、Nginx特点 占有内存少，CPU、内存等资源消耗也少； 运行稳定，并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。 （底层使用C语言编写） Tomcat的最高并发量为250个 3、Nginx ==VS== Apache（1）nginx相对于apache的优点： 轻量级，同样起web 服务，比apache 占用更少的内存及资源 nginx 处理请求是异步非阻塞（如前端ajax）的，而apache 则是阻塞型的 在高并发下nginx能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单 Nginx 配置简洁, Apache 复杂 （2）apache 相对于nginx 的优点： Rewrite重写 ，比nginx 的rewrite 强大模块超多，基本想到的都可以找到 少bug ，nginx 的bug 相对较多。（出身好起步高） 4、配置搭建Nginx（Linux系统环境下） 资源： Tengine（推荐）：Tengine-2.2.3.tar.gz ​ 其他版本 nginx：nginx/Windows-1.8.1 1）安装依赖 命令： 12&gt; yum -y install gcc openssl-devel pcre-devel zlib-devel&gt; 2）解压tar包 命令： 12&gt; tar -zxvf Tengine-2.2.3.tar.gz&gt; 3）configure配置：在解压后的源码目录中 两种方案： 命令： 12&gt; ./configure&gt; 默认配置/usr/soft/nginx 命令 : 12&gt; ./configure –profix==/usr/soft/nginx&gt; 配置在指定路径 4）编译并安装(默认会在/usr/local下生成nginx目录) 12&gt; make &amp;&amp; make install&gt; 5）配置nginx服务在/etc/rc.d/init.d/目录中建立文本文件nginx 在文件中粘贴下面的内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid # Source function library.. /etc/rc.d/init.d/functions # Source networking configuration.. /etc/sysconfig/network # Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0 nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot; [ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125; start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125; stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125; restart() &#123; configtest || return $? stop sleep 1 start&#125; reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125; force_reload() &#123; restart&#125; configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125; rh_status() &#123; status $prog&#125; rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125; case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac 6）修改nginx文件的执行权限 命令 ： 12&gt; chmod +x nginx&gt; 7）添加该文件到系统服务中去 12&gt; chkconfig --add nginx&gt; 8)查看是否添加成功 12&gt; chkconfig --list nginx&gt; 9)启动，停止，重新装载 12&gt; service nginx start|stop&gt; 三、Nginx配置1、查看配置 1234&gt; cd /usr/local/nginx/conf&gt; &gt; vim nginx.conf&gt; 2、配置解析12345678910111213141516171819202122232425262728293031323334#进程数，建议设置和CPU个数一样或2倍worker_processes 2;#日志级别error_log logs/error.log warning;(默认error级别)# nginx 启动后的pid 存放位置#pid logs/nginx.pid;events &#123; #配置每个进程的连接数，总的连接数= worker_processes * worker_connections #默认1024 worker_connections 10240;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on;#连接超时时间，单位秒keepalive_timeout 65; server &#123; listen 80; server_name localhost #默认请求 location / &#123; root html; #定义服务器的默认网站根目录位置 index index.php index.html index.htm; #定义首页索引文件的名称 &#125; #定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; 3、负载均衡配置1）安装Tomcat，参考 Tomcat配置 2）多负载均执行一下操作： 多负载的情况下，打开指定虚拟机器 open node01 node01 为指定虚拟机器的别名，在hosts文件中配置的 启动Tomcat 在Tomcat解压的目录下 ./startup.sh 注意： 记得关闭虚拟机器的防火墙 service iptables stop 浏览器访问 虚拟机器IP地址：8080 默认负载均衡配置（轮询） 1234567891011121314151617181920&gt; http &#123;&gt; upstream shsxt&#123; &gt; # 以下均为实际执行服务的服务器&gt; #只有当hosts文件中给ip地址配置了别名，这里server后面才能用别名，&gt; #否则跟IP地址&gt; server node01; &gt; server node02; &gt; &#125; &gt; &gt; server &#123; &gt; #指定访问端口为80 ，那么Tomcat服务器端的port也要改为80&gt; listen 80; &gt; server_name localhost;&gt; location / &#123;&gt; proxy_pass http://shsxt; &gt; # shsxt 是指定的代理服务器&gt; &#125;&gt; &#125; &gt; &#125;&gt; 查看使用 80端口的程序 12&gt; netstat -anp |grep 80&gt; 配置文件编辑结束后，启动nginx服务 12&gt; service nginx start&gt; （1）轮询负载均衡（默认）1- 对应用程序服务器的请求以循环方式分发 （2）加权负载均衡 通过使用服务器权重，还可以进一步影响nginx负载均衡算法， 谁的权重越大，分发到的请求就越多。 权重总数：10 在nginx.conf文件中修改： 12345upstream shsxt &#123; server node01 weight=3;//域名为在/etc/hosts文件中取的别名 server node02; server node03; &#125; 配置修改之后，重启 12&gt; service nginx restart&gt; （3）最少连接负载平衡 在连接负载最少的情况下，nginx会尽量避免将过多的请求分发给繁忙的应用程序服务器， 而是将新请求分发给不太繁忙的服务器，避免服务器过载。 在nginx.conf文件中修改： 123456upstream shsxt &#123; least_conn; server node00; server node01; server node02; &#125; （4）保持会话持久性——ip-hash负载平衡机制特点：保证相同的客户端总是定向到相同的服务; (此方法可确保来自同一客户端的请求将始终定向到同一台服务器，除非此服务器不可用。) 在nginx.conf文件中修改： 123456upstream shsxt&#123; ip_hash; server （IP地址|别名）; server （IP地址|别名）; server （IP地址|别名）;&#125; （5）Nginx的访问控制 Nginx还可以对IP的访问进行控制，allow代表允许，deny代表禁止. 12345678location / &#123;deny 192.168.2.180;allow 192.168.78.0/24;allow 10.1.1.0/16;allow 192.168.1.0/32;deny all;proxy_pass http://shsxt;&#125; 12345从上到下的顺序，匹配到了便跳出。如上的例子先禁止了1个，接下来允许了3个网段，其中包含了一个ipv6，最后未匹配的IP全部禁止访问]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据思想]]></title>
    <url>%2F2018%2F12%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[[TOC] 1、大数据核心问题：==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）== 2、大数据思维分而治之 把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q） enter description here 3、业务场景仓储、数牌 业务一：找{重复行}(chongfuhang)++现有1TB的TXT文件 ;格式：数字+字符 ；网速：500M/s ；服务器内存大小：128M ；条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++ enter description here ==方法== 答：共需要2次IO：2*30min=1h ==第一次IO==： 给每一行内容加上唯一标记（hashcode（内容），value（行号））。对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。 `对每一行的hash值进行取模运算，并放置于归类分区的小文件中`。由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。 ==第二次IO==： 在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。 业务二：快{排序}(paixu)++现有1TB的TXT文件 ;格式：数字；网速：500M/s ；服务器内存大小：128M ；条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++ 两次IO，2 * 30分钟 = 1小时 enter description here ==方法一：先全局有序后局部有序== 1.对全局按分区排序（由大到小）。​ 用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················） 2.对局部进行排序（由大到小）。​ 对每个分区进行排序。 enter description here ==方法二：先局部有序后全局有序== 先实现局部有序(小–&gt;大)。将文件划分为N个分区，在每个分区内部进行排序 使用归并实现全局有序。每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。 知否]]></content>
      <categories>
        <category>头脑风暴</category>
      </categories>
      <tags>
        <tag>分而治之</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（三）]]></title>
    <url>%2F2018%2F12%2F29%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A03%2F</url>
    <content type="text"><![CDATA[[TOC] 一、服务操作 列出 所有 服务 chkconfig 查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。 服务 操作 仅适用于当前 ：service 服务名 start\ stop\ status\ restart 防火墙的服务名为：iptables service iptables status\ start\ stop\ restart 查看状态\ 开启\ 关闭\ 重启 永久关闭\ 打开 ：（重启后生效） chkconfig iptables on\ off注意：学习期间直接把防火墙关掉就是，工作期间也是运维人员来负责防火墙的。 添加 服务 1) 编辑脚本：vim myservice.sh 编辑内容： （在最前面加一下两句） #chkconfig: 2345 80 90 #description:auto_run (自己的服务脚本：开机时同步时间） result=’ntpdate cn.ntp.org.cn’ 退出编辑并保存：按esc键 按 ：wq 在ntpdate.log文件中输出打印：echo $result &gt; /usr/ntpdate.log 2) 修改权限，使其拥有可执行权限: chmod 700 myservice.sh 3) 将脚本拷贝到/etc/init.d目录： 4) 加入服务：chkconfig –add myservice.sh 5) 重启服务器，验证服务是否添加成功：date 6）/usr目录下产生ntpdate.log 删除 服务 chkconfig –del name 更改 服务初 执行 等级 chkconfig –level 2345 服务名 off\ on 若不加级别，默认是2345级别 chkconfig 服务名 on\ of f 各数字代表的系统初始化级别含义： ​ 0：停机状态 1：单用户模式，root账户进行操作 2：多用户，不能使用net file system，一般很少用 3：完全多用户，一部分启动，一部分不启动，命令行界面 4：未使用、未定义的保留模式 5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。 6：停止所有进程，卸载文件系统，重新启动(reboot) 这些级别中1、2、4很少用，相对而言0、3、5、6用的会较多。3级别和5级别除了桌面相关的进程外没有什么区别。为了减少资源占用，推荐都用3级别. 注意 ：linux默认级别为3，不要把/etc/inittab 中initdefault 设置为0 和 6 二、定时调度 编辑定时任务 crontab –e 格式：minute hour day month dayofweek command 举例 * echo “hello” 每分钟打印“hello” 时间 一到， 执行 操作 命令后 会出现：You have new mail in /var/spool/mail/root 查看任务执行情况 vim /var/spool/mail/root 查看所有用户的定时任务 ll /var/spool/cron 查看当前用户的定时任务 contab –l 注意 “”代表任意的数字, “/”代表”每隔多久”, “-”代表从某个数字到某个数字, “,”分开几个离散的数字 如： 30-40 12 echo “hello” ——–每天12点30分至40分期间，每分钟执行一次命令 30,40 ——–每天12点30分和12点40分 0/5 ——–每天的12点整至12点55分期间，每隔5分钟执行一次命令 三、进程操作 查看 进程 ps -aux -a 列出所有 -u 列出用户 -x 详细列出，如cpu、内存等 -e 显示所有进程 -f 全格式 ps - ef \ grep ssh 查看所有进程里CMD是ssh 的进程信息 ，进程号（PID） ps -aux –sort –pcpu 根据 CPU 使用来升序排序 使程序 后台 运行 只需要在命令后添加 &amp; 符号 echo “hello” &amp; jobs –l –列出当前连接的所有后台进程（jobs仅适用于当前端） ps -ef \ grep 进程名 —-（推荐）列出后台进程 杀死 进程 （强制）kill -9 pid ps 命令先查出对应程序的PID或PPID ，然后杀死掉进程 四、其他命令 wget 1） 安装：yum install wget –y 2） 用法：wget [option] 网址 -O 指定下载保存的路径 3） More Actions一个从网络上自动下载文件的自由工具；支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议，可以使用 HTTP 代理；也可用于做爬虫4）举例：wget www.baidu.com -O baidu.html yum 1） 备份原镜像： cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOSBase.repo.backup 2） 下载新镜像： wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 3） 查看文件内容：vim /etc/yum.repos.d/CentOS-Base.repo 4） 生成缓存：yum makecache5）查看当前源:yum list \ head -50 rpm yum list \ head -501） 安装 rpm –ivh rpm包 2） 查找已安装的rpm包：rpm –q ntp 3） 卸载：rpm –e ntp-4.2.6p5-10.el6.centos.2.x86_64（全名） tar 1） 解压：tar -zvxf xxxx.tar.gz 2） 压缩：tar -zcf 压缩包命名 压缩目标 3） 例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61 4） -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加 -x 解压 -c 压缩 -f 目标文件，压缩文件新命名或解压文件名 -v 解压缩过程信息打印 zip 1） 安装zip：yum install zip –y 2） 压缩命令：zip -r 包名 目标目录 3） 安装 ：unzip,yum install unzip –y 4） 解压 ：unzip filename 五、安装部署JDK 部署11) 解压: tar -zxf jdk-7u80-linux-x64.tar.gz 2) 配置环境变量 编辑配置文件：vim /etc/profile 编辑内容 ： JAVA_HOME= /usr/soft/jdk1.7.0_75 PATH=$PATH:$JAVA_HOME/bin 3) 重新加载环境变量：source /etc/profile 4) 验证: java -version mysql部署1yum安装 mysql 1) yum install mysql-server -y 2) yum install mysql-devel -y 3) service mysqld start 4) mysql -uroot -p 5) mysqladmin -u root password 123456 Tomcat部署12345678910111)下载tomcat:http://tomcat.apache.org/2)启动tomcat在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务3)关闭tomcat服务，可以用shutdown.sh命令或者ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令。4)jps查看系统当前运行在jvm上的进程情况jps是JDK1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。Bootstrap是tomcat的进程名字，后面跟的是它的PID5）验证先把防火墙关了（service iptables stop），然后浏览器端访问虚拟机IP的8080端口 克隆虚拟机12345修改配置：1)网卡ip /etc/sysconfig/network-sr…/ifcfg-eth0 2)hostname /etc/sysconfig/network3)删除网络规则 rm -rf /etc/udev/rules.d/70--….-net.rules4)重启生效 reboot 六、免密登录1、工作原理： 1.Server A向Server B发送一个连接请求。 2.Server B得到Server A的信息后，在本地的authorized_keys文件中查找A存放在B上的公钥，如果有相应的公钥，则随机生成一个字符串，并用Server A的公钥加密，接着发送给Server A。 3.Server A得到Server B发来的消息后，使用私钥进行解密，然后将解密后的字符串发送给Server B。Server B用原来随机生成的字符串和A发过来的字符串进行对比，如果一致，则允许免登录。 总结：A要免密码登录到B，B首先要拥有A的公钥，然后B要做一次加密验证。对于非对称加密，公钥加密的密文不能公钥解开，只能私钥解开。 2、方法一： 1） 生成公钥和密钥： 命令： 1ssh-keygen -t rsa 并且回车3次 （在用户的root目录生成一个 “.ssh”的文件夹） 2） 查看公钥和私钥： 1ll ~/.ssh （目录中会有以下几个文件） authorized_keys:存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥 id_rsa : 生成的私钥文件 id_rsa.pub ： 生成的公钥文件 know_hosts : 已知的主机公钥清单 如果希望ssh公钥生效需满足至少下面两个条件： 1&gt; .ssh目录的权限必须是700 * 2&gt; .ssh/authorized_keys文件权限必须是600 3） 将A的.ssh目录下的公钥追加拷贝到B的authorized_keys文件里 法一： 1scp -p ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:/root/.ssh/authorized_keys 举例： 123456789[root@test .ssh]# scp -p ~/.ssh/id_rsa.pub root@192.168.91.135:/root/.ssh/authorized_keysroot@192.168.91.135's password: id_rsa.pub 100% 408 0.4KB/s 00:00 [root@test .ssh]# [root@test .ssh]# [root@test .ssh]# [root@test .ssh]# ssh root@192.168.91.135Last login: Mon Oct 10 01:27:02 2016 from 192.168.91.133[root@localhost ~]# 法二： 分为两步操作： 12$ scp ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:pub_key //将文件拷贝至远程服务器$ cat ~/pub_key &gt;&gt;~/.ssh/authorized_keys //将内容追加到authorized_keys文件中， 不过要登录远程服务器来执行这条命令 法三：手动复制：如果有多台节点时：A → B &lt; 1、拷贝A的.ssh目录下的公钥： 1vim id_rsa.pub &lt;2、将A的公钥复制好后，在主机B上的~/.ssh目录下，创建并编辑authorized_keys文件，接着黏贴进去。 1vim authorized_keys 4) 验证： 用Scp远程拷贝命令，在A主机上随便拷贝一份文件到B主机上，如果不需要密码，则说明免密码登录配置成功。 3、方法二： 通过ssh-copy-id的方式前提：公钥和私钥已经生成 命令： 1ssh-copy-id -i ~/.ssh/id_rsa.pub &lt;romte_ip&gt; 举例： 123456789101112&gt; [root@test .ssh]# ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.91.135 &gt; root@192.168.91.135's password: &gt; Now try logging into the machine, with "ssh '192.168.91.135'", and check in:&gt; &gt; .ssh/authorized_keys&gt; &gt; to make sure we haven't added extra keys that you weren't expecting.&gt; &gt; [root@test .ssh]# ssh root@192.168.91.135&gt; Last login: Mon Oct 10 01:25:49 2016 from 192.168.91.133&gt; [root@localhost ~]#&gt; 常见错误： [root@test ~]# ssh-copy-id -i ~/.ssh/id_rsa.pub 192.168.91.135 -bash: ssh-copy-id: command not found //提示命令不存在 解决办法：yum -y install openssh-clients 4、（此方法有待考究）法三：通过Ansible实现 批量 免密 1）、 将需要做免密操作的机器hosts添加到/etc/ansible/hosts下： 1234[Avoid close] 192.168.91.132 192.168.91.133 192.168.91.134 ​ 2）、 执行命令进行免密操作 ：​ 1ansible &lt;groupname&gt; -m authorized_key -a &quot;user=root key=&apos;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;) &#125;&#125;&apos;&quot; -k ​ 示例： 1234567891011121314[root@test sshpass-1.05]# ansible test -m authorized_key -a "user=root key='&#123;&#123; lookup('file','/root/.ssh/id_rsa.pub') &#125;&#125;'" -k SSH password: -----&gt;输入密码 192.168.91.135 | success &gt;&gt; &#123; "changed": true, "key": "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEArZI4kxlYuw7j1nt5ueIpTPWfGBJoZ8Mb02OJHR8yGW7A3izwT3/uhkK7RkaGavBbAlprp5bxp3i0TyNxa/apBQG5NiqhYO8YCuiGYGsQAGwZCBlNLF3gq1/18B6FV5moE/8yTbFA4dBQahdtVP PejLlSAbb5ZoGK8AtLlcRq49IENoXB99tnFVn3gMM0aX24ido1ZF9RfRWzfYF7bVsLsrIiMPmVNe5KaGL9kZ0svzoZ708yjWQQCEYWp0m+sODbtGPC34HMGAHjFlsC/SJffLuT/ug/hhCJUYeExHIkJF8OyvfC6DeF7ArI6zdKER7D8M0SM WQmpKUltj2nltuv3w== root@localhost.localdomain", "key_options": null, "keyfile": "/root/.ssh/authorized_keys", "manage_dir": true, "path": null, "state": "present", "unique": false, "user": "root" &#125; [root@test sshpass-1.05]#]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（二）]]></title>
    <url>%2F2018%2F12%2F28%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A02%2F</url>
    <content type="text"><![CDATA[​​ [TOC] 一、磁盘指令1、查看硬盘信息 命令：df （默认大小以kb显示） df -k（以kb为单位） df -m（ 以mb为单位） df –h （易于阅读） 2、查看文件/目录的大小 命令：du filename|foldername （默认单位为kb）-k kb单位 -m mb单位 -a 所有文件和目录 -h 更易于阅读 ​ --max-depth=0 目录深度 二、网络指令1、查看网络配置信息 12&gt; 命令:ifconfig&gt; 箭头1指向的是本机IP，箭头2为广播地址，箭头3位子网掩码。 2、测试与目标主机的连通性 命令：ping remote_ip ctrl + c :结束ping进程 可以ping通Windows系统的IP 输入ping 192.168.78.192代表测试本机和192主机的网络情况， 箭头1表示一共接收到了3个包，箭头2表示丢包率为0，表示两者之间的网络顺畅。 注意：linux系统的ping命令会一直发送数据包，进行测试，除非认为的按ctrl + c停止掉， ​ windows系统默认只会发4个包进行测试，以下为windows的dos命令。 3、显示各种网络相关信息 命令： 12&gt; netstat -anpt&gt; 12345678910111213-a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-l 仅列出有在 Listen (监听) 的服務状态-p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令。提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到 查看端口号（是否被占用） (1)、lsof -i:端口号 （需要先安装lsof） (2)、netstat -tunlp|grep 端口号 4、测试远程主机的网络端口安装telnet: 1yum install telnet -y 查看本机能否连上远程主机的端口号: 1telnet ip port 测试成功后，按ctrl + ] 键，然后弹出telnet&gt;时，再按q退出 5、http请求模拟1命令: curl [option] [url] 举例： 模拟请求百度 1curl -X get www.baidu.com 用法解释： -X/–request [GET|POST|PUT|DELETE|…] 使用指定的http method发出 http request -H/–header 设定request里的header -i/–include 显示response的header -d/–data 设定 http parameters -v/–verbose 输出比较多的信息 -u/–user 使用者账号，密码 -b/–cookie cookie 参数-X跟–request兩个功能是一样的 举例： 123456curl -X GET http://www.baidu.com/ curl --request GET http://www.baidu.com/ curl -X GET "http://www.rest.com/api/users"curl -X POST "http://www.rest.com/api/users"curl -X PUT "http://www.rest.com/api/users"curl -X DELETE "http://www.rest.com/api/users" 三、系统管理指令1、用户操作12345678910 操作 命令创建用户 useradd|adduser username修改密码 passwd username删除用户 userdel –r username修改用户（已下线）： 修改用户名: usermod –l new_name oldname 锁定账户: usermod –L username 解除账户： usermod –U username查看当前登录用户 仅root 用户：whoami | cat /etc/shadow 普通用户：cat /etc/pqsswd 2、用户组操作12345 操作 命令 创建用户组 groupadd groupname删除用户组 groupdel groupname修改用户组 groupmod –n new_name old_name查看用户组 groups （查看的是当前用户所在的用户组） 3、用户+用户组12345 操作 命令 修改用户的主组 usermod –g groupname username给用户追加附加组 usermod –G groupname username查看用户组中用户数 cat /etc/group注意：创建用户时，系统默认会创建一个和用户名字一样的主组 4、系统权限12345678910 操作 命令 查看/usr下所有权限 ll /usr 权限类别 r（读取：4） w（写入：2） x（执行：1） 三个为一组，无权限用 —代替 UGO模型 U（User） G(Group) O(其他)权限修改 修改所有者：chown username file|folder (递归)修改所有者和所属组： chown -r username：groupname file|folder 修改所属组：chgrp groupname file|folder 修改权限：chmod ugo+rwx file|folder 四、系统配置指令0.1环境变量全局变量、局部变量 首先考虑一个问题，为什么我们先前敲的yum, service,date,useradd等等，可以直接使用，系统怎么知道这些命令对应的程序是放在哪里的呢？ 这是由于无论是windows系统还是linux系统，都有一个叫做path的系统环境变量，当我们在敲命令时，系统会到path对应的目录下寻找，找到的话就会执行，找不到就会报没有这个命令。如下图： 配置系统环境变量，使得某些命令在执行时，系统可以找到命令对应的执行程序，命令才能正常执行。 我们可以查看一下，系统一共在哪些目录里寻找命令对应的程序。 命令： 1echo $PATH 可以看到path里有很多路径，路径之间有冒号隔开。当用户敲命令时，系统会从左往右依次寻找对应的程序，有的话则运行该程序，没有的就报错，command not found. 配置全局环境变量： 12&gt; vim /etc/profile #全局环境变量所在的文件&gt; 在文件中： PATH=$PATH:(命令所在目录) 退出文件编辑后： source /etc/profile (重新加载资源，有的可能需要重启机器，这不适用于实际状况) 配置局部环境变量：（推荐，限当前登录用户使用） 查看所有文件(root目录下) ls -a (发现隐藏文件 .bash.profile) 12&gt; vim ~/ bash_profile #局部变量所在的文件&gt; 在文件中： export PATH =$PATH:(命令所在目录) 0.2脚本运行编辑脚本： 1vim test.sh 脚本内容： 12#！/bin/shecho " hello test sh" 给脚本添加权限： 1chmod 700 test.sh 运行： 1234567一种是到脚本的目录下执行：运行命令 ： ./test.sh ,代表执行当前目录里的脚本test.sh一种是敲脚本的绝对路径：运行命令 ：/usr/test/test.sh第三种方式添加到环境变量中 以上两种运行方式都不是很简便，因为先前我们执行yum service命令等，都是直接敲对应的命令的。所以我们也可以参照这样子做，只要我们配一个环境变量就好。 编辑环境变量： 1vim /etc/profile 将test.sh 所在路径放到path后面即可 编辑完之后，执行命令， 1source /etc/profile 重新加载环境变量，此时会发现PATH路径多了一个/usr/test。 最后验证一下，直接执行test.sh 1.修改主机名123 编辑文件： 命令： vim /etc/sysconfig/network 文件内容： HOSTNAME=node00（重启生效)reboot 2.DNS配置: /etc/resolv.conf 为DNS服务器的地址文件 123456789101112查看DNS服务器的地址： cat /etc/resolv.conf修改DNS服务器地址： 法一：编辑文件： vim /etc/sysconfig/network.scripts/ifconfig-eth0在配置网关时，配置DNS1=114.114.114.114（不推荐，江苏南京的IP）法二：编辑文件： vim /etc/resolv.conf文件内容：（用本地网关解析） nameserver 192.168.198.0( 此为虚拟机中的网关地址) 3.sudo权限配置1234567891011121314151617 操作 命令编辑权限配置文件： vim /etc/sudoers格式： 授权用户 主机=[(切换到哪些用户或用户组)] [是否需要密码验证] 路径/命令举例： test ALL=(root) /usr/bin/yum,/sbin/service解释： test用户就可以用yum和servie命令， 但是，使用时需要在前面加上sudo再敲命令。 第一次使用需要输入用户密码,且每个十五分钟需要一次密码验证修改： test ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service这样就不需要密码了将权限赋予某个组，%+组名%group ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service列出用户所有的sudo权限 sudo –l 4.系统时间12345678操作 命令查看系统时间 date ---查看当前时间详情 cal ---查看当前月日历 cal 2018 ---查看2018年完整日历 cal 12 2018 ---查看指定年月的日历 更新系统时间（推荐） yum install ntpdate –y ---安装ntp服务 ntpdate cn.ntp.org.cn ---到域名为cn.ntp.org.cn的时间服务器上同步时间 5.关于hosts配置 相当于给IP地址其别名，可以通过别名访问 路径： Windows系统 C:/Windows/System32/drivers/etc/hosts 文件 Linux系统 /etc/hosts文件：vim +路径 统一 编辑格式 IP地址 别名：192.168.198.128 node00 6.关于hostname配置相当于给对应的虚拟机器起别名 Linux系统： vi /etc/sysconfig/network 编辑内容： HOSTNAME=node01 五、重定向与管道符 输出 重定向 输出重定向到一个文件或设备： &gt; 覆盖原来的文件 &gt;&gt; 追加原来的文件 举例： ls &gt; log — 在log文件中列出所有项，并覆盖原文件 echo “hello”&gt;&gt;log —将hello追加到log文件中 输入 重定向 &lt; 输入重定向到一个程序 举例：cat &lt; log —将log文件作为cat命令的输入，查看log文件的内容 标准 输出 重定向 1 &gt; 或 &gt; 含义： 输出重定向时，只用正确的输出才会重定向到指的文件中 错误的则会直接打印到屏幕上 错误 输出 重定向 2 &gt; 含义： 错误的输出会重定向到指定文件里，正确的日志则直接打印到屏幕上。 结合 使用 2&gt;&amp;1 含义： 将无论是正确的输出还是错误的输出都重定向到指定文件 管道 **\ ** 含义： 把前一个输出当做后一个输入 grep 通过正则搜索文本，并将匹配的行打印出来 netstat -anp \ grep 22 把netstat –anp 命令的输出 当做是grep 命令的输入 命令 执行 控制 &amp;&amp; 前一个命令执行成功才会执行后一个命令 **\ \ ** 前一个命令执行失败才会执行后一个命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（一）]]></title>
    <url>%2F2018%2F12%2F27%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A01%2F</url>
    <content type="text"><![CDATA[[TOC] 一、命令指南（manual）：man安装 man： 1yum install man –y （-y 表示获得允许，无需确认） 查看ls命令指南： 1man ls 二、目录操作命令切换目录： 1cd + 目录的路径 查看当前目录所在的完整路径： 1pwd 新建目录： 1mkdir +目录名字 查看 —当前目录所用有的子目录和文件： 1ls ，ll等价于 ls –l ​ 查看—目录下的所有东西（包括隐藏文件）： 1ls –al 等价于 ll -a 拷贝目录或文件： 1cp –r install.log install2.log 复制文件，-r 表示递归复制，此时，可用于复制整个目录 删除目录或文件： 1rm -r install.log 此时需要手动输入y ，代表确认删除。可加 –f参数，直接删除，无需确认。 当需要一个目录下所有东西时，加-r参数，代表遍历删除。 (rmdir只能删除空目录) 移动目录或文件： 1mv + 目录/文件名字 + 其他路径 ​ 将test目录移动到 根目录/ 下 : 1mv test / （如果移动到当前目录，用另外一个名称，则可以实现重命名的效果） 更改文件或目录的名字： 1mv + 旧目录名字 + 新目录名 ( -r 用于递归的拷贝，删除，移动目录) 三、文件操作命令1、一般文件操作在Linux中：一切皆文件 新建文件：touch install.log​ (vim install.log 编辑文件，如果文件不存在，就会新建一个对应的文件，并进入文件的编辑模式，如果按 :wq 会保存文件并退出，如果按 :q 则不保存退出)​查看文件内容：cat +（文件名）（一次性显示整个文件的内容，文件内容过多时文本在屏幕上迅速闪过（滚屏），用,户体验不好） 一次命令显示一屏文本： 满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比。 1234567 more +（文件名） 按键 效果 Space 显示下一屏文本内容B 显示上一屏文本内容Enter 显示下一行文本内容Q 退出查看 less+（文件名） 按键 效果 Q 退出less 命令 h 显示帮助界面 u 向后滚动半页 d 向前翻半页 e | Enter（回车） 向后翻一行文本 space(空格键) 滚动一页 b 向后翻一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 上下键， 向上一行，向下一行 从头部打印文件内容： 1head -10 +（文件名） 打印文件1到10行 从尾部打印文件内容​ 1tail -10 +（文件名）打印文件最后10行 tail还可用来查看文件内容的更新变化 tail -f (文件名) 查找文件或目录​ 1find +（路径名） –name +（文件名） ​ 举例： 1find / -name profile ​ 在/(根目录)目录下查找 名字为profile的文件或目录 也可利用正则：​ 举例： 1find /etc -name pro* ​ 在/etc目录下查找以pro开头的文件或目录 注意： 路径越精确，查找的范围越小，速度越快 查找的目录必须是文件所在目录的父级目录 2、文件编辑vi（1） vi 进入编辑模式 —–&gt;按i 进入插入模式 ——-&gt; 按Esc 退出编辑模式 1234vi filename :打开或新建文件，并将光标置于第一行首 vi +n filename ：打开文件，并将光标置于第n行首 vi + filename ：打开文件，并将光标置于最后一行首 vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的字符串所在的行首 filename 为文件名 （2）在文件vi（文件编辑）模式下 命令行模式 123456789:w 保存:q 退出:wq 保存并退出:q! 强制退出，不保存:set nu |ctrl+g 显示文本行数:set nonu 去除显示的行数:s/p1/p2/g 将当前行中所有p1均用p2替代 :n1,n2s/p1/p2/g 将第n1至n2行中所有p1均用p2替代 :g/p1/s//p2/g 将文件中所有p1均用p2替换 一般模式 12345678910111213141516171819202122232425262728293031按键：yy 复制光标所在行(常用) nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) p|P p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)G 光标移至第最后一行nG 光标移动至第N行行首n+ 光标下移n行 n- 光标上移n行 H 光标移至屏幕顶行 M 光标移至屏幕中间行 L 光标移至屏幕最后行 dd 删除所在行 x或X 删除一个字符，x删除光标后的，而X删除光标前的 u 撤销(常用)：N,Md 删除第N行到第M行：,$-1d 删除当前光标到倒数第一行数据按键： i: 在当前光标所在字符的前面，转为输入模式； a: 在当前光标所在字符的后面，转为输入模式； o: 在当前光标所在行的下方，新建一行，并转为输入模式； I：在当前光标所在行的行首，转换为输入模式 A：在当前光标所在行的行尾，转换为输入模式 O：在当前光标所在行的上方，新建一行，并转为输入模式；---逐字符移动：h: 左 l: 右j: 下 k: 上 （不同：在我的xshell中是 yyn实现复制所在行的向下n行） （nB|nb:光标向上移动n行） vimVim是从 vi 发展出来的一个文本编辑器 安装vim 软件： 1yum install vim -y 用vim 打开/etc/profile 文件， 特点：编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强 ，其他均与vi相同 3、文件上传下载安装上传下载命令： 1yum install lrzsz -y 上传文件：（windows—&gt;linux） 命令 ：rz 弹出windows上传文件窗口 下载文件：(linux—&gt;windows)注意：sz命令只能下载文件，不能下载目录，推荐将目录压缩成tar包或使用工具软件：Winscp【Xftp】 命令：sz （文件名） 弹出windows下载窗口,下载文件到指定文件目录,下载完之后，按ctrl+c结束。 4、文件传输(1）本地→远程 文件 ： scp local_file remote_username@remote_ip:remote_folder 目录 ： scp -r local_folder remote_username@remote_ip:remote_folder 举例： 1234567891011方式一： scp -rf /etc/profile root@192.168.198.128:/etc/方式二: scp -r /etc/profile root@node01 /etc/ 方式三： scp -r /etc/profile node01:/etc/ 方式四： scp -r /ec/profile node01:'pwd' scp ：远程传输文件命令 -r ：- 指的是后面跟的是参数 r 指的是遍历指定文件 f 指的是不用询问 /etc/profile : 是指定传输的文件 root： 远程机器的账户名 @ 远程机器的IP地址 ： /etc/ 远程机器上指定的目录 node01：远程机器的别名 ‘pwd’： 本地要远程传输文件所在的目录 第一次远程拷贝时，需要在箭头1初输入yes确认一下，验证一下远程主机。然后在箭头2处输入一下远程主机的密码。 (2）远程→本地 文件 ： scp remote_username@remote_ip:remote_file local_folder 目录 ： scp remote_username@remote_ip:remote_folder local_folder 四、文件系统12345678910111213141516171819202122232425262728Linux目录结构： bin 存放二进制可执行文件(ls,cat,mkdir等) boot 存放用于系统引导时使用的各种文件 dev 用于存放设备文件 etc 存放系统配置文件 home 存放所有用户文件的根目录 lib 存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt 系统管理员安装临时文件系统的安装点 opt 额外安装的可选应用程序包所放置的位置 proc 虚拟文件系统，存放当前内存的映射 root 超级用户目录 sbin 存放二进制可执行文件，只有root才能访问 tmp 用于存放各种临时文件 usr 用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录 var 用于存放运行时需要改变数据的文件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统数据库MySQL安装]]></title>
    <url>%2F2018%2F12%2F26%2FLinux%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[[TOC] Linux系统数据库MySQL安装一、第一次安装MySQL1、yum安装 命令 ： yum -y install mysql-server mysql-devel 2、登录 命令 ： mysql -u -p 显示： 1mysql&gt; 3、查看数据库(注意用‘ ; ’结束 ) 命令 ： show databases; 4、退出： 命令： quit； 5、创建用户： 命令 ： mysqladmin -uroot password 123456 6、再登录： 命令 ：mysql -u root -p 显示： 1mysql&gt; 说明成功了！ 7、数据库操作： 命令 ： use mysql; 显示： 1234Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 8、查看用户数据表： 命令 ： show tables; 9、查询user表部分字段： 命令 ： select host,user,password from user; 10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确（1）推荐现将user表中其他无密码的记录删除 命令 ： delete from user where password = ‘ ‘; (2)更新有密码的记录的host字段值 命令 ： update user set host = “%”; (3)刷新权限 命令 ：flush privileges; (4)退出 命令 ：quit; 二、Linux系统登录数据库MySQL报错报错一：1、登录 mysqld -uroot -p123456 报错： 1Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (111) 解决： 1）、先删除mysql.sock cd /var/lib/mysql mv mysql.sock mysql.sock.bak 2）、再次登陆 mysql -uroot -p123456 报错： 1Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (2) 3）、看看mysql的状态， /etc/rc.d/init.d/mysqld status 显示： mysqld is stopped 4）、看看是不是mysql的权限问题在/var/lib目录下： ls -lt|grep mysql 显示： drwxr-xr-x. 4 mysql mysql 4096 Jan 6 11:09 mysql 5）、说明mysql服务没有启动 2、启动mysql服务 /etc/init.d/mysqld start 或 service mysqld start 显示： 1Starting mysqld: [ OK ] 3、再次登录： mysql -uroot -p123456 显示： 12345678910111213Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; 4、退出 quit 5、解决出现mysql.sock的问题 （1）、vim /etc/mycnf 编辑内容： 12[mysqld]skip_name_resolve=on innodb_file_per_table=on 按esc :wq 保存并退出 （2）使用命令： mysql_secure_installation （3）直接[ enter ] 键，输入密码， (另推荐：Jakie_ZHF老师的博客) 报错二：1ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Linux系统环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习之CentOS 6系统安装]]></title>
    <url>%2F2018%2F12%2F25%2FLinux%E7%B3%BB%E7%BB%9FCentOS%206%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、安装 资源准备： CentOS-6.6-x86_64-minimal.iso（简易迷你版） ： CentOS-6.7-x86_64-bin-DVD1.iso（完整版）： 1、点击新建虚拟机img 2、选择典型。（专业人士使用的话建议选择高级）img 3. 选择稍后安装操作系统img 4. 选择操作系统类型，选择linux,centos 64位img 5. 选择虚拟机安装位置和名称。img 6. 指定磁盘容量，默认20GB。img 7. 选择自定义硬件img 8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。img 9.点击完成。img 二、配置虚拟机1. 启动虚拟机。指定虚拟机：点击 “开启此虚拟机” 注意：如果启动虚拟机时，发生以下问题，说明是你的电脑默认未开启虚拟化技术。 img 此时你应该把机器重启并进入bios界面（不同的机器进入bios界面的快捷键不同，一般为F1~F10键中的某个键，如果都不行，就得自己百度一下你的机器型号进入bios界面的快捷方式）。 ​ 当进入bios界面后，把虚拟机化选项（virtualization technology）打开,通过回车键，把disabled改成enabled,然后保存并重启机器。我这边是按F10，不同机器可能不一样，看右下角的提示信息。 img 2.Test Media, 如果不需要的话，点Skipimg 3、单击Next按钮继续img 4. 选择安装期间显示的语言img 5、选择键盘语言img 6、选择存储介质的类别。如果是将CentOS 6安装到本地硬盘上，选择 Basic Storage Devices，如果安装到网络存储介质如SANs上，选择 Specialized Storage Devices img 7.选择 yes,discard any dataimg 8. 设定主机名称（hostname）img 9. 设定时区，选择 Asia/Shanghai 10. 设定root帐户的密码尽量使用较复杂的密码安装（根据实际情况，密码简单时，会有提示，点击user anyway 就行） 11. 选择安装类型，这里我选择 “Use All Space” img 12. 选择 “Write changes to disk”，将分区数据写入硬盘img 13. 开始安装，此时只需等待即可img 14. 安装完结后，点击Reboot按钮三、网络配置1、查看网关 2、配置静态IP(NAT模式)1、编辑配置文件,添加修改以下内容 1vim /etc/sysconfig/network-scripts/ifcfg-eth0 2、按i 进入文本编辑模式，出现游标，左下角会出现INSERT,即可以编辑 12345678DEVICE=eth0 #网卡设备名,请勿修改名字TYPE=Ethernet #网络类型，以太网BOOTPROTO=static #启用静态IP地址ONBOOT=yes #开启自动启用网络连接 IPADDR=192.168.198.128 #设置IP地址NETMASK=255.255.255.0 #设置子网掩码GATEWAY=192.168.198.2 #设置网关DNS1=114.114.114.114 #设置备DNS 按ESC退出编辑模式 1:wq #保存退出 3、重启网络连接 12service network restartifconfig #查看IP地址 4、单独配置DNS解析器 1vi /etc/resolv.conf 编辑内容：i 1nameserver 网关 :wq #保存并退出 5、验证 12ping 虚拟网关 | 物理机（笔记本）IP | 外网ping www.baidu.com 6、注意 a.保证VMware的虚拟网卡没有被禁用 b.网关IP不能被占用 3、桥接和NAT模式的区别桥接：​ 结构：网络与物理机同一个网段（会占用外部IP） ​ 特点： ​ 1.外网能够访问 ​ 2.能够访问外网 注意：桥接模式下的虚拟机网关必须改为与物理机网关一致 NAT模式：​ 结构：构成一个以物理机为网关的子网 ​ 特点： ​ 1.子网的所有的服务器对外不可见 ​ 2.子网能够正常访问外网 ​ 安全！！！ ​ 节省IP资源 四、拍快照（保存当时计算机所出状态的各种配置和资源，适度使用） 选中指定虚拟计算机——鼠标右击—–选中“快照” ——“拍摄快照‘—-在页面中找到”拍摄快照“，并添加名称和描述 也可以删除，找到页面中的删除按钮]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS 6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动安装maven坐标依赖]]></title>
    <url>%2F2018%2F12%2F24%2F%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85maven%E5%9D%90%E6%A0%87%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[一、事件原因：学习quartz框架时，在maven项目的pom.xml文件中添加quartz所需要的坐标依赖时，显示jar包不存在。 12345678910111213提示："Dependency 'xxxx‘ not found"， 并且添加的如下两个坐标依赖均报红。 &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 分析： 1、maven项目所需要的jar包均存放在maven的F:\m2\repository(项目所需的jar包仓库)文件夹中 2、在F:\apache-maven-3.5.4\conf的settings.xml文件中有如下设置：（由于使用远程仓库太慢，阿里云给我们提供了一个镜像仓库，便于我们使用，且只包含central仓库中的jar） 1234567&lt;!--文件中原有的配置：远程仓库---&gt;&lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; 1234567&lt;!--文件中自己手动配置：阿里镜像仓库---&gt;&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 3.可是我们在https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧 （如果有小伙伴有别的解决方案，还请指点一二。） 1&lt;!--more--&gt; 二、解决方案1、首先，我们需要从maven Repository中下载我们需要的jar包（需要的两个jar包，下载原理相同） 2、注意我们的maven安装，需要配置环境变量，才能在dos窗口，指令安装jar包 因为我之前查资料时，有小伙伴说，java的环境变量配置也会影响，所以，我在这里也把java的环境变量配置也贴出来 1 1544699916763 1544699989775 JAVA_HOME F:\Java\jdk1.8.0_131（ 根据自己的jdk安装目录） CLASSPATH .;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar MAVEN_HOME F:\apache-maven-3.5.4（ 根据自己maven安装目录） Path（注意配置的时候，一定要和配置home时的变量名一致，如MAVEN_HOME,我配置成了%MVN_HOME%\bin;） %JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%MYSQL_HOME%\bin;%MAVEN_HOME%\bin; 配置这些环境变量，在dos窗口才能使java ，mvn 之类的指令可以用； 否则会出现如下显示。 ‘mvn’ 不是内部或外部命令，也不是可运行的程序 (这就是环境变量没有配成功的结果) 3.安装 C:\Users\Administrator&gt;mvn -v 1544701045091 C:\Users\Administrator&gt;mvn install:install-file -Dfile=F:/apache-maven-3.5.4/m2/quartz-2.3.0.jar（jar包所在路径） -DgroupId=org.quartz-scheduler -DartifactId=quartz -Dversion=2.3.0 -Dpackaging=jar （根据下面所示的配置groupId、artifactId、version） 12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 1544702128551 如图所示，安装成功。 1544702179172]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>

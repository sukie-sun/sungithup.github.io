<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark学习(二)]]></title>
    <url>%2F2019%2F02%2F17%2FSpark%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、控制算子1、概念： 控制算子有三种，cache、persist、checkpoint 以上算子都可以将RDD 持久化，持久化的单位是 partition。 cache 和 persist 都是懒 执行的。 必须有一个 action 类算子触发执行。 cache 和 persist 算子的返回值可赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了 checkpoint 算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系（所有父RDD）。 错误：rdd.cache().count() 返回的不是持久化的 RDD，而是一个数值了。 2、详解 :one:​ cache默认将 RDD 的数据持久化到内存中。cache 是懒执行。  注意： chche () =persist()=persist(StorageLevel.Memory_Only) :two: persist 支持指定持久化级别 useOffHeap 使用堆外内存 disk、memory、offheap、deserialized（不序列化）、replication（副本数，默认为1） 序列化：压缩数据（节省空间，使用数据时要反序列化，会额外消耗CPU性能） none 、disk_only、disk_only_2、memeory_only 、memeory_only _ser 、 memory_and_disk 、 memory_and_disk_2 :three: checkpoint checkpoint 将 RDD 持久化到磁盘，还可以切断 RDD 之间的依赖关系。 checkpoint 的执行原理： 当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。 当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的RDD 做一个标记。 Spark 框架会自动启动一个新的 job，重新计算这个 RDD 的数据，将数据持久化到 HDFS 上。 优化： 对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS上就可以，省去了重新计算这一步。 持久化级别：如下 1234567891011121314151617181920212223242526val cocnf = new SparkConf()conf.setMaster("local").setAppname("count")val context = new SparkContext()//设置CP在HDFS上的路径context.setCheckPointDir("")val lineADD = context.textFile("./countword.txt")val time1 = System.currentTimeMillis()val c = lineADD.count()val time2 = System.currentTimeMillis()val t1 = time2 - time1//做缓存(persisit（m_o）)linelineADD = lineADD.cache()//做持久化lineADD.persisit(StorageLevel.memory_only)//checkpoint 容错,最好还有cachelineADD.checkpoint()val time3 = System.currentTimeMillis()val c = lineADD.count()val time4 = System.currentTimeMillis()val t2 = time4 - time3//t1 远大于 t2 二、算子补充transformation转换算子 12&gt; join,leftOuterJoin,rightOuterJoin,fullOuterJoin&gt; 作用在 K,V 格式的 RDD 上。根据 K 进行连接，对（K,V）join(K,W)返回（K,(V,W)） join 后的分区数与父 RDD 分区数多的那一个相同 12&gt; union&gt; 合并两个数据集。两个数据集的类型要一致。 返回新的 RDD 的分区数是合并 RDD 分区数的总和。 12&gt; intersection&gt; 取两个数据集的交集 12&gt; subtract&gt; 取两个数据集的差集 12&gt; mapPartition&gt; 与 map 类似，遍历的单位是每个 partition 上的数据。 12&gt; distinct(map+reduceByKey+map)&gt; 12&gt; cogroup&gt; 当调用类型（K,V）和（K，W）的数据上时，返回一个数据集（K，（Iterable,Iterable）） action触发算子 12&gt; foreachPartition&gt; 遍历的数据是每个 partition 的数据。 三、集群搭建及测试Standalone1、下载安装包、解压Spark历史版本下载 注意： 与Hadoop的版本保持对应。 此处使用： spark-1.6.0-bin-hadoop2.6.tgz 1tar -zvxf spark-1.6.0-bin-hadoop2.6.tgz 2、改名1mv spark-1.6.0-bin-hadoop2.6 spark-1.6.0 3、修改slaves进入安装包的conf目录下，修改slaves.template文件，添加从节点。并保存。 123#备份cp slaves.template slavesvim slaves 常驻进程：master、worker 配置slaves（与worker对应） 12node01node02 4、修改 spark-env.sh改名（备份） 1cp spark-env.sh.template spark-env.sh 配置spark-env.sh（注意与虚拟机实际配置对应） 1234567891011121314151617#locally#cluster#YARN client#standalone deploy#配置 java_home 路径JAVA_HOME=/usr/soft/jdk1.8.0_191#master 的 ipSPARK_MASTER_IP=192.168.198.128#提交任务的端口，默认是 7077SPARK_MASTER_PORT=7077#每个 worker 从节点能够支配的 core 的个数SPARK_WORKER_CORES=2#每个 worker 从节点能够支配的内存数SPARK_WORKER_MEMORY=1024m#配置yarnHADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5/etc/hadoop 5、其他节点将spark解压文件发送到其他两个节点 12[root@node00 soft]# scp -r spark-1.6.0-bin-hadoop2.6 node01:`pwd`[root@node00 soft]# scp -r spark-1.6.0-bin-hadoop2.6 node02:`pwd` 6、配置环境变量（可不配，因为bin路径中包含start-all ，该命令与hdfs中的命令会冲突） 7、启动：(node00)在spark的解压文件的/sbin 目录下 1./start-all.sh 停止 1./stop-all.sh 显示： [root@node00 sbin]# ./start-all.shstarting org.apache.spark.deploy.master.Master, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploymaster.Master-1-node00.out node01: starting org.apache.spark.deploy.worker.Worker, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spar.deploy.worker.Worker-1-node01.out node02: starting org.apache.spark.deploy.worker.Worker, logging to /usr/soft/spark-1.6.0-bin-hadoop2.6/logs/spark-root-org.apache.spar.deploy.worker.Worker-1-node02.out 查看三台节点的进程 node00（命令启动的节点） 12&gt; [root@node00 sbin]# jps&gt; 2343 Master2408 Jps nose01(配置的从节点) 12&gt; [root@node01 ~]# jps&gt; 2292 Jps2229 Worker node02(从节点) 12&gt; [root@node02 ~]# jps&gt; 6216 Worker6266 Jps 注意： Worker在这里不是真正干活的进程，而是相当于Yarn中的NM。 它是负责管理所在节点资源的、向Master汇报所在节点的信息（如核数、内存数） Master： 监控任务、分发任务、回收计算结果 8、搭建客户端 将 spark 安装包原封不动的拷贝到一个新的节点上，然后，在新的节点上提交任务即可。 注意：8080 是Spark WEBUI页面的端口 ； 7077 是Spark任务提交的端口 web页面访问：ip:8080 修改master的WEBUI端口， 方法一（永久）：通过修改start-master.sh 文件（在/sbin目录下） 1vim start-master.sh 找到文件内容如下的部分： 123if [ &quot;$SPARK_MASTER_WEBUI_PORT&quot; = &quot;&quot; ]; then SPARK_MASTER_WEBUI_PORT=8080fi 方法二：在 Master 节点上导入临时环境变量，只作用于当前进程，重启就无效了。 1[root@node00 sbin]# export SPARK_MASTER_WEBUI_PORT=8080 删除临时变量 1[root@node00 sbin]# export -n SPARK_MASTER_WEBUI_PORT Yarn1、步骤1。2。3。4。5。8。同standalone 不用Master和Worker，所以不用第7步，我们使用的是yarn中的RM和NM 2、配置添加 HADOOP_CONF_DIR配置 （在使用Yarn时，就能找到关于hdfs的所有配置，其中就包括IP 和Port） 方式一： 编辑spark-env.sh文件 方式二： 1export HADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5/etc/hadoop 测试：求π值Pi案例： 源码案例：路径：在spark解压路径spark-1.6.0-bin-hadoop2.6中 spark-1.6.0-bin-hadoop2.6/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala 原理：随机产生无穷多个点落入如上图形中，求落入圆中的概率：$$概率 p = πrr/(2r*2r)=π$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */// scalastyle:off printlnpackage org.apache.spark.examplesimport scala.math.randomimport org.apache.spark._/** Computes an approximation to pi */object SparkPi &#123; def main(args: Array[String]) &#123; val conf = new SparkConf("local").setAppName("Spark Pi") val spark = new SparkContext(conf) // args 运行时传入的参数 slices 分区数量 (决定task数量) val slices = if (args.length &gt; 0) args(0).toInt else 2 //MaxValue 一个无限大的数 n 随机产生的十万个的数 val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow //parallelize可以获得RDD ，将1~n个数字放到RDD中 //val count :[Int] = spark.parallelize(1 until n, slices) val count = spark.parallelize(1 until n, slices).map &#123; i =&gt; val x = random * 2 - 1 val y = random * 2 - 1 if (x*x + y*y &lt; 1) 1 else 0 &#125;.reduce(_ + _) println("Pi is roughly " + 4.0 * count / n) spark.stop() &#125;&#125;// scalastyle:on println 所需使用的jar包：spark-examples-1.6.0-hadoop2.6.0.jar 位置：解压目录的lib路径下 在任一节点的/bin路径下上执行如下命令：（node00） Standalone 提交命令:12345./spark-submit #提交spark --master spark://node1:7077 #spark主节点的地址和端口 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100 # 指明运行的jar包+路径 和 jar包中执行的包名+类名 100 为传入的参数./spark-submit --master spark://node00:7077 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 1000 显示： 提交命令的节点（node00主节点） 会显示执行日志、运算结果 12345678910111213141516171819202122232425262728293031323334&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Starting task 999.0 in stage 0.0 (TID 999, node02, partition 999,PROCESS_LOCAL, 2158 bytes)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 995.0 in stage 0.0 (TID 995) in 68 ms on node02 (996/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 997.0 in stage 0.0 (TID 997) in 131 ms on node01 (997/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 996.0 in stage 0.0 (TID 996) in 147 ms on node01 (998/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 999.0 in stage 0.0 (TID 999) in 112 ms on node02 (999/1000)&gt; 19/02/13 23:27:31 INFO scheduler.TaskSetManager: Finished task 998.0 in stage 0.0 (TID 998) in 115 ms on node02 (1000/1000)&gt; 19/02/13 23:27:31 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:36) finished in 79.202 s&gt; 19/02/13 23:27:31 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool &gt; 19/02/13 23:27:31 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:36, took 82.641779 s&gt; &gt; Pi is roughly 3.14148344 #运算结果&gt; &gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/metrics/json,null&#125;&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/stages/stage/kill,null&#125;&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/api,null&#125;&gt; 。。。。。。。。。。。。。。。。。。。。。。。。。&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/jobs/json,null&#125;&gt; 19/02/13 23:27:32 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&#123;/jobs,null&#125;&gt; 19/02/13 23:27:32 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.198.128:4040&gt; 19/02/13 23:27:32 INFO cluster.SparkDeploySchedulerBackend: Shutting down all executors&gt; 19/02/13 23:27:32 INFO cluster.SparkDeploySchedulerBackend: Asking each executor to shut down&gt; 19/02/13 23:27:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&gt; 19/02/13 23:27:33 INFO storage.MemoryStore: MemoryStore cleared&gt; 19/02/13 23:27:33 INFO storage.BlockManager: BlockManager stopped&gt; 19/02/13 23:27:33 INFO storage.BlockManagerMaster: BlockManagerMaster stopped&gt; 19/02/13 23:27:33 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&gt; 19/02/13 23:27:33 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.&gt; 19/02/13 23:27:33 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.&gt; 19/02/13 23:27:34 INFO spark.SparkContext: Successfully stopped SparkContext&gt; 19/02/13 23:27:34 INFO util.ShutdownHookManager: Shutdown hook called&gt; 19/02/13 23:27:34 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7c2019e-10f4-4b31-9308-5a94603de113&gt; 19/02/13 23:27:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f7c2019e-10f4-4b31-9308-5a94603de113/httpd-39b8b4b3-9b80-4247-9c7e-ed6bd2dc389f&gt; &gt; 在命令执行期间： 在三个节点敲如下命令：jps，会显示： node00： 123456&gt; [root@node00 ~]# jps&gt; 4903 Jps&gt; 2343 Master&gt; 4764 SparkSubmit #代表是提交spark的节点 (与主从无关)&gt; &gt; node01和node02： 12345&gt; [root@node01 bin]# jps&gt; 2229 Worker&gt; 5096 CoarseGrainedExecutorBackend #代表是干活的节点 （仅为从节点进程）&gt; 5167 Jps&gt; 如果提交命令的节点是从节点（node01），则在该节点上会显示执行日志、运算结果 则在提交过程中，敲命令：jps 该节点会显示 1234567&gt; [root@node01 ~]# jps&gt; 5298 CoarseGrainedExecutorBackend #代表是干活的节点 （仅为从节点进程）&gt; 2229 Worker&gt; 5323 Jps&gt; 5213 SparkSubmit #代表是提交spark的节点 (与主从无关)&gt; &gt; YARN 提交命令：基于Hadoop ： NN DN JN ZKFC ZK RM RM node00 √ √ √ √ √ √ node01 √ √ √ √ √ √ √ node02 √ √ √ √ √ 启动zookeeper ：（3台） 1zkServer.sh start 启动hdfs ：（1台） 1start-all.sh 相当于：Instead use start-dfs.sh and start-yarn.sh 启动resourcemanager ：(在RM的主节点上启动 ：1台) 1yarn-daemon.sh start resourcemanager 在任一节点的/bin路径下执行：（node01） 12345./spark-submit--master yarn #HADOOP_CONF_DIR配置使得在使用Yarn时能找到hdfs的所有配置，其中就有IP 和Port--class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 1000 显示 执行日志、计算结果会在执行提交命令的节点上显示 在命令提交过程中在三台节点上敲命令：jps 会显示 node02： [root@node02 ~]# jps3406 DataNode3491 JournalNode1681 QuorumPeerMain4133 CoarseGrainedExecutorBackend # 真正干活的进程4092 ExecutorLauncher # 启动executor3585 NodeManager3942 SparkSubmit #提交spark的进程4217 Jps 四、Standalone 模式两种提交任务方式1、Standalone-client 提交任务方式(1)命令提交 在/sbin路径下： 1./start-all.sh 提交spark 方式一： 12345./spark-submit--master spark://node00:7077--class org.apache.spark.examples.SparkPi../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 方式二： 123456./spark-submit--master spark://node1:7077--deploy-mode client--class org.apache.spark.examples.SparkPi../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 (2)执行原理图 （3）执行流程 client 模式提交任务后，会在客户端启动 Driver 进程。 Driver 会向 Master 申请启动 Application 启动的资源。 client 模式提交任务后，会在客户端启动 Driver 进程。 Driver 会向 Master 申请启动 Application 启动的资源。 （4）总结 client 模式适用于测试调试程序。 Driver 进程是在客户端启动的，这里的客户端就是指提交应用程序的当前节点。 在 Driver 端可以看到 task 执行的情况。生产环境下不能使用 client 模式， 是因为： 假设要提交 100 个 application 到集群运行，Driver 每次都会在client 端启动，那么就会导致客户端 100 次网卡流量暴增的问题。 2、Standalone-cluster 提交任务方式（1）命令提交 在/sbin路径下： 1./start-all.sh 提交spark 123456./spark-submit--master spark://node00:7077--deploy-mode cluster--class org.apache.spark.examples.SparkPi../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 注意： ​ Standalone-cluster 提交方式，应用程序使用的所有 jar 包和文件，必须保证所有的 worker 节点都要有，因为此种方式，spark 不会自动上传包。 解决方式： 将所有的依赖包和文件打到同一个包中，然后放在 hdfs 上。 将所有的依赖包和文件各放一份在 worker 节点上。 （2）执行原理图 （3）执行流程 cluster 模式提交应用程序后，会向 Master 请求启动 Driver. Master 接受请求，随机在集群一台节点启动 Driver 进程。 Driver 启动后为当前的应用程序申请资源。 Driver 端发送 task 到 worker 节点上执行。 worker 将执行情况和执行结果返回给 Driver 端。 （4）总结 Driver 进程是在集群某一台 Worker 上启动的，在客户端是无法查看 task 的执行情况的。假设要提交 100个 application 到集群运行,每次 Driver 会随机在集群中某一台 Worker 上启动，那么这 100 次网卡流量暴增的问题就散布在集群上 总结 StandaloneStandalone 两种方式提交任务，Driver 与集群的通信包括： Driver 负责应用程序资源的申请 任务的分发。 结果的回收。 监控 task 执行情况。 五、Yarn 模式两种提交任务方式1、yarn-client 提交任务方式（1）命令提交 提交spark 方式一： 12345./spark-submit--master yarn--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar100 方式二： 12345./spark-submit--master yarn–client--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar100 方式三： 123456./spark-submit--master yarn--deploy-mode client--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar100 （2）执行原理图 （3）执行流程 客户端提交一个 Application，在客户端启动一个 Driver 进程。 应用程序启动后会向 RS(ResourceManager)发送请求，启动AM(ApplicationMaster)的资源。 RS 收到请求，随机选择一台 NM(NodeManager)启动 AM。这里的 NM 相当于 Standalone 中的 Worker 节点。 AM启动后，会向RS请求一批container资源，用于启动Executor. RS 会找到一批 NM 返回给 AM,用于启动 Executor。 AM 会向 NM 发送命令启动 Executor。 Executor 启动后，会反向注册给 Driver，Driver 发送 task 到Executor,执行情况和结果返回给 Driver 端。 （4）总结 Yarn-client 模式同样是适用于测试，因为 Driver 运行在本地，Driver会与 yarn 集群中的 Executor 进行大量的通信，会造成客户机网卡流量的大量增加. ApplicationMaster 的作用： 为当前的 Application 申请资源 给 NodeManager 发送消息启动 Executor。 注意： ApplicationMaster 有 launchExecutor 和申请资源的功能，并没有作业调度的功能 2、yarn-cluster 提交任务方式（1）命令提交 提交spark 方式一： 123456./spark-submit--master yarn--deploy-mode cluster--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 方式二: 12345./spark-submit--master yarn-cluster--classorg.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar1000 （2）执行原理图 （3）执行流程 客户机提交 Application 应用程序，发送请求到RS(ResourceManager),请求启动 AM(ApplicationMaster)。 RS 收到请求后随机在一台 NM(NodeManager)上启动 AM（相当于 Driver 端）。 AM 启动，AM 发送请求到 RS，请求一批 container 用于启动Excutor。 RS 返回一批 NM 节点给 AM。 AM 连接到 NM,发送请求到 NM 启动 Excutor。 Excutor 反向注册到 AM 所在的节点的 Driver。Driver 发送 task到 Excutor。 （4）总结 Yarn-Cluster 主要用于生产环境中，因为 Driver 运行在 Yarn 集群中某一台 nodeManager 中，每次提交任务的 Driver 所在的机器都是随机的，不会产生某一台机器网卡流量激增的现象，缺点是任务提交后不能看到日志。只能通过 yarn 查看日志。 ApplicationMaster 的作用： 为当前的 Application 申请资源 给 NodeManger 发送消息启动 Excutor。 任务调度。 停止集群任务命令：yarn application -kill applicationID 总结yarn]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>算子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习]]></title>
    <url>%2F2019%2F02%2F16%2FSpark%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Spark简介1、什么是Spark？ Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。 大数据分析引擎：计算框架 Job的中间结果可以保存在内存中，所以不需要读取HDFS 能更好的的适用于数据挖掘与机器学习等需要迭代的算法 快速（100倍）:用于逻辑回归算法（机器学习）,迭代算法（在计算结果的基础上再计算）时：中间结果值在内存中流转，屏蔽磁盘开销；DAG调度 mr：离线，（迭代时：磁盘IO，较慢） storm：流式 Spark是用Scala编写的，方便快速编程 2、与MapReduce的区别 MapReduce Spark 区别： 同：分布式计算框架 不同： Spark基于内存，MR基于HDFS Spark处理数据的能力是MR的十倍以上 Spark除了基于内存计算之外，还有DAG有向无环图来切分任务的执行顺序 Spark API 的使用语言 Scala（很好）Python(不错)Java(…) 3、Spark运行模式 local 多用于本地测试，如在 eclipse，idea 中写程序测试 standalone standalone是Spark自带的资源调度框架，它支持完全分布式 yarn Hadoop生态圈的资源调度框架，Spark也是可以基于yarn来计算的 基于yarn来进行资源调度，必须实现ApplicationMaster接口，Spark实现的这个接口，所以可以使用 mesos 资源调度框架 二、Sparkcore1、RDD（1）概念：RDD(Resilient Distributed Dateset)弹性分布式数据集 （2）五大特性 RDD 是由一系列的 partition 组成的。 函数是作用在每一个 partition（split）上的。 RDD 之间有一系列的依赖关系。 分区器是作用在 K,V 格式的 RDD 上。 RDD 提供一系列最佳的计算位置。 （3）RDD理解图 理论注解 RDD 实际上不存储数据，这里方便理解，暂时理解为存储数据。 textFile 方法底层封装的是MR 读取文件的方式，读取文件之前先 split，默认 split 大小是一个 block 大小。 ​ RDD 提供计算最佳位置，体现了数据本地化。体现了大数据中“计算移动数据不移动”的理念。 ❔ 哪里体现 RDD 的分布式？ 👆 RDD 是由 Partition 组成，partition 是分布在不同节点上的。 ❔ 哪里体现 RDD 的弹性（容错）？ 👆 partition 数量，大小没有限制,默认和split（block）一致，体现了 RDD 的弹性。👆 RDD 之间依赖关系，可以基于上一个 RDD 重新计算出 RDD。 ❔ 什么是 K,V 格式的 RDD? 👆 如果 RDD 里面存储的数据都是二元组对象，那么这个 RDD 我们就叫做 K,V 格式的 RDD。 👆 MR有分区器（根据key值求hash，来决定数据存放在哪个分区中，所以分区器必须作用在K，V格式的RDD上） 2、Spark任务执行原理 Driver：（相当于ApplicationMaster） Worker：（相当于NodeManager） 以上图中有四个机器节点， Driver 和 Worker 是启动在节点上的进程，运行在 JVM 中的进程。 Driver 与集群节点之间有频繁的通信。 Driver：任务的调度（监控任务、 负责任务(tasks)的分发和结果的回收）。如果 task的计算结果非常大就不要回收了。会造成 oom。 Worker 是 Standalone 资源调度框架里面资源管理的从节点。也是JVM 进程。 Master 是 Standalone 资源调度框架里面资源管理的主节点。也是JVM 进程。 3、Spark代码流程以用Scala编写WordCount为例1、创建 SparkConf 对象 可以设置 Application name。 可以设置运行模式及资源需求。 123456789val conf = new SparkConf() /** * 几种运行方式： * 1.本地运行 * 2.yarn * 3.standalone * 4.mesos */ conf.setMaster("local").setAppName("wc") 2、创建 SparkContext 对象 1val context = new SparkContext(conf) 3、基于 Spark 的上下文创建一个 RDD，对 RDD 进行处理。 12345678910//获取文件中每一行数据的ADD val lineADD = context.textFile("./wc.txt")//获取每一行数据按空格切分后的ADD val wordADD = lineADD.flatMap(x=&gt;&#123;x.split(" ")&#125;)//获取每个单词加上,1 后的ADD（K,V格式） val KVADD = wordADD.map(x=&gt;&#123;(x,1)&#125;)//获取将相同key的value相加后的ADD（K,V格式），相当于Tuple2 val resultADD = KVADD.reduceByKey((x,y)=&gt;&#123;x+y&#125;)//降序排序 val sortADD = resultADD.sortBy(_._2,false) 4、应用程序中要有 Action 类算子来触发 Transformation 类算子执行。 1sortADD.foreach(println) 5、关闭 Spark 上下文对象 SparkContext。 1context.stop() 4、Transformations 转换算子（1）概念Transformations 类算子是一类算子（函数）叫做转换算子，如map,flatMap,reduceByKey 等。Transformations 算子是延迟执行，也叫懒加载执行。 有action触发算子任务才能提交，才会执行runjob 算子必须作用在RDD上 （2）Transformation 类算子 :arrow_up_small: filter过滤符合条件的记录数，true 保留，false 过滤掉。 🔼 contains 作为条件，是否包含，返回true|false :arrow_up_small:map将一个 RDD 中的每个数据项，通过 map 中的函数映射变为一个新的元素。特点：输入一条，输出一条数据。 :arrow_up_small: flatMap先 map 后 flat。与 map 类似，每个输入项可以映射为 0 到多个输出项。 :arrow_up_small:sample随机抽样算子，根据传进去的小数按比例进行，有放回或者无放回的抽样。 :arrow_up_small:reduceByKey将相同的 Key 根据相应的逻辑进行处理。 :arrow_up_small:sortByKey/sortBy作用在 K,V 格式的 RDD 上，对 key 进行升序或者降序排序。 1234567891011121314151617181920212223242526272829303132object WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("WC") val context = new SparkContext(conf) //用于了解集群 val linesRDD :RDD[String] = sc.textFile("./words.txt") // lineRDD.filter(x=&gt;&#123;// x.contains("sh")// &#125;).foreach(println)// lineRDD.sample(true,0.2).foreach(println) // lineRDD.map((_,1)).reduceByKey(_ + _).sortBy(_._2,false).foreach(println)// lineRDD.map((_,1)).sortByKey().foreach(println) val wordRDD :RDD[String] = linesRDD.flatMap&#123;lines =&gt; &#123; lines.split(" ") //匿名函数 &#125;&#125; val KVRDD:RDD[(String,Int)] = wordRDD.map&#123; x =&gt; (x,1) &#125; val result:RDD[(String,Int)] = KVRDD.reduceByKey&#123;(a,b)=&gt; &#123; println("a:"+a+",b:"+b) a+b &#125;&#125; 补充 parallelizePairs join ：保留公共元素 （K,V） leftOutJoin ：保留左边的元素 rightOutJoin ：保留右边元素 fullOutJoin ：去重保留 （保留最大分区数） union ：都保留 （保留总分区数） intersection ：取交集 subtract ： 取差集 distinct ： 去重 cogroup ： 分组 mapPartition ： 与map类似，转换算子，一进一出 foreachPartition 123456789101112 val conf = new SparkConf() conf.setMaster("local").setAppName("WC") val context = new SparkContext(conf) //用于了解集群 //parallelizePairs //joinOptional.absent(0)optional.isPresent()optinal.get() 5、Action 行动算子（1）概念Action 类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count 等。 Transformations 类算子是延迟执行，Action 类算子是触发执行（立即）。 一个 application 应用程序中有几个 Action 类算子执行，就有几个 job 运行。 （2）Action 类算子 :arrow_up_small: count返回数据集中的元素数。会在结果计算完成后回收到 Driver 端。:arrow_up_small: take(n)返回一个包含数据集前 n 个元素的集合。:arrow_up_small: firstfirst=take(1),返回数据集中的第一个元素 :arrow_up_small: foreach循环遍历数据集中的每个元素，运行相应的逻辑。:arrow_up_small: collect将计算结果回收到 Driver 端。 12345678910111213141516171819 val conf = new SparkConf() conf.setMaster("local").setAppName("transf") val context = new SparkContext(conf) val lineADD = context.textFile("./wc.txt") val wordADD = lineADD.flatMap(x=&gt;&#123;x.split(" ")&#125;) // println(wordADD.count())//lineADD中数据回收 val arr= lineADD.collect() arr.foreach(println)// val takes: Array[String] = lineRDD.take(5)// takes.foreach(println)// val str: String = lineRDD.first()// println(str) WordCount以用Java编写为例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.shsxt.spark.java;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.*;import scala.Tuple2;import java.util.Arrays;import java.util.List;public class WordCount &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf(); conf.setMaster("local").setAppName("wc"); JavaSparkContext context = new JavaSparkContext(conf); JavaRDD&lt;String&gt; rdd = context.textFile("./wc.txt"); long count = rdd.count(); List&lt;String&gt; collect = rdd.collect(); List&lt;String&gt; take = rdd.take(5); String first = rdd.first(); JavaRDD&lt;String&gt; wordRDD = rdd.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterable&lt;String&gt; call(String line) throws Exception &#123; String[] split = line.split(" "); List&lt;String&gt; list = Arrays.asList(split); return list; &#125; &#125;);// wordRDD.map(new Function&lt;String&gt;() &#123;// @Override// public String call(String v1) throws Exception &#123;// return null;// &#125;// &#125;) JavaPairRDD&lt;String, Integer&gt; pairRDD = wordRDD.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2(word, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; resultRDD = pairRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; reverseRDD = resultRDD.mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123; @Override public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; tuple2) throws Exception &#123; return new Tuple2&lt;&gt;(tuple2._2, tuple2._1); &#125; &#125;); JavaPairRDD&lt;Integer, String&gt; sortByKey = reverseRDD.sortByKey(false); JavaPairRDD&lt;String, Integer&gt; result = sortByKey.mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; tuple2) throws Exception &#123; return new Tuple2&lt;&gt;(tuple2._2, tuple2._1); &#125; &#125;); result.foreach(new VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void call(Tuple2&lt;String, Integer&gt; tuple2) throws Exception &#123; System.out.println(tuple2); &#125; &#125;); &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>计算引擎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List方法]]></title>
    <url>%2F2019%2F02%2F16%2FList%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137def +(elem: A): List[A]前置一个元素列表def ::(x: A): List[A]在这个列表的开头添加的元素。def :::(prefix: List[A]): List[A]增加了一个给定列表中该列表前面的元素。def ::(x: A): List[A]增加了一个元素x在列表的开头def addString(b: StringBuilder): StringBuilder追加列表的一个字符串生成器的所有元素。def addString(b: StringBuilder, sep: String): StringBuilder追加列表的使用分隔字符串一个字符串生成器的所有元素。def apply(n: Int): A选择通过其在列表中索引的元素def contains(elem: Any): Boolean测试该列表中是否包含一个给定值作为元素。def copyToArray(xs: Array[A], start: Int, len: Int): Unit列表的副本元件阵列。填充给定的数组xs与此列表中最多len个元素，在位置开始。def distinct: List[A]建立从列表中没有任何重复的元素的新列表。def drop(n: Int): List[A]返回除了第n个的所有元素。def dropRight(n: Int): List[A]返回除了最后的n个的元素def dropWhile(p: (A) =&gt; Boolean): List[A]丢弃满足谓词的元素最长前缀。def endsWith[B](that: Seq[B]): Boolean测试列表是否使用给定序列结束。def equals(that: Any): Booleanequals方法的任意序列。比较该序列到某些其他对象。def exists(p: (A) =&gt; Boolean): Boolean测试谓词是否持有一些列表的元素。def filter(p: (A) =&gt; Boolean): List[A]返回列表满足谓词的所有元素。def forall(p: (A) =&gt; Boolean): Boolean测试谓词是否持有该列表中的所有元素。def foreach(f: (A) =&gt; Unit): Unit应用一个函数f以列表的所有元素。def head: A选择列表的第一个元素def indexOf(elem: A, from: Int): Int经过或在某些起始索引查找列表中的一些值第一次出现的索引。def init: List[A]返回除了最后的所有元素def intersect(that: Seq[A]): List[A]计算列表和另一序列之间的多重集交集。def isEmpty: Boolean测试列表是否为空def iterator: Iterator[A]创建一个新的迭代器中包含的可迭代对象中的所有元素def last: A返回最后一个元素def lastIndexOf(elem: A, end: Int): Int之前或在一个给定的最终指数查找的列表中的一些值最后一次出现的索引def length: Int返回列表的长度def map[B](f: (A) =&gt; B): List[B]通过应用函数以g这个列表中的所有元素构建一个新的集合def max: A查找最大的元素def min: A查找最小元素def mkString: String显示列表的字符串中的所有元素def mkString(sep: String): String显示的列表中的字符串中使用分隔串的所有元素def reverse: List[A]返回新列表，在相反的顺序元素def sorted[B &gt;: A]: List[A]根据排序对列表进行排序def startsWith[B](that: Seq[B], offset: Int): Boolean测试该列表中是否包含给定的索引处的给定的序列def sum: A概括这个集合的元素def tail: List[A]返回除了第一的所有元素def take(n: Int): List[A]返回前n个元素def takeRight(n: Int): List[A]返回最后n个元素def toArray: Array[A]列表以一个数组变换def toBuffer[B &gt;: A]: Buffer[B]列表以一个可变缓冲器转换def toMap[T, U]: Map[T, U]此列表的映射转换def toSeq: Seq[A]列表的序列转换def toSet[B &gt;: A]: Set[B]列表到集合变换def toString(): String列表转换为字符串]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Map方法]]></title>
    <url>%2F2019%2F02%2F16%2FMap%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138Scala Map 方法下表列出了 Scala Map 常用的方法：序号 方法及描述def ++(xs: Map[(A, B)]): Map[A, B]返回一个新的 Map，新的 Map xs 组成def -(elem1: A, elem2: A, elems: A*): Map[A, B]返回一个新的 Map, 移除 key 为 elem1, elem2 或其他 elems。 def --(xs: GTO[A]): Map[A, B]返回一个新的 Map, 移除 xs 对象中对应的 keydef get(key: A): Option[B]返回指定 key 的值def iterator: Iterator[(A, B)]创建新的迭代器，并输出 key/value 对def addString(b: StringBuilder): StringBuilder将 Map 中的所有元素附加到StringBuilder，可加入分隔符def addString(b: StringBuilder, sep: String): StringBuilder将 Map 中的所有元素附加到StringBuilder，可加入分隔符 def apply(key: A): B返回指定键的值，如果不存在返回 Map 的默认方法def clone(): Map[A, B]从一个 Map 复制到另一个 Mapdef contains(key: A): Boolean如果 Map 中存在指定 key，返回 true，否则返回 false。def copyToArray(xs: Array[(A, B)]): Unit复制集合到数组def count(p: ((A, B)) =&gt; Boolean): Int计算满足指定条件的集合元素数量def default(key: A): B定义 Map 的默认值，在 key 不存在时返回。def drop(n: Int): Map[A, B]返回丢弃前n个元素新集合def dropRight(n: Int): Map[A, B]返回丢弃最后n个元素新集合def dropWhile(p: ((A, B)) =&gt; Boolean): Map[A, B]从左向右丢弃元素，直到条件p不成立def empty: Map[A, B]返回相同类型的空 Mapdef equals(that: Any): Boolean如果两个 Map 相等(key/value 均相等)，返回true，否则返回falsedef exists(p: ((A, B)) =&gt; Boolean): Boolean判断集合中指定条件的元素是否存在def filter(p: ((A, B))=&gt; Boolean): Map[A, B]返回满足指定条件的所有集合def filterKeys(p: (A) =&gt; Boolean): Map[A, B]返回符合指定条件的的不可变 Mapdef find(p: ((A, B)) =&gt; Boolean): Option[(A, B)]查找集合中满足指定条件的第一个元素def foreach(f: ((A, B)) =&gt; Unit): Unit将函数应用到集合的所有元素def init: Map[A, B]返回所有元素，除了最后一个def isEmpty: Boolean检测 Map 是否为空 def keys: Iterable[A]返回所有的key/p&gt;def last: (A, B)返回最后一个元素def max: (A, B)查找最大元素def min: (A, B)查找最小元素def mkString: String集合所有元素作为字符串显示def product: (A, B)返回集合中数字元素的积。def remove(key: A): Option[B]移除指定 keydef retain(p: (A, B) =&gt; Boolean): Map.this.type如果符合满足条件的返回 truedef size: Int返回 Map 元素的个数def sum: (A, B)返回集合中所有数字元素之和def tail: Map[A, B]返回一个集合中除了第一元素之外的其他元素def take(n: Int): Map[A, B]返回前 n 个元素def takeRight(n: Int): Map[A, B]返回后 n 个元素def takeWhile(p: ((A, B)) =&gt; Boolean): Map[A, B]返回满足指定条件的元素def toArray: Array[(A, B)]集合转数组def toBuffer[B &gt;: A]: Buffer[B]返回缓冲区，包含了 Map 的所有元素def toList: List[A]返回 List，包含了 Map 的所有元素def toSeq: Seq[A]返回 Seq，包含了 Map 的所有元素def toSet: Set[A]返回 Set，包含了 Map 的所有元素def toString(): String返回字符串对象]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组方法]]></title>
    <url>%2F2019%2F02%2F16%2F%E6%95%B0%E7%BB%84%E6%96%B9%E6%B3%95%20%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849序号 方法和描述def apply( x: T, xs: T* ): Array[T]创建指定对象 T 的数组, T 的值可以是 Unit, Double, Float, Long, Int, Char, Short, Byte, Boolean。def concat[T]( xss: Array[T]* ): Array[T]合并数组def copy( src: AnyRef, srcPos: Int, dest: AnyRef, destPos: Int, length: Int ): Unit复制一个数组到另一个数组上。相等于 Java's System.arraycopy(src, srcPos, dest, destPos, length)。def empty[T]: Array[T]返回长度为 0 的数组def iterate[T]( start: T, len: Int )( f: (T) =&gt; T ): Array[T]返回指定长度数组，每个数组元素为指定函数的返回值。以上实例数组初始值为 0，长度为 3，计算函数为a=&gt;a+1：scala&gt; Array.iterate(0,3)(a=&gt;a+1)res1: Array[Int] = Array(0, 1, 2) def fill[T]( n: Int )(elem: =&gt; T): Array[T]返回数组，长度为第一个参数指定，同时每个元素使用第二个参数进行填充。 def fill[T]( n1: Int, n2: Int )( elem: =&gt; T ): Array[Array[T]]返回二数组，长度为第一个参数指定，同时每个元素使用第二个参数进行填充。def ofDim[T]( n1: Int ): Array[T]创建指定长度的数组def ofDim[T]( n1: Int, n2: Int ): Array[Array[T]]创建二维数组def ofDim[T]( n1: Int, n2: Int, n3: Int ): Array[Array[Array[T]]]创建三维数组 def range( start: Int, end: Int, step: Int ): Array[Int]创建指定区间内的数组，step 为每个元素间的步长def range( start: Int, end: Int ): Array[Int]创建指定区间内的数组def tabulate[T]( n: Int )(f: (Int)=&gt; T): Array[T]返回指定长度数组，每个数组元素为指定函数的返回值，默认从 0 开始。以上实例返回 3 个元素：scala&gt; Array.tabulate(3)(a =&gt; a + 5)res0: Array[Int] = Array(5, 6, 7)def tabulate[T]( n1: Int, n2: Int )( f: (Int, Int ) =&gt; T): Array[Array[T]]返回指定长度的二维数组，每个数组元素为指定函数的返回值，默认从 0 开始。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set方法]]></title>
    <url>%2F2019%2F02%2F16%2FSet%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147Scala Set 常用方法下表列出了 Scala Set 常用的方法：序号 方法及描述def +(elem: A): Set[A]为集合添加新元素，x并创建一个新的集合，除非元素已存在 def -(elem: A): Set[A]移除集合中的元素，并创建一个新的集合 def contains(elem: A): Boolean如果元素在集合中存在，返回 true，否则返回 false。def &amp;(that: Set[A]): Set[A]返回两个集合的交集 def &amp;~(that: Set[A]): Set[A]返回两个集合的差集def +(elem1: A, elem2: A, elems: A*): Set[A]通过添加传入指定集合的元素创建一个新的不可变集合 def ++(elems: A): Set[A]合并两个集合def -(elem1: A, elem2: A, elems: A*): Set[A]通过移除传入指定集合的元素创建一个新的不可变集合def addString(b: StringBuilder): StringBuilder将不可变集合的所有元素添加到字符串缓冲区def addString(b: StringBuilder, sep: String): StringBuilder将不可变集合的所有元素添加到字符串缓冲区，并使用指定的分隔符def apply(elem: A)检测集合中是否包含指定元素def count(p: (A) =&gt; Boolean): Int计算满足指定条件的集合元素个数def copyToArray(xs: Array[A], start: Int, len: Int): Unit复制不可变集合元素到数组def diff(that: Set[A]): Set[A]比较两个集合的差集def drop(n: Int): Set[A]]返回丢弃前n个元素新集合 def dropRight(n: Int): Set[A]返回丢弃最后n个元素新集合def dropWhile(p: (A) =&gt; Boolean): Set[A]从左向右丢弃元素，直到条件p不成立def equals(that: Any): Booleanequals 方法可用于任意序列。用于比较系列是否相等。def exists(p: (A) =&gt; Boolean): Boolean判断不可变集合中指定条件的元素是否存在。def filter(p: (A) =&gt; Boolean): Set[A]输出符合指定条件的所有不可变集合元素。def find(p: (A) =&gt; Boolean): Option[A]查找不可变集合中满足指定条件的第一个元素 def forall(p: (A) =&gt; Boolean): Boolean查找不可变集合中满足指定条件的所有元素def foreach(f: (A) =&gt; Unit): Unit将函数应用到不可变集合的所有元素 def head: A获取不可变集合的第一个元素def init: Set[A]返回所有元素，除了最后一个def intersect(that: Set[A]): Set[A]计算两个集合的交集def isEmpty: Boolean判断集合是否为空def iterator: Iterator[A]创建一个新的迭代器来迭代元素def last: A返回最后一个元素def map[B](f: (A) =&gt; B): immutable.Set[B]通过给定的方法将所有元素重新计算def max: A查找最大元素 def min: A查找最小元素 def mkString: String集合所有元素作为字符串显示def mkString(sep: String): String使用分隔符将集合所有元素作为字符串显示def product: A返回不可变集合中数字元素的积。def size: Int返回不可变集合元素的数量def splitAt(n: Int): (Set[A], Set[A])把不可变集合拆分为两个容器，第一个由前 n 个元素组成，第二个由剩下的元素组成def subsetOf(that: Set[A]): Boolean如果集合A中含有子集B返回 true，否则返回falsedef sum: A返回不可变集合中所有数字元素之和 def tail: Set[A]返回一个不可变集合中除了第一元素之外的其他元素def take(n: Int): Set[A]返回前 n 个元素def takeRight(n: Int):Set[A]返回后 n 个元素def toArray: Array[A]将集合转换为数组def toBuffer[B &gt;: A]: Buffer[B]返回缓冲区，包含了不可变集合的所有元素def toList: List[A]返回 List，包含了不可变集合的所有元素def toMap[T, U]: Map[T, U]返回 Map，包含了不可变集合的所有元素 def toSeq: Seq[A]返回 Seq，包含了不可变集合的所有元素def toString(): String返回一个字符串，以对象来表示]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String 方法]]></title>
    <url>%2F2019%2F02%2F16%2FString%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139String 方法 char charAt(int index)返回指定位置的字符 从0开始 int compareTo(Object o)比较字符串与对象 int compareTo(String anotherString)按字典顺序比较两个字符串 int compareToIgnoreCase(String str)按字典顺序比较两个字符串，不考虑大小写 String concat(String str)将指定字符串连接到此字符串的结尾 boolean contentEquals(StringBuffer sb)将此字符串与指定的 StringBuffer 比较。 static String copyValueOf(char[] data)返回指定数组中表示该字符序列的 String static String copyValueOf(char[] data, int offset, int count)返回指定数组中表示该字符序列的 String boolean endsWith(String suffix)测试此字符串是否以指定的后缀结束 boolean equals(Object anObject)将此字符串与指定的对象比较 boolean equalsIgnoreCase(String anotherString)将此 String 与另一个 String 比较，不考虑大小写 byte getBytes()使用平台的默认字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 byte[] getBytes(String charsetName使用指定的字符集将此 String 编码为 byte 序列，并将结果存储到一个新的 byte 数组中 void getChars(int srcBegin, int srcEnd, char[] dst, int dstBegin)将字符从此字符串复制到目标字符数组 int hashCode()返回此字符串的哈希码16 int indexOf(int ch)返回指定字符在此字符串中第一次出现处的索引（输入的是ascii码值） int indexOf(int ch, int fromIndex)返返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索 int indexOf(String str)返回指定子字符串在此字符串中第一次出现处的索引 int indexOf(String str, int fromIndex)返回指定子字符串在此字符串中第一次出现处的索引，从指定的索引开始 String intern()返回字符串对象的规范化表示形式 int lastIndexOf(int ch)返回指定字符在此字符串中最后一次出现处的索引 int lastIndexOf(int ch, int fromIndex)返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索 int lastIndexOf(String str)返回指定子字符串在此字符串中最右边出现处的索引 int lastIndexOf(String str, int fromIndex)返回指定子字符串在此字符串中最后一次出现处的索引，从指定的索引开始反向搜索 int length()返回此字符串的长度 boolean matches(String regex)告知此字符串是否匹配给定的正则表达式 boolean regionMatches(boolean ignoreCase, int toffset, String other, int ooffset, int len)测试两个字符串区域是否相等28 boolean regionMatches(int toffset, String other, int ooffset, int len)测试两个字符串区域是否相等 String replace(char oldChar, char newChar)返回一个新的字符串，它是通过用 newChar 替换此字符串中出现的所有 oldChar 得到的 String replaceAll(String regex, String replacement使用给定的 replacement 替换此字符串所有匹配给定的正则表达式的子字符串 String replaceFirst(String regex, String replacement)使用给定的 replacement 替换此字符串匹配给定的正则表达式的第一个子字符串 String[] split(String regex)根据给定正则表达式的匹配拆分此字符串 String[] split(String regex, int limit)根据匹配给定的正则表达式来拆分此字符串 boolean startsWith(String prefix)测试此字符串是否以指定的前缀开始 boolean startsWith(String prefix, int toffset)测试此字符串从指定索引开始的子字符串是否以指定前缀开始。 CharSequence subSequence(int beginIndex, int endIndex)返回一个新的字符序列，它是此序列的一个子序列 String substring(int beginIndex)返回一个新的字符串，它是此字符串的一个子字符串 String substring(int beginIndex, int endIndex)返回一个新字符串，它是此字符串的一个子字符串 char[] toCharArray()将此字符串转换为一个新的字符数组 String toLowerCase()使用默认语言环境的规则将此 String 中的所有字符都转换为小写 String toLowerCase(Locale locale)使用给定 Locale 的规则将此 String 中的所有字符都转换为小写 String toString()返回此对象本身（它已经是一个字符串！） String toUpperCase()使用默认语言环境的规则将此 String 中的所有字符都转换为大写 String toUpperCase(Locale locale)使用给定 Locale 的规则将此 String 中的所有字符都转换为大写 String trim()删除指定字符串的首尾空白符 static String valueOf(primitive data type x)返回指定类型参数的字符串表示形式]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F15%2FScala%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Scala官网6个特征。简捷，快速1、Java 与Java无缝整合，运行在JVM上，编译形成.class文件 2、类型 类型自动推断:var 变量类型 val 常量类型 （var s = 1 自动推断s 为int类型 ） dos窗口运行Scala语言（cmd - &gt;scala） 3、并发 底层有actor，天生用于高并发和分布式 4、继承 trait 特征特质（Java中接口和抽象类的结合体？？？两者区别？？单继承多实现）​ 静态语言 （Java静态语言 shell 、Python动态语言） 5、匹配 模式匹配:(Java中的switch case类型必须一致)可匹配多种类型 6、高阶 高阶函数（函数式编程）函数可以作为参数传入方法中（Jdk 8 stream流，莱姆塔表达式） ​ 工具类中方法为静态的 二、Scala安装1、windows安装,配置环境变量Ø 官网下载scala2.10：（因为spark需要的是这个版本的Scala） http://www.scala-lang.org/download/2.10.4.html Ø 下载好后安装。双击msi包安装,记住安装的路径。 Ø 配置环境变量（和配置jdk一样） 新建SCALA_HOME 编辑Path变量，在后面追加如下： 1;%SCALA_HOME%\bin Ø 打开cmd,输入： 1scala - version 看是否显示版本号，确定是否安装成功 2、eclipse 配置scala插件Ø 下载插件（一定要对应eclipse版本下载）,并解压 http://scala-ide.org/download/prev-stable.html Ø 将解压目录下的features和plugins两个文件夹拷贝到eclipse安装目录中的”dropins/scala”目录下。 进入dropins，新建scala文件夹，将两个文件夹拷贝到“dropins/scala”下 3、Scala编辑器：scala ide下载网址：http://scala-ide.org/download/sdk.html 4、Idea 中配置scala插件Ø 打开idea,close项目后，点击Configure-&gt;Plugins Ø 搜索scala，点击Install安装 Ø 设置jdk，打开Project Structure,点击new 选择安装好的jdk路径 Ø 新建Scala项目 三、Scala基础语法1、数据类型 2、变量和常量的声明123456789101112 /** * 定义变量和常量 * 变量 :用 var 定义 ，可修改 * 常量 :用 val 定义，不可修改 */ var name = "zhangsan" println(name) name ="lisi" println(name) val gender = "m"// gender = "m"//错误，不能给常量再赋值// 定义变量或者常量的时候，也可以写上返回的类型，一般省略，如：val a:Int = 10 3、类和对象Ø 创建类 1234567class Person&#123; val name = "zhangsan" val age = 18 def sayName() = &#123; "my name is "+ name &#125;&#125; Ø 创建对象 1234567object Lesson_Class &#123; def main(args: Array[String]): Unit = &#123; val person = new Person() println(person.age); println(person.sayName()) &#125;&#125; Ø 伴生类和伴生对象 12345678910111213141516171819202122232425class Person(xname :String , xage :Int)&#123; var name = Person.name val age = xage var gender = "m" def this(name:String,age:Int,g:String)&#123; this(name,age) gender = g &#125; def sayName() = &#123; "my name is "+ name &#125;&#125;object Person &#123; val name = "zhangsanfeng" def main(args: Array[String]): Unit = &#123; val person = new Person("wagnwu",10,"f") println(person.age); println(person.sayName()) println(person.gender) &#125;&#125; 注意 建议类名首字母大写 ，方法首字母小写，类和方法命名建议符合驼峰命名法。 .一行结束，不需要分号。如果一行里有多个语句，则之间用分号隔开。 scala 中的object是单例对象，相当于java中的工具类，它里面的方法可以看成都是static静态的。object不可以传参数。另：Trait不可以传参数 scala中的class类默认可以传参数，默认的传参数就是默认的构造函数。 重写构造函数的时候，必须要先调用默认的构造函数。 class 类属性自带getter ，setter方法。 使用object时，不用new，使用class时要new ,并且new的时候，class中除了方法不执行，其他都执行。 如果在同一个文件中，object对象和class类的名称相同，则这个对象就是这个类的伴生对象，class称为object对象的伴生类，object 称为class类的伴生对象，他们可以直接访问对方的私有变量。 3. if else1234567891011/** * if else */val age =18 if (age &lt; 18 )&#123; println("no allow")&#125;else if (18&lt;=age&amp;&amp;age&lt;=20)&#123; println("allow with other")&#125;else&#123; println("allow self")&#125; 4. for ,while,do…while to和until 的用法（不带步长，带步长区别） 123456789101112131415 /** * to和until * 例： * 1 to 10 返回1到10的Range数组，包含10 * 1 until 10 返回1到10 Range数组 ，不包含10 */ println(1 to 10 )//打印： Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) println(1.to(10))//同上println(1 to (10 ,2))//步长为2，从1开始打印： Range(1, 3, 5, 7, 9) println(1.to(10, 2)) //同上println(1 until 10 ) //不包含最后一个数，打印1,2,3,4,5,6,7,8,9 println(1.until(10))//同上 println(1 until (10 ,3 ))//步长为2，从1开始打印，打印1,4,7 创建for循环 1234567/** * for 循环 * */ for( i &lt;- 1 to 10 )&#123; println(i) &#125; 创建多层for循环 123456789101112131415161718192021222324252627282930313233343536//可以分号隔开，写入多个list赋值的变量，构成多层for循环 //scala中 不能写count++ count-- 只能写count+ var count = 0; for(i &lt;- 1 to 10; j &lt;- 1 until 10)&#123; println("i="+ i +", j="+j) count += 1 &#125; println(count); //例子： 打印小九九 for(i &lt;- 1 until 10 ;j &lt;- 1 until 10)&#123; if(i&gt;=j)&#123; print(i +" * " + j + " = "+ i*j+" ") &#125; if(i==j )&#123; println() &#125; &#125;//九九乘法表 //方法一// for(i&lt;- 1 to 9 )&#123;// for (j &lt;- 1 to i)&#123;// print(j+"*"+i+"="+i*j+"\t")// &#125;// println()// &#125; //方法二// for(i&lt;- 1 to 9 )&#123;// for (j&lt;- 1 to 9)&#123;// if(j&lt;=i)&#123;// print(j+"*"+i+"="+i*j+"\t")// &#125;//// if(i==j)println()// &#125;// &#125; for循环中可以加条件判断，分号隔开 1234 //可以在for循环中加入条件判断 for(i&lt;- 1 to 10 ;if (i%2) == 0 ;if (i == 4) )&#123; println(i)&#125; scala中不能使用count++，count只能使用count = count+1 ，count += 1 while循环，while（）{}，do {}while() 12345678910111213141516171819 //将for中的符合条件的元素通过yield关键字返回成一个集合 val list = for(i &lt;- 1 to 10 ; if(i &gt; 5 )) yield i //&lt;- 后面是一个集合 for( w &lt;- list )&#123; println(w)&#125; /** * while 循环 */ var index = 0 while(index &lt; 100 )&#123; println("第"+index+"次while 循环") index += 1 &#125; index = 0 do&#123; index +=1 println("第"+index+"次do while 循环")&#125;while(index &lt;100 ) 四、Scala函数1、函数定义1234567def fun (a: Int , b: Int ) : Unit = &#123; println(a+b) &#125;fun(1,1) def fun1 (a : Int , b : Int)= a+b println(fun1(1,2)) 语法解释 函数定义语法 用def来定义 可以定义传入的参数，要指定传入参数的类型 scala中函数中如果返回返回值类型为Unit ，即无返回值 12345&gt; def add(x:Int=10,y:Int=11):Unit =&#123;&gt; x+y&gt; &#125;&gt; //写返回值类型是=时，一定要记得写 ：（冒号）&gt; scala中函数有返回值时，可以写return，也可以不写return： 省略return的时候，函数自动回将最后一行的表达式的值，作为返回值 123456789&gt; def max(x:Int,y:Int )=&#123;&gt; if (x&gt;y)&gt; x&gt; else&gt; y&gt; &#125;&gt; println(max(5,7))&gt; 结果显示：7&gt; * * 如果函数有retrun,则必须写返回类型。 12345678&gt; def min(m:Int,n:Int):Int=&#123;&gt; if(m&gt;n)&gt; return n&gt; else&gt; return m&gt; &#125;&gt; println(min(5,7))&gt; * scala中函数有返回值时，可以写返回值的类型，也可以省略，因为scala可以类型自动推断，有时候不能省略，必须写， * * 比如在递归函数中或者函数的返回值是函数类型的时候。 12345678&gt; def num(x: Int): Int = &#123;&gt; if (x == 1)&gt; 1&gt; else &#123;&gt; x * num(x - 1)&gt; &#125;&gt; &#125;&gt; * 函数定义的时候，如果去掉 = ，那么这个方法返回类型必定是Unit的。 这种说法无论方法体里面什么逻辑都成立，scala可以把任意类型转换为Unit。 假设，里面的逻辑最后返回了一个string，那么这个返回值会被转换成Unit，并且值会被丢弃。 则相当于，函数就将返回值去掉，即无返回值。 * {}里的代码，如果只有一行，则可以省略{} * 传递给方法的参数可以在方法中使用，并且scala规定方法的传过来的参数为val类型，不能修改，不是var。 2、递归函数1234567891011/** * 递归函数 * 5的阶乘 */ def fun2(num :Int) :Int= &#123; if(num ==1) num else num * fun2(num-1) &#125; print(fun2(5)) 3、包含参数默认值的函数12345678910/** * 包含默认参数值的函数 * 注意： * 1.默认值的函数中，如果传入的参数个数与函数定义相同，则传入的数值会覆盖默认值 * 2.如果不想覆盖默认值，传入的参数个数小于定义的函数的参数，则需要指定参数名称 */ def fun3(a :Int = 10,b:Int) = &#123; println(a+b) &#125; fun3(b=2) 4、可变参数个数的函数12345678910111213/** * 可变参数个数的函数 * 注意：多个参数逗号分开 */def fun4(elem :Int*)=&#123; var sum = 0; for(e &lt;- elem)&#123; sum += e &#125; sum&#125;println(fun4(1,2,3,4)) 5、匿名函数 有参匿名函数 无参匿名函数 有返回值的匿名函数 可以将匿名函数返回给val定义的值 匿名函数不能显式声明函数的返回类型 123456789101112131415161718//有参数匿名函数val fun1 = (a : Int ， b : Int) =&gt; &#123; println(a+b)&#125;value1(1,2)//无参数匿名函数val fun2 = ()=&gt;&#123; println("啦啦啦")&#125;fun2()//有返回值的匿名函数val fun3 = (a:Int,b:Int) =&gt;&#123; a+b&#125;println(fun3(4,4)) 6、 嵌套函数123456789101112131415/** * 嵌套函数 * 例如：嵌套函数求5的阶乘 */ def fun5(num:Int)=&#123; def fun6(a:Int,b:Int):Int=&#123; if(a == 1)&#123; b &#125;else&#123; fun6(a-1,a*b) &#125; &#125; fun6(num,1) &#125; println(fun5(5)) 7、偏应用函数123456789101112131415161718192021222324import java.util.Date /** * 偏应用函数是一种表达式 * 不需要提供函数需要的所有参数， * 只需要提供部分，或不提供所需参数。 */ def log(date :Date, s :String)= &#123; println("date is "+ date +",log is "+ s) &#125; val date = new Date() log(date ,"log1") log(date ,"log2") log(date ,"log3") //想要调用log，以上变化的是第二个参数，可以用偏应用函数处理,来优化log方法 /* 绑定第一个 date 参数， * 第二个参数使用下划线(_)替换缺失的参数列表， * 并把这个新的函数值的索引的赋给变量。 */ val logWithDate = log(date,_:String) logWithDate("log11") logWithDate("log22") logWithDate("log33") 8、高阶函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970 /** * 高阶函数:就是操作其他函数的函数 * 函数的参数是函数 * 或者函数的返回是函数 * 或者函数的参数和返回都是函数 *///函数作为参数或者返回累心时，只需指明函数中的类型 //函数的参数是函数： //f : (Int,Int) =&gt;Int （两个Int型参数、Int型返回值） def hightFun(f : (Int,Int) =&gt;Int, a:Int ) : Int = &#123; f(a,100) &#125; def f(v1 :Int,v2: Int):Int = &#123; v1+v2 &#125; println(hightFun(f, 1)) //*************** def test1(x: Int, f: (Int, Int) =&gt; Int) = &#123; var a = x + 100 a * f(1, 3) &#125; def sum(x: Int, y: Int): Int = &#123; x - y &#125; println(test6(10,sum))//------------------------------------------------ //函数的返回值类型为函数 ：(Int,Int)=&gt;Int f2 //1，2,3,4相加 def hightFun2(a : Int,b:Int) : (Int,Int)=&gt;Int = &#123; def f2 (v1: Int,v2:Int) :Int = &#123; v1+v2+a+b &#125; f2 &#125; println(hightFun2(1,2)(3,4)) //*********** def test2(x: Int): (Int) =&gt; String = &#123; def concat(y: Int): String = &#123; x + " !! " + y &#125; concat &#125; val concat: Int =&gt; String = test2(5) println(concat(7))// println(test2(5)(7)) //------------------------------------------------- //函数的参数是函数: f : (Int ,Int) =&gt; Int) //函数的返回是函数: (Int,Int) =&gt; Int def hightFun3(f : (Int ,Int) =&gt; Int) : (Int,Int) =&gt; Int = &#123; f &#125; println(hightFun3(f)(100,200)) println(hightFun3((a,b) =&gt;&#123;a+b&#125;)(200,200)) //以上这句话还可以写成这样 //如果函数的参数在方法体中只使用了一次 那么可以写成_表示 println(hightFun3(_+_)(200,200)) //********** def test3(y: Int, f: (Int) =&gt; Int): (Int) =&gt; Int = &#123; var sum = f(1) + y def sum4(x: Int) = &#123; x + sum &#125; sum4 &#125;// def f2 (x: Int) =&#123;// x + 10// &#125; val f = test3(10,(x:Int)=&gt;&#123;x + 10&#125;)// println(f(5)) 9、柯里化函数12345678910111213/** * 柯里化函数 * 可以理解为高阶函数的简化 */ def fun1(a :Int,b:Int)(c:Int,d:Int) = &#123; a+b+c+d &#125; println(fun1(1,2)(3,4)) //******* def fun(a :Int)(c:Int) = &#123; a+c &#125; println(fun(1)(3)) 五、字符串1、string | stringBuilder (可变)2、操作方法Ø 比较:equals Ø 比较忽略大小写:equalsIgnoreCase Ø indexOf：如果字符串中有传入的assci码对应的值，返回下标 12345678910111213141516171819202122232425262728293031323334353637383940 /** * String &amp;&amp; StringBuilder */ val str = "abcd" val str1 = "ABCD" println(str.indexOf(97)) println(str.indexOf("b")) println(str==str1) println(str.equals(str1)) println(str.equalsIgnoreCase(str1)) /** * compareToIgnoreCase * * 如果参数字符串等于此字符串，则返回值 0； * 如果此字符串小于字符串参数，则返回一个小于 0 的值； * 如果此字符串大于字符串参数，则返回一个大于 0 的值。 * */ println(str.compareToIgnoreCase(str1)) val strBuilder = new StringBuilder() //括号可省 strBuilder.append("abc")//一个 + 只能跟单个字符且必须用单引号，否则无效 // strBuilder.+('d') strBuilder+ 'd'// strBuilder.+=('h') strBuilder+= 'h' //两个+ 可跟多个字符且必须用双引号// strBuilder.++=("efg") strBuilder++= "efg"//StringBuilder可以追加浮点数 strBuilder.append(1.0) strBuilder.append(18f) println(strBuilder) println(strBuilder.toString()) 六、集合1、数组（1）创建一维数组1234567891011121314/** * 创建数组两种方式： * 1.new Array[String](3) * 2.直接Array */ //创建类型为Int 长度为3的数组 val arr1 = new Array[Int](3) //创建String 类型的数组，直接赋值 val arr2 = Array[String]("s100","s200","s300") //赋值 arr1(0) = 100 arr1(1) = 200 arr1(2) = 300 （2）数组遍历123456789101112131415161718/** * 遍历两种方式 * for * foreach */ for(i &lt;- arr1)&#123; println(i) &#125; arr1.foreach(i =&gt; &#123; println(i) &#125;) for(s &lt;- arr2)&#123; println(s) &#125; arr2.foreach &#123; x =&gt; println(x) &#125; （3）数组操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Array.concat：合并数组Array.fill(5)(“shsxt”)：创建初始值的定长数组def main(args: Array[String]): Unit = &#123; val a = new Array[Int](3) a(0) = 1 a(1) = 2 a(2) = 3//判断数组中符合条件的元素的个数// val n = a.count(x=&gt;&#123;// if(x&gt;2)// true// else// false// &#125;)//// println(n) val b = Array(4,5,6)//将两个数组的元素合并在一个新的数组中 val ints = Array.concat(a,b)//遍历数组 ints.foreach(println)//创建包含5个指定元素的数组 val strings = Array.fill(5)("shsxt") strings.foreach(println)//// for(i &lt;- a)&#123;// println(i)// &#125;//// val b = Array(4,5,6)//// for(i &lt;- b)&#123;// println(i)// &#125;// a.foreach(x=&gt; &#123; println(x) &#125; )// a.foreach(println(_))// a.foreach(println)// val c = new Array[Array[Int]](3)// c(0) = Array(1,2,3)// c(1) = Array(4,5,6)// c(2) = Array(7,8,9)// c.foreach(x=&gt;&#123;// x.foreach(y=&gt;&#123;// print(y + "\t")// &#125;)// println()// &#125;)// for(i&lt;-c)&#123;// for(j &lt;-i)&#123;// print(j + "\t")// &#125;// println()// &#125; &#125; （4）创建二维数组123456789101112131415161718192021222324252627282930313233343536/** * 创建二维数组和遍历 */ val arr3 = new Array[Array[String]](3) arr3(0)=Array("1","2","3") arr3(1)=Array("4","5","6") arr3(2)=Array("7","8","9") for(i &lt;- 0 until arr3.length)&#123; for(j &lt;- 0 until arr3(i).length)&#123; print(arr3(i)(j)+" ") &#125; println() &#125; var count = 0 for(arr &lt;- arr3 ;i &lt;- arr)&#123; if(count%3 == 0)&#123; println() &#125; print(i+" ") count +=1 &#125; arr3.foreach &#123; arr =&gt; &#123; arr.foreach &#123; println &#125; &#125;&#125; val arr4 = Array[Array[Int]](Array(1,2,3),Array(4,5,6)) arr4.foreach &#123; arr =&gt; &#123; arr.foreach(i =&gt; &#123; println(i) &#125;) &#125;&#125; println("-------") for(arr &lt;- arr4;i &lt;- arr)&#123; println(i) 2、list（1）创建list1val list = List(1,2,3,4) 备注 Nil 长度为0的list （2）list遍历 foreach ，for （3）list操作12345678910111213141516171819202122232425262728 //创建 val list = List(1,2,3,4,5) //遍历 list.foreach &#123; x =&gt; println(x)&#125;// list.foreach &#123; println&#125; //filter 过滤元素 val list1 = list.filter &#123; x =&gt; x&gt;3 &#125; list1.foreach &#123; println&#125; //count 计算符合条件的元素个数 val value = list1.count &#123; x =&gt; x&gt;3 &#125; println(value) //map 对元素操作 如切分 val nameList = List( "hello shsxt", "hello xasxt", "hello shsxt" ) val mapResult:List[Array[String]] = nameList.map&#123; x =&gt; x.split(" ") &#125; mapResult.foreach&#123;println&#125; //flatmap 将元素压扁平,先map再flat val flatMapResult : List[String] = nameList.flatMap&#123; x =&gt; x.split(" ") &#125; flatMapResult.foreach &#123; println &#125; 3、set注意：set集合自动去重 1234567891011121314151617181920212223242526272829303132val s = Set(1,2,3,3)val s1 = Set(1，2，3，4)//遍历时，自动去重s.foreach(println) for(x&lt;- s)&#123;println(x)&#125;//交集val inse = s.intersect(s1)val ins = s.&amp;(s1)inse.foreach(println)inse.foreach&#123;println&#125;//差集val di = s.diff(s1)val de = s.&amp;~(s1)//子集:s是不是s1的子集val boo: Boolean = s.subsetof(s1)//max ,minval max = s.maxval min = s.minprintln(max + ":" + min)//转成数组，lists.toArray.foreach(println)s.toList.foreach(println)//mkString , 元素以！分隔拼成字符串println(s.mkString)println(s.mkstring("!")) 4、map1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//map创建时，若key相同，则都会被后一个key覆盖val map =Map（"1"-&gt;"a",("3","c"),"2"-&gt;"b"）//获取map值println(map.get("1"))println(map.get("1").get)//获取指定key，若没有，就用另一个指定值代替println(map.getOrElse("5","100"))val res = map.get("8").getOrElse("800")println(res)//遍历map//x为map中的一组键值对for(x-&gt; map)&#123;println("key:"+x._1+",value:"+ x._2) &#125;map.foreach(x=&gt;&#123; println("key:"+x._1+",value:"+ x._2) &#125;)//遍历keyval keyIteratable = map.keyskeyIteratable.foreach(k=&gt;&#123;println("key"+k+",value"+map.get(k).get)&#125;)//遍历valueval valueIteratable = map.valuesvalueIteratable.foreach&#123;v=&gt;&#123; println("value"+v)&#125;&#125;//合并mapval map1 = Map（(1，"a"),(2,"b"),(3,"c")）val map2 = Map（(1,"aa"),(2,"bb"),(3,"cc")）map1.++(map2).foreach(println)map1.++:(map2).oreach(println)//map操作方法//filter:过滤，留下符合条件的元素val result = map.filter(x=&gt;&#123;if(x._1.equals("1")) trueelse false&#125;)result.foreach(println)map.filter(_._2.equals("a")).foreach(println)//count：统计符合条件的元素个数val countResult = map.count（x=&gt;&#123; x._1.equals("1")）println(countResult) //contains：判断map中是否包含某个keymap.contains("3")//exist：判断符合条件的元素是存在//若遇到条件返回结果为true，就停止迭代map.exists(x=&gt;&#123;if（x._1.equals("3"))&#123;println("存在喽")&#125;elseprintln("不存在喽")&#125;) 5、元组Tuple同：与列表类似 不同：元组可以包含不同类型的元素；元组的值是通过将单个值包含在圆括号中构成 注意：Tuple最多支持22个参数 1234567891011121314151617181920212223//元素个数取决于Tuple后面的数字val t1 = new Tuple1(1)val t2 = new Tuple2(1,2)val t3 = Tuple3(1,2,3) val t5 = (1,2,3，4,5)val t22 = new Tuple22(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22)//获取元组中的值 ：用 ._XX println（t5._4）val t = Tuple2((1,2),("zhangsan","lisi"))println(t._1._2)//元组遍历：元素迭代器val interator = t5.productorInteratorinterator.foreach(println)while(interator.hasNext)&#123; println(intrator.next())&#125;//元素翻转 swap， 只针对二元数组println(t2.swap)//转换成字符串println(t5.toString()) 七、traitScala：只有extends ，可以继承多个Trait Scala Trait(特征) 相当于 Java 的接口和抽象类的结合 可以定义属性和方法 注意： 若继承的多个trait中包含同名的属性和方法，就必须在在类中使用override 重新定义 若重新定义的属性是使用var定义的则会报错，所以必须是使用val定义的属性才可使用override trait中不能传参数 举例：trait中带属性带方法实现 1234567891011121314151617181920212223242526272829303132trait Read&#123; var name = "read" val age = 18 def read():Unit=&#123; println("read---") &#125;&#125;trait Write&#123; var name = "write" val age = 19 def write():Unit=&#123; println("write----") &#125;&#125;class Human extends Read with Write&#123; //前提name必须是用val定义的 // override var name = "person" override val age = 20 name = "person"&#125;object Trait&#123; def main(args:Array[String]):Unit = &#123; val human = new Human() println(human.name) human.read() human.write() &#125;&#125; 举例：trait中带方法不实现 12345678910111213141516171819202122232425262728293031trait Equal&#123; //抽象方法 def isEqual(p:Point):Boolean def isNoEqual(p:Point):Boolean=&#123; !isEqual(p) &#125;&#125;class Point(x:Int,y:Int) extends Equal &#123; var xx = x var yy = y override def isEqual (p:Point) = &#123; p.xx == xx &amp; p.yy ==yy &#125; def isEqule(p:Any) = &#123; p.isInstanceOf[Point] &amp;&amp; p.asInstanceOf[Point].xx==xx &#125;&#125;object Trait2&#123; def main(args:Array[String]):Unit = &#123; val p1 = new Point(3,5) val p2 = new Point(4,5) ptintln(p1.isNoEqual(p2)) ptintln(p1.isEqual(p2)) &#125;&#125; 八、模式匹配match1、概念理解 一个模式匹配包含多个备选项，且每个都以关键字case开始 每个备选项都包含一个模式和多个表达式，且用箭头符号 =&gt; 隔开了模式和表达式。 Ø 模式匹配不仅可以匹配值还可以匹配类型 Ø 从上到下顺序匹配，如果匹配到则不再往下匹配 Ø 都匹配不上时，会匹配到case_ ,相当于default Ø match 的最外面的”{ }”可以去掉看成一个语句 123456789101112131415161718192021object Match&#123; def main(args:Array[String]):Unit = &#123; val p1 =(1,2,3f,"4","abc",55d) p1.foreach(x=&gt;&#123; println(mymatch(x)) &#125;) &#125; def mymatch(x:Any)=&#123; x match &#123; case "4" =&gt;println("macth 4--") case x:String =&gt; println("match String") case 1 =&gt; println("1--") case 2 =&gt; println("2--") case 3f =&gt; println("3f--") // case x :Double =&gt; println("type is Double") case _ =&gt; println("no match--") &#125; &#125;&#125; 九、样例类1、概念理解 使用了case关键字的类定义就是样例类(case classes) 样例类是种特殊的类。实现了类构造参数的getter方法（构造参数默认被声明为val），当构造参数是声明为var类型的，它将帮你实现setter和getter方法。 Ø 样例类默认帮你实现了toString,equals，copy和hashCode等方法。 Ø 样例类可以new, 也可以不用new 2、举例：结合模式匹配的代码 1234567891011121314151617181920212223object Match2&#123; def main(args:Array[String]):Unit = &#123; val zs = new Woman(18,"zs") println(zs.age + " : "+ zs . name) val mm = Woman(19,"meimei") val nn = Woman(19,"meimei") println(mm.equals(zs)) println(mm.equals(nn)) val wo = List(ls,mm) wo.foreach(x=&gt;&#123; x match &#123; case x:Woman =&gt; println("match Woman--") case Woman(18,"ls") =&gt; println("ls") case Woman(19,"mm") =&gt; println("mm") case Woman(19,"zs") =&gt; println("zs") &#125; &#125;) &#125;&#125;case class Woman (age:Int ,name:String) 十、Actor Model\1. 概念理解 Actor Model是用来编写并行计算或分布式系统的高层次抽象（类似java中的Thread）让程序员不必为多线程模式下共享锁而烦恼,被用在Erlang 语言上, 高可用性99.9999999 % 一年只有31ms 宕机Actors将状态和行为封装在一个轻量的进程/线程中，但是不和其他Actors分享状态，每个Actors有自己的世界观，当需要和其他Actors交互时，通过发送事件和消息，发送是异步的，非堵塞的(fire-andforget)，发送消息后不必等另外Actors回复，也不必暂停，每个Actors有自己的消息队列，进来的消息按先来后到排列，这就有很好的并发策略和可伸缩性，可以建立性能很好的事件驱动系统。 Actor的特征： Ø ActorModel是消息传递模型,基本特征就是消息传递 Ø 消息发送是异步的，非阻塞的 Ø 消息一旦发送成功，不能修改 Ø Actor之间传递时，自己决定决定去检查消息，而不是一直等待，是异步非阻塞的 什么是Akka Akka 是一个用 Scala 编写的库，用于简化编写容错的、高可伸缩性的 Java 和Scala 的 Actor 模型应用，底层实现就是Actor,Akka是一个开发库和运行环境，可以用于构建高并发、分布式、可容错、事件驱动的基于JVM的应用。使构建高并发的分布式应用更加容易。 spark1.6版本之前，spark分布式节点之间的消息传递使用的就是Akka，底层也就是actor实现的。1.6之后使用的netty传输。 12345678910111213141516171819202122class MyActor extends Actor&#123;override def act():Unit=&#123; while(true)&#123; receive &#123; case "hello"=&gt;println("hello") case x:String =&gt; println("save String ="+ x) case x:Int =&gt; println("save Int") case _ =&gt; println("save default") &#125; &#125;&#125;object Actor&#123; def main(args:Array[String]):Unit = &#123; val actor = new MyActor() //启动 actor.start() //发送消息 actor ! "hello" &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445case class Msg(actor)()class WoActor() extends Actor&#123; oevrride def act():Unit =&#123; while(true)&#123; receive &#123; case msg:Msg =&gt;&#123; msg.mes //收到的消息 msg.actor //回复的消息 println("too") msg.actor ! "hahaha" &#125; &#125; &#125; &#125;&#125;class HuActor(wo:WoActor) extends Actor&#123; oevrride def act():Unit =&#123; while(true)&#123; receive &#123; case "hello" =&gt;&#123; println("too") val msg = Msg(this,"rrr") wo ! msg &#125; case "rrr" =&gt;&#123; println("uuu") val msg = Msg(this,"heheh") wo ! msg &#125; &#125; &#125; &#125;&#125;object Actor2&#123; def main(args:Array[String]):Unit = &#123; val woman = new WoActor() val human = new HuActor(woman) human.start() woman.start() human ! "hello" &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243case class Message(actor:Actor,msg:Any)class Actor1 extends Actor&#123; def act()&#123; while(true)&#123; receive&#123; case msg :Message =&gt; &#123; println("i sava msg! = "+ msg.msg) msg.actor!"i love you too !" &#125; case msg :String =&gt; println(msg) case _ =&gt; println("default msg!") &#125; &#125; &#125;&#125;class Actor2(actor :Actor) extends Actor&#123; actor ! Message(this,"i love you !") def act()&#123; while(true)&#123; receive&#123; case msg :String =&gt; &#123; if(msg.equals("i love you too !"))&#123; println(msg) actor! "could we have a date !" &#125; &#125; case _ =&gt; println("default msg!") &#125; &#125; &#125;&#125;object Lesson_Actor2 &#123; def main(args: Array[String]): Unit = &#123; val actor1 = new Actor1() actor1.start() val actor2 = new Actor2(actor1) actor2.start() &#125;&#125; 十一、WordCount1234567891011121314151617181920212223242526272829303132333435363738//在lib文件中添加spark的jar包 ，并addLibimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport org.apache.spark.rdd.RDDimport org.apache.spark.rdd.RDD.rddToPairRDDFunctionsobject WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("local").setAppName("WC") val sc = new SparkContext(conf) //用于了解集群 //RDD:集合 ，抽象的 val lines :RDD[String] = sc.textFile("./words.txt") //方式一： val word :RDD[String] = lines.flatMap&#123;lines =&gt; &#123; lines.split(" ") //匿名函数 &#125;&#125; val pairs:RDD[(String,Int)] = word.map&#123; x =&gt; (x,1) &#125; //map :一进一出 flatmap：一进多出 x：每个单词 val result:RDD[(String,Int)] = pairs.reduceByKey&#123;(a,b)=&gt; &#123; println("a:"+a+",b:"+b) a+b //相当于 a = a + b用于下一个统计 &#125;&#125; //优化：排序 false（降序） 第一个_ 代表每个元素 result.sortBy(_._2,false).foreach(println) //方式二：简化写法 lines.flatMap &#123; _.split(" ")&#125;.map &#123; (_,1)&#125;.reduceByKey(_+_).foreach(println) &#125;&#125;]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>静态</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习]]></title>
    <url>%2F2019%2F02%2F14%2FRedis%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Redis的简介1、前情提要磁盘数据库： MySQL（关系型数据库） Hbase 内存数据库： Redis memcached （成本高、性能好、读写速度快、数据安全性低、直接把值放到内存里面，内存数据库就直接把值取到） 2、用作数据库、缓存和消息中间件 二八法则：80%是经常被查询，使用内存数据库做缓存中间件，读取性能高 二、Redis的特点1、数据结构丰富Redis虽然是键值对数据库，但他支持多种类型的数据结构（主要是不同类型的value） 字符串、散列（hashes），列表（lists），集合（sets），有序集合（sorted sets） 2、数据的持久化Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载到内存进行使用 3、数据的备份Redis 支持数据的备份，即 master-slave 模式的数据备份。 三、Redis的安装1、下载安装包redis-3.2.9.tar.gz 网址：http://www.redis.cn/download.html 2、依赖软件安装1yum install gcc tcl -y 3、解压 redis并进入解压目录 1tar -zvxf redis-3.2.9.tar.gz 4、 执行 编译命令（注意：当前路径下需包含 makefile文件 ： 用于手动编译） 1make &amp;&amp; make install 5、修改 redis 的配置文件 redis.config ( 先 备份一个原厂配置文件) 1vim redis.config 修改运行模式为后台运行（如果为no，启动redis-server后，控制台就会卡在启动状态）daemonize守护进程 daemonize yes 指定redis数据持久化的路径(在执行redis-cli命令的当前路径，会生成dump.rdb文件) dir ./ 使用默认的 redis.conf 文件，redis 默认只能通过 127.0.0.1:6379 这个地址访问，这样就只能在本机上操作了 想要远程操作，需要修改redis.conf 这个配置文件，在配置文件中添加相应的 ip 地址， 在bind 127.0.0.1 后面追加 bind 127.0.0.1 192.168.198.128 6、启动server服务（如在redis的解压目录下） 命令： redis-server 配置文件的地址 1redis-server redis-conf 终止服务：（查询redis进程号，然后，kill该进程） 12ps -ef | grep rediskill 进程号 7、启动客户端服务1redis-cli 显示： 127.0.0.1:6379&gt; 四、Redis的使用0、前情提要Redis的key 值是二进制安全的，这意味着可以用任何二进制序列作为 key值。 1、切换数据库（实例） select databaseid 默认共有 16 个实例库， 登录时是 ID 为0 的数据库，总共有 16 个 1select 0 2、Key操作：（前提：是针对已经存在key） KEYS pattern查找所有符合给定模式 pattern 的 key 。 12&gt; keys * &gt; 列出所有Key EXISTS key检查给定 key 是否存在。 12&gt; EXISTS name&gt; 若显示0，则不存在；若显示1，则存在 EXPIRE key seconds为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 12&gt; EXPIRE age 1&gt; 若设置成功，显示1；显示0 ，则为失败，可能是该key不存在 TTL key以秒为单位，返回给定 key 的剩余生存时间 MOVE key db 将当前数据库的 key 移动到给定的数据库 db 当中。如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果 12&gt; move name 1&gt; name 是数据库0中已存在的key，移动到数据库1中后，0中就不存在该key TYPE key 返回 key 所储存的值的类型。 12&gt; DEL key [key ...]&gt; 删除给定的一个或多个 key 。不存在的 key 会被忽略 3、String 操作字符串是一种最基本的 Redis 值类型。Redis 字符串是二进制安全的，这意味着一个 Redis 字符串能包含任意类型的数据 12&gt; SET key value [EX seconds][PX milliseconds] [NX|XX]&gt;  EX 设置过期时间，秒，等同于 SETEX key seconds value PX 设置过期时间，毫秒，等同于 PSETEX key milliseconds value NX 键不存在，才能设置，等同于 SETNX key value XX 键存在时，才能设置 注意 将字符串值 value 关联到 key 。如果 key 已经持有其他值， SET 就覆写旧值，无视类型。对于某个原本带有生存时间（TTL）的键来说， 当 SET 命令成功在这个键上执行时， 这个键原有的 TTL 将被清除。 12&gt; GET key&gt; 返回 key 所关联的字符串值。如果 key 不存在那么返回特殊值 nil 。假如 key 储存的值不是字符串类型，返回一个错误，因为 GET 只能用于处理字符串值 12&gt; APPEND key value&gt; 如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾。如果 key 不存在， APPEND 就简单地将给定 key 设为 value ，就像执行 SET key value 一样。 12&gt; STRLEN key&gt; 返回 key 所储存的字符串值的长度。当 key 储存的不是字符串值时，返回一个错误。 12&gt; INCR key&gt; 将 key 中储存的数字值增一，并显示。如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCR 操作。如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。 错误： (error) ERR value is not an integer or out of range 12&gt; INCRBY key increment&gt; 将 key 所储存的值加上增量 increment ，并显示。如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行INCRBY 命令。如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。 12&gt; DECR key&gt; 将 key 中储存的数字值减一。 12&gt; DECRBY key decrement&gt; 将 key 所储存的值减去减量 decrement 。 12&gt; GETRANGE key start end&gt; 返回 key 中字符串值的子字符串，字符串的截取范围由 start 和end 两个偏移量决定(包括 start 和 end 在内)。负数偏移量表示从字符串最后开始计数， -1 表示最后一个字符， -2 表示倒数第二个，以此类推 12&gt; SETRANGE key offset value&gt; 用 value 参数覆写(overwrite)给定 key 所储存的字符串值，从偏移量 offset 开始。不存在的 key 当作空白字符串处理。 12&gt; SETEX key seconds value&gt; 将值 value 关联到 key ，并将 key 的生存时间设为 seconds如果 key 已经存在， SETEX 命令将覆写旧值。这个命令类似于以下两个命令： 123&gt; SET key value&gt; EXPIRE key seconds # 设置生存时间&gt; 不同之处是， SETEX 是一个原子性(atomic)操作，关联值和设置生存时间两个动作会在同一时间内完成，该命令在 Redis 用作缓存时，非常实用。 12&gt; SETNX key value&gt; 将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 12&gt; MGET key [key ...]&gt; 返回所有(一个或多个)给定 key 的值。如果给定的 key 里面，有某个 key 不存在，那么这个 key 返回特殊值 nil 。因此，该命令永不失败 12&gt; MSET key value [key value ...]&gt; 同时设置一个或多个 key-value 对。如果某个给定 key 已经存在，那么 MSET 会用新值覆盖原来的旧值，如果这不是你所希望的效果，请考虑使用 MSETNX 命令：它只会在所有给定 key 都不存在的情况下进行设置操作。 12&gt; MSETNX key value [key value ...]&gt; 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。即使只有一个给定 key 已存在， MSETNX 也会拒绝执行所有给定 key 的设置操作。MSETNX 是原子性的，因此它可以用作设置多个不同 key 表示不同字段(field)的唯一性逻辑对象(unique logic object)，所有字段要么全被设置，要么全不被设置。 4、 List 操作 12&gt; LPUSH key value [value ...]&gt; 将一个或多个值 value 插入到列表 key 的表头。（从左边插入）如果有多个 value 值，那么各个 value 值按从左到右的顺序依次 插 入 到 表 头 ： 比 如 说 ， 对 空 列 表 mylist 执 行 命令 LPUSH mylist a b c ，列表的值将是 c b a ， 这等同于原 子 性 地 执行 LPUSH mylist a 、 LPUSH mylist b 和 LPUSH mylistc 三个命令。 12&gt; RPUSH key value [value ...]&gt; 将一个或多个值 value 插入到列表 key 的表尾。（从右边插入）如果有多个 value 值，那么各个 value 值按从左到右的顺序依次 插 入 到 表 尾 ： 比 如 对 一 个 空 列 表 mylist 执行 RPUSH mylist a b c ，得出的结果列表为 a b c ， 等同于 执 行 命令 RPUSH mylist a 、 RPUSH mylist b 、 RPUSH mylist c 。 12&gt; LRANGE key start stop&gt; 返 回 列 表 key 中 指 定 区 间 内 的 元 素 ， 区 间 以 偏 移量 start 和 stop 指定。（从左至右）下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。-1 表示最后一个元素 12&gt; LPOP key&gt; 移除并返回列表 key 的头元素。（从列表key的左边开始弹出元素） 12&gt; RPOP key&gt; 移除并返回列表 key 的尾元素。（从列表key的右边开始弹出元素） 备注： 同向为栈，异向为队列栈（lpush lpop ； rpush rpop） 队列（lpush rpop ； rpush lpop ） ArrayList : 数组（查找快，增删慢）LinkList : 链表（查找慢，增删快） 12&gt; LINDEX key index&gt; 返回列表 key 中，下标为 index 的元素。下标(index)参数 start 和 stop 都以 0 为底，也就是说，以 0 表示列表的第一个元素，以 1 表示列表的第二个元素，以此类推。你也可以使用负数下标，以 -1 表示列表的最后一个元素， -2 表示列表的倒数第二个元素，以此类推 12&gt; LLEN key&gt; 返回列表 key 的长度。如果 key 不存在，则 key 被解释为一个空列表，返回 0 . 12&gt; LREM key count value&gt; 根据参数 count 的值，移除列表中与参数 value 相等的元素。count 的值可以是以下几种：count &gt; 0 : 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count 。count &lt; 0 : 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。count = 0 : 移除表中所有与 value 相等的值。 12&gt; LTRIM key start stop&gt; 对一个列表进行修剪(trim)，就是说，让列表只保留指定区间内的元素， 不在指定区间之内的元素都将被删除。举 个 例 子 ， 执 行 命 令 LTRIM list 0 2 ， 表 示 只 保 留 列表 list 的前三个元素，其余元素全部删除 12&gt; RPOPLPUSH source destination&gt; 命令 RPOPLPUSH 在一个原子时间内，执行以下两个动作：将列表 source 中的最后一个元素(尾元素)弹出，并返回给客户端。将 source 弹 出 的 元 素 插 入 到 列 表 destination ， 作为 destination 列表的的头元素。举 个 例 子 ， 你 有 两 个 列表 source 和 destination ， source 列 表 有 元素 a, b, c ， destination 列表有元素 x, y, z ，执行 RPOPLPUSH sourcedestination 之后， source 列表包含元素 a, b ， destination 列表包含元素 c, x, y, z ，并且元素 c 会被返回给客户端。 12&gt; LSET key index value&gt; 将列表 key 下标为 index 的元素的值设置为 value 。当 index 参数超出范围，或对一个空列表( key 不存在)进行 LSET 时，返回一个错误。 12&gt; LINSERT key BEFORE|AFTER pivot value&gt; 将值 value 插入到列表 key 当中，位于值 pivot 之前或之后。当 pivot 不存在于列表 key 时，不执行任何操作。当 key 不存在时， key 被视为空列表，不执行任何操作。 5、Set操作 12&gt; SADD key member [member ...]&gt; 将一个或多个 member 元素加入到集合 key 当中（无序），已经存在于集合的 member 元素将被忽略。假如 key 不存在，则创建一个只包含 member 元素作成员的集合。 12&gt; SMEMBERS key&gt; 返回集合 key 中的所有成员。 不存在的 key 被视为空集合。 12&gt; SISMEMBER key member&gt; 判断 member 元素是否是集合 key 的成员。 12&gt; SCARD key&gt; 返回集合 key 的基数(集合中元素的数量)。 12&gt; SREM key member [member ...]&gt; 移 除 集 合 key 中 的 一 个 或 多 个 member 元 素 ， 不 存 在的 member 元素会被忽略。 12&gt; SPOP key （抽奖场景）&gt; 移除并返回集合中的一个随机元素。如果只想获取一个随机元素，但不想该元素从集合中被移除的话，可以使用 SRANDMEMBER 命令。 12&gt; SMOVE source destination member&gt; 将 member 元素从 source 集合移动到 destination 集合。SMOVE 是原子性操作。如果 source 集合不存在或不包含指定的 member 元素，则 SMOVE 命令不执行任何操作，仅返回 0 。否则， member 元素从 source 集合中被移除，并添加到 destination 集合中去。当 destination 集合已经包含 member 元素时， SMOVE 命令只是简单地将 source 集合中的 member 元素删除。当 source 或 destination 不是集合类型时，返回一个错误。 12&gt; SDIFF key [key ...]&gt; 求差集：从第一个 key 的集合中去除其他集合和自己的交集部分 sdiff求两个set的差集，有先后顺序，以靠前的为基准，列出前一个set有的，而后一个set没有的元素 12&gt; SINTER key [key ...] （微博求共同关注场景）&gt; 返回一个集合的全部成员，该集合是所有给定集合的交集。不存在的 key 被视为空集。当给定集合当中有一个空集时，结果也为空集(根据集合运算定律)。 12&gt; SUNION key [key ...]&gt; 返回一个集合的全部成员，该集合是所有给定集合的并集。不存在的 key 被视为空集。 6、Sorted set操作类似 Sets,但是每个字符串元素都关联到一个叫 score 浮动数值。里面的元素总是通过 score 进行着排序，所以不同的是，它是可以检索的一系列元素。 注意： 在 set 基础上，加上 score 值， 之前 set 是key value1 value2….现在 Zset 是 key score1 value1 score2 value2 12&gt; ZADD key score member [[score member][score member] ...]&gt; 将 一 个 或 多 个 member 元 素 及 其 score 值 加 入 到 有 序集 key 当中。 显示 redis&gt; ZADD page_rank 9 baidu.com 8 bing.com 10 google.com(integer) 2redis&gt; ZRANGE page_rank 0 -1 WITHSCORES 1) “bing.com”2) “8”3) “baidu.com”4) “9”5) “google.com”6) “10” 12&gt; ZRANGE key start stop [WITHSCORES]&gt; 返回有序集 key 中，指定区间内的成员。其中成员的位置按 score 值递增(从小到大)来排序。具有相同 score 值的成员按字典序(lexicographical order )来排列。如果你需要成员按 score 值递减(从大到小)来排列，请使用 ZREVRANGE 命令。下标参数 start 和 stop 都以 0 为底， 也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。超出范围的下标并不会引起错误。比 如 说 ， 当 start 的 值 比 有 序 集 的 最 大 下 标 还 要 大 ， 或是 start &gt; stop 时， ZRANGE 命令只是简单地返回一个空列表。另一方面，假如 stop 参数的值比有序集的最大下标还要大，那么Redis 将 stop 当作最大下标来处理。可以通过使用 WITHSCORES 选项，来让成员和它的 score 值一并返回， 返回列表以 value1,score1, …, valueN,scoreN 的格式表示。 12&gt; ZREVRANGE key start stop [WITHSCORES]( ( 音乐排行榜场景) )&gt; 返回有序集 key 中，指定区间内的成员。其中成员的位置按 score 值递减(从大到小)来排列。 12&gt; ZREM key member [member ...]&gt; 移除有序集 key 中的一个或多个成员，不存在的成员将被忽略。 12&gt; ZREMRANGEBYSCORE key min max&gt; 移除有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。 12&gt; ZSCORE key member&gt; 返回有序集 key 中，成员 member 的 score 值。 12&gt; ZCARD key&gt; 返回有序集 key 的基数（包含的元素的个数）。 12&gt; ZCOUNT key min max&gt; 返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。 12&gt; ZRANK key member&gt; 返回有序集 key 中，成员 member 的 score 值。ZCARD key返回有序集 key 的基数（包含的元素的个数）。ZCOUNT key min max返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量。ZRANK key member 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。 7、 Hash 操作 （散列）KV 模式不变，但是 V 是一个键值对 12&gt; HSET key field value&gt; 将哈希表 key 中的域 field 的值设为 value 。 12&gt; HGET key field&gt; 返回哈希表 key 中给定域 field 的值。 12&gt; HMSET key field value [field value ...]&gt; 同时将多个 field-value (域-值)对设置到哈希表 key 中。此命令会覆盖哈希表中已存在的域。 12&gt; HMGET key field [field ...]&gt; 返回哈希表 key 中，一个或多个给定域的值。 12&gt; HGETALL key&gt; 返回哈希表 key 中，所有的域和值。 12&gt; HKEYS key&gt; 返回哈希表 key 中的所有域。 12&gt; HVALS key&gt; 返回哈希表 key 中所有域的值。 12&gt; HSETNX key field value&gt; 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在。若域 field 已经存在，该操作无效。 12&gt; HEXISTS key field&gt; 查看哈希表 key 中，给定域 field 是否存在。 12&gt; HDEL key field [field ...]&gt; 删除哈希表 key 中的一个或多个指定域，不存在的域将被忽略。 12&gt; HINCRBY key field increment&gt; 为哈希表 key 中的域 field 的值加上增量 increment 。增量也可以为负数，相当于对给定域进行减法操作。 12&gt; HINCRBYFLOAT key field increment&gt; 增加浮点数场景：用户维度统计统计数包括：关注数、粉丝数、喜欢商品数、发帖数用户为 Key，不同维度为 Field，Value 为统计数 五、Redis 的持久化1、Redis 持久化方式 RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。 2、Rdb:（1）在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的 snapshot 快照，它恢复时就是将快照文件直接读到内存里。 Rdb 保存的是 dump.rdb 文件 （2）如何触发 RDB 快照 Save：save 时只管保存，其他不管，全部阻塞。Bgsave：redis 会在后台进行快照操作，快照操作的同时还可以响应客户端的请求，可以通过 lastsave 命令获取最后一次成功执行快照的时间。 （3）如何停止 静态停止：将配置文件里的 RDB 保存规则改为 save “”动态停止 ： config set save “ ” 3、AOF(Append Only File) （1）以日志的形式来记录每个写操作，将 redis 执行过的所有写指令记录下来(读操作不记录)。只许追加文件但不可以改写文件，redis 启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次一完成数据恢复工作。======APPEND ONLY MODE======开启 aof ：appendonly yes (默认是 no) （2）Aof 策略 Appendfsync 参数： Always ：同步持久化 每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性较好。Everysec： 出厂默认推荐，异步操作，每秒记录，如果一秒宕机，有数据丢失No：从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。 （3）Rewrite 概念：AOF 采用文件追加方式，文件会越来越来大为避免出现此种情况，新增了重写机制，aof 文件的大小超过所设定的阈值时，redis 就会自动将 aof文件的内容压缩，只保留可以恢复数据的最小指令集，可以使用命令bgrewirteaof。 触发机制：redis 会记录上次重写的 aof 的大小，默认的配置当 aof 文件大小为上次 rewrite 后大小的一倍且文件大于 64M 触发。 重写原理：aof 文件持续增长而大时，会 fork 出一条新进程来将文件重写(也就是先写临时文件最后再 rename)，遍历新进程的内存中的数据，每条记录有一条 set 语句，重写 aof 文件的操作，并没有读取旧的的 aof 文件，而是将整个内存的数据库内容用命令的方式重写了一个新的 aof 文件，这点和快照有点类似。 no-appendfsync-on-rewrite no : 重写时是否可以运用 Appendfsync 用默认 no 即可，保证数据安全 auto-aof-rewrite-percentage 倍数 设置基准值auto-aof-rewrite-min-size 设置基准值大小 （4）flushall刷新内存中的数据，即清空数据可通过删除aof文件中该操作的记录来恢复数据rdb文件无法实现数据恢复 （5）aof 特点 aof优点：可用于数据恢复缺点：体积大，速度慢 aof文件体积会越来越大 （6）aof文件优化重写：当文件大小达到一定阈值，启动压缩文件 netstat -anpt查看所有端口的使用情况 4、备份 Redis 数据建议：创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。 六、Redis 主从复制0、前情提要Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(masterserver)的精确复制品 。 1、关于 Redis 复制功能的几个重要方面： Redis 使用异步复制。 一个主服务器可以有多个从服务器。 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器，多个从服务器之间可以构成一个图状结构。 复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次同步， 主服务器也可以继续处理命令请求。不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。 复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT 命令可以交给附属节点去运行。 2、从服务器配置方式一：编辑配置文件 （永久生效） 添加主服务器的IP slaveof 192.168.198.128 6379 方式二：调用 SLAVEOF 命令，输入主服务器的 IP 和端口，然后同步就会开始 前提：主从服务器的redis-server均启动了 （仅适用于当前线程的服务，同步时，会将自己原来的key值数据清空，并且在主服务器页面会显示关于从服务器的日志信息） 127.0.0.1:6379&gt; SLAVEOF 192.168.198.128 6379 若想取消该服务器的主从关系，使用如下命令，且还会保存当前数据 127.0.0.1:6379&gt; SLAVEOF no one 连接远程节点的redis服务（端口默认为6379） redis-cli -h 192.168.198.128 3、只读服务器从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。 只读模式配置 方式一：由 redis.conf 文件中的 slave-read-only 选项控制 slave-read-only yes 方式二：通过 CONFIG SET 命令来开启或关闭这个模式  只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。  另外，对一个从属服务器执行命令 SLAVEOF NO ONE 将使得这个从属服务器关闭复制功能，并从从属服务器转变回主服务器，原来同步所得的数据集不会被丢弃。  利用『 SLAVEOF NO ONE 不会丢弃同步所得数据集』这个特性，可以在主服务器失败的时候，将从属服务器用作新的主服务器，从而实现无间断运行。 4、从服务器相关配置：设置密码： ​ 主服务器： 通过 requirepass 选项设置密码 ​ 从服务器： 方式一： 服务器正在运行，使用客户端输入以下命令：config set masterauth 方式二： 将它加入到配置文件中：masterauth 5、主服务器配置只在有至少 N 个从服务器的情况下，才执行写操作 从 Redis 2.8 开始， 为了保证数据的安全性，可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。 min-slaves-to-write min-slaves-max-lag 至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag 秒， 那么主服务器就会执行客户端请求的写操作。 七、Redis-sentinel( 哨兵)0、前情提要Redis 的 Sentinel 系统用于管理多个 Redis 服务器，该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时，Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时，集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 是一个分布式系统，你可以在一个架构中运行多个 Sentinel 进程， 这些进程使用流言协议（gossipprotocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移，以及选择哪个从服务器作为新的主服务器。 1、启动 Sentinel方式一： 对于 redis-sentinel 程序， 可以用以下命令来启动Sentinel 系统： redis-sentinel /path/to/sentinel.conf 方式二： 对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器： redis- - server /path/to/sentinel.conf – sentinel 注意： 启动 Sentinel 实例必须指定相应的配置文件， 系统会使用配置文件来保存 Sentinel 的当前状态， 并在 Sentinel 重启时通过载入配置文件来进行状态还原。如果启动 Sentinel 时没有指定相应的配置文件， 或者指定的配置文件不可写（not writable）， 那么 Sentinel 会拒绝启动。 2 、配置 SentinelRedis 源码中包含了一个名为 sentinel.conf 的文件，这个文件是一个带有详细注释的 Sentinel 配置文件示例。运行一个 Sentinel 所需的最少配置如下所示： 12345sentinel monitor mymaster 192.168.198.128 6379 2 sentinel down-after-milliseconds mymaster 30000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1protected-mode no / bind 本机 IP 注意：主服务器无密码时，记得在 sentinel 配置里配上bind 本机 ip ，或者关掉保护模式 protected-mode no 配置解释： 第一行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6379 ， 而将这个主服务器判断为失效至少需要 2 个Sentinel 同意 （只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行）。 down-after-milliseconds 选项 指定了 Sentinel 认为服务器已经断线所需的毫秒数。 如果服务器在给定的毫秒数之内， 没有返回 Sentinel发送的 PING 命令的回复， 或者返回一个错误， 那么 Sentinel 将这个服务器标记为主观下线 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 你可以通过将这个值设为 1 来保证每次只有一个从服务器处于不能处理命令请求的状态。]]></content>
      <categories>
        <category>内存数据库</category>
      </categories>
      <tags>
        <tag>Linux系统环境</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm学习]]></title>
    <url>%2F2019%2F01%2F29%2FStorm%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Storm简介官网：Storm 1、Storm ：流式计算处理框架 2、 特点： 实时 分布式 高容错 storm常驻内存（7*24h:意味着永久运行，需人为关闭） 数据不经过磁盘，在内存中处理 高可靠性：异常处理、消息可靠性保障机制 可维护性：StormUI图形化监控接口 3、应用 （1）流式处理：（异步） ​ 客户端提交数据进行计算，并不会等待计算结果 举例： 逐条处理：ETL（用于数据清洗） 统计分析 （2）实时请求应答服务：（同步） 客户端提交数据后，立刻取得计算结果并返回给客户端 举例： 二、Storm架构（从进程角度） 1、nimbus : (主)接收客户端提交的请求； 资源调度、任务分配、接收jar包 2、supervisor：(从) 接收nimbus分配的任务、启停worker 3、worker– 运行具体处理运算组件的进程（每个Worker对应执行一个Topology的子集）– worker任务类型，即spout任务、bolt任务两种– 启动executor（executor即worker JVM进程中的一个java线程，一般默认每个executor负责执行一个task任务） 4、zookeeper存储心跳信息、任务信息 数据传输 ZMQ ：ZeroMQ开源的消息传递框架。并不是消息队列 Netty ：基于NIO的网络框架，高效（Storm 0.9之后使用） 三、Storm编程模型 Topology（DAG有向无环图的实现） 是对Storm实时计算逻辑的封装，由一系列通过数据流相互关联的Spout、Bolt组成的拓扑结构 生命周期： 此拓扑结构只要启动，就会在集群中一直运行，直到手动将其kill，否则不会终止。 （与MapReduce中Job的区别，MR中Job在计算完成后就会终止） spout（数据源：发送数据） 一般会从指定的数据源读取元祖（Tuple）发送到拓扑（Topology）中 一个Spout可以发送多个Stream （通过OutputFieldDeclare的declare方法声明不同的数据流，发送数据时通过SpoutOutputCollector中的emit方法指定 StreamId将数据发送出去） Spout中的核心方法是nextTuple，该方法会被Storm线程不断调用、主动从数据源拉取数据、再通过emit方法将数据生成元祖（Tuple）发送给之后的Bolt计算 bolt（数据处理组件：计算数据，个数不限） 单个Bolt可实现简单的任务或数据流转换 复杂的场景需要多个Bolt分多个步骤完成 一个Bolt可以发送多个Stream （通过OutputFieldDeclare的declare方法声明不同的数据流，发送数据时通过SpoutOutputCollector中的emit方法指定 StreamId将数据发送出去） Bolt中的核心方法是execute，该方法通过接收一个元组数据、实现核心业务逻辑 tuple（Stream中最小的数据组成单位） Stream（数据流） 从Spout中传递数据给Bolt、上一个Bolt传递数据给下一个Blot，这样形成的数据通道叫做Stream Stream声明时需要给其指定Id（默认为Default，实际开发中多使用单一数据流，无需指定StreamId） Stream Grouping – 数据流分组（即数据分发策略） 四、Storm安装部署（完全分布式） 环境：zookeeper集群（node00，node01，node02） 1、解压安装 版本：apache-storm-0.10.0.tar.gz 1tar -zvxf apache-storm-0.10.0.tar.gz 2、在storm目录中创建logs目录 1mkdir logs 3、编辑配置文件解压路径/config/storm.yaml 1234567891011storm.zookeeper.servers:- &quot;node00&quot;- &quot;node01&quot;- &quot;node02&quot;storm.local.dir: &quot;/opt/storm&quot;nimbus.host: “node00&quot;supervisor.slots.ports:- 6700- 6701- 6702- 6703 将安装包发送到其他节点 4、启动服务 （1）启动zookeeper‘ 1zkServer.sh start (2)启动nimbus（在node00上：storm安装目录下） 1nohup ./bin/storm nimbus &gt;&gt; ./logs/nimbus.out 2&gt;&amp;1 &amp; nohup:防止被操作系统意外挂起 2&gt;&amp;1：标准错误输出重定向 &amp;：后台运行 (3)启动supervisor（在node01 ，node02上：storm安装目录下） 1./bin/storm supervisor&gt;&gt; ./logs/supervisor.out 2&gt;&amp;1 &amp; (4)查看进程 ：jps 显示： node00： [root@node00 apache-storm-0.10.0]# jps8869 QuorumPeerMain9093 Jps9083 config_value node01 、 node02 [root@node01 apache-storm-0.10.0]# jps7280 config_value7290 Jps7230 QuorumPeerMain 且三个节点的logs目录下也相应的生成了指定的日志文件 5、Storm UI（从浏览器访问，在前台页面上查看详情） （1）启动服务（可在node00节点：storm安装目录） 1./bin/storm ui &gt;&gt; ./logs/ui.out 2&gt;&amp;1 &amp; （2）浏览器访问 http://node00:8080 五、Storm重点概念详解1、grouping 数据流分组（即数据分发策略） 2、并发机制topology负载均衡 3、容错机制4、demoAPIStorm WordCount（数据累加） 5、与MapReduce的区别 Storm MapReduce 流式处理 批处理 （毫）秒级 分钟级 DAG模型 Map+Reduce模型 常驻内存运行 反复启停 6、和Spark Streaming的区别 Storm Spark Streaming 流式处理 微批处理 （毫）秒级 秒级 成熟稳定 稳定性改进中 独立系统 专为流式计算设计 Spark核心的一种计算模型能与其他组件很好的结合 小记 1、storm源码中包含后缀名为.clj的文件，这是一种Clojure编程语言，它是一种运行在JVM上的Lisp方言。而Lisp是一种以表达性和功能强大著称的编程语言。 2、阿里巴巴在Storm的基础上使用Java代码并做了相关的改进，开发了JStorm，和Storm一样都是开源的。（反哺行为，包括将Flink→Blink） $$ $$ st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);]]></content>
      <categories>
        <category>分布式框架</category>
      </categories>
      <tags>
        <tag>Storm，流式处理框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume学习]]></title>
    <url>%2F2019%2F01%2F18%2FFlume%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、分类exec： Unix等操作系统执行命令行，如tail ，cat 。可监听文件 netcat 监听一个指定端口，并将接收到的数据的每一行转换为一个event事件 avro 序列化的一种，实现RPC（一种远程过程调用协议）。 监听AVRO端口来接收外部AVRO客户端事件流 capacity：默认该通道中最大的可以存储的event数量是1000 Trasaction Capacity：每次最大可以source中拿到或者送到sink中的event数量也是100 二、操作1、 netcat（监听端口，在本地控制台打印）（1） vim netcat_logger1234567891011121314151617181920212223# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 （2）命令操作 （在会话1端） 在node00节点的控制台输入启动命令： (方式一：指定配置文件的路径+文件名) flume-ng agent –conf-file /root/flume/netcat_logger –name a1 -Dflume.root.logger=INFO,console （方式二：配置文件在当前目录） flume-ng agent –conf ./ –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console 特别注意： #####官网方式######### flume-ng agent –conf conf –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console 解释：此命令适用于将配置文件放在flume解压安装目录的conf中（不常用） 控制台显示: 1​````` 19/01/18 12:27:31 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 12:27:31 INFO node.Application: Starting Sink k119/01/18 12:27:31 INFO node.Application: Starting Source r119/01/18 12:27:31 INFO source.NetcatSource: Source starting19/01/18 12:27:31 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]123456789101112131415161718* (在会话2端)&gt; 在node00节点的控制台输入命令：&gt;&gt; 1、在节点上安装telnet：&gt;&gt; yum install -y telnet&gt;&gt; yum -y install telnet-server&gt;&gt; 2、启动：&gt;&gt; telnet localhost 44444 &gt;&gt; `注意：`：&gt;&gt; a1.sources.r1.bind = localhost12345678910&gt;&gt; 前提是/etc/hosts中已经配置&gt;&gt; 如果此处配置localhost 那么启动时，localhost 或127.0.0.1都可以，node00就不行&gt;&gt; 如果此处配置node00那么启动时，node00或ip都可以，localhost就不行&gt;&gt; 3、在控制台输入任何内容;&gt;&gt; 都会在会话1端显示，且会话1端（ctrl+c）退出服务，会话2端也自动结束 yum list telnet* 查看telnet相关的安装包 直接yum –y install telnet 就OK yum -y install telnet-server 安装telnet服务 yum -y install telnet-client 安装telnet客户端(大部分系统默认安装) 1234### 2、avro（监听远程发送文件，在本地控制台打印）#### （1）vim avro_logger #test avro sources ##使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示 ##当前flume节点执行： #flume-ng agent –conf ./ –conf-file avro_loggers –name a1 -Dflume.root.logger=INFO,console ##其他flume节点执行： #flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./logs a1.sources=r1a1.channels=c1a1.sinks=k1 a1.sources.r1.type = avroa1.sources.r1.bind=192.168.198.128a1.sources.r1.port=55555 a1.sinks.k1.type=logger a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100 a1.sources.r1.channels=c1a1.sinks.k1.channel=c1123456789101112131415161718实现功能：&gt; 使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示#### （2）命令操作##### `启动` （在会话1端）在node00上* ##当前flume节点执行（配置文件在当前目录）：* &gt; flume-ng agent --conf ./ --conf-file avro_logger --name a1 -Dflume.root.logger=INFO,console`显示：` 19/01/18 13:53:16 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 13:53:16 INFO node.Application: Starting Sink k119/01/18 13:53:16 INFO node.Application: Starting Source r119/01/18 13:53:16 INFO source.AvroSource: Starting Avro source r1: { bindAddress: 192.168.198.128, port: 55555 }…19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 13:53:17 INFO source.AvroSource: Avro source r1 started. 12345678910##### 发送(在会话2端)在node00上发送文件到node00`启动`##可在本地和其他flume节点执行（配置文件在当前目录）： flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./flume.log12(在会话1端) 19/01/18 14:12:57 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata }12345678910时刻监听传输文件的内容`注意`&gt; 该过程也可应用于不同节点之间### 3、exec（监听某一命令，在本地控制台打印）#### （1）vim exec_logger #单节点flume配置 example.conf: A single-node Flume configuration#给agent三大结构命名 Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 #描述source的配置：类型、命令（监听/root/flume.log文件） Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /root/flume.log #描述sink的配置：类型 Describe the sinka1.sinks.k1.type = logger #在内存中使用一个channel缓存事件 Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 #将source和sink绑定到channel上 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1234567891011121314151617181920212223242526（xshell会话1：）&gt; 在node00上：`启动`&gt;&gt; 在exec_logger文件所在的目录下&gt;&gt; 命令：flume-ng agent --conf-file exec_logger --name a1 -Dflume.root.logger=INFO,console&gt;&gt; r1 启动&gt; （复制会话：会话2）&gt;&gt; 在node00上：&gt;&gt; 在root目录下&gt;&gt; 命令：echo hello bigdata &gt;&gt;flume.log&gt; 之后在会话2上&gt;&gt; `logger本地控制台打印：`&gt;&gt; 19/01/18 12:03:23 INFO sink.LoggerSink:Event:{ headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata }123456### 4、netcat–hdfs(监听数据，传到hdfs上)#### （1）vim netcat_hdfs a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1 a1.sources.r1.type = netcata1.sources.r1.bind = node00a1.sources.r1.port = 41414 a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://Sunrise/myflume/%y-%m-%da1.sinks.k1.hdfs.useLocalTimeStamp=true Define a memory channel called c1 on a1a1.channels.c1.type = memory #默认值，可省 #a1.channels.c1.capacity = 1000 #a1.channels.c1.transactionCapacity = 100 Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c112345678910111213141516#### (2)操作在node00的会话1上启动&gt; 在node00上：&gt;&gt; 在netcat_hdfs文件所在的目录下&gt;&gt; 命令：flume-ng agent --conf-file netcat_hdfs --name a1 -Dflume.root.logger=INFO,console显示： 19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 14:34:44 INFO node.Application: Starting Sink k119/01/18 14:34:44 INFO node.Application: Starting Source r119/01/18 14:34:44 INFO source.NetcatSource: Source starting19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started19/01/18 14:34:44 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/192.168.198.128:41414] 12345678910在node00的会话2上启动&gt; telnet node00 41414显示 Trying 192.168.198.128…Connected to node00.Escape character is ‘^]’. 1234输入任意内容在node00会话1端会显示 19/01/18 14:36:50 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false19/01/18 14:36:51 INFO hdfs.BucketWriter: Creating hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Closing hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Renaming hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp to hdfs://Sunrise/myflume/19-01-18/FlumeData.154782221025919/01/18 14:37:29 INFO hdfs.HDFSEventSink: Writer callback called. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758在HDF分布式系统上会显示，生成的文件![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4acaz5fj30u10blmzc.jpg)![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4bsv8d2j30wd08k74i.jpg)`注意：`这种情况会在hdfs上生成很多小文件，[在官网](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html)HDFS Sink：[**](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html#hdfs-sink)有很多关于文件生成过程中的配置| Name | Default | Description || ---------------------- | ------------ | ------------------------------------------------------------ || **channel** | – | || **type** | – | The component type name, needs to be `hdfs` || **hdfs.path** | – | HDFS directory path (eg hdfs://namenode/flume/webdata/) || hdfs.filePrefix | FlumeData | Name prefixed to files created by Flume in hdfs directory || hdfs.fileSuffix | – | Suffix to append to file (eg `.avro` - *NOTE: period is not automatically added*) || hdfs.inUsePrefix | – | Prefix that is used for temporal files that flume actively writes into || hdfs.inUseSuffix | `.tmp` | Suffix that is used for temporal files that flume actively writes into || hdfs.rollInterval | 30 | Number of seconds to wait before rolling current file (0 = never roll based on time interval) || hdfs.rollSize | 1024 | File size to trigger roll, in bytes (0: never roll based on file size) || hdfs.rollCount | 10 | Number of events written to file before it rolled (0 = never roll based on number of events) || hdfs.idleTimeout | 0 | Timeout after which inactive files get closed (0 = disable automatic closing of idle files) || hdfs.batchSize | 100 | number of events written to file before it is flushed to HDFS || hdfs.codeC | – | Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy || hdfs.fileType | SequenceFile | File format: currently `SequenceFile`, `DataStream` or `CompressedStream` (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC || hdfs.maxOpenFiles | 5000 | Allow only this number of open files. If this number is exceeded, the oldest file is closed. || hdfs.minBlockReplicas | – | Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath. || hdfs.writeFormat | Writable | Format for sequence file records. One of `Text` or `Writable`. Set to `Text` before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive. || hdfs.callTimeout | 10000 | Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring. || hdfs.threadsPoolSize | 10 | Number of threads per HDFS sink for HDFS IO ops (open, write, etc.) || hdfs.rollTimerPoolSize | 1 | Number of threads per HDFS sink for scheduling timed file rolling || hdfs.kerberosPrincipal | – | Kerberos user principal for accessing secure HDFS || hdfs.kerberosKeytab | – | Kerberos keytab for accessing secure HDFS || hdfs.proxyUser | | || hdfs.round | false | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) || hdfs.roundValue | 1 | Rounded down to the highest multiple of this (in the unit configured using `hdfs.roundUnit`), less than current time. || hdfs.roundUnit | second | The unit of the round down value - `second`, `minute` or `hour`. || hdfs.timeZone | Local Time | Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles. || hdfs.useLocalTimeStamp | false | Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. || hdfs.closeTries | 0 | Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart. || hdfs.retryInterval | 180 | Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension. || serializer | `TEXT` | Other possible options include `avro_event` or the fully-qualified class name of an implementation of the `EventSerializer.Builder` interface. |netcat-hdfs （配置方式二） a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1 a1.sources.r1.type = avroa1.sources.r1.bind=node01a1.sources.r1.port=55555 a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://shsxt/hdfsflume Define a memory channel called c1 on a1a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100 Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c1123456### 5、结合版（netcat-avro）#### （1）vim netcat2_logger（node00） example.conf: A single-node Flume configuration#flume-ng agent –conf ./ –conf-file netcat2_logger –name a1 -Dflume.root.logger=INFO,console #flume-ng –conf conf –conf-file /root/flume_test/netcat_hdfs -n a1 -Dflume.root.logger=INFO,console #telnet 192.168.235.15 44444 Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = node00 a1.sources.r1.port = 44444 Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = node01 a1.sinks.k1.port = 60000 Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 #————————— #flume-ng agent –conf-file etect2_logger –name a1 -#Dflume.root.logger=INFO,console #flume-ng agent –conf conf –conf-file netcat_logger –name a1 -#Dflume.root.logger=INFO,console12（node01） #flume-ng agent –conf ./ –conf-file avro2 -n a1a1.sources = r1a1.sinks = k1a1.channels = c1 a1.sources.r1.type = avroa1.sources.r1.bind = node01a1.sources.r1.port = 60000 a1.sinks.k1.type = logger Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1234567891011121314(2)操作&gt; 先启动后面的flume节点node01 ，在启动node00，最后启动node02在node01上`启动`&gt; flume-ng &gt;&gt; --conf conf --conf-file avro2 -n a1 -Dflume.root.logger=INFO,console显示 19/01/18 23:22:27 INFO node.Application: Starting Channel c119/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 23:22:28 INFO node.Application: Starting Sink k119/01/18 23:22:28 INFO node.Application: Starting Source r119/01/18 23:22:28 INFO source.AvroSource: Starting Avro source r1: { bindAddress: node01, port: 60000 }…19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 23:22:30 INFO source.AvroSource: Avro source r1 started. 1234567891011121314151617181920212223242526在node00上`启动`&gt; flume-ng agent&gt;&gt; --conf ./ --conf-file netcat2_logger --name a1 -Dflume.root.logger=INFO,console在node02上`启动`&gt; telnet node00 44444然后输入数据文件最后在node01节点上显示文件信息 19/01/18 23:33:01 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 0D hello world. } ` flume-ng agent –conf-file flumeproject –name a1 -Dflume.root.logger=INFO,console]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase性能优化]]></title>
    <url>%2F2019%2F01%2F17%2FHBase%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[HBase性能优化方案（一）、表的设计一、Pre-Creating Regions 预分区 解决海量导入数据时的热点问题 背景： 在创建HBase表的时候默认一张表只有一个region， 所有的put操作都会往这一个region中填充数据， 当这一个region过大时就会进行split。 如果在创建HBase的时候就进行预分区 则会减少当数据量猛增时由于region split带来的资源消耗。 注意： 每个region都有一个startKey和一个endKey来表示该region存储的rowKey范围。 1&gt; create &apos;t1&apos;, &apos;cf&apos;, SPLITS =&gt; [&apos;20150501000000000&apos;, &apos;20150515000000000&apos;, &apos;20150601000000000&apos;] 或者 123456&gt; create &apos;t2&apos;, &apos;cf&apos;, SPLITS_FILE =&gt; &apos;/home/hadoop/splitfile.txt&apos; /home/hadoop/splitfile.txt中存储内容如下： 201505010000000002015051500000000020150601000000000 二、row key HBase中row key用来检索表中的记录，支持以下三种方式： · 通过单个row key访问：即按照某个row key键值进行get操作； · 通过row key的range进行scan：即通过设置startRowKey和endRowKey，在这个范围内进行扫描；过滤器 · 全表扫描：即直接扫描整张表中所有行记录。 （二）、写表操作一、多HTable客户端并发写 二、HTable参数设置 三、批量写 四、多线程并发写 （三）、读表操作一、多HTable客户端并发读 二、HTable参数设置 三、批量读 四、多线程并发读 五、缓存查询结果 六、 Blockcache]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase学习]]></title>
    <url>%2F2019%2F01%2F15%2FHBase%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[非关系型数据库 官网 一、对数据库的 基本了解 1、简介 基于Hadoop 的分布式数据库 特点： 1、高可靠性 2、高性能 （以上两点：基于分布式的特点） 3、面向列 （以（K,V）存储，有唯一标记的rowkey，value包含是数据库中的列值） 4、可伸缩 （搭建在集群上） 5、实时读写 （用时间戳唯一标记每一版本的数据记录） 2、工作结构 1、利用Hadoop的HDFS作为其文件存储系统 2,利用Hadoop的MapReduce来处理HBase中的海量数据 3,利用Zookeeper作为其分布式协同服务 4,主要用来存储非结构化和半结构化的松散数据（NoSQL非关系型数据库有redis、MongoDB等 3、关系型数据库 1、定义 关系模型指的就是二维表格模型； 而一个关系型数据库就是由二维表及其之间的联系所组成的一个数据组织 2、三大优点 容易理解 使用方便 易于维护 3、三大瓶颈 高并发读写需求 硬盘I/O是一个很大的瓶颈，并且很难能做到数据的强一致性。 海量数据的读写性能低 在一张包含海量数据的表中查询，效率是非常低的。 ​ 扩展性和可用性差 丰富的完整性使得横向扩展把难度加大了 4、非关系型数据库 1、存储格式：key value键值对，文档，图片等等 结构不固定 2、可以减少一些时间和空间的开销，仅需要根据id取出相应的value就可以完成查询。 3、一般不支持ACID特性，无需经过SQL解析，读写性能高 4、不提供where字段条件过滤 5、难以体现设计的完整性，只适合存储一些较为简单的数据 二、对HBase的基本里了解1、数据结构组成（ 1）Row key : 唯一标记决定一行数据按照字典排序最大只能存储64KB的字节数据设计非常关键 （2）Column Family列族 &amp; qualifier列 列族必须作为表模式(schema)定义的一部分预先给出， 表中的每个列都归属于某个列族； 权限控制、存储以及调优都是在列族层面进行的； 列名以列族作为前缀，每个“列族”都可以有多个列成员(column)； 新的列可以随后按需、动态加入； （3）Cell单元格 由行和列的坐标交叉决定； 单元格是有版本的（有时间戳决定）； 单元格的内容是未解析的字节数组；cell中的数据是没有类型的，全部是字节码形式存贮。 由{rowkey， column( = +)， version} 唯一确定的单元。 （4）Timestamp时间戳 在HBase每个cell存储单元对同一份数据有多个版本， 根据唯一的时间戳来区分每个版本之间的差异， 不同版本的数据按照时间倒序排序，最新的数据版本排在最前面 时间戳的类型是64位整型。时间戳可以由HBase(在数据写入时自动)赋值，精确到毫秒 时间戳。也可以由客户显式赋值，但必须唯一性 （5）HLog(WAL log) HLog文件就是一个普通的Hadoop SequenceFile HLog Sequence File的Key是HLogKey对象 ​ **HLogKey中记录了写入数据的归属信息，包括table和region名字，sequence number（起始值为0或是最近一次存入文件系统中sequence number）和timestamp（写入时间） HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue ​ **存储hbase表的操作记录，KV数据信息 2、体系架构 （1）Client 包含访问HBase的接口并维护cache来加快对HBase的访问 （2）Zookeeper 保证任何时候，集群中只有一个master； 存贮所有Region的寻址入口。 实时监控Region server的上线和下线信息。并实时通知Master 存储HBase的schema和table元数据 （3）Master 为Region server分配region； 负责Region server的负载均衡； 发现失效的Region server并将其上的region重新分配； 管理用户对table的增删改操作； （4）RegionServer 维护region，处理对这些region的IO请求 负责切分在运行过程中变得过大的region （5）Region 保存一个表里面某段连续的数据，每个表一开始只有一个region； 随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变） （HBase自动把表水平划分成多个区域(region)） 当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上 （6）Memstore与storefile 一个region由2-3store组成，一个store对应一个CF（列族） store包括位于内存中的memstore和位于磁盘的storefile。 写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile； 当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、majorcompaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile 当一个region所有storefile的大小和数量超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡 客户端检索数据，先在memstore找，找不到再找storefile HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegion server上。 每个Strore又由一个memStore和0至多个StoreFile组成,StoreFile以HFile格式保存在HDFS上(HFile)。 三、Hbase 安装部署完全分布式搭建 1、安装包准备 Hbase（本文使用0.98版本） 将tar上传至Linux系统，进行解压安装 2、修改配置文件hbase-env.sh（在Hbase的解压目录的conf目录中） 修添加JAVA_HOME环境变量 123# The java implementation to use. Java 1.7+ required.# export JAVA_HOME=/usr/java/jdk1.6.0/export JAVA_HOME=/usr/soft/jdk1.8.0_191 不使用HBase的默认zookeeper配置，（使用自己的）： 12# Tell HBase whether it should manage it&apos;s own instance of Zookeeper or not. export HBASE_MANAGES_ZK=false 3、修改配置hbase-site.xml（在Hbase的解压目录的conf目录中） 123456789101112131415161718192021&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;!--Hdfs配置时的集群名--&gt;&lt;value&gt;hdfs://Sunrise:8020/hbase&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--zookeeper的三台节点--&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;node00,node01,node02&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--配置http访问的port---&gt;&lt;name&gt;hbase.master.info.port&lt;/name&gt;&lt;value&gt;60010&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 注意：（会出bug的地方）： 1、问题描述： HBase启动时，警告：Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0 解决方案： 原因：由于使用了JDK8 ，需要在HBase的配置文件中hbase-env.sh，注释掉两行。 123# Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+#export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m&quot;#export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m&quot; 重新启动HBase。 2、问题描述： 配置好HBase后，各项服务正常，但想从浏览器通过端口60010看下节点情况，但是提示拒绝访问 检测：在服务器上netstat -natl|grep 60010 发现并没有60010端口 原因：HBase 1.0 之后的版本都需要在hbase-site.xml中配置端口，如下 1234&lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt;&lt;/property&gt; 重新启动HBase,在浏览器再次访问，就ok了 4、添加配置regionservers 文件（在Hbase的解压目录的conf目录中） 添加配置的regionservers 的主机名 regionservers 123node00node01node02 5、添加配置backup-masters 添加配置的master备份的主机名（在Hbase的解压目录的conf目录中） backup-masters 1node02 6、将Hadoop安装解压目录/etc/hadoop目录下的hdfs-site.xml文件 拷贝到Hbase的解压目录的conf目录中 7、配置环境变量 ~/.bash_profile 123456789101112JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/binHIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/binSQOOP_HOME=/usr/soft/sqoop-1.4.6export PATH=$PATH:$SQOOP_HOME/binHBASE_HOME=/usr/soft/hbase-1.2.9export PATH=$PATH:$HBASE_HOME/bin source ~/.bash_profile 8、将如上配置远程发送至其他节点（Hbase 、 ./bash_profile） 9、各个节点注意要做时间同步 1ntpdate cn.ntp.org.cn 10、启动HDFS集群： 12zkServer.sh startstart-hdfs.sh 11、启动：start-hbase.sh 12345starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node00.outnode02: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node02.outnode01: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node01.outnode00: starting regionserver, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-regionserver-node00.outnode02: starting master, logging to /usr/soft/hbase-1.2.9/bin/../logs/hbase-root-master-node02.out 12、查看进程：jps 13、浏览器访问：node00:60010 14、关闭：stop-hbase.sh 四、通过hbase shell命令进入HBase 命令行接口通过help可查看所有命令的支持以及帮助手册 帮助创建 hbase(main):007:0&gt; help create 名称 Shell命令 举例 创建表 create ‘表名’, ‘列族名1’[,…] create ‘t1’，‘cf1’ 列出所有表 list list 添加记录 put ‘表名’, ‘RowKey’, ‘列族名称:列名’, ‘值’ put ‘t1’,‘rk_00101’,‘cf1:name’,‘zs’ 查看记录 get ‘表名’, ‘RowKey’, ‘列族名称:列名’ get ‘t1’,‘rk_00101’ get ‘t1’,‘rk_00101’,‘cf1:name’ 查看表中 记录总数 count ‘表名’ count ‘t1’ 删除记录 delete ‘表名’ , ‘RowKey’, ‘列族名称:列名’ delete ‘t1’,‘rk_00101’,‘cf1:name’ 删除一张表 先要屏蔽该表，才能对该表进行删除。 第一步 disable ‘表名称’ 第二步 drop ‘表名称’ disable ‘t1’ drop ‘t1’ 查看所有 记录 scan ‘表名 ‘ scan ‘t1’ create ‘t2’, {NAME =&gt; ‘cf1’, VERSIONS =&gt; 2}, METADATA =&gt; { ‘mykey’ =&gt; ‘myvalue’ } 查看未加工的数据中指定版本记录 scan ‘t1’, {RAW =&gt; true, VERSIONS =&gt; 3} raw 未加工的 3 查看保存 版本记录 scan ‘t1’, {VERSIONS =&gt; 2} 2 五、HBase优化详见HBase性能优化文档]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive优化]]></title>
    <url>%2F2019%2F01%2F14%2FHive%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、核心思想： 把Hive SQL 当做MapReduce程序进行优化 注意：以下不能HQL转化为Mapreduce任务运行 —select 仅查询本表字段 —where 仅对本表字段做条件过滤 二、explain 用以显示任务执行计划 格式： EXPLAIN [EXTENDED|DEPENDENCY|AUTHORIZATION] query 语法解释 从语法组成可以看出来是一个“explain ”+三个可选参数+查询语句。大家可以积极尝试一下，后面两个显示内容很简单的，我介绍一下第一个 extended 这个可以显示hql语句的语法树 其次，执行计划一共有三个部分： 这个语句的抽象语法树 这个计划不同阶段之间的依赖关系 对于每个阶段的详细描述 例子： 12&gt; hive&gt; explain select * from log;&gt; 拓展课下查询MySQl的执行计划。 三、Hive运行方式集群模式：12执行hql：hive&gt; select count(*) from log; 结论： 函数（如count）是在reduce阶段进行默认提交到yarn所在的节点上运行， 优化一:设置 本地模式（运行速度加快。但对加载文件有限制） 1234hive&gt;set hive.exec.mode.local.auto=true;查看：hive&gt;set hive.exec.mode.local.auto 但是如果加载文件的最大值大于配置（默认配置为100M），仍会使用集群模式运行（在yarn所在的节点） 12345查看最大加载文件hive&gt; set hive.exec.mode.local.auto.inputbytes.max;显示：hive.exec.mode.local.auto.inputbytes.max=134217728 优化二：设置 严格模式 123通过设置以下参数开启严格模式[防止误操作]：hive&gt; set hive.mapred.mode=strict;（默认为：nonstrict非严格模式） 但是存在查询限制 1、对分区表查询时，必须添加where对于分区字段的条件过滤； 12&gt; hive&gt; select * from day_table where dt='2019-01-13';&gt; 2、order by语句必须包含limit输出限制； 12345&gt; hive&gt; select * from log order by id limit 1;&gt; 这里的1， 表示显示前多少条记录，只能设一个数字&gt; 和Mysql（可以从0 开始）不同的是，它只能从1开始&gt; mysql可以有两个数字，表示从第几条开始，显示几条&gt; 3、限制执行笛卡尔积的查询 imit 四、Hive排序1、Order By— 对于查询结果做全局排序，只允许有一个reduce处理（当数据量较大时，reduce数量有限，应慎用。 ​ 严格模式下，必须结合limit来使用） 1select * from log order by id； 2、Sort By– 对于单个reduce的数据进行排序 –局部（单个reduce）有序，全局无序 1可以通过设置mapred.reduce.tasks的值来控制reduce的数，然后对reduce输出的结果做二次排序 案例 1select * from log sort by id; (结果无序) 显示 Time taken: 147.077 seconds, Fetched: 7 row(s) 3、Distribute By– 分区排序，经常和 Sort By 结合使用 全局有序，局部无序 1select * from log distribute by id; （结果无序） Time taken: 144.708 seconds, Fetched: 7 row(s) 注意：hive要求DISTRIBUTE BY语句出现在SORT BY语句之前 1Distribute By可以将Map阶段输出的数据按指定的字段划分到不同的reduce文件中，然后，sort by 对reduce阶段的输出数据做排序。 案例: 1select * from log distribute by id sort by id asc; （结果无序） 1select * from (select * from log distribute by id) a sort by a.id ;s 改良 123select a.* from (select * from log cluster by id ) a order by a.id limit 9 ; (结果有序)9 在这里是表中数据记录的总条数 显示： Time taken: 234.593 seconds, Fetched: 7 row(s) 1select * from log order by id limit 9; （结果有序） 显示： Time taken: 102.065 seconds, Fetched: 7 row(s) 4、Cluster By– 相当于 Sort By + Distribute By（Cluster By不能通过asc、desc的方式指定排序规则；可通过 distribute by column sort by column asc|desc 的方式） 1select * from log cluster by id； （结果无序） 五、Hive Join （重难点）1、Join 连接时，将小表（驱动表）放在join的左边2、Map Join ： 因为Map Join 是在Map端且在内存中进行的，所以不需要启动Reduce任务，也没有shuffle阶段，从而在一定程度上节省资源，提高Join效率。 方式：（两种）1、SQL方式：​ 在HQl语句中添加MapJoin标记（mapjoin）(将小表加入到内存，注意小表的大小) ​ 语法： 12SELECT /*+ MAPJOIN(smallTable) */ smallTable.key, bigTable.value FROM smallTable JOIN bigTable ON smallTable.key = bigTable.key; 案例： 12SELECT /*+ MAPJOIN(log1) */ log.id,log1.name FROM log JOIN log1 ON log.id = log1.id; 2、自动的MapJoin​ 通过修改以下配置启用自动的mapjoin： 1hive&gt; set hive.auto.convert.join = true; ​ （ 该参数为true时，Hive自动对左边的表统计数据量，如果是小表就加入内存，即对小表使用Map join）其他相关配置参数： 1hive&gt; set hive.mapjoin.smalltable.filesize; （默认：大表小表判断的阈值25MB左右，如果表的大小小于该值则会被加载到内存中运行，可自定义） 1hive&gt; set hive.ignore.mapjoin.hint; （默认值：true；是否忽略mapjoin hint 即mapjoin标记；如果为false，这则需要添加-MapJoin标记，mapjoin（smalltable）） 1hive&gt; set hive.auto.convert.join.noconditionaltask; （默认值：true；将普通的join转化为普通的mapjoin时，是否将多个mapjoin转化为一个mapjoin） 1hive&gt; set hive.auto.convert.join.noconditionaltask.size; （默认：10M；将多个mapjoin转化为一个mapjoin时，其表的最大值为10M，可自定义） 六、Map-Side聚合 相当于聚合函数：count（） 设置参数，开启在Map端的聚合 1set hive.map.aggr=true; 相关配置参数： 1set hive.groupby.mapaggr.checkinterval； （默认为：100000，表示 map端group by执行聚合时处理的多少行数据） 1set hive.map.aggr.hash.min.reduction； （默认为：0.5，进行聚合的最小比例，预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合） 1set hive.map.aggr.hash.percentmemory; （默认： 0.5 ，map端聚合使用的内存的最大值） 1set hive.map.aggr.hash.force.flush.memory.threshold; （默认为：0.9，map端做聚合操作是hash表的最大可用内容，大于该值则会触发flush 1set hive.groupby.skewindata； （默认为：false，是否对GroupBy产生的数据倾斜做优化） 附加： 数据倾斜问题解决：多种方式（使用MapJoin、使用MapSide） 参考 http://www.sohu.com/a/224276626_543508 七、控制Hive中Map和Reduce的数量1、Map数量相关的参数1set mapred.max.split.size; （默认为：256M，一个split的最大值，即每个map处理文件的最大值） 1set mapred.min.split.size.per.node; (一个节点上最小split数：1个) 1set mapred.min.split.size.per.rack; (一个机架上最小split数：1个) 2、Reduce数量相关的参数1set mapred.reduce.tasks; (默认为：-1，强制指定reduce任务的数量。-1，是未定义，不发挥作用。如果指定了，就会按指定的数量执行) 1set hive.exec.reducers.bytes.per.reducer; （默认为：256M ，每个reduce任务处理的数据量） 1set hive.exec.reducers.max; （默认为：1009个，每个任务最大的reduce数 [Map数量 &gt;= Reduce数量 ]） 八、Hive - JVM重用适用场景：1、小文件个数过多2、task个数过多 原理： hadoop默认配置是使用派生JVM来执行map和reduce任务的，JVM重用可以使得JVM实例在同一个JOB中重新使用N次 1set mapred.job.reuse.jvm.num.tasks; (默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM) 缺点： 设置开启之后，task插槽会一直占用资源，不论是否有task运行，直到所有的task即整个job全部执行完成时，才会释放所有的task插槽资源！]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx入门学习（一）]]></title>
    <url>%2F2019%2F01%2F12%2FNginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E5%9B%9E%E5%90%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Nginx[^开发者]: 由俄罗斯的程序设计师Igor Sysoev所开发 [TOC] 一、产生背景1.巨大流量—海量的并发访问 2.单台服务器资源和能力有限 引发服务器宕机而无法提供服务 二、负载均衡(Load Balance)1、高并发（1）、高（大量的） （2）、并发就是可以使用多个线程或者多个进程，同时处理（就是并发）不同的操作 （3）、简而言之就是每秒内有多少个请求同时访问。 2、负载均衡（1）、将请求/数据【均匀】分摊到多个操作单元上执行 （2）、关键在于【均匀】,也是分布式系统架构设计中必须考虑的因素之一。 3、互联网分布式架构常见，分为客户端层、反向代理nginx层、站点层、服务层、数据层。只需要实现“将请求/数据 均匀分摊到多个操作单元上执行”，就能实现负载均衡。 三、对Nginx的基本了解1、什么是Nginx？1一款轻量级的Web 服务器/反向代理服务器【后面有介绍】及电子邮件（IMAP/POP3）代理服务器。 ​ 特点 12*是占有内存少，CPU、内存等资源消耗却非常低，*运行非常稳定并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。 2、Nginx VS Apache（1）、nginx相对于apache的优点：1234*轻量级，同样起web 服务，比apache 占用更少的内存及资源高并发，*nginx 处理请求是异步非阻塞（如前端ajax）的，而apache 则是阻塞型的，*在高并发下nginx能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单*Nginx 配置简洁, Apache 复杂 （2）、apache 相对于nginx 的优点：12* Rewrite重写 ，比nginx 的rewrite 强大模块超多，*基本想到的都可以找到少bug ，nginx 的bug 相对较多。（出身好起步高） 四、安装Nginx这里以安装tengine为例 1、安装之前准备配置依赖 gcc openssl-devel pcre-devel zlib-devel 安装： yum install gcc openssl-devel pcre-devel zlib-devel -y 2、下载（目前最新版）：tengine-2.2.3.tar 3、 解压缩 tar -zvxf tengine-2.2.3.tar 4、安装Nginx 在Nginx解压的目录下运行： ./configure make &amp;&amp; make install 默认安装目录：/usr/local/nginx 5、配置Nginx为系统服务，以方便管理（1）、在/etc/rc.d/init.d/目录中建立文本文件nginx（2）、在文件中粘贴下面的内容：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid # Source function library.. /etc/rc.d/init.d/functions # Source networking configuration.. /etc/sysconfig/network # Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0 nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot; [ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125; start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125; stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125; restart() &#123; configtest || return $? stop sleep 1 start&#125; reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125; force_reload() &#123; restart&#125; configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125; rh_status() &#123; status $prog&#125; rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125; case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac （3）、修改nginx文件的执行权限 chmod +x nginx （4）、添加该文件到系统服务中去 chkconfig –add nginx 查看是否添加成功 chkconfig –list nginx 启动，停止，重新装载 service nginx start|stop 五、Nginx配置1、查看配置 cd /usr/local/nginx/conf vim nginx.conf 2、配置解析12345678910111213141516171819202122232425262728293031323334#进程数，建议设置和CPU个数一样或2倍worker_processes 2;#日志级别error_log logs/error.log warning;(默认error级别)# nginx 启动后的pid 存放位置#pid logs/nginx.pid;events &#123; #配置每个进程的连接数，总的连接数= worker_processes * worker_connections #默认1024 worker_connections 10240;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on;#连接超时时间，单位秒keepalive_timeout 65; server &#123; listen 80; server_name localhost #默认请求 location / &#123; root html; #定义服务器的默认网站根目录位置 index index.php index.html index.htm; #定义首页索引文件的名称 &#125; #定义错误提示页面 error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; 3、负载均衡配置1）安装Tomcat，参考 Tomcat配置 2）多负载均执行一下操作： 多负载的情况下，打开指定虚拟机器 open node01 node01 为指定虚拟机器的别名，在hosts文件中配置的 启动Tomcat 在Tomcat解压的目录下 ./startup.sh 注意： 记得关闭虚拟机器的防火墙 service iptables stop 浏览器访问 虚拟机器IP地址：8080 默认负载均衡配置 123456789101112131415161718192021&gt; http &#123; &gt; upstream shsxt&#123; &gt; # 以下均为实际执行服务的服务器&gt; #只有当hosts文件中给ip地址配置了别名，这里server后面才能用别名，&gt; #否则跟IP地址&gt; server node01; &gt; server node02; &gt; server node03; &gt; &#125; &gt; &gt; server &#123; &gt; #指定访问端口为80 ，那么Tomcat服务器端的port也要改为80&gt; listen 80; &gt; server_name localhost;&gt; location / &#123;&gt; proxy_pass http://shsxt; &gt; # shsxt 是指定的代理服务器&gt; &#125;&gt; &#125; &gt; &#125;&gt; 查看使用 80端口的程序 netstat -anp |grep 80 配置文件编辑结束后，启动nginx服务 service nginx start （1）、轮询负载均衡（默认）1- 对应用程序服务器的请求以循环方式分发 （2）、加权负载均衡 通过使用服务器权重，还可以进一步影响nginx负载均衡算法， 谁的权重越大，分发到的请求就越多。 权重总数：10 在nginx.conf文件中修改： 12345upstream shsxt &#123; server srv1.example.com weight=3;//域名为在/etc/hosts文件中取的别名 server srv2.example.com; server srv3.example.com; &#125; 配置修改之后，重启 service nginx restart （3）、最少连接负载平衡 在连接负载最少的情况下，nginx会尽量避免将过多的请求分发给繁忙的应用程序服务器， 而是将新请求分发给不太繁忙的服务器，避免服务器过载。 在nginx.conf文件中修改： 123456upstream shsxt &#123; least_conn; server srv1.example.com; server srv2.example.com; server srv3.example.com; &#125; （4）、会话持久性——ip-hash负载平衡机制特点：保证相同的客户端总是定向到相同的服务; (此方法可确保来自同一客户端的请求将始终定向到同一台服务器，除非此服务器不可用。) 在nginx.conf文件中修改： 123456upstream shsxt&#123; ip_hash; server （IP地址|别名）; server （IP地址|别名）; server （IP地址|别名）;&#125; (5)、Nginx的访问控制 Nginx还可以对IP的访问进行控制，allow代表允许，deny代表禁止. 12345678location / &#123;deny 192.168.2.180;allow 192.168.78.0/24;allow 10.1.1.0/16;allow 192.168.1.0/32;deny all;proxy_pass http://shsxt;&#125; 12345从上到下的顺序，匹配到了便跳出。如上的例子先禁止了1个，接下来允许了3个网段，其中包含了一个ipv6，最后未匹配的IP全部禁止访问]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive学习]]></title>
    <url>%2F2019%2F01%2F11%2FHive%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、Hive是什么？1、基于 Hadoop 的一个数据仓库工具 可以将结构化的数据文件映射为一张hive数据库表； 这张Hive数据库表保存不了metadata元数据信息，而是将metadata存储在本地磁盘上的MySQL（关系型数据库）中 并提供简单的 sql 查询功能； 可以将 sql 语句转换为 MapReduce 任务进行运行。 2、快速实现简单的MapReduce 统计的工具 方便非Java编程者对HDFS的数据做mapreduce操作； 学习成本低，十分适合数据仓库的统计分析。 3、什么是数据仓库？ Data Warehouse(DW 或DWH）是为企业所有级别的决策制定过程，提供所有类型数据支持的战略集合。 单个数据存储，出于分析性报告和决策支持目的而创建。 为需要业务智能的企业，提供指导业务流程改进、监视时间、成本、质量以及控制. 数据仓库是用来做查询分析的数据库，基本不用来做插入，修改，删除操作。 4、数据处理的两大分类oltp+olap 联机事务处理OLTP（on-line transaction processing） OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。 OLTP系统强调数据库内存效率，强调内存各种指标的命令率，强调绑定变量，强调并发操作； 联机分析处理OLAP（On-Line Analytical Processing） OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 OLAP系统则强调数据分析，强调SQL执行市场，强调磁盘I/O，强调分区等。 数据文件按结构的分类 结构化数据：关系型 半结构化数据：K-V 松散型： 二、Hive架构原理用户接口主要有三个：CLI，Client 和 WUI。 三、Hive搭建及三种模式1、Hive的安装配置：（1）基本环境：Hadoop集群环境（至少3个节点） Hive是依赖于hadoop系统的，因此在运行Hive之前需要保证已经搭建好hadoop集群环境。 （2）安装一个关系型数据mysql 因为Hive数据仓库的元数据信息是存放在本地磁盘的关系数据库上的 安装步骤：详见 “Linux系统数据库MySQL安装.md” （3）解压安装（按需在指定节点上） 解压apache-hive-1.2.1-bin.tar.gz （4）追加配置环境变量 vim ~/.bash_profile 1234HIVE_HOME=Hive的解压路径HIVE_HOME=/usr/soft/apache-hive-1.2.1-binexport PATH=$PATH:$HIVE_HOME/bin （5）替换和添加相关jar包 修改HADOOP_HOME/share/hadoop/yarn/lib目录下的jline-*.jar 将其替换成HIVE_HOME/lib下的jline-2.12.jar。 –将如下(hive连接mysql)的jar包拷贝到hive解压目录的lib目录下 mysql-connector-java-5.1.32-bin.jar （6）修改配置文件（选择3种模式里哪一种）见三种安装模式 （7）启动 hive 2、三种模式 三种模式 A、内嵌模式（元数据保存在内嵌的derby中，允许一个会话链接，尝试多个会话链接时会报错）【了解】 B、本地模式（本地安装mysql 替代derby存储元数据）【重要】 C、远程模式（远程安装mysql 替代derby存储元数据）【重要】 （1）内嵌Derby单用户模式（了解） 元数据是内嵌在Derby数据库中的，只能允许一个会话连接，数据会存放到HDFS上。 存储方式简单，只需要hive-site.xml 注：使用 derby存储方式时，运行 hive 会在当前目录生成一个 derby 文件和一个metastore_db hive-site.xml ： 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.local&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;&lt;value&gt;/user/hive/warehouse&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; （2）本地用户模式（重要，多用于本地开发测试） 与嵌入式的区别 不再使用内嵌的Derby作为元数据的存储介质，而是使用其他数据库比如MySQL来存储元数据且是一个多用户的模式，运行多个用户client连接到一个数据库中。这种方式一般作为公司内部同时使用Hive。 这里有一个前提，每一个用户必须要有对MySQL的访问权利，即每一个客户端使用者需要知道MySQL的用户名和密码才行。 需要在本地运行一个 mysql 服务器 在node00上（与MySQL在同一个节点上）解压安装Hive MySQL Hive : node00 需要将 mysql 的 jar 包（mysql-connector-java-5.1.32-bin.jar）拷贝到$HIVE_HOME/lib 目录下 hive-site.xml 12345678910111213141516171819202122232425262728&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive_local/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node00/hive_remote?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （3）远程模式（重要） remote 一体 将Hive解压安装与MySQL不同的节点上 MySQL ：node00 Hive ： node02 需要在 Hive服务器启动 meta服务 hive-site.xml (hadoop 2.6.5) 12345678910111213141516171819202122232425262728&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive1/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 如果在hadoop5.X环境下还需要添加 1234&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node01:9083&lt;/value&gt;&lt;/property&gt; 注：这里把hive的服务端和客户端都放在同一台服务器上了。服务端和客户端可以拆开 Remote 分开(公司企业经常用) 将 hive-site.xml 配置文件拆为如下两部分（此时不与MySQL在同一台节点上） MySql ： node00 服务端 ： node02 客户端 ： node01 1）、服务端配置文件（node02） hive-site.xml 123456789101112131415161718192021222324&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive2/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node00:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2）、客户端配置文件（node01） hive-site.xml 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive2/warehouse&lt;/value&gt; &lt;!--注意这里的路径要和服务端一致---&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node02:9083&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 服务端启动 hive 程序 hive –service metastore 客户端直接使用 hive 命令即可 Hive常见问题总汇：http://blog.csdn.net/freedomboy319/article/details/44828337 四、HQL详解Hql 就是HiveQl语句 1、DDL语句（数据库定义语言）（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDLHive的数据定义语言 （LanguageManual DDL;)） 重点 hive 的建表语句和分区。 （2）创建/删除/修改/使用数据库 创建数据库 （Hive搭建完毕后，会创建一个默认的数据库） 查看 show databases； CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment]; 举例： create database attribute; create database attr; 注意：创建数据时，数据库名不要和系统关键字冲突，否则会报错； 如下： 123456789101112131415161718192021命令：hive&gt; create database out;报错：FAILED: ParseException line 1:16 Failed to recognize predicate 'out'. Failed rule: 'identifier' in create database statement原因：在Hive1.2.0版本开始增加了如下配置选项，默认值为true：hive.support.sql11.reserved.keywords该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。解决：法一：弃用这个关键字，换个名字法二：弃用对保留关键字的支持在conf下的hive-site.xml配置文件中修改配置选项：&lt;property&gt; &lt;name&gt;hive.support.sql11.reserved.keywords&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 删除数据库 DROP (DATABASE|SCHEMA) [IF EXISTS] database_name; 举例： drop database attribute; 修改数据库(了解) ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, …); ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; 使用数据库 （进入某一数据库。如果没有这步操作，会进入默认default数据库） USE database_name; 举例： use attr； （3）创建/删除/表（重点） 创建表（重要！） 数据类型： data_type : primitive_type 原始数据类型 | array_type 数组 | map_type map | struct_type | union_type – (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION | STRING 基本可以搞定一切 | BINARY | TIMESTAMP | DECIMAL | DECIMAL(precision, scale) | DATE | VARCHAR | CHAR array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], …&gt; union_type : UNIONTYPE &lt; data_type, data_type, … &gt; 1、准备数据 1231,zshang,18,game-girl-book,stu_addr:beijing-work_addr:shanghai2,lishi,16,shop-boy-book,stu_addr:hunan-work_addr:shanghai3,wang2mazi,20,fangniu-eat,stu_addr:shanghai-work_addr:tianjing 2、创建表 (如果没有指定进入某一数据库，就会在默认数据库中创建) 1234567891011create table log( id int, name string, age int, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; lines terminated by &apos;\n&apos;; 导入数据（属于DML但是为了演示需要在此应用） 123456LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [LOCAL]:从本地 | 若无，则为从HDFS [OVERWRITE] ： 会覆盖Hive表中的数据 | 若无则会追加 [PARTITION....] ： 创建分区 将log1文件中的数据填加到log表中 （log1中数据的格式要和log表格式保持一致，否则会乱；若文件已存在，则会自动重命名） 本地加载（相当于复制）数据到Hive的制定表中 12&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log;&gt; HDFS加载（相当于剪切）数据到Hive的制定表中 12&gt; LOAD DATA INPATH &apos;/root/su/log1&apos; INTO TABLE log ;&gt; 查看表中数据 1234567&gt; 对本表查询不会产生MapReduce任务&gt; hive&gt; select * from log;&gt; 使用函数查询会产生MapReduce任务&gt; hive&gt; select count(*) from log;&gt; 查询表的字段信息&gt; hive&gt; desc log;&gt; 第一个查询结果： 12341 zshang 18 [&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;1 zhaoliu 18 [&quot;game&quot;,&quot;girl&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;beijing&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;2 lishi 16 [&quot;shop&quot;,&quot;boy&quot;,&quot;book&quot;] &#123;&quot;stu_addr&quot;:&quot;hunan&quot;,&quot;work_addr&quot;:&quot;shanghai&quot;&#125;3 wang2mazi 20 [&quot;fangniu&quot;,&quot;eat&quot;] &#123;&quot;stu_addr&quot;:&quot;shanghai&quot;,&quot;work_addr&quot;:&quot;tianjing&quot;&#125; 第二个查询结果： 14 附加题 查询表中likes字段中有girl的人 1hive&gt; select name from log2 where likes[1]=&quot;girl&quot;; 查询表中address字段有stu_addr为beijing的人 1hive&gt; select name from log2 where address[&quot;stu_addr&quot;]=&quot;beijing&quot;; 3、删除表 12&gt; DROP TABLE [IF EXISTS] table_name [PURGE];&gt; 举例： （用drop命令删除表，会将表中数据一并删除，其对应在MySQl中的表的元数据信息也会随之删除； ​ 用hdfs命令删除表对应的文件目录，表中数据也一并删除，但其元数据信息依然保存在My SQL上， ​ 再load数据，可恢复该表） 12&gt; drop table log1；&gt; 123456&gt; hdfs dfs -rmr /user/hive_local/warehouse/attr.db/log1&gt; &gt; hive&gt; use attr;&gt; hive&gt; LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1;&gt; &gt; 创建外部表（重要） 外部关键字EXTERNAL允许您创建一个表,并提供一个位置,以便hive不使用这个表的默认位置。这方便如果你已经生成了数据，当删除一个外部表,表中的数据不会从文件系统中删除。外部表指向任何HDFS的存储位置,而不是存储在配置属性指定的文件夹 hive.metastore.warehouse.dir;).中 创建表： 1234567891011create EXTERNAL table log1( id int, name string, age int, likes array&lt;string&gt;, address map&lt;string,string&gt; ) row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; lines terminated by &apos;\n&apos;; 加载数据： 1LOAD DATA LOCAL INPATH &apos;/root/su/log1&apos; INTO TABLE log1; 删除外部表（相当于删除的是表的元数据信息，而表中的数据还保存） 1drop table log1； 结果： hive&gt; show tables; 无log1 MySQl中也无此表元数据信息 但是， 在HDFS文件系统中，此表数据依然存在 也就是说，词表还可以恢复 恢复表： 1重新创建log1表，该表即可恢复 （4）修改表,更新，删除数据(这些很少用)重命名表 1234&gt; ALTER TABLE table_name RENAME TO new_table_name;&gt; &gt; Eg: alter table meninem rename to jacke;&gt; 更新数据 1UPDATE tablename SET column = value [, column = value ...][WHERE expression] 删除数据 1DELETE FROM tablename [WHERE expression] 2、DML语句（数据库管理语言）（1）具体参见：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML 重点是数据加载和查询插入语法 Hive数据操作语言（LanguageManual DML;)） （2）四种插入/导入数据(重要) Hive不能很好的支持用insert语句一条一条的进行插入操作，不支持update操作。数据是以load的方式加载到建立好的表中。数据一旦导入就不可以修改。 1234567create table log3( id int, name string, age int ) row format delimited fields terminated by &apos;,&apos; lines terminated by &apos;\n&apos;; 1.直接加载数据12LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]load data local inpath &apos;/root/su/log1&apos; into table log1; 2.将表1查询结果插入表2注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 1234567创建person2表，然后从表person1查询数据导入：覆盖：INSERT OVERWRITE TABLE person2 [PARTITION(dt=&apos;2008-06-08&apos;, country)] SELECT id,name, age From ppt;追加：INSERT INTO TABLE log3 SELECT id,name, age From log; 3.将表1、表2查询结果插入表3、表4注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 1234567891011121314FROM person t1INSERT OVERWRITE | INTO TABLE person1 [PARTITION(dt=&apos;2008-06-08&apos;, country)] SELECT t1.id, t1.name, t1.age ; FROM log t1,log1 t2 INSERT OVERWRITE TABLE log4 SELECT t1.id,t1.name,t2.age ; 是否存在笛卡尔积：？？？？存在。 为了防止笛卡尔积： FROM log t1,log1 t2 INSERT OVERWRITE TABLE log4 SELECT t1.id,t1.name,t2.age where t1.id =t2.id; 1234【from放前面好处就是后面可以插入多条语句 】FROM abc t1,sufei t2 INSERT OVERWRITE TABLE qidu SELECT t1.id,t1.name,t1.age,t2.likes,t2.address ; 12345FROM abc t1,sufei t2 INSERT OVERWRITE TABLE qidu SELECT t1.id,t1.name,t1.age,t1.likes,t1.address where…INSERT OVERWRITE TABLE wbb SELECT t2.id,t2.name,t2.age,t2.likes,t2.address where…; 4.直接列出数据插入表中（大量数据时不推荐）注意：查询结果的字段个数、类型 要与插入的表的字段一 一匹配对应 12INSERT INTO TABLE students VALUES (1,&apos;zs&apos;,18,&apos;boy&apos;,&apos;beijng&apos;),(2,&apos;wh&apos;,&apos;girl&apos;,&apos;stu_addr&apos;:shanghai&apos;); 本地load数据和从HDFS上load加载数据的过程有什么区别？ 本地： local 会自动复制到HDFS上的hive的**目录下 Hdfs导入 后移动到hive的**目录下 （3）查询数据并保存 保存数据到本地： 123456789101112insert overwrite local directory &apos;/opt/datas/hive_exp_emp2&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; select * from db_1128.emp ;留意两种的区别：保存的数据格式insert overwrite local directory &apos;/sun/temp/hive_save1&apos; row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; select * from log2 ; 这里如果将 overwrite 改为into 会报错。 12//查看数据!cat /sun/temp/hive_save1/000000_0; 保存数据到HDFS上： 12345678910insert overwrite directory &apos;/user/beifeng/hive/hive_exp_emp&apos; select * from db_1128.emp ;insert overwrite directory &apos;/sun/hive/temp/hive_save1&apos; row format delimited fields terminated by &apos;,&apos; COLLECTION ITEMS TERMINATED by &apos;-&apos; map keys terminated by &apos;:&apos; select * from log2 ; 这里如果将 overwrite 改为into 会报错。 在外部shell中将数据重定向到文件中： 123(注意：需要指明是哪个数据库的表)# hive -e &quot;select * from attr.log;&quot; &gt; /sun/hive/temp/hive_save2# cat /sun/hive/temp/hive_save2 （4）备份数据或还原数据（在HDFS上） 备份数据（包括表的元数据和表中的数据）： 1EXPORT TABLE log to &apos;/sun/hive/datas/export/cp1&apos; 删除再还原数据： 12345先删除表。drop table log;show tables ;再还原数据：IMPORT FROM &apos;/sun/hive/datas/export/cp1&apos; ; （5）其他Hql操作Hive的group by\join(left join right join等)\having\sort by \order by等操作和MySQL没有什么大的区别：http://www.2cto.com/kf/201609/545560.html 3、Hive SerDe（序列化、反序列化）(1)定义Hive SerDe - Serializer and Deserializer SerDe 用于做序列化和反序列化。 构建在数据存储和执行引擎之间，对两者实现解耦。 对数据实现序列化，清洗数据，使之成为有效数据并加载。 Hive通过ROW FORMAT DELIMITED以及SERDE进行内容的读写。 （2）实现1234567row_format: DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] : SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 1234567891011121314151617181920212223242526272829303132Hive正则匹配（实现数据清洗）创建表 logtbl： CREATE TABLE logtbl ( host STRING, identity STRING, t_user STRING, time STRING, request STRING, referer STRING, agent STRING) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos; WITH SERDEPROPERTIES ( &quot;input.regex&quot;=&quot;([^ ]*) ([^ ]*) ([^ ]*) \\[(.*)\\] \&quot;(.*)\&quot; (-|[0-9]*) (-|[0-9]*)&quot;) STORED AS TEXTFILE; 加载数据:load data local inpath &apos;/root/su/localhost_access_log.2016-02-29&apos; into table logtbl;查看数据：select * from logtbl;显示：192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /bg-upper.png HTTP/1.1 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /bg-nav.png HTTP/1.1 304 -192.168.57.4 - - 29/Feb/2016:18:14:35 +0800 GET /asf-logo.png HTTP/1.1 304 -...(省略。。。) 表数据见数据文件：localhost_access_log.2016-02-29.txt 12345678910111213141516171819202122192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:35 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /asf-logo.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-middle.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-nav.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET / HTTP/1.1&quot; 200 11217192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.css HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /tomcat.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-button.png HTTP/1.1&quot; 304 -192.168.57.4 - - [29/Feb/2016:18:14:36 +0800] &quot;GET /bg-upper.png HTTP/1.1&quot; 304 - 五、Beeline和Hiveserver2（Hive的升级）1、Hiveserver2直接启动（只能在服务端启动，相当于服务端）1# ./hiveserver2 若已经配置环境变量则启动方式为： 1# hivesever2 2、启动 beeline（可在服务端|客户端启动，相当于客户端） 因为beeline是在Hive安装目录的/bin下，所以只要有hive包都可以启动 1234567891011121314151617181920212223242526272829303132333435363738# ./beelinebeeline&gt; !connect jdbc:hive2://node00:10000 root 123456显示：Connecting to jdbc:hive2://node00:10000Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://node00:10000&gt;使用：列出数据库0: jdbc:hive2://node00:10000&gt; show databases;+----------------+--+| database_name |+----------------+--+| attr || attribute || default |+----------------+--+3 rows selected (7.493 seconds)0: jdbc:hive2://node00:10000&gt;而在服务端：显示：[root@node00 ~]# hiveserver219/01/07 08:52:09 WARN conf.HiveConf: HiveConf of name hive.metastore.local does not existOKOKOKOK退出：服务端：ctrl + c客户端：！quit； 或 ctrl + c作用：对操作结果添加了美化。不过不太常用，耗内存，数据大的时候，还影响页面。 六、Hive的JDBC 一般是平台使用展示或接口，服务端启动hiveserver2后，在java代码中通过调用hive的jdbc访问默认端口10000进行连接、访问 12345678910111213141516171819public class HivejdbcClient &#123; private static String driverName = "org.apache.hive.jdbc.HiveDriver"; public static void main(String[] args)&#123; try&#123; Class.forName(driverName); &#125;catch (ClassNotFoundException)&#123; e.printStackTrace(); System.exit(1); &#125;// repalace "hive" here with the name of user the queries should run as Connection con = DriverManager.getConnection("jdbc:hive2://node00:10000/default","root","123456"); Statement stmt = con.createStatement(); String sql = "select * from log limit 0"; ResultSet rs = stmt.executeQuery(sql); while(rs.next())&#123; System.out.println(rs.getInt(1)+"-"+rs.getString("name")); &#125; &#125;&#125; 七、Hive分区与自定义函数UDF UDAF UDTF1、Hive的分区partition（重要） 功能： 为了方便海量数据的管理和查询，可以对数据建立分区（可按日期、部门、类型等具体业务）。进行分门别类的管理。 注意： 必须在定义表的时候创建partition分区 存储数据时，添加分区字段的数据，直接将数据按分区进行存储。 添加分区时： ​ 时间的格式：/ ： 存储时会乱码，用 - 不会。 ​ 需要指定分区 ​ 多个分区时，存在父子目录关系，按顺序对应，对应父子 ​ 创建表时，已经指定分区个数，就只能填加指定个数的字段数据 删除分区时： ​ 若该分区是父分区的最后一个子区，则父分区也会被删除 ​ 若删除父分区，其所有子分区也都会备删除 ​ 若删除的分区，分别在多个不同父分区中存在，则都会被删除 重命名分区时： ​ 修改之后的名字不能是已经存在的 注意：在创建 删除多分区等操作时一定要注意分区的先后顺序，他们是父子节点的关系。分区字段不要和表字段相同 类别： 单分区和多分区 静态分区和动态分区 （1）创建分区 单分区建表 123456create table day_table(id int, content string) partitioned by (dt string) row format delimited fields terminated by &apos;,&apos;; 注意：【单分区表，按天分区，在表结构中存在id，content，dt三列；以dt为文件夹区分】 双分区建表 123456create table day_hour_table (id int,content string) partitioned by (dt string, hour string) row format delimited fields terminated by &apos;,&apos;; 注意： 【双分区表，按天和小时分区，在表结构中新增加了dt和hour两列；先以dt为文件夹，再以hour子文件夹区分】 （2）添加分区表的分区（表已创建，在此基础上添加分区：按什么分区）： 注意：报错：此时添加，要注意分区的个数相对应，否则会报错： 1FAILED: ValidationFailureSemanticException Partition spec &#123;dt=2008-08-08, hour=08&#125; contains non-partition columns 注意：报错此时添加，要注意分区的字段名要对应添加，否则会保如下错误： 1FAILED: ValidationFailureSemanticException Partition spec &#123;d=2008-08-08&#125; contains non-partition columns 注意：一定是存在分区，才可添加 添加分区： 12ALTER TABLE table_nameADD partition_spec [ LOCATION &apos;location1&apos; ] partition_spec [ LOCATION &apos;location2&apos; ] ... 123例： ALTER TABLE day_table ADD PARTITION (dt=&apos;2028-08-08&apos;, hour=&apos;08&apos;);ALTER TABLE day_table ADD PARTITION (dt=&apos;2028-08-08&apos;); （3）删除分区语法：（– 用户可以用 ALTER TABLE DROP PARTITION 来删除分区。分区的元数据和数据将被一并删除。） 删除如双分区中的子级分区时，如果仅剩一个子分区，那么父级分区也会被删除。（连坐） 1ALTER TABLE table_name DROP partition_spec, partition_spec,... 1234例：ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);ALTER TABLE day_hour_table DROP PARTITION (dt=&apos;2008-08-08&apos;); （4）数据加载进分区表中语法： 1LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1,partcol2=val2 ...)] 12345HDFS：LOAD DATA INPATH &apos;/user/pv.txt&apos; INTO TABLE day_hour_table PARTITION(dt=&apos;2008-08-08&apos;, hour=&apos;08&apos;);本地：LOAD DATA local INPATH &apos;/user/hua/*&apos; INTO TABLE day_hour partition(dt=&apos;2010-07-07&apos;); （5）查看表的所有分区123hive&gt; show partitions day_hour_table;show partitions day_table; （6）重命名分区语法： 1ALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec; 12例：ALTER TABLE day_table PARTITION (tian=&apos;2018-05-01&apos;) RENAME TO PARTITION (tain=&apos;2018-06-01&apos;); Hive的函数课参考官网，用时查阅即可： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF （7）动态分区(重要)–注意外部表 在本地文件/home/grid/a.txt中写入以下4行数据 aaa,US,CA aaa,US,CB bbb,CA,BB bbb,CA,BC 建立非分区表并加载数据 创建表 123456CREATE TABLE info1 ( name STRING, cty STRING, st STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 加载数据 1LOAD DATA LOCAL INPATH &apos;/root/su/a&apos; INTO TABLE info1; 查看 1SELECT * FROM info1; 建立外部分区表并动态加载数据 （注意删除外部表的相关事项） 123456&gt; CREATE EXTERNAL TABLE info2 (&gt; name STRING&gt; ) &gt; PARTITIONED BY (country STRING, state STRING); &gt; &gt; 这时候就需要使用动态分区来实现，使用动态分区需要注意设定以下参数： hive.exec.dynamic.partition 默认值：false 是否开启动态分区功能，默认false关闭。 使用动态分区时候，该参数必须设置成true; hive.exec.dynamic.partition.mode 默认值：strict 动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。 一般需要设置为nonstrict hive.exec.max.dynamic.partitions.pernode 默认值：100 在每个执行MR的节点上，最大可以创建多少个动态分区。 该参数需要根据实际的数据来设定。 比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。 hive.exec.max.dynamic.partitions 默认值：1000 在所有执行MR的节点上，最大一共可以创建多少个动态分区。 同上参数解释。 hive.exec.max.created.files 默认值：100000 整个MR Job中，最大可以创建多少个HDFS文件。 一般默认值足够了，除非你的数据量非常大，需要创建的文件数大于100000，可根据实际情况加以调整。 hive.error.on.empty.partition 默认值：false 当有空分区生成时，是否抛出异常。 一般不需要设置。 1234567891011set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; set hive.exec.max.dynamic.partitions.pernode=1000; INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; INSERT INTO TABLE info2 PARTITION (country, state) SELECT name, cty, st FROM info1; SELECT * FROM info2; 2、自定义函数UDF UDAF UDTF 自定义函数包括三种 UDF、UDAF、UDTF UDF：一进一出 UDAF：聚集函数，多进一出。如：Count/max/min UDTF：一进多出，如 lateralview explore()，（类似于mysql中的视图） 使用方式 ：在HIVE会话中add自定义函数的jar 文件，然后创建 function 继而使用函数 （1）UDF 开发（用的多一点）1、UDF函数可以直接应用于 select 语句，对查询结构做格式化处理后，再输出内容。 2、编写 UDF 函数的时候需要注意一下几点： a）自定义 UDF 需要继承 org.apache.hadoop.hive.ql.UDF。 b）需要实现 evaluate 函数，evaluate 函数支持重载。 3、步骤 a）把程序打包放到目标机器上去； （需要hive和hadoop，jdk 的相关jar包） 函数一：脱敏处理 123456789101112131415161718192021222324package com.bigdata.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public class TuoMing extends UDF &#123; private Text res = new Text(); public Text evaluate(String string) &#123; // 校验参数是否为空 if(string==null)&#123; return null; &#125; // 若为单个字符 if(string.length()==1)&#123; res.set("*"); &#125; String str1 = string.substring(0,1); String str2 = string.substring(string.length()-1,string.length()); res.set(str1+"***"+str2); return res; &#125; &#125; 函数二：add函数 12345678910111213141516171819202122package com.bigdata.hive.udf;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text;public class Add extends UDF &#123; private Text res = new Text(); public Text evaluate(String num1,String num2) &#123; // 校验参数是否为空 if(num1==null)&#123; return null; &#125;else if(num2==null)&#123; res.set(num1); return res; &#125; int n = Integer.parseInt(num1)+Integer.parseInt(num2); String str =n+""; res.set(str); return res; &#125; &#125; b）进入 hive 客户端，添加 jar 包 1234hive&gt;add jar /root/su/TuoMing.jar;(相当于添加到环境变量中)(清除缓存时记得删除jar包： delete jar /*)delete jar /jar/udf_test.jar; c）创建临时函数： 123hive&gt;CREATE TEMPORARY FUNCTION add_example AS &apos;hive.udf.add&apos;;CREATE TEMPORARY FUNCTION tm_example AS &apos;com.bigdata.hive.udf.TuoMing&apos;;（as 后面添加的是：包名+类名） d）查询 HQL 语句： 12345SELECT add_example(8, 9) FROM scores;SELECT add_example(scores.math, scores.art) FROM scores;SELECT tm_example(id) FROM log; e）销毁临时函数： 1hive&gt; DROP TEMPORARY FUNCTION tm_example; （2）UDAF自定义集函数(用的少) 多行进一行出，如 sum()、min()，用在 group by 时 1.必须继承org.apache.hadoop.hive.ql.exec.UDAF(函数类继承) org.apache.hadoop.hive.ql.exec.UDAFEvaluator(内部类 Eval uator 实现 UDAFEvaluator 接口) 2.Evaluator 需要实现 init、iterate、terminatePartial、merge、terminate 这几个函数 init():类似于构造函数，用于 UDAF 的初始化 iterate():接收传入的参数，并进行内部的轮转，返回 boolean terminatePartial():无参数，其为 iterate 函数轮转结束后，返回轮转数据， 类似于 hadoop 的Combinermerge():接收 terminatePartial 的返回结果，进行数据 merge 操作， ​ 其返回类型为 boolean terminate():返回最终的聚集函数结果 开发一个功能同： Oracle 的 wm_concat()函数 Mysql 的 group_concat() Hive UDF 的数据类型： Hive UDF 的数据类型： （3）UDTF（用的少一点）UDTF：一进多出，如 lateral view explode( ) 返回一个数组表 Hive Lateral View 视图 Lateral View用于和UDTF函数（explode、split）结合来使用。 首先通过UDTF函数拆分成多行，再将多行结果组合成一个支持别名的虚拟表。 主要解决 在select使用UDTF做查询过程中，查询只能包含单个UDTF，不能包含其他字段、以及多个UDTF的问题 语法： LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,’ columnAlias) 例： 统计人员表中共有多少种爱好、多少个城市? 1234&gt; select count(distinct(myCol1)), count(distinct(myCol2))，count(distinct(myCol3))from log2 &gt; LATERAL VIEW explode(likes) myTable1 AS myCol1 &gt; LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3;&gt; 123select myCol1, myCol2 from log2 LATERAL VIEW explode(likes) myTable1 AS myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; distinct(myCol1) 表示去重 LATERAL VIEW explode(likes) myTable1 AS myCol1 将likes查询结果放到mytable1表中，作为字段myCol1 LATERAL VIEW explode(address) myTable2 AS myCol2, myCol3; 将address查询结果放到myTable2 表中，作为字段myCol2，myCol3，因为address是包含K-V的（两个） 八、Hive索引(知道) 一个表上创建索引： 使用给定的列表的列作为键创建一个索引。 详见创建索引;)设计文档。 12345678910111213CREATE INDEX index_name ON TABLE base_table_name (col_name, ...) AS index_type [WITH DEFERRED REBUILD] [IDXPROPERTIES (property_name=property_value, ...)] [IN TABLE index_table_name] [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] [LOCATION hdfs_path] [TBLPROPERTIES (...)] [COMMENT &quot;index comment&quot;]; 九、案例实践案例一：(基站掉话率)基站掉话率 1、创建表cell_monitor表 1234567891011121314create table cell_monitor( record_time string, imei string, cell string, ph_num int, call_num int, drop_num int, duration int, drop_rate DOUBLE, net_type string, erl string)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;STORED AS TEXTFILE; 结果表cell_drop_monitor 12345678create table cell_drop_monitor(imei string,total_call_num int,total_drop_num int,d_rate DOUBLE) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;STORED AS TEXTFILE; 2、load数据1LOAD DATA LOCAL INPATH &apos;/root/su/cdr_summ_imei_cell_info.csv&apos; OVERWRITE INTO TABLE cell_monitor; 3、找出掉线率最高的基站12345from cell_monitor cm insert overwrite table cell_drop_monitor select cm.imei ,sum(cm.drop_num),sum(cm.duration),sum(cm.drop_num)/sum(cm.duration) d_rate group by cm.imei sort by d_rate desc; 案例二：（单词统计）1、建表12create table docs(line string);create table wc(word string, totalword int); 2、加载数据1load data local inpath &apos;/tmp/wc&apos; into table docs; 3、统计12345from (select explode(split(line, &apos; &apos;)) as word from docs) w insert into table wc select word, count(1) as totalword group by word order by word; 4、查询结果1select * from wc; 十、分桶（重要）1、概念 主要应用于数据抽样。 通过对列值取哈希值的方式，将不同数据放到不同的文件中存储。 对Hive中每个表、分区都可以进行分桶。 列的哈希值 /桶的个数→决定每条数据划分到哪个桶中 2、开启支持分桶1hive&gt; set hive.enforce.bucketing=true; 默认：false； 设置为true之后，mr运行时会根据bucket的个数自动分配reduce task个数。 （用户也可以通过mapred.reduce.tasks自己设置reduce任务个数，但分桶时不推荐使用） 一次作业产生的桶数 = reducde task数 3、往分桶表中加载数据12insert into table bucket_table select columns from tbl;insert overwrite table bucket_table select columns from tbl; 4、桶表抽样查询1select * from bucket_table tablesample(bucket 1 out of 4 on columns); TABLESAMPLE语法： 12&gt; TABLESAMPLE(BUCKET x OUT OF y)&gt; x：表示从哪个bucket开始抽取数据，x&lt;=y y：必须为该表总bucket数的倍数或因子 理解： 分桶表已经按age分为4桶，然后，有y个人去抽，从第(x 取模 桶数)桶中抽 5、实战创建普通表 123456CREATE TABLE mm( id INT, name STRING, age INT)ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 测试数据 123456781,tom,112,cat,223,dog,334,hive,445,hbase,556,mr,667,alice,778,scala,88 加载数据： 1load data local inpath &apos;/root/su/mm&apos; into table mm; 创建分桶表 1234567CREATE TABLE psnbucket( id INT, name STRING, age INT)CLUSTERED BY (age) INTO 4 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos;; 加载数据： 1insert into table psnbucket select id, name, age from mm; 抽样 1select id, name, age from psnbucket tablesample(bucket 2 out of 4 on age); 注意： hive&gt; select id, name, age from psnbucket tablesample(bucket 4 out of 2 on age);FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table psnbucket denominator : 分母 十一、运行方式1、Hive运行模式 – 命令行方式cli：控制台模式 1234567--与hdfs交互 * 执行dfs命令 * 例 ：hive&gt; dfs -ls / --与Linux交互 * ！ 开头 * hive&gt; !pwd –脚本运行方式：（生产中常用） 123456789101112131415161718192021222324在外部shell中执行,指定数据库,分号可加可不加# hive -e &quot;select * from attr.log &quot;# hive -e &quot;select * from attr.log；select * from default.log2&quot;--------------------------------------------------------------将执行结果重定向到指定文件：# hive -e &quot;select * from attr.log &quot; &gt;&gt;log1--------------------------------------------------------------静默模式执行，不打印log日志# hive -S -e &quot;select * from attr.log &quot; &gt;&gt;log1--------------------------------------------------------------脚本执行先编辑脚本问价# vim file1编辑内容select * from attr.log where id = 1;select * from attr.log where id &lt; 3;执行脚本# hive -f file1--------------------------------------------------------------?? 使用命令文件执行hive-init.sql?? # hive -i /home/hive-init.sql--------------------------------------------------------------在hive cli中执行脚本文件hive&gt; source file1 ？未解决？ ?? 使用命令文件执行hive-init.sql?? # hive -i /home/hive-init.sql 十二、hive的GUI接口（web页面）Hive Web GUI接口 web界面安装：1、下载源码包apache-hive-1.2.1-src.tar.gz, 2、在本地Windows系统中解压 并将\apache-hive-1.2.1-src\hwi\web路径中所有的文件打成war包 制作方法： war包 1、到\apache-hive-1.2.1-src\hwi\web路径下 2、在路径栏输入命令：jar -cvf hive-hwi.war * 3、即可生成文件：hive-hwi.war 3、将hwi-war包放在$HIVE_HOME/lib/中（Linux系统） 4、复制tools.jar(在jdk的lib目录下)到$HIVE_HOME/lib下 5、修改hive-site.xml 路径：/usr/soft/apache-hive-1.2.1-bin/conf/hive-site.xml 123456789101112&lt;property&gt; &lt;name&gt;hive.hwi.listen.host&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.port&lt;/name&gt; &lt;value&gt;9999&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;lib/hive-hwi.war&lt;/value&gt; &lt;/property&gt; 6、启动hwi服务(端口号9999) 1hive --service hwi 7、浏览器通过以下链接来访问 http://node00:9999/hwi/ 8、登录页面： USER: GROUPS: 自已定义 十三、权限管理Hive - SQL Standards Based Authorization in HiveServer2 （1）三种授权模型 （2）常用：基于SQL标准的完全兼容SQL的授权模型特点： 支持对于用户的授权认证 支持角色role的授权认证 role可理解为是一组权限的集合，通过role为用户授权 一个用户可以具有一个或多个角色 ​ 默认包含俩种角色：public、admin 限制 （3）操作在hive服务端修改配置文件hive-site.xml添加以下配置内容： 1234567891011121314151617181920&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.enable.doAs&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.users.in.admin.role&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator&lt;/value&gt;&lt;/property&gt; 服务端启动hiveserver2；客户端通过beeline进行连接 角色的添加、删除、查看、设置： 第一次操作无权限： 需要：CREATE ROLE admin； 12345CREATE ROLE role_name; -- 创建角色DROP ROLE role_name; -- 删除角色SET ROLE (role_name|ALL|NONE); -- 设置角色SHOW CURRENT ROLES; -- 查看当前具有的角色SHOW ROLES; -- 查看所有存在的角色 【官网：权限】 Action Select Insert Update Delete Owership Admin URL Privilege(RWX Permission + Ownership) ALTER DATABASE Y ALTER INDEX PROPERTIES Y ALTER INDEX REBUILD Y ALTER PARTITION LOCATION Y Y (for new partition location) ALTER TABLE (all of them except the ones above) Y ALTER TABLE ADD PARTITION Y Y (for partition location) ALTER TABLE DROP PARTITION Y ALTER TABLE LOCATION Y Y (for new location) ALTER VIEW PROPERTIES Y ALTER VIEW RENAME Y ANALYZE TABLE Y Y CREATE DATABASE Y (if custom location specified) CREATE FUNCTION Y CREATE INDEX Y (of table) CREATE MACRO Y CREATE TABLE Y (of database) Y (for create external table – the location) CREATE TABLE AS SELECT Y (of input) Y (of database) CREATE VIEW Y + G DELETE Y DESCRIBE TABLE Y DROP DATABASE Y DROP FUNCTION Y DROP INDEX Y DROP MACRO Y DROP TABLE Y DROP VIEW Y DROP VIEW PROPERTIES Y EXPLAIN Y INSERT Y Y (for OVERWRITE) LOAD Y (output) Y (output) Y (input location) MSCK (metastore check) Y SELECT Y SHOW COLUMNS Y SHOW CREATE TABLE Y+G SHOW PARTITIONS Y SHOW TABLE PROPERTIES Y SHOW TABLE STATUS Y TRUNCATE TABLE Y UPDATE Y 十四、Hive优化（重点）详见Hive优化文档 HIve常用函数： https://www.cnblogs.com/kimbo/p/6288516.html https://www.iteblog.com/archives/2258.html#3_avg https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inFunctions MapReducde底层源码： http://note.youdao.com/noteshare?id=212e4a69d7bf8fc30979f1e4fc39ff0f&amp;sub=EA7C15DC72DF45248721CE2AD3F93CDD http://note.youdao.com/noteshare?id=86ca5c96d13413f789164ff92f9ab4f9&amp;sub=7F20006D1D714D77AEDE5242603786C1 http://note.youdao.com/noteshare?id=a518dfed10d824b0995380669ddd28c9&amp;sub=3526F532CDAA49628FEA1FE3A61239F3 http://note.youdao.com/noteshare?id=95d458d779f8e9f391d6ea06b6c6d122&amp;sub=0A13B59E10D94C96ABF5388CE4EB89D4]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 入门学习（二）]]></title>
    <url>%2F2019%2F01%2F11%2FLinux%20%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%9B%9E%E5%90%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2018年12月18日 周二 晴 今日学习要点： [TOC] Linux后半程一、Linux系统配置1.主机名配置： vim /etc/sysconfig/network 1545214876278 配置完成之后需要重启机器才能生效 reboot 2.DNS配置 查看DNS服务器的地址cat /etc/resolv.conf 修改DNS服务器地址方式一：vim /etc/sysconfig/network.scripts/ifconfig-eth0 ​ 在配置网关时，配置DNS1=114.114.114.114（不推荐，江苏南京的IP） 方式二：vim /etc/resolv.conf （用本地网关解析） ​ nameserver 192.168.198.0 ( 此为虚拟机中的网关地址) 3.环境变量 配置系统环境变量，使得某些命令在执行时，系统可以找到命令对应的执行程序，命令才能正常执行。 查看系统一共在哪些目录里寻找命令对应的程序 命令：echo $PATH 1545215894631 注意：路径之间有冒号隔开，系统会从左往右依次寻找对应的程序 ​ 一般命令会存放在 bin目录，或sbin目录 配置全局环境变量： vim /etc/profile 在文件中： PATH=$PATH:(命令所在目录) 退出文件编辑后： source /etc/profile (重新加载资源，有的可能需要重启机器，这不适用于实际状况) 配置局部环境变量：（推荐，限当前登录用户使用） 查看所有文件(root目录下) ls -a (发现隐藏文件 .bash.profile) vim ~/ bash_profile 在文件中： export PATH =$PATH:(命令所在目录) 4.拍快照（保存当时计算机所出状态的各种配置和资源，适度使用） 选中指定虚拟计算机——鼠标右击—–选中“快照” ——“拍摄快照‘—-在页面中找到”拍摄快照“，并添加名称和描述 也可以删除，找到页面中的删除按钮 二、服务操作1、查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。 命令：chkconfig 1545219619458 12345678各数字代表的系统初始化级别： 0：停机状态 1：单用户模式，root账户进行操作 2：多用户，不能使用net file system，一般很少用 3：完全多用户，一部分启动，一部分不启动，命令行界面 4：未使用、未定义的保留模式 5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。 6：停止所有进程，卸载文件系统，重新启动(reboot) 1、2、4很少用，0、3、5、6常用，3级别和5级别除了桌面相关的进程外没有什么区别，推荐都用3级别； linux默认级别为3； 不要把 /etc/inittab 中 initdefault 设置为0 和 6； 2、服务操作 service 服务名 start/stop/status/restart 举例：对防火墙服务进行操作 防火墙的服务名为：iptables 查看防火墙服务运行状态 service iptables status 关闭防火墙 service iptables stop 开启防火墙 service iptables start 永久开启/关闭防火墙 chkconfig iptables on/off 3、服务初执行等级更改 chkconfig –level 2345 name off|on​ （ 服务名） 举例：防火墙 chkconfig –level 2345 iptables off 若不加级别，默认是2345级别 命令：chkconfig name on|off​ （服务名） 三、linux进程操作1、查看所有进程 命令： ps -aux 12345 -a 列出所有-u 列出用户-x 详细列出，如cpu、内存等 -e select all processes 相当于-a -f does full-format listing 将所有格式详细列出来 查看所有进程里CMD是ssh 的进程信息（包括pid 进程号） 命令： ps - ef | grep ssh （| 管道符 ：前一个输出，变为后一个的输入） 举例： ps -ef | grep redis 2、杀死进程kill 命令：kill pid -9 强制杀死 用法：用ps 命令先查出对应程序的PID或PPID ，然后用kill杀死掉进程。 四、其他常用命令1、yum 基于RPM包管理 能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装 跟换yum下载源（默认是到国外网站下载） 第一步：备份你的原镜像文件，以免出错后可以恢复 cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 第二步：下载新的CentOS-Base.rep到/etc/yum.repos.d/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 下载完之后，查看一下文件内容 vim /etc/yum.repos.d/CentOS-Base.repo 第三步：生成缓存 运行yum makecache 查看当前源 yum list | head -50 2、 wget 一个从网络上自动下载文件的自由工具 支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议，可以使用 HTTP 代理 安装： yum install wget –y 用法： wget [option] 网址 -O 指定下载保存的路径 举例： wget www.baidu.com -O baidu.html 3、tar12345 -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加-x 解压-c 压缩-f 目标文件，压缩文件新命名或解压文件名-v 解压缩过程信息打印 解压命令：tar -zvxf xxxx.tar.gz 压缩命令：tar -zcf 压缩包命名 压缩目标举例： tar -zcf tomcat.tar.gz apache-tomcat-7.0.61将 apache-tomcat-7.0.61 目录压缩成tomcat.tar.gz包 4、man作用：用于查看指定命令的具体解释 安装 yum install man -y (下载并安装man 并确认) 使用 man ps 五、JDK部署1、准备JDK安装包：（这是使用 .rpm 格式的安装包） 官网下载：http://www.oracle.com/technetwork/java/javase/downloads/index.html 云盘资源： jdk-8u191-linux-x64.rpm ： ​ 根据用户喜好放到虚拟机器的文件目录中 2、解压并安装，展示编译过程 rpm -ivh jdk-8u191-linux-x64.rpm 安装放到了 /usr 目录下，有/java目录 3、配置环境变量 vim ~/.bash_profile 在文件中： JAVA_HOME=(jdk文件所在的路径+jdk文件名) export PATH=$PATH:$JAVA_HOME/bin 注意： 新的path路径必须要包含旧的PATH路径，且每个路径之间以冒号隔开，而不是分号 配置完成，退出编辑框后 source ~/.hash_profile 4、测试： java -version 或 echo $JAVA_HOME echo 标准输出，打印 六、Tomcat部署1、官网下载http://tomcat.apache.org/ 云盘资源：apache-tomcat-7.0.61.tar 2、上传并解压 tar -zvxf apache-tomcat-7.0.61.tar 3、启动tomcat在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务 ./startup.sh 4、关闭tomcat服务方式一：可以用shutdown.sh命令 方式二：ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令 5、验证先把防火墙关了（service iptables stop），然后访问虚拟机IP的8080端口]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 入门学习（一）]]></title>
    <url>%2F2019%2F01%2F11%2FLinux%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%2B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0(%E7%AC%AC%E4%B8%80%E5%9B%9E%E5%90%88)%2F</url>
    <content type="text"><![CDATA[Linux网络配置+常用命令学习[TOC] 一、 Linux概述1.1. 简介imgLinux是一个自由的，免费的，源码开放的操作系统。也是开源软件中最著名的例子。其最主要的目的就是为了建立不受任何商品化软件版权制约的，全世界都能使用的类Unix兼容产品.而我们将服务器部署在Linux将会更加的稳定、安全、高效以及出色的性能这时windows无法比的。 1.2.Linux作者img 林纳斯·本纳第克特·托瓦兹（Linus Benedict Torvalds, 1969年~ ），著名的电脑程序员、黑客。Linux内核的发明人及该计划的合作者。托瓦兹利用个人时间及器材创造出了这套当今全球最流行的操作系统（作业系统）内核之一。现受聘于开放源代码开发实验室（OSDL：Open Source Development Labs, Inc），全力开发Linux内核。 1.3.Linux 发行版发行版是基于 Linux 内核的一个操作系统。它带有用户可以使用的软件集合。更多的，它还包含系统管理包。目前有许多 Linux 发行版。因为我们不能数清目前所有的 Linux 发行版，所以我们来看一下一些有名的版本： Ubuntu、Fedora、Opensuse、Red hat Linux 和 Debian 等是几个非常受欢迎的 Linux 发行版。 img C**entos** img img Ubuntu imgimg Rehat img img 1.4.Linux的特点开放性，多用户，多任务，丰富的网络功能，可靠的系统安全，良好的可移植性，具有标准兼容性 二、环境准备2.1. Vmware2.1.1 Vmware简介大多数服务器的容量（CPU,内存，磁盘等）利用率不足 30%，这不仅导致了资源浪费，也加大了服务器的数量。实现服务器虚拟化后，多个操作系统可以作为虚拟机在单台物理服务器上运行，并且每个操作系统都可以访问底层服务器的计算资源，从而解决效率低下问题。 Vmware虚拟机化技术由此诞生，它可以将一台服务器虚拟化出多台虚拟机，供多人同时使用，提高资源利用率。 2.1.2 Vmware workstation安装详细见vmware安装文档 2.2. linux安装详细见Linux安装文档 2.3.网络配置2.3.1 查看网关img img 2.3.2 配置静态IP(NAT模式)1.编辑配置文件,添加修改以下内容 vi /etc/sysconfig/network-scripts/ifcfg-eth0 按i 进入文本编辑模式，出现游标，左下角会出现INSERT,即可以编辑 img 应包含以下配置，除此之外的可以删除掉。 123456789101112131415DEVICE=eth0 #网卡设备名,请勿修改名字TYPE=Ethernet #网络类型，以太网BOOTPROTO=static #启用静态IP地址ONBOOT=yes #开启自动启用网络连接 IPADDR=192.168.78.100 #设置IP地址NETMASK=255.255.255.0 #设置子网掩码GATEWAY=192.168.78.2 #设置网关DNS1=114.114.114.114 #设置备DNS 按ESC退出编辑模式 :wq #保存退出 2.修改完后执行以下命令 service network restart #重启网络连接 ifconfig #查看IP地址 3.验证是否配置成功: 虚拟机能ping通虚拟网关 img 虚拟机与物理机（笔记本）相互可ping通 img 虚拟机与公网上的百度网址相互可ping通（此步ping通，才说明网络配置成功，Ctrl键+C停止） 命令：ping www.baidu.com 注意： a.保证VMware的虚拟网卡没有被禁用img b.网关IP不能被占用 2.4.XShell安装与使用2.4.1安装步骤除了安装路径需要修改，其他一直下一步。 2.4.2 连接虚拟机 打开xshell软件新建一个会话 img 填写所要连接的虚拟机IP，会话名称可改可不改，点击确定。 img 3.连接虚拟机。 img 4．输入root用户名，可以勾选”记住用户名” img 5.填写密码，可以勾选“记住密码” img 6.登录成功。 img 三、文件系统Linux文件系统中的文件是数据的集合，文件系统不仅包含着文件中的数据而且还有文件系统的结构，所有Linux 用户和程序看到的文件、目录、软连接及文件保护信息等都存储在其中。 Linux目录结构： img bin 存放二进制可执行文件(ls,cat,mkdir等) boot 存放用于系统引导时使用的各种文件 dev 用于存放设备文件 etc 存放系统配置文件 home 存放所有用户文件的根目录 lib 存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt 系统管理员安装临时文件系统的安装点 opt 额外安装的可选应用程序包所放置的位置 proc 虚拟文件系统，存放当前内存的映射 root 超级用户目录 sbin 存放二进制可执行文件，只有root才能访问 tmp 用于存放各种临时文件 usr 用于存放系统应用程序，比较重要的目录/usr/local 本地管理员软件安装目录 var 用于存放运行时需要改变数据的文件 3.1目录操作3.1.1**切换目录命令：cd + 目录的路径 查看当前目录的完整路径 ：pwd img 命令 cd .. 返回到父目录 img 3.1.2**新建目录命令：mkdir+ 目录名字 查看当前目录下拥有的子目录和文件: ls img 3.1.3 拷贝目录cp source dest -r img 3.1.4删除目录rmdir directory img 注意：rmdir只能删除空目录,若要删除非空目录则用rm命令 rm -rf dir 3.1.5移动/更改 目录​ 移动文件或目录：mv + 目录/文件名字 + 其他路径 ​ mv test / 将test目录移动到 根目录/ 下 ​ img ​ 更改文件或目录的名字：mv + 旧目录名字 + 新目录名字。 ​ img 3.2.文件操作3.2.1新建文件：（一切皆文件）touch web.log 创建一个空文件。 img 3.2.2 复制文​ cp web.log web_cp.log img 复制文件，加个-r 参数，代表遍历复制，此时可用于复制一个目录。 3.2.3删除文件rm web_cp.log img 此时需要手动输入y ，代表确认删除。可加 –f参数，直接删除，无需确认。当需要一个目录下所有东西时，加-r参数，代表遍历删除。 rm -f web.log img 3.2.4**查看3.2.4.1**查看目录下的东西​ ls / ll 命令 ls -l 等价于 ll img 查看目录下的所有东西（包括隐藏文件） 命令：ls –al 等价于 ll –a img 3.2.4.2查看文件内容cat filename: 一次性显示整个文件的内容 img 注意：当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。 因此，一般用more等命令分屏显示. more filename 该命令一次显示一屏文本，满屏后停下来，并且在屏幕的底部出现一个提示信息，给出至今己显示的该文件的百分比。 按Space键，显示文本的下一屏内容。按Enter键，只显示文本的下一行内容。 按B键，显示上一屏内容。 按Q键，退出。 命令：more /etc/profile img 显示的内容： img ​ less命令 与 more命令 非常类似 less filename: ​ h 显示帮助界面 Q 退出less 命令 u 向后滚动半页 d 向前翻半页 空格键 滚动一页 b 向后翻一页 回车键 滚动一行 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 以及上下键，向上一行，向下一行 3.2.4.3从头打印**文件内容​ head -10 filename 打印文件1到10行 img 3.2.4.4从尾部打印文件内容 tail -10 filename 打印文件最后10行 img 注意：tail 还经常可以拿来查看文件的内容变化 加-f参数，tail –f filename 3.2.5查找文件或目录​ find pathname –name filename ​ 例子：find / -name profile ​ 该命令表示为，在/目录下查找 名字为profile的文件或目录，最后列出它的绝对路径 ​ img ​ 最后发现，linux系统根目录/ 下 一共有两个名字为profile， ​ 其中/etc/profile是一个文件，/etc/lvm/profile为目录 还可以按正则表达式来查找，且pathname越精确，查找的范围越小，速度越快。 ​ find /etc -name pro* 注意：（命令执行时，其查找的目录必须是所在目录的父级目录） ​ 该命令表示为：在/etc目录下查找以pro开头的文件或目录。 img 四、文本编辑4.1.vi编辑模式 vi filename :打开或新建文件，并将光标置于第一行首 vi +n filename ：打开文件，并将光标置于第n行首 vi + filename ：打开文件，并将光标置于最后一行首 vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的串处 • q!：不保存文件并退出vi – 在VI的命令模式下输入“:set nu”，就有行号了。 – 在VI的命令模式下输入“:set nonu”，取消行号。 一般模式 • yy 复制光标所在行(常用) • nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) （不同：在我的xshell中是 yyn实现复制所在行的向下n行） • p,P p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用) G:光标移至第最后一行 nG：光标移动至第N行行首 n（shift）+：光标下移n行 （nB|nb:光标向上移动n行） n-：光标上移n行 H ：光标移至屏幕顶行 M ：光标移至屏幕中间行 L ：光标移至屏幕最后行 • dd：删除 行 x或X：删除一个字符，x删除光标后的，而X删除光标前的 • u 恢复前一个动作(常用) 删除第N行到第M行： :N,Md 4.2.vimVim是从 vi 发展出来的一个文本编辑器。代码补完、语法高亮、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用. 安装vim 软件 yum install vim img 按y确认, 这中间一共要按两次确认 img 可以在书写命令时就加y,这样就不用逐一确认。 yum install vim -y 用vim 打开/etc/profile 文件，会发现编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强 命令：vim /etc/profile img 五、文件传输5.1.远程拷贝5.1.1将本地文件复制到远程机器 举例： 123456789101112&gt; 方式一：&gt; scp -rf /etc/profile root@192.168.198.128:/etc/&gt; &gt; 方式二:&gt; scp -r /etc/profile root@node01 /etc/ &gt; &gt; 方式三：&gt; scp -r /etc/profile node01:/etc/&gt; &gt; 方式四：&gt; scp -r /ec/profile node01:&apos;pwd&apos;&gt; scp ：远程传输文件命令 -r ：- 指的是后面跟的是参数 r 指的是遍历指定文件 f 指的是不用询问 /etc/profile : 是指定传输的文件 root： 远程机器的账户名 @ 远程机器的IP地址 ： /etc/ 远程机器上指定的目录 node01：远程机器的别名 ‘pwd’： 本地要远程传输文件所在的目录 scp local_file remote_username@remote_ip:remote_folder img 第一次远程拷贝时，需要在箭头1初输入yes确认一下，验证一下远程主机。然后在箭头2处输入一下远程主机的密码。 5.1.2将本地目录复制到远程机器scp -r local_folder remote_username@remote_ip:remote_folder img 在test目录下创建一个myfile文件，然后将test目录远程拷贝到192虚拟机的根目录下。 5.2.上传​ 需先安装好lrzsz : yum install lrzsz -y 安装好后，输入上传的命令rz,弹出一下界面： img 选择一个windows系统里的文件上传至虚拟机的当前目录下,然后ll命令，查看结果 img 5.3.下载 下载命令为sz，sz命令只能下载文件，不能是目录，可先将目录压缩成一个包，再下载至windows系统。下载完之后，按ctrl+c结束。 img 5.4 Xftp的安装与使用​ 除了可以用rz sz命令进行本地windows系统和虚拟机之间的文件传输，还可以使用XFTP软件。 六、网络指令6.1.查看网络配置信息命令:ifconfig img 箭头1指向的是本机IP，箭头2为广播地址，箭头3位子网掩码。 6.2.测试与目标主机的连通性命令：ping remote_ip（可以ping通Windows系统的IP） img 输入ping 192.168.78.192代表测试本机和192主机的网络情况， 箭头1表示一共接收到了3个包，箭头2表示丢包率为0，表示两者之间的网络顺畅。 注意：linux系统的ping命令会一直发送数据包，进行测试，除非认为的按ctrl + c停止掉， ​ windows系统默认只会发4个包进行测试，以下为windows的dos命令。 img 6.3.显示各种网络相关信息​ 命令：netstat –a n p t -a (all)显示所有选项，默认不显示LISTEN相关-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-l 仅列出有在 Listen (监听) 的服務状态 -p 显示建立相关链接的程序名-r 显示路由信息，路由表-e 显示扩展信息，例如uid等-s 按各个协议进行统计-c 每隔一个固定时间，执行该netstat命令。 提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到 img 七、系统配置7.1 主机名配置 若要修改主机名字，可在/etc/sysconfig/network文件里修改. vim /etc/sysconfig/network ​ img 机器重启才能生效 7.2 DNS配置 /etc/resolv.conf 为DNS服务器的地址文件 img 7.3 环境变量Linux系统的环境变量是在/etc/profile文件里配置。 首先考虑一个问题，为什么我们先前敲的yum, service,date,useradd等等，可以直接使用，系统怎么知道这些命令对应的程序是放在哪里的呢？ 这是由于无论是windows系统还是linux系统，都有一个叫做path的系统环境变量，当我们在敲命令时，系统会到path对应的目录下寻找，找到的话就会执行，找不到就会报没有这个命令。如下图： img 我们可以查看一下，系统一共在哪些目录里寻找命令对应的程序。 命令：echo $PATH img 可以看到path里有很多路径，路径之间有冒号隔开。当用户敲命令时，系统会从左往右依次寻找对应的程序，有的话则运行该程序，没有的就报错，command not found. 那如果我写了一个脚本（脚本后面会专门讲），我该怎样运行它呢？ img img 对test.sh添加可执行权限，chmod 700 test.sh img 运行方法有三种： 一种是到脚本的目录下执行： 运行命令 ： ./test.sh ,代表执行当前目录里的脚本test.sh img 一种是敲脚本的绝对路径：/usr/test/test.sh img 以上两种运行方式都不是很简便，因为先前我们执行yum service命令等，都是直接敲对应的命令的。所以我们也可以参照这样子做，只要我们配一个环境变量就好。 编辑： vim /etc/profile 将test.sh所在目录添加到PATH里就OK，我这里test.sh是在/usr/test目录（通过pwd查看）下。 img 编辑完之后，执行source /etc/profile命令，重新加载环境变量，此时会发现PATH路径多了一个/usr/test。 img ​ 最后验证一下，直接执行test.sh ​ img 八、服务操作8.1 列出所有服务命令：chkconfig 查询操作系统在每一个执行等级中会执行哪些系统服务，其中包括各类常驻服务。 img 各数字代表的系统初始化级别含义： ​ 0：停机状态 1：单用户模式，root账户进行操作 2：多用户，不能使用net file system，一般很少用 3：完全多用户，一部分启动，一部分不启动，命令行界面 4：未使用、未定义的保留模式 5：图形化，3级别中启动的进程都启动，并且会启动一部分图形界面进程。 6：停止所有进程，卸载文件系统，重新启动(reboot) 这些级别中1、2、4很少用，相对而言0、3、5、6用的会较多。3级别和5级别除了桌面相关的进程外没有什么区别。为了减少资源占用，推荐都用3级别. 注意 ：linux默认级别为3，不要把/etc/inittab 中initdefault 设置为0 和 6 8.2 服务操作service 服务名 start/stop/status/restart 例子：对防火墙服务进行操作，防火墙的服务名为：iptables. ​ 查看防火墙服务运行状态。 img 关闭防火墙. img 开启防火墙 img 8.3 关闭防火墙service iptables start/stop/status 注：学习期间直接把防火墙关掉就是，工作期间也是运维人员来负责防火墙的。 永久开启/关闭防火墙 chkconfig iptables on/off 8.4 服务初执行等级更改chkconfig –level 2345 name off|on ​ （服务名） img 若不加级别，默认是2345级别 命令：chkconfig name on|off ​ （服务名） 九、linux进程操作9.1 查看所有进程命令： ps -aux ​ -a 列出所有 ​ -u 列出用户 ​ -x 详细列出，如cpu、内存等 e ​ -f img 命令： ps - ef | grep ssh 查看所有进程里CMD是ssh 的进程信息。 img 其中箭头所指的是sshd服务进程的进程号（PID） 9.2 杀死进程Kill 用法 kill pid -9：强制杀死 ps 命令先查出对应程序的PID或PPID ，然后杀死掉进程。 img 十、 其他常用命令10.1 yumyum是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 由于centos系统的yum默认是到国外网站下载，有时下载速度会很慢，故我们可以换一个yum的下载源，这里我们换一个国内的下载源 阿里云镜像。 第一步：备份你的原镜像文件，以免出错后可以恢复。 cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup img 第二步：下载新的CentOS-Base.rep到/etc/yum.repos.d/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo img 下载完之后，vim /etc/yum.repos.d/CentOS-Base.repo 查看一下文件内容。 img 第三步：运行yum makecache生成缓存 img 查看当前源 yum list | head -50 10.2 wgetwget 是一个从网络上自动下载文件的自由工具，支持通过 HTTP、HTTPS、FTP 三个最常见的 TCP/IP协议 下载，并可以使用 HTTP 代理 需先安装 yum install wget –y wget用法:wget [option] 网址 -O 指定下载保存的路径 img img wget 工具还可以用来做一些简单的爬虫，这里不是我们的学习重点，如果想做爬虫，可以用java或python语言来做。 img 10.3 tartar ​ -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加 ​ -x 解压 ​ -c 压缩 ​ -f 目标文件，压缩文件新命名或解压文件名 ​ -v 解压缩过程信息打印 解压命令：tar -zvxf xxxx.tar.gz 例子：先用rz命令或wscp上传一个tar包，然后解压。 img img 解压后： img 压缩命令：tar -zcf 压缩包命名 压缩目标 例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61 将 apache-tomcat-7.0.61 目录压缩成tomcat.tar.gz包。 img 十一、JDK部署11.1 官网下载http://www.oracle.com/technetwork/java/javase/downloads/index.html img img 11.2 上传并解压用xftp将jdk包上传到linux系统里，我这里上传到/usr/soft目录下。 然后解压: tar -zxf jdk-7u80-linux-x64.tar.gz img 11.3配置环境变量配置全局JAVA_HOME，并在PATH路径里加入java_home/bin. 注意：新的path路径必须要包含旧的PATH路径，且每个路径之间以冒号隔开，而不是分号 vim /etc/profile JAVA_HOME= /usr/soft/jdk1.7.0_75 PATH=$PATH:$JAVA_HOME/bin img 重新加载环境变量：source /etc/profile img 11.4 验证java -version img 如出现上图，则表示java环境变量配置成功。 十二、部署Tomcat12.1 官网下载下载tomcat http://tomcat.apache.org/ 12**.2 上传并解压**我这里上传至/usr/soft目录下，然后解压。 img 12.3 启动tomcat在tomcat的bin目录下有个startup.sh 脚本可以直接启动tomcat服务 img 关闭tomcat服务，可以用shutdown.sh命令。 或者ps -ef | grep tomcat 查看出tomcat进程号后，用kill命令。 12.4 jpsjps是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。 img 如上图所示，jps命令显示出了，系统当前运行在jvm上的进程情况。其中Bootstrap是tomcat的进程名字，1996是tomcat的PID 13.5验证先把防火墙关了（service iptables stop），然后访问虚拟机IP的8080端口 img]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>系统学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统CentOS 6]]></title>
    <url>%2F2019%2F01%2F11%2FLinux%E7%B3%BB%E7%BB%9FCentOS%206%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、安装 资源准备： CentOS-6.6-x86_64-minimal.iso（简易迷你版） ： CentOS-6.7-x86_64-bin-DVD1.iso（完整版）： 1、点击新建虚拟机img 2、选择典型。（专业人士使用的话建议选择高级）img 3. 选择稍后安装操作系统img 4. 选择操作系统类型，选择linux,centos 64位img 5. 选择虚拟机安装位置和名称。img 6. 指定磁盘容量，默认20GB。img 7. 选择自定义硬件img 8. 点击CD/DVD,然后选择操作系统的ISO映像文件，选择完后，点击关闭。img 9.点击完成。img 二、配置虚拟机1. 启动虚拟机。img注意：如果启动虚拟机时，发生以下问题，说明是你的电脑默认未开启虚拟化技术。 img 此时你应该把机器重启并进入bios界面（不同的机器进入bios界面的快捷键不同，一般为F1~F10键中的某个键，如果都不行，就得自己百度一下你的机器型号进入bios界面的快捷方式）。 ​ 当进入bios界面后，把虚拟机化选项（virtualization technology）打开,通过回车键，把disabled改成enabled,然后保存并重启机器。我这边是按F10，不同机器可能不一样，看右下角的提示信息。 img 2.Test Media, 如果不需要的话，点Skipimg 3、单击Next按钮继续img 4. 选择安装期间显示的语言img 5、选择键盘语言img 6、选择存储介质的类别。如果是将CentOS 6安装到本地硬盘上，选择 Basic Storage Devices，如果安装到网络存储介质如SANs上，选择 Specialized Storage Devices img 7.选择 yes,discard any dataimg 8. 设定主机名称（hostname）img 9. 设定时区，选择 Asia/Shanghaiimg 10. 设定root帐户的密码尽量使用较复杂的密码安装（根据实际情况，密码简单时，会有提示，点击user anyway 就行） img 11. 选择安装类型，这里我选择 “Use All Space” img 12. 选择 “Write changes to disk”，将分区数据写入硬盘img 13. 开始安装，此时只需等待即可img 14. 安装完结后，点击Reboot按钮img]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>CentOS 6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统数据库MySQL安装]]></title>
    <url>%2F2019%2F01%2F10%2FLinux%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93MySQL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[[TOC] Linux系统数据库MySQL安装一、第一次安装MySQL1、yum安装 命令 ： yum -y install mysql-server mysql-devel 2、登录 命令 ： mysql -u -p 显示： 1mysql&gt; 3、查看数据库(注意用‘ ; ’结束 ) 命令 ： show databases; 4、退出： 命令： quit； 5、创建用户： 命令 ： mysqladmin -uroot password 123456 6、再登录： 命令 ：mysql -u root -p 显示： 1mysql&gt; 说明成功了！ 7、数据库操作： 命令 ： use mysql; 显示： 1234Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changed 8、查看用户数据表： 命令 ： show tables; 9、查询user表部分字段： 命令 ： select host,user,password from user; 10、通过修改user表的host字段，设置数据库访问权限，只要用户名和密码正确（1）推荐现将user表中其他无密码的记录删除 命令 ： delete from user where password = ‘ ‘; (2)更新有密码的记录的host字段值 命令 ： update user set host = “%”; (3)刷新权限 命令 ：flush privileges; (4)退出 命令 ：quit; 二、Linux系统登录数据库MySQL报错报错一：1、登录 mysqld -uroot -p123456 报错： 1Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (111) 解决： 1）、先删除mysql.sock cd /var/lib/mysql mv mysql.sock mysql.sock.bak 2）、再次登陆 mysql -uroot -p123456 报错： 1Can&apos;t connect to local MySQL server through socket &apos;/var/lib/mysql/mysql.sock&apos; (2) 3）、看看mysql的状态， /etc/rc.d/init.d/mysqld status 显示： mysqld is stopped 4）、看看是不是mysql的权限问题在/var/lib目录下： ls -lt|grep mysql 显示： drwxr-xr-x. 4 mysql mysql 4096 Jan 6 11:09 mysql 5）、说明mysql服务没有启动 2、启动mysql服务 /etc/init.d/mysqld start 显示： 1Starting mysqld: [ OK ] 3、再次登录： mysql -uroot -p123456 显示： 12345678910111213Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.1.73 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; 4、退出 quit 5、解决出现mysql.sock的问题 （1）、vim /etc/mycnf 编辑内容： 12[mysqld]skip_name_resolve=on innodb_file_per_table=on 按esc :wq 保存并退出 （2）使用命令： mysql_secure_installation （3）直接[ enter ] 键，输入密码， (另推荐：Jakie_ZHF老师的博客) 报错二：1ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Linux系统环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据思想]]></title>
    <url>%2F2019%2F01%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[[TOC] 1、大数据核心问题：==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）== 2、大数据思维分而治之 把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q） enter description here 3、业务场景仓储、数牌 业务一：找{重复行}(chongfuhang)++现有1TB的TXT文件 ;格式：数字+字符 ；网速：500M/s ；服务器内存大小：128M ；条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++ enter description here ==方法== 答：共需要2次IO：2*30min=1h ==第一次IO==： 给每一行内容加上唯一标记（hashcode（内容），value（行号））。对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。 `对每一行的hash值进行取模运算，并放置于归类分区的小文件中`。由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。 ==第二次IO==： 在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。 业务二：快{排序}(paixu)++现有1TB的TXT文件 ;格式：数字；网速：500M/s ；服务器内存大小：128M ；条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++ 两次IO，2 * 30分钟 = 1小时 enter description here ==方法一：先全局有序后局部有序== 1.对全局按分区排序（由大到小）。​ 用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················） 2.对局部进行排序（由大到小）。​ 对每个分区进行排序。 enter description here ==方法二：先局部有序后全局有序== 先实现局部有序(小–&gt;大)。将文件划分为N个分区，在每个分区内部进行排序 使用归并实现全局有序。每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。 知否]]></content>
      <categories>
        <category>头脑风暴</category>
      </categories>
      <tags>
        <tag>分而治之</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce学习]]></title>
    <url>%2F2019%2F01%2F05%2FMapReduce%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[一、MapReduce是什么1、概念 MapReduce是一种分布式离线计算框架，是一种编程模型，用于在分布式系统上大规模数据集(大于1TB)的并行运算。 分布式编程： 借助一个集群，通过多台机器去并行处理大规模数据集，从而获得海量计算能力。 2、理解 Map(映射) Reduce(归约) 指定一个Map(映射)函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce(归约)函数，用来保证所有映射的键值对中的每一个共享相同的键组。 二、MapReduce设计理念1、分布式计算 分布式计算将该应用分解成许多小的部分，分配给多台计算机节点进行处理。这样可以节约整体计算时间，大大提高计算效率。 分而治之 策略： 一个存储在分布式文件系统中的大规模数据集， 会被切分成许多独立的分片（split）， 这些分片可以被 多个Map任务并行处理 2、移动计算，而分移动数据 将计算程序应用移动到具有数据的集群计算机节点之上进行计算操作； 将有用、准确、及时的信息提供给任何时间、任何地点的任何客户。 3、Master/Slave架构 包括一个Master和若干个Slave。Master上运行JobTracker，Slave上运行TaskTracker 三、MapReduce计算框架的组成 MR 1、 Mapper负责“分”，即把得到的复杂的任务分解为若干个“简单的任务”执行。 ​ “简单的任务”： 数据或计算规模相对于原任务要大大缩小； 就近计算，即会被分配到存放了所需数据的节点进行计算； 每个map任务之间可以并行计算，不产生任何通信。 split 2、Split规则：（取三者的中间值） – max.split(100M) – min.split(10M) – block(64M) max(min.split,min(max.split,block)) split实际大小=block大小（2.X：128M） Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数 3、Reduce详解（总·重要） – Reduce的任务是对map阶段的结果进行“汇总”并输出。 Reducer的数目由mapred-site.xml配置文件里的项目mapred.reduce.tasks决定。缺省值为1，用户可自定义。 4、Shuffle详解（总·核心） – 在mapper和reducer中间的一个步骤 可以把mapper的输出按照某种key值重新切分和组合成n份，把key值符合某种范围的输出送到特定的reducer那里去处理。 – 可以简化reducer过程 Partitoner ： hash(key) mod R 四、MapReduce架构1、非共享式架构每个节点都有自己的内存，容错性比较好。 2、一主多从架构可扩展性好，硬件要求易达到。 – 主 JobTracker:（ResourceManager资源管理） 负责调度分配每一个子任务task运行于TaskTracker上， 如果发现有失败的task就重新分配其任务到其他节点。 每一个hadoop集群中只一个 JobTracker, 一般它运行在Master节点上。 – 从TaskTracker:（NodeManager） TaskTracker主动与JobTracker通信，接收作业，并负责直接执行每一个任务， 为了减少网络带宽TaskTracker最好运行在HDFS的DataNode上。 MapReduce的体系结构MapReduce主要有以下4个部分组成 1234567891011121314151617181 ）Client•用户编写的MapReduce程序通过Client提交到JobTracker端•用户可通过Client提供的一些接口查看作业运行状态2 ）JobTracker•JobTracker负责资源监控和作业调度•JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点•JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源3 ）TaskTracker•TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）•TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask和Reduce Task使用（所以最好放在DataNode上）4 ）TaskTask 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 五、MapReduce搭建1、节点分布情况 NN DN JN ZK ZKFC RM node00 √ √ node01 √ √ node02 √ 2、配置文件 修改配置文件 (1)mapred-site.xml:（配置mapreudce需要的框架环境） 路径：F:\hadoop-2.6.5\etc\hadoop\mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; （2）yarn-site.xml:（配置yarn的任务调度的计算框架） 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 因为ResourceManager 和NodeManager主从结构，RM存在单点故障，要对它做HA（通过ZK） 修改yarn-site.xml配置文件,完整的内容如下： 12345678910111213141516171819202122232425262728 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;Sunrise&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;node03&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;node04&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;node01:2181,node02:2181,node03:2181&lt;/value&gt; &lt;/property&gt; 六、个人理解 基于源码，对mapreduce的工作流程的描述： 12345678910111213141516171819一个应用程序要进行大规模数据处理分析数据文件保存在HDFS中，分块存储在分布式节点上首先是将数据文件切分成许多split切片每一个split切片单独启动一个map任务，所以会启动多个map任务map阶段的输入是诸多(key,value),输出是新的（key,value）,然后被拉去到不同的reduce上并行处理操作所以每个map的输出阶段都执行分区操作，并决定reduce任务的个数然后对map输出结果进行排序、归并、合并，这个过程叫map阶段的shuffleshuffle结束后，将相应的结果分发给reduce，让reduce完成后续的工作 结束后，将结果输出给HDFS。不同的map之间不会通信，不同的reduce也不会通信，整个过程对用户透明。 shuffle MapReduce执行的各个阶段： 123456789101112131、从HDFS中加载文件，加载读取由INputFormat模块来完成，对输入负责格式验证，同时，对数据进行逻辑上切分成split2、由record read具体根据分片的位置长度信息去找各个block，以（key，value）输出，作为map的输入，3、map中有用户自定义的map函数就可以进行相应的数据处理，并输出一堆（key，value），作为中间结果4、之后，是shuffle（洗牌）过程对这中间结果进行分区、排序、合并，并溢写到磁盘，5、相应的reduce任务就会来fetch对应的分区（key，value（list））6、reduce中有用户自定义的reduce函数就可以完成对数据的分析，结果以新的（key，value）输出7、输出结果借助OutputFormat模块对输出格式进行检查，以及相关目录是否存在等，最后写入到HDFS中。 split 关于split的切分的理解： 1234561、InputFormat将大的数据文件分成很多split2、文件在HDFS中是以很多个物理块block分布式存储不同的节点上3、切片是用户自定义的逻辑分片4、split的数量决定map任务的数量5、切片过多会导致map任务启动过多，map任务之间切换的时候就会耗费相关的管理资源，所以切片过多会影响执行效率6、 切片过少又会影响任务执行的并行度，所以理想情况用block块的大小作为切片的大小。 关于shuffle的理解 123456789map端shuffle1、从HDFS输入数据和执行map任务，在map任务执行之前，RecordReader阅读器还将数据变成满足Map函数所需的（K，V）形式，然后InputFormat会将其切分成若干切片（一堆（K，V））。2、每个切片会分配一个map任务，每个map任务会分配一个默认的缓存，一般默认缓存为100M.map的输出键值对作为中间结果先写入到缓存（直接写入磁盘会增加寻址开销，所以集中写入磁盘一次寻址就可以完成批量写入，就可以将寻址开销分摊到大量数据中，这就是缓存的作用）。3、当写入的内容达到缓存空间的一定比例后（溢写比，一般为0.8，就是80M的时候，为了不影响map任务的继续执行），会启动溢写进程，把缓存中相关数据写入磁盘。4、在溢写过程中，会执行分区（partition）、排序（sort，按照key值）和可能的合并（combine，为了减少溢写到磁盘的数据量，慎用）操作，写入磁盘，生成磁盘的溢写文件。5、在map任务运行结束前，系统会对溢写文件进行归并（merge），形成大文件（里面的键值对是分区，排序的）,文件格式为（key,value&lt;list&gt;），归并时如果溢写文件大于预定值（默认为3），会再次合并reduce端shuffle1、reduce任务会询问JobTracker，去拉取map机器上的属于自己的分区，对来自不同机器的数据进行归并、合并，然后输入到reduce函数中进行数据的处理分析，再写入磁盘 我 MapReduce应用程序执行过程]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop2.X]]></title>
    <url>%2F2019%2F01%2F04%2FHadoop2.X%2F</url>
    <content type="text"><![CDATA[[TOC] 一、Hadoop 2.x产生背景1、Hadoop 1.0存在的问题（1）HDFS存在的问题 NameNode单点故障，难以应用于在线场景 NameNode（一个）压力过大，内存受限，影响系统扩展性 （2）MapReduce存在的问题 JobTracker访问压力大，影响系统扩展性 难以支持MapReduce以外的计算框架，比如Spark、Storm 2、Hadoop 2.0分支HDFS：分布式文件存储系统MapReduce：计算框架YARN：资源管理系统 3、特点 1）. 解决单点故障：HDFS HA（高可用） 通过主备NameNode解决，如果主NameNode发生故障，就切换到备NameNode上 | 2).解决内存受限问题：HDFS Federation（联邦制）、HA HA：两个NameNode (3.0就实现了一组多从：水平扩展，支持多个NameNode；每个NameNode分管一部分目录；所有NameNode共享所有DataNode资源) 3).仅架构上发生变化使用方式不变 二、HDFS HA结构及功能**HADN：DataNode（数据节点） 存放数据block块；遵循心跳机制向NN Active和NN Standby汇报block块信息，但只执行active的命令 主备NN：NameNode Active 和 NameNode Standby （主备名称节点） 主NN对外提供读写服务，备NN同步主NN元数据，以待切换，所有的DN同时向两个NN汇报数据块信息 元数据信息加载到主NN，并写入JN（至少写两台：过半原则）； 备NN可以从JN中同步元数据信息； 解决单点故障； –两种切换方式： 手动：通过命令实现主备切换 自动：基于Zookeeper实现（详情见搭建步骤） JN：JournalNode（至少3台） 存储主NN元数据信息，实现主备NN间数据共享； （遵循过半原则：至少有过半的数量参与投票） ZKFC：FailoverController（竞争锁） 谁拿到了这个所，谁就是active NN 心跳机制监控主备NN状态，一旦出现一台挂机，就会释放锁，另一个NN就会立即启动竞争锁，成为active NN ZK：Zookeeper（至少3台） （实现主备NN切换） **联邦 通过多个namenode/namespace把元数据的存储和管理分散到多个节点中，使到namenode/namespace可以通过增加机器来进行水平扩展 通过多个namespace来隔离不同类型的应用，把不同类型应用的HDFS元数据的存储和管理分派到不同的namenode中。 三、YARN(资源管理)???????详见Yarn学习.md 1、核心思想：SourceManager（资源管理）+ReplicationMaster（任务调度） 2.yarn的引入使得多个计算框架可以应用到一个集群中 四、Zookeeper工作原理详见Zookeeper学习.md 五、Hadoop2.X 集群搭建1、linux环境下搭建 NN DN JN ZKFC ZK SM RM node00 √ √ √ √ √ √ √ node01 √ √ √ √ √ √ node02 √ √ √ √ 0.在搭建环境之前的准备 三台虚拟机： 1234567关闭防火墙安装jdk编辑/etc/hosts/给各个节点服务器起别名时间服务器：ntpdate 安装：yum install ntpdate -y 生成：ntpdate cn.ntp.org.cn免密登录环境准备 在hadoop安装目录下hadoop-2.6.5/etc/hadoop/ 编辑hadoop-env.sh 1export JAVA_HOME=/usr/soft/jdk1.8.0_191 2.编辑core-site.xml 123456789101112131415&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://Sunrise&lt;/value&gt;&lt;!--配置集群的名字--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node00:2181,node01:2181,node02:2181&lt;/value&gt; &lt;!--配置zookeeper：三个节点--&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop&lt;/value&gt;&lt;!--配置hadoop基础配置存放的路径--&gt;&lt;/property&gt;&lt;/configuration&gt; 3.编辑hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;configuration&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;sxt&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.Sunrise&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node01:8020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn1&lt;/name&gt; &lt;value&gt;node00:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.Sunrise.nn2&lt;/name&gt; &lt;value&gt;node01:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定namenode元数据存储在journalnode中的路径 --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node00:8485;node01:8485;node02:8485/sxt&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 指定HDFS客户端连接active namenode的java类 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.Sunrise&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 配置隔离机制为ssh 防止脑裂：保证activeNN仅有一台--&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 指定秘钥的位置 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_dsa&lt;/value&gt; &lt;!--免密登录是生成的文件，有的是id_rsa--&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 指定journalnode日志文件存储的路径 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;!-- 开启自动故障转移 --&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 配置hadoop中的slaves（主从架构：datanode） 123node00node01node02 5.准备zookeeper 三台zookeeper：node00，node01，node02 编辑zookeeper-3.4.13/conf/zoo.cfg 123456789tickTime=2000initLimit=10syncLimit=5dataDir=/usr/soft/zookeeper-3.4.13/datadataLogDir=/usr/soft/zookeeper-3.4.13/logsclientPort=2181server.1=node00:2888:3888server.2=node01:2888:3888server.3=node02:2888:3888 在dataDir目录中创建文件myid，三台节点的文件内容分别为1，2，3 6.配置环境变量 vim ~/.bash_profile 123456JAVA_HOME=/usr/soft/jdk1.8.0_191export PATH=$PATH:$JAVA_HOME/binHADOOP_HOME=/usr/soft/hadoop-2.6.5export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinZOOKEEPER_HOME=/usr/soft/zookeeper-3.4.13export PATH=$PATH:$ZOOKEEPER_HOME/bin source ~/.bash_profile 使其成为资源文件，发送到其他节点后，也需要此操作 7.将以上配置文件远程发送至其他节点服务器 scp -r filename nodename:pwd 8.命令操作： 123456789101112131415161. 启动三个zookeeper：./zkServer.sh start2. 启动三个JournalNode：./hadoop-daemon.sh start journalnode3. （生成fsimage文件）在其中一个namenode上格式化： hdfs namenode -format4. 把刚刚格式化之后的元数据拷贝到另外一个namenode上 a) 启动刚刚格式化的namenode : hadoop-daemon.sh start namenode b) （同步fsimage文件）在另一个（没有格式化的）namenode上执行： hdfs namenode -bootstrapStandby c) 启动没格式化的namenode： hadoop-daemon.sh start namenode5. （初始化竞争锁zookeeper）在其中一个namenode上初始化zkfc： hdfs zkfc -formatZK6. 停止上面节点：stop-dfs.sh7. 全面启动（三个节点）：start-dfs.sh8. 启动yarn资源管理器 yarn-daemon.sh start resourcemanager (yarn resourcemanager ) 2、使用（启动步骤） 1234(1)关闭防火墙：service iptables stop （3台）(2)启动zookeeper:zkServer.sh start （3台）(3)启动集群：start-dfs.sh |（start-all.sh : 同时启动hdfs和yarn)(4)启动yarn：yarn-daemon.sh start resourcemanager （可3台） （关闭步骤） 123(1)关闭yarn：yarn-daemon.sh stop resourcemanager （开几台关几台）(2)关闭集群：stop-dfs.sh |（stop-all.sh :同时关闭hdfs和yarn） （3台）(3)关闭zookeeper：zkServer.sh stop （3台） 12345678有可能会出错的地方1， 确认每台机器防火墙均关掉2， 确认每台机器的时间是一致的3， 确认配置文件无误，并且确认每台机器上面的配置文件一样4， 如果还有问题想重新格式化，那么先把所有节点的进程关掉5， 删除之前格式化的数据目录hadoop.tmp.dir属性对应的目录，所有节点同步都删掉，别单删掉之前的一个，删掉三台JN节点中dfs.journalnode.edits.dir属性所对应的目录6， 接上面的第6步又可以重新格式化已经启动了7， 最终Active Namenode停掉的时候，StandBy可以自动接管！]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS学习]]></title>
    <url>%2F2019%2F01%2F03%2FHDFS%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] Hadoop学习一、分布式文件存储系统HDFS1、什么是分布式？ 定义：将海量的数据，复杂的业务分发到不同的计算机节点和服务器上分开处理和计算。 特点： 多副本，提高服务的容错率、安全性、高可靠性 适合批处理，提高服务的效率和速度， 减轻单台服务的压力 具有很好的可扩展性 计算向数据靠拢，安全，高效 大数据三驾马车：GFS、MapReduce、Bigtable 2、什么是HDFS？（1）HDFS为什么会出现？ 主要解决大量【pb级以上】的大数据的分布式存储问题 （2）HDFS的特点 $$ 分布式特性： 适合大数据处理：GB、TB、PB以上的数据 百万规模以上的文件数量:10K+ 节点 适合批处理：移动计算而非数据(MR),数据位置暴露给计算框架 $$ 自身特性： 可构建在廉价机器上 高可靠性：通过多副本提提高 高容错性：数据自动保存多个副本；副本丢失后，自动恢复,提供了恢复机制 $$ 缺点： —–低延迟高数据吞吐访问问题（不适合低延迟数据访问，Hbase适合） 不支持毫秒级 吞吐量大但有限制于其延迟（瓶颈：低延迟无法突破） —–小文件存取占用NameNode大量内存(寻道时间超过读取时间,约占99%) ——-不支持多用户写入及任意修改文件 不支持文件修改：一个文件只能有一个写者 文件仅支持append不支持修改 （其实本身是支持的，主要为了用空间换时间，节约成本） $$ 实现目标： 兼容廉价的硬件设施 实现流数据读写 支持大数据集 支持简单的文件模型 强大的跨平台兼容性 （3）HDFS架构图HDFS架构图 HDFS架构图 关系型数据库：安全，存储在磁盘中；如MySql、Oracle、SQlServer 非关系型数据库：不安全，存储在内存中；如Redis、MemcacheDB、mongDB、Hbase 3、HDFS的功能模块及原理详解 HDFS数据存储模型（block）block （1）文件被线性切分固定大小的数据块：block 通过偏移量offset（单位：byte）标记 默认数据块大小为64MB (hadoop1.x，hadoop2.x默认为128M）)，可自定义配置 若文件大小不到64MB ，则单独存成一个block （2）一个文件存储方式 按大小被切分成若干个block ，存储到不同节点上 默认情况下每个block都有2个副本 共3个副本 副本数不大于节点数 （3）Block大小和副本数通过Client端上传文件时设置， 文件上传成功后副本数可以变更，Block Size大小不可变更 块的大小远远大于普通文件系统，可以最小化寻址开销 NameNode（简称NN） 存储元数据； 元数据保存在内存中； 保存文件、block块、datanode之间的映射关系 1&gt; NN主要功能： 接收客户端的读写服务；接收DN汇报block位置关系 2&gt; NN保存metadate元信息 基于内存存储，不会和磁盘发生交换 ​ metadata元数据信息包括以下 文件的归属（ownership）和权限（permission） 文件大小和写入时间 block列表【偏移量】：即一个完整文件有哪些block（b0+b1+b2+..=file） 位置信息（动态的）：Block每个副本保存在哪个DataNode中 *注意*：位置信息是由DN启动时上报给NN ，因为它会随时变化，所以不会保存在内存和磁盘中 3&gt; NameNode的metadate信息在启动后会加载到内存 同时： metadata信息也会保存fsimage文件中（fsimage文件是位于磁盘上的镜像文件） 对metadata的操作日志也会记录在edits 文件中（edits文件是位于磁盘上的日志文件） SecondaryNameNode（简称SNN）1&gt;SNN主要功能 帮助NameNode合并edits和fsimage文件，减少NN启动时间； SecondaryNameNode一般是单独运行在一台机器上； 它不是NN的备份（但可以做备份)。 2&gt;合并流程SNN合并 123456789101112SecondaryNameNode的工作情况：（1）SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件， 暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成， 上层写日志的函数完全感觉不到差别；（2）SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文 件，并下载到本地的相应目录下；（3）SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件 中的各项更新操作，使得内存中的FsImage保持最新； 这个过程就是EditLog和FsImage文件合并；（4）SecondaryNameNode执行完（3）操作之后， 会通过post方式将新的FsImage文件发送到NameNode节点上（5）NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件， 同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了 3&gt;合并机制 ——-SNN执行合并时间和机制 A、根据配置文件设置的时间间隔fs.checkpoint.period 默认3600秒 B、根据配置文件设置edits log大小 fs.checkpoint.size ​ 规定edits文件的最大值默认是64MB DataNode（简称DN）1&gt; DN主要功能 存储文件内容（block）； 文件内容保存在磁盘； 维护了block id 到datanode本地文件的映射关系 启动DN线程的时候会向NameNode汇报block位置信息 2&gt; DN工作机制12345• 数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，• 会根据客户端或者是名称节点的调度来进行数据的存储和检索，• 并且通过心跳机制向名称节点定期发送自己所存储的块的列表，保持与其联系（3秒一次） （如果NN 10分钟没有收到DN的心跳，则认为其已经lost，并copy其上的block到其它DN）• 每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中 3&gt; block的副本放置策略 – 第一个副本：放置在上传文件的DN（集群内提交）； ​ 如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。 – 第二个副本：放置在于第一个副本不同的机架的节点上。 – 第三个副本：与第二个副本相同机架的不同节点。 – 更多副本：随机节点 block块存放位置 4、HDFS读写流程 读文件过程read 1、首先client端调用FileSystem对象（FS）的open方法，（FS：一个DistributedFileSystem的实例）。2、DistributedFileSystem通过rpc协议从NameNode（NN）获得文件的第一批block的locations，（同一个block按副本数会返回多个locations，因为同一文件的block分布式存储在不同节点上），这些locations按照hadoop拓扑结构排序，距离客户端近的排在前面（就近选择）。 3、前两步会返回一个FSDataInputStream对象，该对象会被封装DFSInputStream对象，DFSInputStream可以方便的管理DN和NN的数据流。客户端调用read方法，DFSInputStream会连接离客户端最近的DN，数据从DN源源不断的流向客户端（对客户端是透明的，只能看到一个读入的Input流）。 4、如果第一批block都读完了， DFSInputStream就会去NN拿下一批block的locations，然后继续读，如果所有的块都读完，这时就会关闭掉所有的流 读 注意： 123456如果在读数据的时候， DFSInputStream和DN的通讯发生异常，就会尝试连接正在读的block的排序第二近的DN,并且会记录哪个DN发生错误，剩余的blocks读的时候就会直接跳过该DN。 DFSInputStream也会检查block数据校验和，如果发现一个坏的block,就会先报告到NN，然后DFSInputStream在其他的DN上读该block的镜像。该设计就是客户端直接连接DN来检索数据，并且NN来负责为每一个block提供最优的DN，NN仅仅处理block location的请求，这些信息都加载在NN的内存中，hdfs通过DN集群可以承受大量客户端的并发访问。* RPC *（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。 写文件过程write 1.客户端通过调用DistributedFileSystem的create方法创建新文件。 2.DistributedFileSystem通过RPC调用NN去创建一个没有blocks关联的新文件，创建前，NN会做各种校验，比如文件是否存在，客户端有无权限去创建等。如果校验通过，NN就会记录下新文件，否则就会抛出IO异常。 3.前两步结束后，会返回FSDataOutputStream的对象，封装在DFSOutputStream，客户端开始写数据到DFSOutputStream，DFSOutputStream会把数据切成一个个小的packet，然后排成队列dataQuene。 4.NN会给这个新的block分配最适合存储的几个datanode，DFSOutputStream把packet包排成一个管道pipeline输出。先按队列输出到管道的第一个datanode中，并将该Packet从dataQueue队列中移到ackQueue队列中，第一个datanode又把packet输出到第二个datanode中，以此类推。 5.DFSOutputStream中的ackQuene，也是由packet组成，等待DN的收到响应，当pipeline中的DN都表示已经收到数据的时候，这时ackQuene才会把对应的packet包移除掉。 如果在写的过程中某个DN发生错误，会采取以下几步： ​ 1) pipeline被关闭掉； ​ 2)为了防止丢包，ackQuene里的packet会同步到dataQuene里;新建pipeline管道接到其他正常DN上 ​ 4)剩下的部分被写到剩下的正常的datanode中； ​ 5)NN找到另外的DN去创建这个块的复制。（对客户端透明） 6.客户端完成写数据后调用close方法关闭写入流 注意：客户端执行write操作后，写完的block才是可见的，正在写的block对客户端是不可见的 写 5.HDFS文件权限和安全模式？？HDFS文件权限？？– 与Linux文件权限类似 • r: read; w:write; x:execute，权限x对于文件忽略，对于文件夹表示是否允许访问其内容 – 如果Linux系统用户zs使用hadoop命令创建一个文件，那么这个 文件在HDFS中owner就是zs。 – HDFS的权限目的：阻止好人做错事，而不是阻止坏人做坏事。 ？？安全模式？？ NN启动的时候，首先将映像文件(fsimage)载入内存，并执行编辑日志(edits)中的各项操作。 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的fsimage文件(这个操作不需要SecondaryNameNode)和一个空的编辑日志。 此刻namenode运行在安全模式。即namenode的文件系统对于客服端来说是只读的。(显示目录，显示文件内容等。写、删除、重命名都会失败)。 在此阶段Namenode收集各个datanode的报告，当数据块达到最小副本数以上时，会被认为是“安全”的， 在一定比例（可设置）的数据块被确定为“安全”后，再过若干时间，安全模式结束 当检测到副本数不足的数据块时，该块会被复制直到达到最小副本数，系统中数据块的位置并不是由namenode维护的，而是以块列表形式存储在datanode中。 异常 二、完全分布式搭建及eclipse插件1、完全分布式搭建（必备）(1)环境的准备 Linux (前面已经安装好了) JDK（前面已经安装好了） 准备至少3台机器（通过克隆虚拟机；) (网络配置、JDK搭建、hosts配置，保证节点间能互ping通） 时间同步 (ntpdate time.nist.gov) ssh免秘钥登录 (两两互通免秘钥) （2）完全分布式搭建步骤详情见Hadoop2.X.md文件 2、HDFS命令(0) 命令 ：hdfs dfs(1)上传文件到HDFS： hdfs dfs -put fileName[本地文件名] PATH【hdfs的文件路劲】 上传本地文件install.log到/myhdfs目录下 hdfs dfs -put install.log /myhdfs ​ （文件路径) (上传目录） (2)创建文件夹 hdfs dfs -mkdir[-p] (3)删除文件或文件夹 hdfs dfs -rm -r /myhadoop1.0 123456789hdfs dfs -du -s URI[URI ...] 显示文件(夹)大小. hdfs dfs -cp -f]URI[URI...]&lt;dest&gt; 复制文件(夹)，可以覆盖，可以保留原有权限信息hdfs dfs -count -q&lt;paths&gt;列出文件夹数量、文件数量、内容大小.hdfs dfs -chown -R[:[GROUP]]URI[URI] 修改所有者.hdfs dfs -chmod [-R]&lt;MODE[,MODE]...|OCTALMODE&gt;URI[URI ...] 修改权限. （4）指定block大小 123456789产生100000条数据：for i in `seq 100000`;do echo &quot;hello sxt $i&quot; &gt;&gt; test.txt;done上传文件test.txt到指定的Java22目录下，并指定block块的大小1M：hdfs dfs -D dfs.blocksize=1048576-put test.txt /java22-D ----设置属性 3、eclipse插件安装配置（1）、导入插件 将以下jar包放入eclipse的plugins文件夹中 ​ hadoop-eclipse-plugin-2.6.0.jar 启动eclipse：出现界面如下： 插件应用 （2）配置环境变量Eclipse插件安装完后修改windows下的用户名，然后重启Eclipse： 环境变量 （3）新建Java项目 三、网盘1、代码编写 新建Java项目，导入所需要的jar包 1234567hadoop中的share\hadoop\hdfshadoop中的share\hadoop\hdfs\libhadoop中的share\hadoop\commonhadoop中的share\hadoop\common\lib下的jar包。 block底层—offset偏移量来读取字节数组 123456789101112131415161718private static void blk() throws Exception &#123; Path ifile = new Path(""); FileStatus file = fs.getFileStatus(ifile );// 获取block的location信息HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容 BlockLocation[] blk = fs.getFileBlockLocations(file,0, file.getLen()); for (BlockLocation bb : blk) &#123; System.out.println(bb); &#125; FSDataInputStream input = fs.open(ifile); System.out.println((char)input.readByte()); System.out.println((char)input.readByte()); // 指定从哪个offset的位置偏移量来读 input.seek(1048576); System.out.println((char)input.readByte()); input.seek(1048576); System.out.println((char)input.readByte()); &#125; private static void blk() throws Exception { Path ifile = new Path(“”); FileStatus file = fs.getFileStatus(ifile ); // 获取block的location信息 HDFS分布式文件存储系统根据其偏移量的位置信息来读取其内容 BlockLocation[] blk = fs.getFileBlockLocations(file , 0, file.getLen()); for (BlockLocation bb : blk) { System.out.println(bb); } FSDataInputStream input = fs.open(ifile); System.out.println((char)input.readByte()); System.out.println((char)input.readByte()); // 指定从哪个offset的位置偏移量来读 input.seek(1048576); System.out.println((char)input.readByte()); input.seek(1048576); System.out.println((char)input.readByte()); }]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN的入门学习]]></title>
    <url>%2F2019%2F01%2F03%2FYarn%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 一、简介yarn（资源管理器）（1）存在背景：MR1.0存在缺陷： 单点故障： 仅有一个JobTracker负责整个作业的调度、管理、监控、资源调度 （一个作业拿到后会分解多个任务去执行mapduce，JobTracker把任务分配给TaskTracker来具体负责执行相关map或reduce任务） JobTracker‘大包大揽’，管理事项过多 （上限4000个节点） 容易出现内存溢出 资源划分不合理 （强行划分slot，map资源和reduce资源不能互用，导致忙的忙死，闲的闲死） 1既是一个计算框架，也是一个资源管理框架 （2）yarn产生 对JobTracker进行功能分解，将资源管理功能分给ResourceManager，将任务调度和任务监控分给ApplicationMaster，将TaskTracker的任务交给NodeManager 12纯粹的资源管理框架被剥离资源管理调度功能的MapReduce就变成了MR2.0，他就是一个运行在YARN上的一个纯粹的计算框架，由YARN为其提供资源管理调度服务 什么叫纯粹的计算框架？？ 它提供一些计算基类，使用时，编写map类和reduce类的子类，去继承它。然后计算框架去做后台自动分片，shuffle过程。 资源管理框架？？ 它专门管理CPU内存资源的分配 二、YARN设计思路 三、YARN体系结构三大核心： 1、RecourceManager（RM） ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager） 调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行就近选择，从而实现“计算向数据靠拢” 容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量 调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也允许用户根据自己的需求重新设计调度器 应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等 2、ApplicationMaster ResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMasterApplicationMaster的主要功能是：（1）当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，ResourceManager会以容器的形式为ApplicationMaster分配资源； （2）把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源的“二次分配”； （3）与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行失败恢复（即重新申请资源重启任务）； （4）定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信息； （5）当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。 3、NodeManager NodeManager是驻留在一个YARN集群中的每个节点上的代理，有所需数据的节点，主要负责： 容器生命周期管理 监控每个容器的资源（CPU、内存等）使用情况 跟踪节点健康状况 以“心跳”的方式与ResourceManager保持通信 向ResourceManager汇报作业的资源使用情况和每个容器的运行状态 接收来自ApplicationMaster的启动/停止容器的各种请求 需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与NodeManager通信来掌握各个任务的执行状态 四、YARN 工作流程 五、YARN框架与MapReduce1.0框架的对比分析 从MapReduce1.0框架发展到YARN框架，客户端并没有发生变化，其大部分调用API及接口都保持兼容，因此，原来针对Hadoop1.0开发的代码不用做大的改动，就可以直接放到Hadoop2.0平台上运行 总体而言，YARN相对于MapReduce1.0来说具有以下优势： 大大减少了承担中心服务功能的ResourceManager的资源消耗 ApplicationMaster来完成需要大量资源消耗的任务调度和监控 多个作业对应多个ApplicationMaster，实现了监控分布化 MapReduce1.0既是一个计算框架，又是一个资源管理调度框架，但是，只能支持MapReduce编程模型。而YARN则是一个纯粹的资源调度管理框架，在它上面可以运行包括MapReduce在内的不同类型的计算框架，只要编程实现相应的ApplicationMaster YARN中的资源管理比MapReduce1.0更加高效 以容器为单位，而不是以slot为单位 六、YARN 的发展目标YARN 的目标就是实现“一个集群多个框架”？ ，为什么？ 一个企业当中同时存在各种不同的业务应用场景，需要采用不同的计算框架 MapReduce实现离线批处理 使用Impala实现实时交互式查询分析 使用Storm实现流式数据实时分析 使用Spark实现迭代计算 这些产品通常来自不同的开发团队，具有各自的资源调度管理机制 为了避免不同类型应用之间互相干扰，企业就需要把内部的服务器拆分成多个集群，分别安装运行不同的计算框架，即“一个框架一个集群” 导致问题 集群资源利用率低 数据无法共享 维护代价高 YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源调度管理框架YARN，在YARN之上可以部署其他各种计算框架 由YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩 可以实现一个集群上的不同应用负载混搭，有效提高了集群的利用率 不同计算框架可以共享底层存储，避免了数据集跨集群移动]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper学习]]></title>
    <url>%2F2019%2F01%2F03%2FZookeeper%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] 动物园管理员 推荐图书：《从Paxo到Zookeeper》 Zookeeper1、简介 开源的、分布式应用程序，提供一致性服务，是Haoop （实现HA）和Hbase（和zookeeper是强依赖关系）的重要组件 提供的功能： 配置维护 域名维护 分布式的同步 组服务 Zookeeper→提供通用分布式锁服务，用以协调分布式应用 Keepalived→实现节点健康检查，采用优先级监控，没有协同工作，功能单一，可扩展性差。 2、Zookeep而角色 （一般很少配置Observer，因为用的少，而且配置的节点一般为奇数） Zookeeper需保证高可用和强一致性； ​ 为了支持更多的客户端，需要增加更多Server； ​ Server增多，投票阶段延迟增大，影响性能； ​ 权衡伸缩性和高吞吐率，引入Observer ​ Observer不参与投票； ​ Observers接受客户端的连接，并将写请求转发给leader节点； ​ 加入更多Observer节点，提高伸缩性，同时不影响吞吐率。 3、Zookeeper特点 特点 说明 最终一致性 为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能（与强一致性相对） 可靠性 如果消息被到一台服务器接受，那么它将被所有的服务器接受. 实时性 Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 独立性 各个Client之间互不干预 原子性 更新只能成功或者失败，没有中间状态。 顺序性 所有Server，同一消息发布顺序一致。 4、安装部署：官网： 下载： （1）修改配置文件： 在Zokeeper的安装目录中的conf目录下，将zoo_sample.cfg文件改名为zoo.cfg mv zoo_sample.cfg zoo.cfg 编辑： vim /usr/soft/zookeeper-3.4.13/conf/zoo.cfg 12345678910111213#发送心跳的间隔时间，单位：毫秒tickTime=2000 dataDir=/usr/soft/zookeeper-3.4.13/datadataLogDir=/usr/soft/zookeeper-3.4.13/logsdataLogDir=/Users/zdandljb/zookeeper/dataLog#客户端连接 Zookeeper 服务器的端口，clientPort=2181 #Zookeeper 会监听这个端口，接受客户端的访问请求。initLimit=5syncLimit=2server.1=node01:2888:3888server.2=node02:2888:3888server.3=node03:2888:3888 配置解释: initLimit： 这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5 个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 5*2000=10 秒 syncLimit：这个配置项标识 Leader 与 Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的心跳时间长度，总的时间长度就是 2*2000=4 秒 server.A=B：C：D：其 中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号 (2)创建myid文件（在上面配置文件中配置dataDir 的目录下） 123server1机器的内容为：1，server2机器的内容为：2，server3机器的内容为：3 （3）将zookeeper包发到各个节点上 Paxo算法官网： 1、简介一个基于消息传递的一致性算法，广泛应用于分布式计算中，是到目前为止唯一的分布式一致性算法。 前提： Paxos 有一个前提：没有拜占庭将军问题。就是说 Paxos只有在一个可信的计算环境中才能成立，这个环境是不会被入侵所破坏的。 2、结合故事的对应理解 小岛(Island)——ZK Server Cluster议员(Senator)——ZK Server提议(Proposal)——ZNode Change(Create/Delete/SetData…)提议编号(PID)——Zxid(ZooKeeper Transaction Id)正式法令——所有 ZNode 及其数据 总统——ZK Server Leader zookeeper的节点及工作原理1、工作原理 1.每个Server在内存中存储了一份数据； 2.Zookeeper启动时，将从实例中选举一个leader（Paxos协议） 3.Leader负责处理数据更新等操作 4.一个更新操作成功，当且仅当大多数Server在内存中成功修改数据。 Zookeeper的核心是原子广播，这个机制保证了各个server之间的同步。实现这个机制的协议叫做Zab协议。 Zab协议有两种模式，它们分别是恢复模式和广播模式。 当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数server的完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和server具有相同的系统状态。一旦leader已经和多数的follower进行了状态同步后，他就可以开始广播消息了，即进入广播状态。这时候当一个server加入zookeeper服务中，它会在恢复模式下启动，发现leader，并和leader进行状态同步。待到同步结束，它也参与消息广播。Zookeeper服务一直维持在Broadcast状态，直到leader崩溃了或者leader失去了大部分的followers支持. 广播模式需要保证proposal被按顺序处理，因此zk采用了递增的事务id号(zxid)来保证。所有的提议(proposal)都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch。低32位是个递增计数。 当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的server都恢复到一个正确的状态。 2、Znode节点（1）Znode有两种类型，短暂的（ephemeral）和持久的（persistent） Znode的类型在创建时确定并且之后不能再修改。 短暂znode的客户端会话结束时，zookeeper会将该短暂znode删除，短暂znode不可以有子节点 持久znode不依赖于客户端会话，只有当客户端明确要删除该持久znode时才会被删除 （2）Znode有四种形式的目录节点 PERSISTENT、持久的 EPHEMERAL、短暂的 PERSISTENT_SEQUENTIAL、持久且有序的 EPHEMERAL_SEQUENTIAL 短暂且有序的 3、shell操作启动服务端：./zkServer.sh start 停止服务：./zkServer.sh stop 启动客户端：./zkCli.sh -server 127.0.0.1 : 2081 ​ (localhost、node01) ​ （也可连接其他节点） ​ (port默认2081,可省；ip也可省) 退出客户端：quit 操作指南：help 查看根目录：ll / ​ （ll +路径） 获取具体服务内容：get / ​ (get +路径+服务)可查看注册zookeeper服务的节点信息 （如果作为leader的namenode挂了，最新文件会相应的更换数据信息，如果没有nn，那么就没有相应的最新文件，只会有记录上一个阶段数据的文件） 创建服务：create /sun aabbcc ​ (create +路径 + 数据内容) 在其他节点也可启动客户端，创建服务 删除服务：rmr /sun 4、API操作 见代码testzookeeper 总结 Zookeeper 作为 Hadoop 项目中的一个子项目，是Hadoop 集群管理的一个必不可少的模块，它主要用来控制集群中的数据，如它管理 Hadoop 集群中的NameNode，还有 Hbase 中 Master、 Server 之间状态同步等。 ​ Zoopkeeper 提供了一套很好的分布式集群管理的机制，就是它这种基于层次型的目录树的数据结构，并对树中的节点进行有效管理，从而可以设计出多种多样的分布式的数据管理模型。]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习(总)]]></title>
    <url>%2F2019%2F01%2F02%2FNginx%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[[TOC] Nginx学习：大型网站高并发运行处理一、Nginx使用背景1、背景 1）高并发（海量数据，复杂业务，大量线程）集中访问服务器 2)服务器资源和能力有限 使得服务器宕机，无法提供服务 2、概念理解 1)高并发 海量数据访问，多个进程同时处理不同操作 2）负载均衡 均匀分配请求|数据到不同操作单元上 3）常见互联网架构 客户端层→反向代理层→站点层→服务层→数据层 二、Nginx入门1、了解nginx是什么 nginx是一款轻量级（开发方便，配置简捷）的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器 2、特点 占有内存少，并发能力强，nginx的并发能力确实在同类型的网页服务器中表现非常好。 （使用C语言编写） 3、配置搭建Nginx（Linux系统环境下）资源： Tengine（推荐）：Tengine-2.2.3.tar.gz ​ 其他版本 nginx：nginx/Windows-1.8.1 1）安装依赖 命令：yum -y install gcc openssl-devel pcre-devel zlib-devel 2）解压tar包 命令：tar -zxvf Tengine-2.2.3.tar.gz 3）configure配置：在解压后的源码目录中 两种方案： 命令： ./configure 默认配置/usr/soft/nginx 命令 : ./configure –profix==/usr/soft/nginx 配置在指定路径 4）编译并安装(默认会在/usr/local下生成nginx目录) make &amp;&amp; make install 5）配置nginx服务 在/etc/rc.d/init.d/目录中建立文本文件nginx 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#!/bin/sh## nginx - this script starts and stops the nginx daemon## chkconfig: - 85 15 # description: Nginx is an HTTP(S) server, HTTP(S) reverse \# proxy and IMAP/POP3 proxy server# processname: nginx# config: /etc/nginx/nginx.conf# config: /etc/sysconfig/nginx# pidfile: /var/run/nginx.pid # Source function library.. /etc/rc.d/init.d/functions # Source networking configuration.. /etc/sysconfig/network # Check that networking is up.[ &quot;$NETWORKING&quot; = &quot;no&quot; ] &amp;&amp; exit 0 nginx=&quot;/usr/local/nginx/sbin/nginx&quot;prog=$(basename $nginx) NGINX_CONF_FILE=&quot;/usr/local/nginx/conf/nginx.conf&quot; [ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx lockfile=/var/lock/subsys/nginx make_dirs() &#123; # make required directories user=`nginx -V 2&gt;&amp;1 | grep &quot;configure arguments:&quot; | sed &apos;s/[^*]*--user=\([^ ]*\).*/\1/g&apos; -` options=`$nginx -V 2&gt;&amp;1 | grep &apos;configure arguments:&apos;` for opt in $options; do if [ `echo $opt | grep &apos;.*-temp-path&apos;` ]; then value=`echo $opt | cut -d &quot;=&quot; -f 2` if [ ! -d &quot;$value&quot; ]; then # echo &quot;creating&quot; $value mkdir -p $value &amp;&amp; chown -R $user $value fi fi done&#125; start() &#123; [ -x $nginx ] || exit 5 [ -f $NGINX_CONF_FILE ] || exit 6 make_dirs echo -n $&quot;Starting $prog: &quot; daemon $nginx -c $NGINX_CONF_FILE retval=$? echo [ $retval -eq 0 ] &amp;&amp; touch $lockfile return $retval&#125; stop() &#123; echo -n $&quot;Stopping $prog: &quot; killproc $prog -QUIT retval=$? echo [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile return $retval&#125; restart() &#123; configtest || return $? stop sleep 1 start&#125; reload() &#123; configtest || return $? echo -n $&quot;Reloading $prog: &quot; killproc $nginx -HUP RETVAL=$? echo&#125; force_reload() &#123; restart&#125; configtest() &#123; $nginx -t -c $NGINX_CONF_FILE&#125; rh_status() &#123; status $prog&#125; rh_status_q() &#123; rh_status &gt;/dev/null 2&gt;&amp;1&#125; case &quot;$1&quot; in start) rh_status_q &amp;&amp; exit 0 $1 ;; stop) rh_status_q || exit 0 $1 ;; restart|configtest) $1 ;; reload) rh_status_q || exit 7 $1 ;; force-reload) force_reload ;; status) rh_status ;; condrestart|try-restart) rh_status_q || exit 0 ;; *) echo $&quot;Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;&quot; exit 2esac 6）修改nginx文件的权限 命令 ： chmod +x nginx 7）将文件添加到系统服务中 chkconfig –add nginx 8）验证 chkconfig –list nginx 9）启动|停止服务 service nginx start|stop 4、负载均衡配置1）编辑配置文件： 命令 ： vim /usr/local/nginx/conf/nginx.conf 具体负载配置： 1234567891011121314151617181920212223242526272829303132333435363738（1）（默认）轮询负载http &#123; upstream shsxt&#123; server node01; server node02; &#125; server &#123; listen 80; server_name localhost; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125;（2）加权负载upstream shsxt&#123; server sxt1.com weight=3; server sxt2.com;&#125;(3)最少连接负载upstream shsxt&#123; least_conn; servcer node01; server node02;&#125;(4)ip_hash负载（保持回话持久性）upstream shsxt&#123; ip_hash; server node01; server node02;&#125;3. 访问控制（allow 代表允许其访问，deny 禁止其访问）location / &#123; deny 192.168.4.29; allow 192.168.198.0/24; deny all; proxy_pass http://shsxt;&#125; 5、session一致性问题 实现session共享即可解决这个问题 实现工具：memcached 作用：专门管理session的工具 1）在tomcat的lib目录下导入连接memcached所需的jar包 asm-3.2.jar kryo-1.04.jar kryo-serializers-0.11.jar memcached-session-manager-1.7.0.jar memcached-session-manager-tc7-1.8.1.jar minlog-1.2.jar msm-kryo-serializer-1.7.0.jar reflectasm-1.01.jar spymemcached-2.7.3.jar 2）在tomcat的conf目录下编辑context.xml文件 1234567&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.17.9:11211" sticky="true" lockingMode="auto" sessionBackupAsync="false" requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; （注意：配置memcachedNodes属性时，配置其ip和端口，默认为11211，存在多个memecacahed数据库时，用都逗号隔开） 3）验证session：修改index.jsp（在/usr/soft/apache-tomcat-8.5.24/webapps/ROOT/index.jsp），取sessionid看一看 12345SessionID:&lt;%=session.getId()%&gt;&lt;/br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;/br&gt;&lt;h1&gt;tomcat1&lt;/h1&gt; 4)安装： memcached 命令 ：yum install memcached –y 5）启动memcached (IP地址为memcached安装的节点的IP地址) memcached -d -m 128m -p 11211 -l 192.168.198.128 -u root -P /tmp/ 6）在浏览器段访问服务器，默认端口 ： 80 ，对此测验，就会发现sessionID不会改变 三、虚拟主机1、什么是虚拟主机？（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。 （2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。 2、虚拟主有啥特点？（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用 （2）也大大简化了服务器管理的复杂性； 3、虚拟主机有哪些类别？（1）基于域名 1234567891011121314151617181920212223242526http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03; &#125; server &#123; listen 80; //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里 server_name sxt2.com; location / &#123; proxy_pass http://bjsxt; &#125; &#125; server &#123; listen 80; //访问sxt1.com的时候，会把请求导到shsxt的服务器组里 server_name sxt1.com; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; 注意： （1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。 （C:\Windows\System32\drivers\etc\hosts 给IP取别名） 如：192.168.198.130 sxt1.com （2）每台服务器的Tomcat的端口不与配置中的listen一致，那么windows系统浏览器访问时，需要加上Tomcat的端口，（192.168.198.128：8080） ​ 如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80 （2）基于端口 12345678910111213141516171819202122232425http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03 &#125; server &#123; //当访问nginx的80端口时，将请求导给bjsxt组 listen 8080; server_name 192.168.198.128; location / &#123; proxy_pass http://bjsxt; &#125;&#125; server &#123; //当访问nginx的81端口时，将请求导给shsxt组 listen 81; server_name 192.168.198.128; //nginx服务器的IP location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; （3）基于IP ：（不常用） 四、正向代理和反向代理1、正向代理理解： 代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见） 举例： 国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙） 但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口 2、反向代理理解： 代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器 举例： 如我们访问www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。 Nginx就是性能很好的反向代理服务器，用来作负载均衡。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手动安装maven坐标依赖]]></title>
    <url>%2F2018%2F12%2F28%2F%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85maven%E5%9D%90%E6%A0%87%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[一、事件原因：学习quartz框架时，在maven项目的pom.xml文件中添加quartz所需要的坐标依赖时，显示jar包不存在。 12345678910111213提示："Dependency 'xxxx‘ not found"， 并且添加的如下两个坐标依赖均报红。 &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 分析： 1、maven项目所需要的jar包均存放在maven的F:\m2\repository(项目所需的jar包仓库)文件夹中 2、在F:\apache-maven-3.5.4\conf的settings.xml文件中有如下设置：（由于使用远程仓库太慢，阿里云给我们提供了一个镜像仓库，便于我们使用，且只包含central仓库中的jar） 1234567&lt;!--文件中原有的配置：远程仓库---&gt;&lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; 1234567&lt;!--文件中自己手动配置：阿里镜像仓库---&gt;&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 3.可是我们在https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧 （如果有小伙伴有别的解决方案，还请指点一二。） 1&lt;!--more--&gt; 二、解决方案1、首先，我们需要从maven Repository中下载我们需要的jar包（需要的两个jar包，下载原理相同） 2、注意我们的maven安装，需要配置环境变量，才能在dos窗口，指令安装jar包 因为我之前查资料时，有小伙伴说，java的环境变量配置也会影响，所以，我在这里也把java的环境变量配置也贴出来 1 1544699916763 1544699989775 JAVA_HOME F:\Java\jdk1.8.0_131（ 根据自己的jdk安装目录） CLASSPATH .;%JAVA_HOME%\lib;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar MAVEN_HOME F:\apache-maven-3.5.4（ 根据自己maven安装目录） Path（注意配置的时候，一定要和配置home时的变量名一致，如MAVEN_HOME,我配置成了%MVN_HOME%\bin;） %JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;%MYSQL_HOME%\bin;%MAVEN_HOME%\bin; 配置这些环境变量，在dos窗口才能使java ，mvn 之类的指令可以用； 否则会出现如下显示。 ‘mvn’ 不是内部或外部命令，也不是可运行的程序 (这就是环境变量没有配成功的结果) 3.安装 C:\Users\Administrator&gt;mvn -v 1544701045091 C:\Users\Administrator&gt;mvn install:install-file -Dfile=F:/apache-maven-3.5.4/m2/quartz-2.3.0.jar（jar包所在路径） -DgroupId=org.quartz-scheduler -DartifactId=quartz -Dversion=2.3.0 -Dpackaging=jar （根据下面所示的配置groupId、artifactId、version） 12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 1544702128551 如图所示，安装成功。 1544702179172]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（二）]]></title>
    <url>%2F2018%2F12%2F28%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[​​ [TOC] 一、磁盘指令 查看硬盘信息 命令：df （默认大小以kb显示） df -k（以kb为单位） df -m（ 以mb为单位） df –h （易于阅读） 查看文件/目录的大小 命令：du filename|foldername （默认单位为kb）-k kb单位 -m mb单位 -a 所有文件和目录 -h 更易于阅读 ​ --max-depth=0 目录深度 二、网络指令 查看网络配置信息 命令:ifconfig 测试与目标主机的连通性 命令：ping remote_ip ctrl + c :结束ping进程 显示各种网络相关信息 命令：netstat 查看端口号（是否被占用） (1)、lsof -i:端口号 （需要先安装lsof） (2)、netstat -tunlp|grep 端口号 测试远程主机的网络端口 命令： telnet ip port （需要先安装telnet） 测试成功后，按ctrl + ] 键，然后弹出telnet&gt;时，再按q退出 http请求模拟 curl -X get www.baidu.com 模拟请求百度 三、系统管理指令 用户操作 12345678910 操作 命令创建用户 useradd|adduser username修改密码 passwd username删除用户 userdel –r username修改用户（已下线）： 修改用户名: usermod –l new_name oldname 锁定账户: usermod –L username 解除账户： usermod –U username查看当前登录用户 仅root 用户：whoami | cat /etc/shadow 普通用户：cat /etc/pqsswd 用户组操作 12345 操作 命令 创建用户组 groupadd groupname删除用户组 groupdel groupname修改用户组 groupmod –n new_name old_name查看用户组 groups （查看的是当前用户所在的用户组） 用户+用户组 12345 操作 命令 修改用户的主组 usermod –g groupname username给用户追加附加组 usermod –G groupname username查看用户组中用户数 cat /etc/group注意：创建用户时，系统默认会创建一个和用户名字一样的主组 系统权限 12345678910 操作 命令 查看/usr下所有权限 ll /usr 权限类别 r（读取：4） w（写入：2） x（执行：1） 三个为一组，无权限用 —代替 UGO模型 U（User） G(Group) O(其他)权限修改 修改所有者：chown username file|folder (递归)修改所有者和所属组： chown -r username：groupname file|folder 修改所属组：chgrp groupname file|folder 修改权限：chmod ugo+rwx file|folder 四、系统配置指令 1.修改主机名 123 编辑文件： 命令： vim /etc/sysconfig/network 文件内容： HOSTNAME=node00（重启生效)reboot 2.DNS配置 12编辑文件： 命令：vim /etc/resolv.conf文件内容： nameserver 192.168.198.0 3.sudo权限配置 1234567891011121314151617 操作 命令编辑权限配置文件： vim /etc/sudoers格式： 授权用户 主机=[(切换到哪些用户或用户组)] [是否需要密码验证] 路径/命令举例： test ALL=(root) /usr/bin/yum,/sbin/service解释： test用户就可以用yum和servie命令， 但是，使用时需要在前面加上sudo再敲命令。 第一次使用需要输入用户密码,且每个十五分钟需要一次密码验证修改： test ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service这样就不需要密码了将权限赋予某个组，%+组名%group ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service列出用户所有的sudo权限 sudo –l 4.系统时间 12345678操作 命令查看系统时间 date ---查看当前时间详情 cal ---查看当前月日历 cal 2018 ---查看2018年完整日历 cal 12 2018 ---查看指定年月的日历 更新系统时间（推荐） yum install ntpdate –y ---安装ntp服务 ntpdate cn.ntp.org.cn ---到域名为cn.ntp.org.cn的时间服务器上同步时间 5.关于hosts配置 相当于给IP地址其别名，可以通过别名访问 路径： Windows系统 C:/Windows/System32/drivers/etc/hosts 文件 Linux系统 /etc/hosts文件：vim +路径 统一 编辑格式 IP地址 别名：192.168.198.128 node00 6.关于hostname配置 相当于给对应的虚拟机器起别名 Linux系统： vi /etc/sysconfig/network 编辑内容： HOSTNAME=node01 五、重定向与管道符 输出重定向 输出重定向到一个文件或设备： &gt; 覆盖原来的文件 &gt;&gt; 追加原来的文件 举例： ls &gt; log — 在log文件中列出所有项，并覆盖原文件 echo “hello”&gt;&gt;log —将hello追加到log文件中 输入重定向 &lt; 输入重定向到一个程序 举例：cat &lt; log —将log文件作为cat命令的输入，查看log文件的内容 标准 输出 重定向 1 &gt; 或 &gt; 含义： 输出重定向时，只用正确的输出才会重定向到指的文件中 错误的则会直接打印到屏幕上 错误 输出 重定向 2 &gt; 含义： 错误的输出会重定向到指定文件里，正确的日志则直接打印到屏幕上。 结合 使用 2&gt;&amp;1 含义： 将无论是正确的输出还是错误的输出都重定向到指定文件 管道 **\ ** 含义： 把前一个输出当做后一个输入 grep 通过正则搜索文本，并将匹配的行打印出来 netstat -anp \ grep 22 把netstat –anp 命令的输出 当做是grep 命令的输入 命令 执行 控制 &amp;&amp; 前一个命令执行成功才会执行后一个命令 **\ \ ** 前一个命令执行失败才会执行后一个命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（一）]]></title>
    <url>%2F2018%2F12%2F27%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、命令指南（manual）：man 安装：yum install man –y （-y 表示获得允许，无需确认） 查看ls命令指南： man ls 二、目录命令切换目录：cd + 目录的路径 查看当前目录所在的完整路径：pwd 新建目录：mkdir +目录名字 查看当前目录所用有的子目录和文件：ls ，ll等价于 ls –l ​ 查看目录下的所有东西（包括隐藏文件）： ls –al 等价于 ll -a​拷贝目录或文件：cp –r install.log install2.log 删除目录或文件：rm -r install.log (rmdir只能删除空目录) 移动目录或文件：mv + 目录/文件名字 + 其他路径 ​ 将test目录移动到 根目录/ 下 : mv test / （如果移动到当前目录，用另外一个名称，则可以实现重命名的效果） 更改文件或目录的名字：mv + 旧目录名字 + 新目录名 ( -r 用于递归的拷贝，删除，移动目录) 三、文件命令1、一般文件操作新建文件：touch install.log​ (vim install.log 编辑文件，如果文件不存在，就会新建一个对应的文件，并进入文件的编辑模式，如果按 :wq 会保存文件并退出，如果按 :q 则不保存退出)​查看文件内容：cat +（文件名）（一次性显示整个文件的内容，文件内容过多时用户体验不好） 一次命令显示一屏文本： 1234567 more +（文件名） 按键 效果 Space 显示下一屏文本内容B 显示上一屏文本内容Enter 显示下一行文本内容Q 退出查看 less+（文件名） 按键 效果 h 显示帮助界面 u 向后滚动半页 d 向前翻半页 e | Enter 向后翻一行文本 space 滚动一页 b 向后翻一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 上下键，向上一行，向下一行 从头打印文件内容：​ head -10 +（文件名） 打印文件1到10行 从尾部打印文件内容​ tail -10 +（文件名）打印文件最后10行 tail -f (文件名) 常用于查看文件内容的更新变化 查找文件或目录​ find +（路径名） –name +（文件名）​ 举例：find / -name profile​ 在/(根目录)目录下查找 名字为profile的文件或目录 ​ 也可利用正则：​ 举例： find /etc -name pro*​ 在/etc目录下查找以pro开头的文件或目录 路径越精确，查找的范围越小，速度越快 i 2、文件编辑vi（1） vi 进入编辑模式 —–&gt;按i 进入插入模式 ——-&gt; 按Esc 退出编辑模式 1234vi filename :打开或新建文件，并将光标置于第一行首 vi +n filename ：打开文件，并将光标置于第n行首 vi + filename ：打开文件，并将光标置于最后一行首 vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的字符串所在的行首 filename 为文件名 （2）在文件vi（文件编辑）模式下 命令行模式 123456789:w 保存:q 退出:wq 保存并退出:q! 强制退出:set nu |ctrl+g 显示文本行数:set nonu 去除显示的行数:s/p1/p2/g 将当前行中所有p1均用p2替代 :n1,n2s/p1/p2/g 将第n1至n2行中所有p1均用p2替代 :g/p1/s//p2/g 将文件中所有p1均用p2替换 一般模式 12345678910111213141516171819202122232425262728293031按键：yy 复制光标所在行(常用) nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) p|P p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)G 光标移至第最后一行nG 光标移动至第N行行首n+ 光标下移n行 n- 光标上移n行 H 光标移至屏幕顶行 M 光标移至屏幕中间行 L 光标移至屏幕最后行 dd 删除所在行 x或X 删除一个字符，x删除光标后的，而X删除光标前的 u 撤销(常用)删除第N行到第M行：N,Md：,$-1d 删除当前光标到到数第一行数据按键： i: 在当前光标所在字符的前面，转为输入模式； a: 在当前光标所在字符的后面，转为输入模式； o: 在当前光标所在行的下方，新建一行，并转为输入模式； I：在当前光标所在行的行首，转换为输入模式 A：在当前光标所在行的行尾，转换为输入模式 O：在当前光标所在行的上方，新建一行，并转为输入模式；---逐字符移动：h: 左 l: 右j: 下 k: 上 vim 安装：yum install vim -y 用vim 打开/etc/profile 文件， 特点：编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强 ，其他均与vi相同 3、文件上传下载 安装上传下载命令：yum install lrzsz -y 上传文件：（windows—&gt;linux） 命令 ：rz 弹出windows上传文件窗口 下载文件：(linux—&gt;windows) 注意：sz命令只能下载文件，不能下载目录，推荐将目录压缩成tar包或使用工具软件：Winscp【Xftp】 命令：sz （文件名） 弹出windows下载窗口,下载文件到指定文件目录 4、文件传输(1)、本地→远程 文件 ： scp local_file remote_username@remote_ip:remote_folder 目录 ： scp -r local_folder remote_username@remote_ip:remote_folder (2）、远程→本地 文件 ： scp remote_username@remote_ip:remote_file local_folder 目录 ： scp remote_username@remote_ip:remote_folder local_folder]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Linux命令的学习（三）]]></title>
    <url>%2F2018%2F12%2F27%2F%E5%B8%B8%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E7%9A%84%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 一、服务操作 列出 所有 服务 chkconfig 服务 操作 service 服务名 start\ stop\ status\ restart 永久关闭\ 打开 （启动后生效） chkconfig iptables on\ off 添加 服务 1) 编辑脚本：vim myservice.sh 编辑内容： （在最前面加一下两句） #chkconfig: 2345 80 90 #description:auto_run (自己的服务脚本：开机时同步时间） result=’ntpdate cn.ntp.org.cn’ 退出编辑并保存：按esc键 按 ：wq 在ntpdate.log文件中输出打印：echo $result &gt; /usr/ntpdate.log 2) 修改权限，使其拥有可执行权限: chmod 700 myservice.sh 3) 将脚本拷贝到/etc/init.d目录： 4) 加入服务：chkconfig –add myservice.sh 5) 重启服务器，验证服务是否添加成功：date 6）/usr目录下产生ntpdate.log 删除 服务 chkconfig –del name 更改 服务初 执行 等级 chkconfig –level 2345 服务名 off\ on chkconfig 服务名 on\ of f 二、定时调度 编辑定时任务 crontab –e 格式：minute hour day month dayofweek command 举例 * echo “hello” 每分钟打印“hello” 时间一到，执行操作命令后 会出现：You have new mail in /var/spool/mail/root 查看任务执行情况 vim /var/spool/mail/root 查看所有用户的定时任务 ll /var/spool/cron 查看当前用户的定时任务 contab –l 注意 “”代表任意的数字, “/”代表”每隔多久”, “-”代表从某个数字到某个数字, “,”分开几个离散的数字 如： 30-40 12 echo “hello” ——–每天12点30分至40分期间，每分钟执行一次命令 30,40 ——–每天12点30分和12点40分 0/5 ——–每天的12点整至12点55分期间，每隔5分钟执行一次命令 三、进程操作 查看 进程 ps -aux -a 列出所有 -u 列出用户 -x 详细列出，如cpu、内存等 -e 显示所有进程 -f 全格式 ps - ef \ grep ssh 查看所有进程里CMD是ssh 的进程信息 ps -aux –sort –pcpu 根据 CPU 使用来升序排序 使程序 后台运行 只需要在命令后添加 &amp; 符号 echo “hello” &amp; jobs –l –列出当前连接的所有后台进程（jobs仅适用于当前端） ps -ef \ grep 进程名 —-（推荐）列出后台进程 杀死进程 （强制）kill -9 pid —-pid为进程号 四、其他命令 wget 1） 安装：yum install wget –y 2） 用法：wget [option] 网址 -O 指定下载保存的路径 3） 也可用于做爬虫 yum 1） 备份原镜像： cp /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOSBase.repo.backup 2） 下载新镜像： wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 3） 查看文件内容：vim /etc/yum.repos.d/CentOS-Base.repo 4） 生成缓存：yum makecache rpm 1） 安装 rpm –ivh rpm包 2） 查找已安装的rpm包：rpm –q ntp 3） 卸载：rpm –e ntp-4.2.6p5-10.el6.centos.2.x86_64（全名） tar 1） 解压：tar -zvxf xxxx.tar.gz 2） 压缩：tar -zcf 压缩包命名 压缩目标 3） 例子：tar -zcf tomcat.tar.gz apache-tomcat-7.0.61 4） -z gzip进行解压或压缩，带.gz需要加，压缩出来.gz也需要加 -x 解压 -c 压缩 -f 目标文件，压缩文件新命名或解压文件名 -v 解压缩过程信息打印 zip 1）安装zip：yum install zip –y 2）压缩命令：zip -r 包名 目标目录 3）安装 ：unzip,yum install unzip –y 4）解压 ：unzip filename 五、安装部署 JDK 部署 1) 解压: tar -zxf jdk-7u80-linux-x64.tar.gz 2) 配置环境变量 编辑配置文件：vim /etc/profile 编辑内容 ： JAVA_HOME= /usr/soft/jdk1.7.0_75 PATH=$PATH:$JAVA_HOME/bin 3) 重新加载环境变量：source /etc/profile 4) 验证: java -version mysql部署 yum安装 mysql 1) yum install mysql-server -y 2) yum install mysql-devel -y 3) service mysqld start 4) mysql -uroot -p 5) mysqladmin -u root password 123456 六、免密登录| 法一 | 1） 生成公钥和密钥：ssh-keygen -t rsa ，并且回车3次 （在用户的根目录生成一个 “.ssh”的文件夹） 2） 查看公钥和私钥：ll ~/.ssh （目录中会有以下几个文件） authorized_keys:存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥 id_rsa : 生成的私钥文件 id_rsa.pub ： 生成的公钥文件 know_hosts : 已知的主机公钥清单 如果希望ssh公钥生效需满足至少下面两个条件： 1&gt; .ssh目录的权限必须是700 * 2&gt; .ssh/authorized_keys文件权限必须是600 3） 将A的.ssh目录下的公钥追加拷贝到B的authorized_keys文件里 scp -p ~/.ssh/id_rsa.pub root@&lt;remote_ip&gt;:/root/.ssh/authorized_keys 4) 验证：将文件远程拷贝到远程主机上，看是否需要密码 | ——————————————————— :———————————————————– （此方法有待考究）法二：通过Ansible实现 批量 免密 1）、 将需要做免密操作的机器hosts添加到/etc/ansible/hosts下： [&nbsp;Avoid close] 192.168.91.132 192.168.91.133 192.168.91.134 2）、 执行命令进行免密操作 ：​ 1ansible &lt;groupname&gt; -m authorized_key -a &quot;user=root key=&apos;&#123;&#123; lookup(&apos;file&apos;,&apos;/root/.ssh/id_rsa.pub&apos;) &#125;&#125;&apos;&quot; -k ​ |]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx入门学习（二）]]></title>
    <url>%2F2018%2F12%2F20%2FNginx%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%9B%9E%E5%90%88%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2018年12月20日 周四 阴 一、虚拟主机1、什么是虚拟主机？（1）是指在网络服务器上分出一定的磁盘空间，租给用户以放置站点以及应用空间，并提供必要的存储和传输功能。 （2）是被虚拟化的逻辑主机，也可理解为就是把一台物理服务器划分成多个“虚拟“的服务器，各个虚拟主机之间完全独立，对外界呈现的状态也同单独物理主机表现完全相同。 2、虚拟主有啥特点？（1）多台虚拟主机共享一台真实主机资源，大幅度降低了硬件、网络维护、通信线路等的费用 （2）也大大简化了服务器管理的复杂性； 3、虚拟主机有哪些类别？（1）基于域名 1234567891011121314151617181920212223242526http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03; &#125; server &#123; listen 80; //访问sxt2.com的时候，会把请求导到bjsxt的服务器组里 server_name sxt2.com; location / &#123; proxy_pass http://bjsxt; &#125; &#125; server &#123; listen 80; //访问sxt1.com的时候，会把请求导到shsxt的服务器组里 server_name sxt1.com; location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; 注意： （1）基于域名的虚拟机主机 在模拟应用场景时，需要在windows系统的hosts文件里配置域名映射。 （C:\Windows\System32\drivers\etc\hosts 给IP取别名） 如：192.168.198.130 sxt1.com （2）每台服务器的Tomcat的端口不与配置的listen一致，那么windows系统浏览器访问时，需要加上TOmcat的端口，（192.168.198.128：8080） ​ 如果一致，那么就可以不加Tomcat的端口因为Nginx服务器默认端口为80 （2）基于端口 12345678910111213141516171819202122232425http &#123; upstream shsxt&#123; server node01; server node02; &#125; upstream bjsxt&#123; server node03 &#125; server &#123; //当访问nginx的80端口时，将请求导给bjsxt组 listen 8080; server_name 192.168.198.128; location / &#123; proxy_pass http://bjsxt; &#125;&#125; server &#123; //当访问nginx的81端口时，将请求导给shsxt组 listen 81; server_name 192.168.198.128; //nginx服务器的IP location / &#123; proxy_pass http://shsxt; &#125; &#125; &#125; （3）基于IP ：（不常用） 二、正向代理和反向代理1、正向代理理解： 代理客户端，如通过VPN ，隐藏客户端，访问目标服务器（服务端可见） 举例： 国内不能直接访问谷歌，但是可以访问代理服务器，通过代理服务器可以访问谷歌。（就是翻墙） 但是，需要客户端必须设置正向代理服务器，并且要知道正向代理服务器的IP地址和端口 2、反向代理理解： 代理服务端，通过负载均衡服务器（如Nginx），隐藏服务端，分发客户端的不同请求（客户端可见）到内部网络上的服务器 举例： 如我们访问www.baidu.com的时候，它背后有很多台服务器，客户端并不知道具体是哪一台服务器给你提供的服务，只要知道反向代理服务器是谁就好了，反向代理服务器就会把我们的请求转发到真实服务器上。 Nginx就是性能很好的反向代理服务器，用来作负载均衡。 三、Nginx的session一致性问题1、背景：http协议是无状态的，多次访问如果是不同服务器响应请求，就会出现上次访问留下的session或cookie失效。这就引发了session共享的问题。 2、Session一致性解决方案（1）–session复制 tomcat 本身带有复制session的功能。 （2）-共享session 需要专门管理session的软件， memcached 缓存服务，可以和tomcat整合，帮助tomcat共享管理session。 3、安装memcachedmemcached （同redis一样）是基于内存的数据库 1、安装 yum –y install memcached 可以用telnet localhost 11211 启动： memcached -d -m 128m -p 11211 -l 192.168.235.113 -u root -P /tmp/ 2.web服务器连接memcached的jar包拷贝到tomcat的lib目录下 访问Tomcat服务器期间产生的session通过相关jar包，才能写入到memcached数据库中 memcached-session-manager-1.7.0.jar memcached-session-manager-tc7-1.8.1.jar 3.配置tomcat的conf目录下的context.xml 1234567&lt;Manager className="de.javakaffee.web.msm.MemcachedBackupSessionManager" memcachedNodes="n1:192.168.198.128:11211" sticky="true" lockingMode="auto" sessionBackupAsync="false" requestUriIgnorePattern=".*\.(ico|png|gif|jpg|css|js)$"sessionBackupTimeout="1000" transcoderFactoryClass="de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory" /&gt; 配置memcachedNodes属性， 配置memcached数据库的ip和端口，默认11211，多个的话用逗号隔开. 目的是为了让tomcat服务器从memcached缓存里面拿session或者是放session 将配置完成的context.xml发送到其他虚拟机器上 scp -r context.xml root@node01:pwd 或 scp -r context.xml node01:pwd 或 scp -r context.xml root@192.168.198.130:pwd 4.修改tomcat目录中webapps/ROOT下的 index.jsp，取sessionid看一看 12345678&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;html lang="en"&gt;SessionID:&lt;%=session.getId()%&gt;&lt;/br&gt;SessionIP:&lt;%=request.getServerName()%&gt;&lt;/br&gt;&lt;h1&gt;tomcat1&lt;/h1&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F12%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>

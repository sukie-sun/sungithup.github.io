[{"title":"Flume学习","date":"2019-01-17T16:00:00.000Z","path":"2019/01/18/Flume学习/","text":"[TOC] 一、分类exec： Unix等操作系统执行命令行，如tail ，cat 。可监听文件 netcat 监听一个指定端口，并将接收到的数据的每一行转换为一个event事件 avro 序列化的一种，实现RPC（一种远程过程调用协议）。 监听AVRO端口来接收外部AVRO客户端事件流 capacity：默认该通道中最大的可以存储的event数量是1000 Trasaction Capacity：每次最大可以source中拿到或者送到sink中的event数量也是100 二、操作1、 netcat（监听端口，在本地控制台打印）（1） vim netcat_logger1234567891011121314151617181920212223# example.conf: A single-node Flume configuration# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 （2）命令操作 （在会话1端） 在node00节点的控制台输入启动命令： (方式一：指定配置文件的路径+文件名) flume-ng agent –conf-file /root/flume/netcat_logger –name a1 -Dflume.root.logger=INFO,console （方式二：配置文件在当前目录） flume-ng agent –conf ./ –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console 特别注意： #####官网方式######### flume-ng agent –conf conf –conf-file netcat_logger –name a1 -Dflume.root.logger=INFO,console 解释：此命令适用于将配置文件放在flume解压安装目录的conf中（不常用） 控制台显示: 1​````` 19/01/18 12:27:31 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 12:27:31 INFO node.Application: Starting Sink k119/01/18 12:27:31 INFO node.Application: Starting Source r119/01/18 12:27:31 INFO source.NetcatSource: Source starting19/01/18 12:27:31 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]123456789101112131415161718* (在会话2端)&gt; 在node00节点的控制台输入命令：&gt;&gt; 1、在节点上安装telnet：&gt;&gt; yum install -y telnet&gt;&gt; yum -y install telnet-server&gt;&gt; 2、启动：&gt;&gt; telnet localhost 44444 &gt;&gt; `注意：`：&gt;&gt; a1.sources.r1.bind = localhost12345678910&gt;&gt; 前提是/etc/hosts中已经配置&gt;&gt; 如果此处配置localhost 那么启动时，localhost 或127.0.0.1都可以，node00就不行&gt;&gt; 如果此处配置node00那么启动时，node00或ip都可以，localhost就不行&gt;&gt; 3、在控制台输入任何内容;&gt;&gt; 都会在会话1端显示，且会话1端（ctrl+c）退出服务，会话2端也自动结束 yum list telnet* 查看telnet相关的安装包 直接yum –y install telnet 就OK yum -y install telnet-server 安装telnet服务 yum -y install telnet-client 安装telnet客户端(大部分系统默认安装) 1234### 2、avro（监听远程发送文件，在本地控制台打印）#### （1）vim avro_logger #test avro sources ##使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示 ##当前flume节点执行： #flume-ng agent –conf ./ –conf-file avro_loggers –name a1 -Dflume.root.logger=INFO,console ##其他flume节点执行： #flume-ng avro-client –conf ./ -H 192.168.198.128 -p 55555 -F ./logs a1.sources=r1a1.channels=c1a1.sinks=k1 a1.sources.r1.type = avroa1.sources.r1.bind=192.168.198.128a1.sources.r1.port=55555 a1.sinks.k1.type=logger a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100 a1.sources.r1.channels=c1a1.sinks.k1.channel=c1123456789101112131415161718实现功能：&gt; 使用avro方式在某节点上将文件发送到本服务器上且通过logger方式显示#### （2）命令操作##### `启动` （在会话1端）在node00上* ##当前flume节点执行（配置文件在当前目录）：* &gt; flume-ng agent --conf ./ --conf-file avro_logger --name a1 -Dflume.root.logger=INFO,console`显示：` 19/01/18 13:53:16 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 13:53:16 INFO node.Application: Starting Sink k119/01/18 13:53:16 INFO node.Application: Starting Source r119/01/18 13:53:16 INFO source.AvroSource: Starting Avro source r1: { bindAddress: 192.168.198.128, port: 55555 }…19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 13:53:17 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 13:53:17 INFO source.AvroSource: Avro source r1 started. 1234567891011121314##### 发送(在会话2端)在node00上发送文件到node00`启动`* ##可在本地和其他flume节点执行（配置文件在当前目录）：* &gt; flume-ng avro-client --conf ./ -H 192.168.198.128 -p 55555 -F ./flume.log(在会话1端) 19/01/18 14:12:57 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata }12345678910时刻监听传输文件的内容`注意`&gt; 该过程也可应用于不同节点之间### 3、exec（监听某一命令，在本地控制台打印）#### （1）vim exec_logger #单节点flume配置 example.conf: A single-node Flume configuration#给agent三大结构命名 Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 #描述source的配置：类型、命令（监听/root/flume.log文件） Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /root/flume.log #描述sink的配置：类型 Describe the sinka1.sinks.k1.type = logger #在内存中使用一个channel缓存事件 Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 #将source和sink绑定到channel上 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1234567891011121314151617181920212223242526（xshell会话1：）&gt; 在node00上：`启动`&gt;&gt; 在exec_logger文件所在的目录下&gt;&gt; 命令：flume-ng agent --conf-file exec_logger --name a1 -Dflume.root.logger=INFO,console&gt;&gt; r1 启动&gt; （复制会话：会话2）&gt;&gt; 在node00上：&gt;&gt; 在root目录下&gt;&gt; 命令：echo hello bigdata &gt;&gt;flume.log&gt; 之后在会话2上&gt;&gt; `logger本地控制台打印：`&gt;&gt; 19/01/18 12:03:23 INFO sink.LoggerSink:Event:{ headers:{} body: 68 65 6C 6C 6F 20 62 69 67 64 61 74 61 hello bigdata }123456### 4、netcat–hdfs(监听数据，传到hdfs上)#### （1）vim netcat_hdfs a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1 a1.sources.r1.type = netcata1.sources.r1.bind = node00a1.sources.r1.port = 41414 a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://Sunrise/myflume/%y-%m-%da1.sinks.k1.hdfs.useLocalTimeStamp=true Define a memory channel called c1 on a1a1.channels.c1.type = memory #默认值，可省 #a1.channels.c1.capacity = 1000 #a1.channels.c1.transactionCapacity = 100 Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c112345678910111213141516#### (2)操作在node00的会话1上启动&gt; 在node00上：&gt;&gt; 在netcat_hdfs文件所在的目录下&gt;&gt; 命令：flume-ng agent --conf-file netcat_hdfs --name a1 -Dflume.root.logger=INFO,console显示： 19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 14:34:44 INFO node.Application: Starting Sink k119/01/18 14:34:44 INFO node.Application: Starting Source r119/01/18 14:34:44 INFO source.NetcatSource: Source starting19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.19/01/18 14:34:44 INFO instrumentation.MonitoredCounterGroup: Component type: SINK, name: k1 started19/01/18 14:34:44 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/192.168.198.128:41414] 12345678910在node00的会话2上启动&gt; telnet node00 41414显示 Trying 192.168.198.128…Connected to node00.Escape character is ‘^]’. 1234输入任意内容在node00会话1端会显示 19/01/18 14:36:50 INFO hdfs.HDFSSequenceFile: writeFormat = Writable, UseRawLocalFileSystem = false19/01/18 14:36:51 INFO hdfs.BucketWriter: Creating hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Closing hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp19/01/18 14:37:29 INFO hdfs.BucketWriter: Renaming hdfs://Sunrise/myflume/19-01-18/FlumeData.1547822210259.tmp to hdfs://Sunrise/myflume/19-01-18/FlumeData.154782221025919/01/18 14:37:29 INFO hdfs.HDFSEventSink: Writer callback called. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758在HDF分布式系统上会显示，生成的文件![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4acaz5fj30u10blmzc.jpg)![](https://ws1.sinaimg.cn/large/005zftzDgy1fzb4bsv8d2j30wd08k74i.jpg)`注意：`这种情况会在hdfs上生成很多小文件，[在官网](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html)#### HDFS Sink[¶](http://flume.apache.org/releases/content/1.8.0/FlumeUserGuide.html#hdfs-sink)有很多关于文件生成过程中的配置| Name | Default | Description || ---------------------- | ------------ | ------------------------------------------------------------ || **channel** | – | || **type** | – | The component type name, needs to be `hdfs` || **hdfs.path** | – | HDFS directory path (eg hdfs://namenode/flume/webdata/) || hdfs.filePrefix | FlumeData | Name prefixed to files created by Flume in hdfs directory || hdfs.fileSuffix | – | Suffix to append to file (eg `.avro` - *NOTE: period is not automatically added*) || hdfs.inUsePrefix | – | Prefix that is used for temporal files that flume actively writes into || hdfs.inUseSuffix | `.tmp` | Suffix that is used for temporal files that flume actively writes into || hdfs.rollInterval | 30 | Number of seconds to wait before rolling current file (0 = never roll based on time interval) || hdfs.rollSize | 1024 | File size to trigger roll, in bytes (0: never roll based on file size) || hdfs.rollCount | 10 | Number of events written to file before it rolled (0 = never roll based on number of events) || hdfs.idleTimeout | 0 | Timeout after which inactive files get closed (0 = disable automatic closing of idle files) || hdfs.batchSize | 100 | number of events written to file before it is flushed to HDFS || hdfs.codeC | – | Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy || hdfs.fileType | SequenceFile | File format: currently `SequenceFile`, `DataStream` or `CompressedStream` (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC || hdfs.maxOpenFiles | 5000 | Allow only this number of open files. If this number is exceeded, the oldest file is closed. || hdfs.minBlockReplicas | – | Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath. || hdfs.writeFormat | Writable | Format for sequence file records. One of `Text` or `Writable`. Set to `Text` before creating data files with Flume, otherwise those files cannot be read by either Apache Impala (incubating) or Apache Hive. || hdfs.callTimeout | 10000 | Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring. || hdfs.threadsPoolSize | 10 | Number of threads per HDFS sink for HDFS IO ops (open, write, etc.) || hdfs.rollTimerPoolSize | 1 | Number of threads per HDFS sink for scheduling timed file rolling || hdfs.kerberosPrincipal | – | Kerberos user principal for accessing secure HDFS || hdfs.kerberosKeytab | – | Kerberos keytab for accessing secure HDFS || hdfs.proxyUser | | || hdfs.round | false | Should the timestamp be rounded down (if true, affects all time based escape sequences except %t) || hdfs.roundValue | 1 | Rounded down to the highest multiple of this (in the unit configured using `hdfs.roundUnit`), less than current time. || hdfs.roundUnit | second | The unit of the round down value - `second`, `minute` or `hour`. || hdfs.timeZone | Local Time | Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles. || hdfs.useLocalTimeStamp | false | Use the local time (instead of the timestamp from the event header) while replacing the escape sequences. || hdfs.closeTries | 0 | Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart. || hdfs.retryInterval | 180 | Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension. || serializer | `TEXT` | Other possible options include `avro_event` or the fully-qualified class name of an implementation of the `EventSerializer.Builder` interface. |netcat-hdfs （配置方式二） a1 which ones we want to activate.a1.channels = c1a1.sources = r1a1.sinks = k1 a1.sources.r1.type = avroa1.sources.r1.bind=node01a1.sources.r1.port=55555 a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://shsxt/hdfsflume Define a memory channel called c1 on a1a1.channels.c1.type = memorya1.channels.c1.capacity=1000a1.channels.c1.transactionCapacity = 100 Define an Avro source called r1 on a1 and tell ita1.sources.r1.channels = c1a1.sinks.k1.channel = c1123456### 5、结合版（netcat-avro）#### （1）vim netcat2_logger（node00） example.conf: A single-node Flume configuration#flume-ng agent –conf ./ –conf-file netcat2_logger –name a1 -Dflume.root.logger=INFO,console #flume-ng –conf conf –conf-file /root/flume_test/netcat_hdfs -n a1 -Dflume.root.logger=INFO,console #telnet 192.168.235.15 44444 Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = node00 a1.sources.r1.port = 44444 Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = node01 a1.sinks.k1.port = 60000 Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 #————————— #flume-ng agent –conf-file etect2_logger –name a1 -#Dflume.root.logger=INFO,console #flume-ng agent –conf conf –conf-file netcat_logger –name a1 -#Dflume.root.logger=INFO,console12（node01） #flume-ng agent –conf ./ –conf-file avro2 -n a1a1.sources = r1a1.sinks = k1a1.channels = c1 a1.sources.r1.type = avroa1.sources.r1.bind = node01a1.sources.r1.port = 60000 a1.sinks.k1.type = logger Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1234567891011121314(2)操作&gt; 先启动后面的flume节点node01 ，在启动node00，最后启动node02在node01上`启动`&gt; flume-ng &gt;&gt; --conf conf --conf-file avro2 -n a1 -Dflume.root.logger=INFO,console显示 19/01/18 23:22:27 INFO node.Application: Starting Channel c119/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.19/01/18 23:22:28 INFO instrumentation.MonitoredCounterGroup: Component type: CHANNEL, name: c1 started19/01/18 23:22:28 INFO node.Application: Starting Sink k119/01/18 23:22:28 INFO node.Application: Starting Source r119/01/18 23:22:28 INFO source.AvroSource: Starting Avro source r1: { bindAddress: node01, port: 60000 }…19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Monitored counter group for type: SOURCE, name: r1: Successfully registered new MBean.19/01/18 23:22:30 INFO instrumentation.MonitoredCounterGroup: Component type: SOURCE, name: r1 started19/01/18 23:22:30 INFO source.AvroSource: Avro source r1 started. 1234567891011121314151617181920212223242526在node00上`启动`&gt; flume-ng agent&gt;&gt; --conf ./ --conf-file netcat2_logger --name a1 -Dflume.root.logger=INFO,console在node02上`启动`&gt; telnet node00 44444然后输入数据文件最后在node01节点上显示文件信息 19/01/18 23:33:01 INFO sink.LoggerSink: Event: { headers:{} body: 68 65 6C 6C 6F 20 77 6F 72 6C 64 0D hello world. } ` flume-ng agent –conf-file flumeproject –name a1 -Dflume.root.logger=INFO,console","tags":[{"name":"Linux系统环境","slug":"Linux系统环境","permalink":"http://sungithup.github.io/tags/Linux系统环境/"},{"name":"HDFS","slug":"HDFS","permalink":"http://sungithup.github.io/tags/HDFS/"},{"name":"Base","slug":"Base","permalink":"http://sungithup.github.io/tags/Base/"}]},{"title":"大数据思想","date":"2019-01-04T16:00:00.000Z","path":"2019/01/05/大数据思想/","text":"[TOC] 1、大数据核心问题：==海量数据、工业技术落后、硬件损坏常态化（Ctrl+M）== 2、大数据思维分而治之 把一个复杂的问题按一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，然后分别找出各部分的中间结果，最后将各个部分的中间结果组成整个问题的最终结果（Ctrl+Q） 3、业务场景仓储、数牌 业务一：找{重复行}(chongfuhang)++现有1TB的TXT文件 ;格式：数字+字符 ；网速：500M/s ；服务器内存大小：128M ；条件：仅有两行重复 ，内存不能放下全量数据（OM：out of memery）；++ ==方法== 答：共需要2次IO：2*30min=1h ==第一次IO==： 给每一行内容加上唯一标记（hashcode（内容），value（行号））。对每一行内容进行hash运算，得到唯一标识hashcode，作为key，将行号作为value。然而，对于内容完全重复的两行，其hashcode值一定相同。 `对每一行的hash值进行取模运算，并放置于归类分区的小文件中`。由于数据基数过大，就将每行的key值对取模，转化为小文件，如将hash值对100取模，则产生100个小文件分区，取模后相同的放在同一个分区中。 ==第二次IO==： 在每个分区中的小文件遍历，对每一行进行比较，因为重复行一定会在同一个分区中。这样工作量就会大大减小。 业务二：快{排序}(paixu)++现有1TB的TXT文件 ;格式：数字；网速：500M/s ；服务器内存大小：128M ；条件：实现快排序，内存不能放下全量数据（OM：out of memery）；++ 两次IO，2 * 30分钟 = 1小时 ==方法一：先全局有序后局部有序== 1.对全局按分区排序（由大到小）。​ 用if。。else方法对数据进行按范围分类，落到不同的分区中（0~1000、1001~2000、2001~3000··················） 2.对局部进行排序（由大到小）。​ 对每个分区进行排序。 ==方法二：先局部有序后全局有序== 先实现局部有序(小–&gt;大)。将文件划分为N个分区，在每个分区内部进行排序 使用归并实现全局有序。每个分区分别各自取出最小值，拿出来比较，最小值放在一旁，最小值所在的那个分区再拿出剩下数据中的最小值，再进行比较，比较出的最小值，排在上一次的最小值后，依次进行下去，这样就实现了全局有序。","tags":[{"name":"BigData","slug":"BigData","permalink":"http://sungithup.github.io/tags/BigData/"},{"name":"头脑风暴","slug":"头脑风暴","permalink":"http://sungithup.github.io/tags/头脑风暴/"},{"name":"分而治之","slug":"分而治之","permalink":"http://sungithup.github.io/tags/分而治之/"}]},{"title":"手动安装maven坐标依赖","date":"2018-12-27T18:16:47.000Z","path":"2018/12/28/手动安装maven坐标依赖/","text":"手动安装maven坐标依赖一、事件原因：学习quartz框架时，在maven项目的pom.xml文件中添加quartz所需要的坐标依赖时，显示jar包不存在。 12345678910111213提示：\"Dependency 'xxxx‘ not found\"， 并且添加的如下两个坐标依赖均报红。 &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz-jobs&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 分析： 1、maven项目所需要的jar包均存放在maven的F:\\m2\\repository(项目所需的jar包仓库)文件夹中 2、在F:\\apache-maven-3.5.4\\conf的settings.xml文件中有如下设置：（由于使用远程仓库太慢，阿里云给我们提供了一个镜像仓库，便于我们使用，且只包含central仓库中的jar） 1234567&lt;!--文件中原有的配置：远程仓库---&gt;&lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; 1234567&lt;!--文件中自己手动配置：阿里镜像仓库---&gt;&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; 3.可是我们在https://mvnrepository.com/（maven仓库）中发现有我们要的jar包，而且就在Central仓库里，这里我们就很奇怪了，后来就选择还是手动安装jar包吧 （如果有小伙伴有别的解决方案，还请指点一二。） 1&lt;!--more--&gt; 二、解决方案1、首先，我们需要从maven Repository中下载我们需要的jar包（需要的两个jar包，下载原理相同） 2、注意我们的maven安装，需要配置环境变量，才能在dos窗口，指令安装jar包 因为我之前查资料时，有小伙伴说，java的环境变量配置也会影响，所以，我在这里也把java的环境变量配置也贴出来 JAVA_HOME F:\\Java\\jdk1.8.0_131（ 根据自己的jdk安装目录） CLASSPATH .;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar MAVEN_HOME F:\\apache-maven-3.5.4（ 根据自己maven安装目录） Path（注意配置的时候，一定要和配置home时的变量名一致，如MAVEN_HOME,我配置成了%MVN_HOME%\\bin;） %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin;%SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem;%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0\\;%MYSQL_HOME%\\bin;%MAVEN_HOME%\\bin; 配置这些环境变量，在dos窗口才能使java ，mvn 之类的指令可以用； 否则会出现如下显示。 ‘mvn’ 不是内部或外部命令，也不是可运行的程序 (这就是环境变量没有配成功的结果) 3.安装 C:\\Users\\Administrator&gt;mvn -v C:\\Users\\Administrator&gt;mvn install:install-file -Dfile=F:/apache-maven-3.5.4/m2/quartz-2.3.0.jar（jar包所在路径） -DgroupId=org.quartz-scheduler -DartifactId=quartz -Dversion=2.3.0 -Dpackaging=jar （根据下面所示的配置groupId、artifactId、version） 12345&lt;dependency&gt; &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt; &lt;artifactId&gt;quartz&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; 如图所示，安装成功。","tags":[{"name":"maven","slug":"maven","permalink":"http://sungithup.github.io/tags/maven/"}]},{"title":"常用Linux命令的学习（二）","date":"2018-12-27T16:00:00.000Z","path":"2018/12/28/常用Linux命令的学习（二）/","text":"​​ [TOC] 一、磁盘指令 查看硬盘信息 命令：df （默认大小以kb显示） df -k（以kb为单位） df -m（ 以mb为单位） df –h （易于阅读） 查看文件/目录的大小 命令：du filename|foldername （默认单位为kb）-k kb单位 -m mb单位 -a 所有文件和目录 -h 更易于阅读 ​ --max-depth=0 目录深度 二、网络指令 查看网络配置信息 命令:ifconfig 测试与目标主机的连通性 命令：ping remote_ip ctrl + c :结束ping进程 显示各种网络相关信息 命令：netstat 查看端口号（是否被占用） (1)、lsof -i:端口号 （需要先安装lsof） (2)、netstat -tunlp|grep 端口号 测试远程主机的网络端口 命令： telnet ip port （需要先安装telnet） 测试成功后，按ctrl + ] 键，然后弹出telnet&gt;时，再按q退出 http请求模拟 curl -X get www.baidu.com 模拟请求百度 三、系统管理指令 用户操作 12345678910 操作 命令创建用户 useradd|adduser username修改密码 passwd username删除用户 userdel –r username修改用户（已下线）： 修改用户名: usermod –l new_name oldname 锁定账户: usermod –L username 解除账户： usermod –U username查看当前登录用户 仅root 用户：whoami | cat /etc/shadow 普通用户：cat /etc/pqsswd 用户组操作 12345 操作 命令 创建用户组 groupadd groupname删除用户组 groupdel groupname修改用户组 groupmod –n new_name old_name查看用户组 groups （查看的是当前用户所在的用户组） 用户+用户组 12345 操作 命令 修改用户的主组 usermod –g groupname username给用户追加附加组 usermod –G groupname username查看用户组中用户数 cat /etc/group注意：创建用户时，系统默认会创建一个和用户名字一样的主组 系统权限 12345678910 操作 命令 查看/usr下所有权限 ll /usr 权限类别 r（读取：4） w（写入：2） x（执行：1） 三个为一组，无权限用 —代替 UGO模型 U（User） G(Group) O(其他)权限修改 修改所有者：chown username file|folder (递归)修改所有者和所属组： chown -r username：groupname file|folder 修改所属组：chgrp groupname file|folder 修改权限：chmod ugo+rwx file|folder 四、系统配置指令 1.修改主机名 123 编辑文件： 命令： vim /etc/sysconfig/network 文件内容： HOSTNAME=node00（重启生效)reboot 2.DNS配置 12编辑文件： 命令：vim /etc/resolv.conf文件内容： nameserver 192.168.198.0 3.sudo权限配置 1234567891011121314151617 操作 命令编辑权限配置文件： vim /etc/sudoers格式： 授权用户 主机=[(切换到哪些用户或用户组)] [是否需要密码验证] 路径/命令举例： test ALL=(root) /usr/bin/yum,/sbin/service解释： test用户就可以用yum和servie命令， 但是，使用时需要在前面加上sudo再敲命令。 第一次使用需要输入用户密码,且每个十五分钟需要一次密码验证修改： test ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service这样就不需要密码了将权限赋予某个组，%+组名%group ALL=(root) NOPASSWD: /usr/bin/yum,/sbin/service列出用户所有的sudo权限 sudo –l 4.系统时间 12345678操作 命令查看系统时间 date ---查看当前时间详情 cal ---查看当前月日历 cal 2018 ---查看2018年完整日历 cal 12 2018 ---查看指定年月的日历 更新系统时间（推荐） yum install ntpdate –y ---安装ntp服务 ntpdate cn.ntp.org.cn ---到域名为cn.ntp.org.cn的时间服务器上同步时间 5.关于hosts配置 相当于给IP地址其别名，可以通过别名访问 路径： Windows系统 C:/Windows/System32/drivers/etc/hosts 文件 Linux系统 /etc/hosts文件：vim +路径 统一 编辑格式 IP地址 别名：192.168.198.128 node00 6.关于hostname配置 相当于给对应的虚拟机器起别名 Linux系统： vi /etc/sysconfig/network 编辑内容： HOSTNAME=node01 五、重定向与管道符 输出重定向 输出重定向到一个文件或设备： &gt; 覆盖原来的文件 &gt;&gt; 追加原来的文件 举例： ls &gt; log — 在log文件中列出所有项，并覆盖原文件 echo “hello”&gt;&gt;log —将hello追加到log文件中 输入重定向 &lt; 输入重定向到一个程序 举例：cat &lt; log —将log文件作为cat命令的输入，查看log文件的内容 标准 输出 重定向 1 &gt; 或 &gt; 含义： 输出重定向时，只用正确的输出才会重定向到指的文件中 错误的则会直接打印到屏幕上 错误 输出 重定向 2 &gt; 含义： 错误的输出会重定向到指定文件里，正确的日志则直接打印到屏幕上。 结合 使用 2&gt;&amp;1 含义： 将无论是正确的输出还是错误的输出都重定向到指定文件 管道 **\\ ** 含义： 把前一个输出当做后一个输入 grep 通过正则搜索文本，并将匹配的行打印出来 netstat -anp \\ grep 22 把netstat –anp 命令的输出 当做是grep 命令的输入 命令 执行 控制 &amp;&amp; 前一个命令执行成功才会执行后一个命令 **\\ \\ ** 前一个命令执行失败才会执行后一个命令","tags":[{"name":"Linux命令","slug":"Linux命令","permalink":"http://sungithup.github.io/tags/Linux命令/"}]},{"title":"常用Linux命令的学习（一）","date":"2018-12-27T12:16:47.000Z","path":"2018/12/27/常用Linux命令的学习（一）/","text":"常用Linux命令的学习（一）[TOC] 一、命令指南（manual）：man 安装：yum install man –y （-y 表示获得允许，无需确认） 查看ls命令指南： man ls 二、目录命令切换目录：cd + 目录的路径 查看当前目录所在的完整路径：pwd 新建目录：mkdir +目录名字 查看当前目录所用有的子目录和文件：ls ，ll等价于 ls –l ​ 查看目录下的所有东西（包括隐藏文件）： ls –al 等价于 ll -a​拷贝目录或文件：cp –r install.log install2.log 删除目录或文件：rm -r install.log (rmdir只能删除空目录) 移动目录或文件：mv + 目录/文件名字 + 其他路径 ​ 将test目录移动到 根目录/ 下 : mv test / （如果移动到当前目录，用另外一个名称，则可以实现重命名的效果） 更改文件或目录的名字：mv + 旧目录名字 + 新目录名 ( -r 用于递归的拷贝，删除，移动目录) 三、文件命令1、一般文件操作新建文件：touch install.log​ (vim install.log 编辑文件，如果文件不存在，就会新建一个对应的文件，并进入文件的编辑模式，如果按 :wq 会保存文件并退出，如果按 :q 则不保存退出)​查看文件内容：cat +（文件名）（一次性显示整个文件的内容，文件内容过多时用户体验不好） 一次命令显示一屏文本： 1234567 more +（文件名） 按键 效果 Space 显示下一屏文本内容B 显示上一屏文本内容Enter 显示下一行文本内容Q 退出查看 less+（文件名） 按键 效果 h 显示帮助界面 u 向后滚动半页 d 向前翻半页 e | Enter 向后翻一行文本 space 滚动一页 b 向后翻一页 [pagedown]： 向下翻动一页 [pageup]： 向上翻动一页 上下键，向上一行，向下一行 从头打印文件内容：​ head -10 +（文件名） 打印文件1到10行 从尾部打印文件内容​ tail -10 +（文件名）打印文件最后10行 tail -f (文件名) 常用于查看文件内容的更新变化 查找文件或目录​ find +（路径名） –name +（文件名）​ 举例：find / -name profile​ 在/(根目录)目录下查找 名字为profile的文件或目录 ​ 也可利用正则：​ 举例： find /etc -name pro*​ 在/etc目录下查找以pro开头的文件或目录 路径越精确，查找的范围越小，速度越快 i 2、文件编辑vi（1） vi 进入编辑模式 —–&gt;按i 进入插入模式 ——-&gt; 按Esc 退出编辑模式 1234vi filename :打开或新建文件，并将光标置于第一行首 vi +n filename ：打开文件，并将光标置于第n行首 vi + filename ：打开文件，并将光标置于最后一行首 vi +/pattern filename：打开文件，并将光标置于第一个与 pattern匹配的字符串所在的行首 filename 为文件名 （2）在文件vi（文件编辑）模式下 命令行模式 123456789:w 保存:q 退出:wq 保存并退出:q! 强制退出:set nu |ctrl+g 显示文本行数:set nonu 去除显示的行数:s/p1/p2/g 将当前行中所有p1均用p2替代 :n1,n2s/p1/p2/g 将第n1至n2行中所有p1均用p2替代 :g/p1/s//p2/g 将文件中所有p1均用p2替换 一般模式 12345678910111213141516171819202122232425262728293031按键：yy 复制光标所在行(常用) nyy 复制光标所在行的向下n行，例如， 20yy则是复制20行(常用) p|P p为复制的数据粘贴在光标下一行， P则为粘贴在光标上一行(常用)G 光标移至第最后一行nG 光标移动至第N行行首n+ 光标下移n行 n- 光标上移n行 H 光标移至屏幕顶行 M 光标移至屏幕中间行 L 光标移至屏幕最后行 dd 删除所在行 x或X 删除一个字符，x删除光标后的，而X删除光标前的 u 撤销(常用)删除第N行到第M行：N,Md：,$-1d 删除当前光标到到数第一行数据按键： i: 在当前光标所在字符的前面，转为输入模式； a: 在当前光标所在字符的后面，转为输入模式； o: 在当前光标所在行的下方，新建一行，并转为输入模式； I：在当前光标所在行的行首，转换为输入模式 A：在当前光标所在行的行尾，转换为输入模式 O：在当前光标所在行的上方，新建一行，并转为输入模式；---逐字符移动：h: 左 l: 右j: 下 k: 上 vim 安装：yum install vim -y 用vim 打开/etc/profile 文件， 特点：编辑器对文本的内容进行了高亮，使整个文件的内容可读性大大加强 ，其他均与vi相同 3、文件上传下载 安装上传下载命令：yum install lrzsz -y 上传文件：（windows—&gt;linux） 命令 ：rz 弹出windows上传文件窗口 下载文件：(linux—&gt;windows) 注意：sz命令只能下载文件，不能下载目录，推荐将目录压缩成tar包或使用工具软件：Winscp【Xftp】 命令：sz （文件名） 弹出windows下载窗口,下载文件到指定文件目录 4、文件传输(1)、本地→远程 文件 ： scp local_file remote_username@remote_ip:remote_folder 目录 ： scp -r local_folder remote_username@remote_ip:remote_folder (2）、远程→本地 文件 ： scp remote_username@remote_ip:remote_file local_folder 目录 ： scp remote_username@remote_ip:remote_folder local_folder","tags":[{"name":"Linux命令","slug":"Linux命令","permalink":"http://sungithup.github.io/tags/Linux命令/"}]}]